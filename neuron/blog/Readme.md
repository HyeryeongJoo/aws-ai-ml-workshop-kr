# AWS Neuron 주요 블로그

Last updated: Feb 24, 2024

---

- (Sep 2023) 루시아 130억 파라미터 크기의 모델을 AWS Inferentia2 액셀러레이터를 통해 사용할 경우 월 운영 비용을 기존 대비 최대 65%까지 절감 사례:  [솔트룩스 inferentia2 적용 사례](https://www.saltlux.com/pr/post/%EC%86%94%ED%8A%B8%EB%A3%A9%EC%8A%A4-%EA%B8%B0%EC%97%85-%EB%A7%9E%EC%B6%A4%ED%98%95-%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8-%EB%A3%A8%EC%8B%9C%EC%95%84luxia%EB%A1%9C-%EA%B5%AD%EB%82%B4-%EB%B0%8F/)
- (Jun 2023) SK텔레콤 의 MLOps 파이프라인에 Inferentia 활용 사례:[SK텔레콤 inferentia 적용 사례](https://aws.amazon.com/ko/blogs/tech/skt-mlops-using-aws-inferentia-stepfunctions/)
- (Apr 2023) Inferentia 기본 개념 및 NVidia Trition 에서 Inferentia 모델 서빙 설명: [하이퍼커넥트의 AWS 기계 학습 추론 가속기 적용을 통한 모델 서빙 비용 절감 사례와 꿀팁](https://www.youtube.com/watch?v=tJkSe-hA9vc)
- (Oct 2022) Neuron 모델을 가지고 부하 테스트를 수행하여 GPU 모델과 비교하였습니다. 실험 결과 80%대의 높은 비용 절감: [하이퍼커넥트 inferentia 적용 사례](https://hyperconnect.github.io/2022/12/13/infra-cost-optimization-with-aws-inferentia.html)
