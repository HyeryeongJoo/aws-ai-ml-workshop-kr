{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 에 저장된 모델을 SageMaker INF2 에 배포하기\n",
    "\n",
    "이 노트북은 [MLP-KTLim/llama-3-Korean-Bllossom-8B](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B) 의 한국어 파인 튜닝 모델을 AWS Neuron Model 로 바꾼 [Gonsoo/AWS-Neuron-llama-3-Korean-Bllossom-8B](https://huggingface.co/Gonsoo/AWS-Neuron-llama-3-Korean-Bllossom-8B) 모델을 사용 합니다.<br>\n",
    "Amazon EC2 Inferentia2 에 Hugging Face Text Generation Inference (HF TGI) 도커 컨테이너를 이용하여, SageMaker Endpoint 에 배포 합니다.\n",
    "\n",
    "#### 코드 참조\n",
    "- [Deploy Llama 3 70B on AWS Inferentia2 with Hugging Face Optimum](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/llama3-70b/deploy-llama-3-70b-inferentia2.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 다음과 같은 패키지를 설치 합니다.\n",
    "- 처음 실행시에 아래의 주석을 제거하고 실행 하세요\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install huggingface_hub\n",
    "# !pip install \"sagemaker>=2.199.0\" transformers --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SageMaker 세션 등 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/base_serializers.py:28: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.22.4)\n",
      "  import scipy.sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::057716757052:role/gen_ai_gsmoon\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hugging Face LLM Inf2 DLC (Deep Learning Container ) 가져오기\n",
    "\n",
    "새로운 Hugging Face TGI Neuronx DLC를 사용하여 AWS Inferentia2에서 추론을 실행할 수 있습니다. sagemaker SDK의 get_huggingface_llm_image_uri 메서드를 사용하여 원하는 backend, session, region, 그리고 version에 따라 적절한 Hugging Face TGI Neuronx DLC URI를 검색할 수 있습니다. 사용 가능한 모든 버전은 여기에서 확인할 수 있습니다.\n",
    "- [중요] 아래에서는 버전 version=\"0.0.24\" 를 사용합니다. 이유는 모델의 뉴런 컴파일 버전과 일치를 시키기 위함 입니다.\n",
    "    - 0.0.24 는 Neuron-cc 2.14 버전이 탑재되어 있습니다.\n",
    "- 참조: [Hugging Face TGI Neuronx DLC URI](https://github.com/aws/deep-learning-containers/releases?q=tgi+AND+neuronx&expanded=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.2-optimum0.0.24-neuronx-py310-ubuntu22.04-v1.0\n"
     ]
    }
   ],
   "source": [
    "# from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# # retrieve the llm image uri\n",
    "# llm_image = get_huggingface_llm_image_uri(\n",
    "#   \"huggingface-neuronx\",\n",
    "#   version=\"0.0.23\"\n",
    "#   # version=\"0.0.24\"\n",
    "# )\n",
    "\n",
    "\n",
    "# 신규 버전 ( 2024년 8월 22일 출시) : Neuron-CC 2.14 버전\n",
    "llm_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.2-optimum0.0.24-neuronx-py310-ubuntu22.04-v1.0\"\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HF 에서 모델 다운로드 및 S3 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 다운로드\n",
    "- NeuronCC-2-14 버전으로 컴파일된 모델을 다운로드 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_folder = \"/home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B\"\n",
    "\n",
    "# model_name = \"Gonsoo/AWS-Neuron-llama-3-Korean-Bllossom-8B\" # Neuron-CC 2.13 버전 에서 컴파일\n",
    "# 아래의 도커의 optimum-cli 를 통하여 컴파일 됨\n",
    "# 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.2-optimum0.0.24-neuronx-py310-ubuntu22.04-v1.0\n",
    "model_name = \"Gonsoo/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B\" # Neuron-CC 2.14 버전 에서 컴파일\n",
    "\n",
    "os.makedirs(download_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files:   0%|                                   | 0/7 [00:00<?, ?it/s]Downloading 'special_tokens_map.json' to '/home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/.cache/huggingface/download/special_tokens_map.json.278b7f0f84be865c4687700ee7b3c63d89a51e18.incomplete'\n",
      "Downloading 'tokenizer_config.json' to '/home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/.cache/huggingface/download/tokenizer_config.json.e0008d220901606b0ef9047ddfe1271849e194ea.incomplete'\n",
      "Downloading 'compiled/675e2498ec72a928a923.neff' to '/home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/.cache/huggingface/download/compiled/675e2498ec72a928a923.neff.30e6a0eb1f82e6197e8e78689fd273a644a416877e2931807cb870b4cc0edb27.incomplete'\n",
      "Downloading 'tokenizer.json' to '/home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/.cache/huggingface/download/tokenizer.json.9e7400021be1f68863fe30bbc1f05cd9d48b4efb.incomplete'\n",
      "Downloading 'generation_config.json' to '/home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/.cache/huggingface/download/generation_config.json.d696a9efff82dff3972e0c39bb5ccaa2e47972fc.incomplete'\n",
      "\n",
      "special_tokens_map.json: 100%|█████████████████| 444/444 [00:00<00:00, 4.81MB/s]\u001b[A\n",
      "Download complete. Moving file to /home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/special_tokens_map.json\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████| 51.1k/51.1k [00:00<00:00, 63.3MB/s]\u001b[A\n",
      "Download complete. Moving file to /home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/tokenizer_config.json\n",
      "\n",
      "675e2498ec72a928a923.neff:   0%|                    | 0.00/10.3M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "tokenizer.json:   0%|                               | 0.00/9.09M [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'compiled/2ae6fb8fd3c66e17e30f.neff' to '/home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/.cache/huggingface/download/compiled/2ae6fb8fd3c66e17e30f.neff.8eb814b5782004bc0b5ea450da96aa840191f69275ec74d0ebe5c9c601388caa.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "2ae6fb8fd3c66e17e30f.neff:   0%|                    | 0.00/10.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "generation_config.json: 100%|██████████████████| 172/172 [00:00<00:00, 2.03MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/generation_config.json\n",
      "Downloading 'config.json' to '/home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/.cache/huggingface/download/config.json.30cbb5ee956fdcfb4e430fcae1550f38bf4a2674.incomplete'\n",
      "675e2498ec72a928a923.neff: 100%|████████████| 10.3M/10.3M [00:00<00:00, 139MB/s]\n",
      "Download complete. Moving file to /home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/compiled/675e2498ec72a928a923.neff\n",
      "\n",
      "config.json: 100%|█████████████████████████| 1.09k/1.09k [00:00<00:00, 12.7MB/s]\u001b[A\n",
      "Download complete. Moving file to /home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/config.json\n",
      "2ae6fb8fd3c66e17e30f.neff: 100%|████████████| 10.9M/10.9M [00:00<00:00, 127MB/s]\n",
      "Download complete. Moving file to /home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/compiled/2ae6fb8fd3c66e17e30f.neff\n",
      "Fetching 7 files:  14%|███▊                       | 1/7 [00:00<00:01,  5.86it/s]\n",
      "\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 40.5MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to /home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B/tokenizer.json\n",
      "Fetching 7 files: 100%|███████████████████████████| 7/7 [00:00<00:00, 25.07it/s]\n",
      "/home/ec2-user/SageMaker/models/AWS-NeuronCC-2-14-llama-3-Korean-Bllossom-8B\n",
      "total 8956\n",
      "drwxrwxr-x  4 ec2-user ec2-user    4096 Aug 30 12:49 .\n",
      "drwxrwxr-x 10 ec2-user ec2-user    4096 Aug 30 12:49 ..\n",
      "drwxrwxr-x  3 ec2-user ec2-user    4096 Aug 30 12:49 .cache\n",
      "drwxrwxr-x  2 ec2-user ec2-user    4096 Aug 30 12:49 compiled\n",
      "-rw-rw-r--  1 ec2-user ec2-user    1093 Aug 30 12:49 config.json\n",
      "-rw-rw-r--  1 ec2-user ec2-user     172 Aug 30 12:49 generation_config.json\n",
      "-rw-rw-r--  1 ec2-user ec2-user     444 Aug 30 12:49 special_tokens_map.json\n",
      "-rw-rw-r--  1 ec2-user ec2-user   51106 Aug 30 12:49 tokenizer_config.json\n",
      "-rw-rw-r--  1 ec2-user ec2-user 9085699 Aug 30 12:49 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli download {model_name} --local-dir {download_folder} --exclude \".cache\" --exclude \".gitattributes\"\n",
    "! ls -al {download_folder} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델을 S3 에 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "desired_s3_uri = f\"s3://{sagemaker_session_bucket}/inf2_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_s3_path:  s3://sagemaker-us-east-1-057716757052/inf2_model\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "file_s3_path = S3Uploader.upload(local_path=download_folder, desired_s3_uri=desired_s3_uri)\n",
    "print(\"file_s3_path: \", file_s3_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-30 12:49:39          1 inf2_model/.cache/huggingface/.gitignore\n",
      "2024-08-30 11:31:03          0 inf2_model/.cache/huggingface/download/README.md.lock\n",
      "2024-08-30 11:31:03        101 inf2_model/.cache/huggingface/download/README.md.metadata\n",
      "2024-08-30 12:49:40          0 inf2_model/.cache/huggingface/download/compiled/2ae6fb8fd3c66e17e30f.neff.lock\n",
      "2024-08-30 12:49:39        125 inf2_model/.cache/huggingface/download/compiled/2ae6fb8fd3c66e17e30f.neff.metadata\n",
      "2024-08-30 12:49:40          0 inf2_model/.cache/huggingface/download/compiled/675e2498ec72a928a923.neff.lock\n",
      "2024-08-30 12:49:40        125 inf2_model/.cache/huggingface/download/compiled/675e2498ec72a928a923.neff.metadata\n",
      "2024-08-30 11:31:03          0 inf2_model/.cache/huggingface/download/compiled/bf8ef7b688d31c9ddf4e.neff.lock\n",
      "2024-08-30 11:31:03        125 inf2_model/.cache/huggingface/download/compiled/bf8ef7b688d31c9ddf4e.neff.metadata\n",
      "2024-08-30 11:31:03          0 inf2_model/.cache/huggingface/download/compiled/d9202ad2e9258377560b.neff.lock\n",
      "2024-08-30 11:31:03        125 inf2_model/.cache/huggingface/download/compiled/d9202ad2e9258377560b.neff.metadata\n",
      "2024-08-30 12:49:39          0 inf2_model/.cache/huggingface/download/config.json.lock\n",
      "2024-08-30 12:49:39        101 inf2_model/.cache/huggingface/download/config.json.metadata\n",
      "2024-08-30 12:49:39          0 inf2_model/.cache/huggingface/download/generation_config.json.lock\n",
      "2024-08-30 12:49:39        100 inf2_model/.cache/huggingface/download/generation_config.json.metadata\n",
      "2024-08-30 12:49:39          0 inf2_model/.cache/huggingface/download/special_tokens_map.json.lock\n",
      "2024-08-30 12:49:39        101 inf2_model/.cache/huggingface/download/special_tokens_map.json.metadata\n",
      "2024-08-30 12:49:39          0 inf2_model/.cache/huggingface/download/tokenizer.json.lock\n",
      "2024-08-30 12:49:39        101 inf2_model/.cache/huggingface/download/tokenizer.json.metadata\n",
      "2024-08-30 12:49:39          0 inf2_model/.cache/huggingface/download/tokenizer_config.json.lock\n",
      "2024-08-30 12:49:39        101 inf2_model/.cache/huggingface/download/tokenizer_config.json.metadata\n",
      "2024-08-30 11:31:01       2583 inf2_model/README.md\n",
      "2024-08-30 12:49:38   10875904 inf2_model/compiled/2ae6fb8fd3c66e17e30f.neff\n",
      "2024-08-30 12:49:39   10271744 inf2_model/compiled/675e2498ec72a928a923.neff\n",
      "2024-08-30 11:31:02   14470144 inf2_model/compiled/bf8ef7b688d31c9ddf4e.neff\n",
      "2024-08-30 11:31:02   12329984 inf2_model/compiled/d9202ad2e9258377560b.neff\n",
      "2024-08-30 12:49:37       1093 inf2_model/config.json\n",
      "2024-08-30 12:49:38        172 inf2_model/generation_config.json\n",
      "2024-08-30 12:49:38        444 inf2_model/special_tokens_map.json\n",
      "2024-08-30 12:49:38    9085699 inf2_model/tokenizer.json\n",
      "2024-08-30 12:49:38      51106 inf2_model/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# ! aws s3 rm {file_s3_path} --recursive\n",
    "! aws s3 ls {file_s3_path} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 한국어 모델을 inferentia2 에 배포하기\n",
    "\n",
    "\n",
    "모델을 Amazon SageMaker에 배포하기 전에 TGI Neuronx 엔드포인트 구성을 정의해야 합니다. 다음과 같은 추가 매개변수를 정의해야 합니다:\n",
    "\n",
    "- HF_NUM_CORES: 컴파일에 사용된 Neuron 코어의 수.\n",
    "- HF_BATCH_SIZE: 모델 컴파일에 사용된 배치 크기.\n",
    "- HF_SEQUENCE_LENGTH: 모델 컴파일에 사용된 시퀀스 길이.\n",
    "- HF_AUTO_CAST_TYPE: 모델 컴파일에 사용된 자동 캐스트 유형.\n",
    "\n",
    "또한 기존의 TGI 매개변수도 정의해야 합니다:\n",
    "\n",
    "- HF_MODEL_ID: \"/opt/ml/model\"\n",
    "    - ECR 에서 추론 이미지를 다운로드 받고, 컨테이너가 생성이 됩니다. 이후에 S3 의 모델 파일은 \"/opt/ml/model\" 로 다운로드가 됩니다. 그래서 이 경로를 기술해야만 합니다.\n",
    "- MAX_BATCH_SIZE: 모델이 처리할 수 있는 최대 배치 크기로, 컴파일에 사용된 배치 크기와 동일합니다.\n",
    "- MAX_INPUT_LENGTH: 모델이 처리할 수 있는 최대 입력 길이.\n",
    "- MAX_TOTAL_TOKENS: 모델이 생성할 수 있는 최대 총 토큰 수로, 컴파일에 사용된 시퀀스 길이와 동일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 배포 파라미터 설정 및 SageMaker HuggingFaceModel 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_data: \n",
      " {'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/inf2_model/', 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.inf2.xlarge\"\n",
    "health_check_timeout=1800 # additional time to load the model\n",
    "volume_size=64 # size in GB of the EBS volume\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"/opt/ml/model\",       # Path to the model in the container\n",
    "    \"HF_NUM_CORES\": \"2\", # number of neuron cores\n",
    "    \"HF_BATCH_SIZE\": \"4\", # batch size used to compile the model\n",
    "    \"HF_SEQUENCE_LENGTH\": \"4096\", # length used to compile the model\n",
    "    \"HF_AUTO_CAST_TYPE\": \"fp16\",  # dtype of the model\n",
    "    \"MAX_BATCH_SIZE\": \"4\", # max batch size for the model\n",
    "    \"MAX_INPUT_LENGTH\": \"4000\", # max length of input text\n",
    "    \"MAX_TOTAL_TOKENS\": \"4096\", # max length of generated text\n",
    "    \"MESSAGES_API_ENABLED\": \"true\", # Enable the messages API\n",
    "    \"HF_TOKEN\":  \"None\"# HF_TOKEN\n",
    "}\n",
    "\n",
    "# model_data 딕셔너리 생성\n",
    "model_data = {\n",
    "    'S3DataSource': {\n",
    "        'S3Uri': f\"{file_s3_path}/\",\n",
    "        'S3DataType': 'S3Prefix',\n",
    "        'CompressionType': 'None'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"model_data: \\n\", model_data) \n",
    "# create HuggingFaceModel with the image uri\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data=model_data,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델을 SageMaker Endpoint 에 배포\n",
    "\n",
    "HuggingFaceModel을 생성한 후에는 deploy 메서드를 사용하여 Amazon SageMaker에 배포할 수 있습니다. 우리는 ml.inf2.xlarge 인스턴스 유형을 사용하여 모델을 배포할 것입니다. TGI는 자동으로 모든 Inferentia 디바이스에 걸쳐 모델을 분산하고 샤딩할 것입니다.\n",
    "- 다음은 약 17분이 소요 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------!"
     ]
    }
   ],
   "source": [
    "# deactivate warning since model is compiled\n",
    "llm_model._is_compiled_model = True\n",
    "\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout,\n",
    "  volume_size=volume_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 추론\n",
    "\n",
    "엔드포인트가 배포된 후에는 이를 통해 추론을 실행할 수 있습니다. 우리는 predictor의 predict 메서드를 사용하여 엔드포인트에서 추론을 실행할 것입니다. 생성에 영향을 미치는 다양한 매개변수로 추론을 수행할 수 있습니다. 매개변수는 페이로드의 parameters 속성에서 정의할 수 있습니다. 지원되는 매개변수는 여기에서 확인할 수 있습니다.\n",
    "메시지 API를 사용하면 대화 방식으로 모델과 상호작용할 수 있습니다. 메시지의 역할과 내용을 정의할 수 있습니다. 역할은 system, assistant 또는 user일 수 있습니다. system 역할은 모델에 컨텍스트를 제공하는 데 사용되고, user 역할은 질문을 하거나 모델에 입력을 제공하는 데 사용됩니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Prompt to generate\n",
    "messages=[\n",
    "    { \"role\": \"system\", \"content\": \"당신은 인공지능 전문가 입니다.\" },\n",
    "    { \"role\": \"user\", \"content\": \"딥러닝이 무엇인지 말해 주세요?\" }\n",
    "  ]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": \"Gonsoo/AWS-Neuron-llama-3-Korean-Bllossom-8B\", # placholder, needed\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 2048,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 테스트 해보시죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딥러닝(Deep Learning)은 인공지능(AI)의 한 분야로, 인공신경망(Artificial Neural Network)을 이용하여 데이터를 학습시키는 기술입니다. 인공신경망은 인간의 뇌를 모방한 구조로, 여러 층(layer)으로 이루어진 다층 신경망을 통해 복잡한 패턴을 학습하고 예측하는 능력을 갖추고 있습니다.\n",
      "\n",
      "딥러닝은 다음과 같은 특징을 가지고 있습니다:\n",
      "\n",
      "1. **다층 구조**: 딥러닝 모델은 여러 층으로 이루어져 있으며, 각 층은 입력 데이터를 더 복잡한 형태로 변환하여 다음 층으로 전달합니다. 이러한 층이 많을수록 모델의 학습 능력이 향상됩니다.\n",
      "\n",
      "2. **자동 특징 추출**: 기존의 기계 학습 알고리즘은 개발자가 수동으로 특징을 추출해야 하지만, 딥러닝은 데이터로부터 자동으로 중요한 특징을 추출할 수 있습니다.\n",
      "\n",
      "3. **대규모 데이터 처리**: 딥러닝 모델은 대규모의 데이터를 학습시켜야 하며, 이를 위해 고성능 컴퓨팅 자원이 필요합니다.\n",
      "\n",
      "딥러닝은 이미지 인식, 음성 인식, 자연어 처리, 자율 주행, 게임 등 다양한 분야에서 널리 사용되고 있습니다. 예를 들어, 이미지 인식 모델은 사진 속 사람, 동물, 사물 등을 인식하고 분류할 수 있으며, 음성 인식 모델은 음성 명령을 이해하고 처리할 수 있습니다. 또한, 딥러닝은 의료 분야에서도 질병 진단, 유전자 분석 등에서 중요한 역할을 하고 있습니다.<|eot_id|>\n",
      "CPU times: user 4.33 ms, sys: 621 μs, total: 4.95 ms\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chat = llm.predict({\"messages\" :messages, **parameters,\"steam\":True})\n",
    "\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 리소스 정리\n",
    "\n",
    "SageMaker HuggingFace Model 및 엔드포인트를 삭제 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
