{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Tensorflow 컨테이너를 사용하여 하이퍼파라미터 튜닝하기\n",
    "## [(원본)](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning/tensorflow_mnist)\n",
    "\n",
    "이 문서는 **SageMaker TensorFlow container**를 사용하여 [MNIST dataset](http://yann.lecun.com/exdb/mnist/)을 훈련시키기 위해 convolutional neural network 모델을 만드는 방법에 초점을 두고 있습니다. \n",
    "이것은 하이퍼파라미터 튜닝을 활용하여 서로 다른 하이퍼파라미터를 조합하여 여러 훈련 Job을 실행함으로써 최상의 모델 훈련 결과를 제공하게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "워크플로우를 시작하기 전에 몇가지 설정이 필요합니다. \n",
    "\n",
    "1. 훈련 데이터셋과 모델 아티펙트가 저장될 s3버킷과 prefix를 지정합니다. \n",
    "2. SageMaker가 s3와 같은 리소스를 접근할 수 있도록 실행 Role을 가져옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "bucket = '<My bucket name>'#sagemaker.Session().default_bucket() # we are using a default bucket here but you can change it to any bucket in your account\n",
    "prefix = 'sagemaker/DEMO-hpo-tensorflow-high' # you can customize the prefix (subfolder) here\n",
    "\n",
    "role = sagemaker.get_execution_role() # we are using the notebook instance role for training in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 필요한 Python 라이브러리를 import 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 데이터셋 다운로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "('Writing', 'data/train.tfrecords')\n",
      "('Writing', 'data/validation.tfrecords')\n",
      "('Writing', 'data/test.tfrecords')\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "\n",
    "utils.convert_to(data_sets.train, 'train', 'data')\n",
    "utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "utils.convert_to(data_sets.test, 'test', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 업로드하기\n",
    " ```sagemaker.Session.upload_data``` 함수를 이용하여 S3경로에 데이터셋을 업로드합니다. 해당 함수의 리턴값은 S3의 경로를 가르킵니다. 이 경로는 훈련 Job을 시작할 때 사용할 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sds-sm-seongshj/sagemaker/DEMO-hpo-tensorflow-high/data/mnist\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker.Session().upload_data(path='data', bucket=bucket, key_prefix=prefix+'/data/mnist')\n",
    "print (inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분산 훈련을 위한 스크립트 작성하기\n",
    "다음은 네트워크 모델의 전체코드입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import tensorflow as tf\n",
      "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
      "\n",
      "INPUT_TENSOR_NAME = 'inputs'\n",
      "SIGNATURE_NAME = 'predictions'\n",
      "\n",
      "def model_fn(features, labels, mode, params):\n",
      "    # this script takes learning_rate as a hyperparameter\n",
      "    learning_rate = params.get(\"learning_rate\",0.1)\n",
      "    \n",
      "    # Input Layer\n",
      "    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n",
      "\n",
      "    # Convolutional Layer #1\n",
      "    conv1 = tf.layers.conv2d(\n",
      "        inputs=input_layer,\n",
      "        filters=32,\n",
      "        kernel_size=[5, 5],\n",
      "        padding='same',\n",
      "        activation=tf.nn.relu)\n",
      "\n",
      "    # Pooling Layer #1\n",
      "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
      "\n",
      "    # Convolutional Layer #2 and Pooling Layer #2\n",
      "    conv2 = tf.layers.conv2d(\n",
      "        inputs=pool1,\n",
      "        filters=64,\n",
      "        kernel_size=[5, 5],\n",
      "        padding='same',\n",
      "        activation=tf.nn.relu)\n",
      "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
      "\n",
      "    # Dense Layer\n",
      "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
      "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
      "    dropout = tf.layers.dropout(\n",
      "        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n",
      "\n",
      "    # Logits Layer\n",
      "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
      "\n",
      "    # Define operations\n",
      "    if mode in (Modes.PREDICT, Modes.EVAL):\n",
      "        predicted_indices = tf.argmax(input=logits, axis=1)\n",
      "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n",
      "\n",
      "    if mode in (Modes.TRAIN, Modes.EVAL):\n",
      "        global_step = tf.train.get_or_create_global_step()\n",
      "        label_indices = tf.cast(labels, tf.int32)\n",
      "        loss = tf.losses.softmax_cross_entropy(\n",
      "            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\n",
      "        tf.summary.scalar('OptimizeLoss', loss)\n",
      "\n",
      "    if mode == Modes.PREDICT:\n",
      "        predictions = {\n",
      "            'classes': predicted_indices,\n",
      "            'probabilities': probabilities\n",
      "        }\n",
      "        export_outputs = {\n",
      "            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n",
      "        }\n",
      "        return tf.estimator.EstimatorSpec(\n",
      "            mode, predictions=predictions, export_outputs=export_outputs)\n",
      "\n",
      "    if mode == Modes.TRAIN:\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
      "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
      "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
      "\n",
      "    if mode == Modes.EVAL:\n",
      "        eval_metric_ops = {\n",
      "            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n",
      "        }\n",
      "        return tf.estimator.EstimatorSpec(\n",
      "            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
      "\n",
      "\n",
      "def serving_input_fn(params):\n",
      "    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n",
      "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
      "\n",
      "\n",
      "def read_and_decode(filename_queue):\n",
      "    reader = tf.TFRecordReader()\n",
      "    _, serialized_example = reader.read(filename_queue)\n",
      "\n",
      "    features = tf.parse_single_example(\n",
      "        serialized_example,\n",
      "        features={\n",
      "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
      "            'label': tf.FixedLenFeature([], tf.int64),\n",
      "        })\n",
      "\n",
      "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
      "    image.set_shape([784])\n",
      "    image = tf.cast(image, tf.float32) * (1. / 255)\n",
      "    label = tf.cast(features['label'], tf.int32)\n",
      "\n",
      "    return image, label\n",
      "\n",
      "\n",
      "def train_input_fn(training_dir, params):\n",
      "    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n",
      "\n",
      "\n",
      "def eval_input_fn(training_dir, params):\n",
      "    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n",
      "\n",
      "\n",
      "def _input_fn(training_dir, training_filename, batch_size=100):\n",
      "    test_file = os.path.join(training_dir, training_filename)\n",
      "    filename_queue = tf.train.string_input_producer([test_file])\n",
      "\n",
      "    image, label = read_and_decode(filename_queue)\n",
      "    images, labels = tf.train.batch(\n",
      "        [image, label], batch_size=batch_size,\n",
      "        capacity=1000 + 3 * batch_size)\n",
      "\n",
      "    return {INPUT_TENSOR_NAME: images}, labels\n"
     ]
    }
   ],
   "source": [
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 스크립트는 [TensorFlow MNIST example](https://github.com/tensorflow/models/tree/master/official/mnist)를 수정한 것입니다. \n",
    "이것은 훈련, 평가, 추론을 위해 사용되는```model_fn(features, labels, mode)`` 를 제공합니다.\n",
    "\n",
    "### 일반적인 ```model_fn```\n",
    "\n",
    "일반적인 **```model_fn```** 은 다음과 같은 패턴을 따릅니다. \n",
    "1. [Neural network 정의](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L96)\n",
    "- [Neural network에 ```features``` 적용](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L178)\n",
    "- [```mode```가 ```PREDICT``` 이면 neural network에서 output 리턴](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L186)\n",
    "- [Output과 ```labels```을 비교하는 loss fuction 계산](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L188)\n",
    "- [Optimizer 생성 및 neural network 개선을 위한 loss function 최소화](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L193)\n",
    "- [Output, optimizer, loss function 리턴](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L205)\n",
    "\n",
    "### 분산 훈련에서의 ```model_fn``` 작성\n",
    "분산 훈련이 일어나면, 동일한 neural network은 여러 훈련 인스턴스로 보내집니다. 개별 인스턴스는 데이터셋의 배치를 예측하고, loss를 계산하고 optimizer를 최소화합니다. 이 프로세스의 전체 루프를 **training step** 이라고 합니다. \n",
    "\n",
    "\n",
    "\n",
    "#### training steps 동기화\n",
    "A [global step](https://www.tensorflow.org/api_docs/python/tf/train/global_step)은 인스턴스 사이에 공유되는 전역 변수입니다. \n",
    "그것은 분산 훈련에서 필요하기 때문에 Optimizer는 실행되는 중간에 **training steps** 의 수를 추척합니다. \n",
    "\n",
    "\n",
    "```python\n",
    "train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step())\n",
    "```\n",
    "\n",
    "이것이 분산훈련을 위해 필요한 유일한 변경입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 튜닝 Job 설정\n",
    "\n",
    "*참고: 아래의 기본 설정에서는 하이퍼파라미터 튜닝 Job이 완료하는데 30분이 소요될 수 있습니다.*\n",
    "\n",
    "이제 다음 단계에 따라 SageMaker Python SDK를 사용하여 하이퍼파라미터 튜닝 Job을 설정합니다.\n",
    "* TensorFlow 훈련 Job 설정을 위한 estimator 생성하기\n",
    "* 튜닝하려는 하이퍼파라미터 범위 정의하기. 이 예제에서는 \"learning_rate\"를 튜닝함 \n",
    "* 최적화할 튜닝 Job의 목표 메트릭 정의하기\n",
    "* 위의 설정과 튜닝 Resource Configuratin으로 하이퍼파라미터 Tuner 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker에서 단일 TensorFlow Job을 훈련하는 것과 유사하게, TensorFlow 스크립트, IAM role, (작업별)하드웨어 구성을 전달하는 TensorFlow estimator를 정의합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='mnist.py',\n",
    "                  role=role,\n",
    "                  framework_version='1.12.0',\n",
    "                  training_steps=1000, \n",
    "                  evaluation_steps=100,\n",
    "                  train_instance_count=1,\n",
    "                  train_instance_type='ml.m4.xlarge',\n",
    "                  base_job_name='DEMO-hpo-tensorflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined our estimator we can specify the hyperparameters we'd like to tune and their possible values.  We have three different types of hyperparameters.\n",
    "\n",
    "estimator를 정의하고 나면 튜닝하려는 하이퍼파라미터과 가능한 값을 지정할 수 있습니다. 하이퍼파라미터는 세 가지 유형이 있습니다.\n",
    "\n",
    "- 범주형 파라미터는 이산형 셋(discrete set)에서 하나의 값을 가져야 합니다. 가능한 값 목록을  `CategoricalParameter(list)`으로 전달하여 정의합니다 \n",
    "- 연속형 파라미터는 `ContinuousParameter(min, max)` 에서 정의한 최소값과 최대값 사이의 실수 값을 가질 수 있습니다. \n",
    "- 정수형 파라미터는 `IntegerParameter(min, max)`에서 정의한 최소값과 최대값 사이의 정수 값을 가질 수 있습니다. \n",
    "\n",
    " \n",
    "*참고: 가능하면 값을 최소한의 restrictive type을 지정하는 것이 거의 항상 좋습니다. 예를 들면, learning rate는 연속값으로 0.01에서 0.2로 튜닝하는 것이 0.01, 0.1, 0.15 혹은 0.2의 범주형 파라미터로 튜닝하는 것보다 더 나은 결과를 얻을 수 있습니다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'learning_rate': ContinuousParameter(0.01, 0.02)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 튜닝을 위한 목표 매트릭과 그것을 정의하기 위한 설정을 진행합니다. 이것은 훈련 Job이 CloudWatch 로그로부터 매트릭을 추출하는데 필요한 정규표현식(Regex)를 포함합니다. 이 경우, 스크립트는 loss값을 방출하고 목표 매트릭은 이를 목표 매트릭으로 사용할 것입니다. 또한 최상의 하이퍼파라미터 설정을 찾을 때, 목표 매트릭을 최소화하기 하이퍼파라미터를 튜닝하기 위해 objective_type은 'minimize'로 설정합니다. default로 objective_type은 'maximize' 설정됩니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'loss'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'loss',\n",
    "                       'Regex': 'loss = ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "이제 `HyperparameterTuner` 객체를 생성할 것이고, 객체에 다음 값들이 전달됩니다.:\n",
    "- 위에서 생성한 TensorFlow estimator\n",
    "- 하이퍼파라미터 범위\n",
    "- 목표 매트릭 이름 및 정의\n",
    "- 총 훈련 Job의 갯수와 병렬적으로 실행할 훈련 Job의 수와 같은 튜닝 resource configurations \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=9,\n",
    "                            max_parallel_jobs=3,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 하이퍼파라미터 튜닝 Job 시작하기\n",
    "마지막으로 `.fit()`을 호출하고 훈련 및 테스트 데이터셋의 S3 경로를 전달함에 따라 하이퍼파라미터 훈련 Job을 시작할 수 있습니다.\n",
    "\n",
    "하이퍼파라미터 튜닝 Job이 생성된 후, 다음 단계에서 진행 상태를 보기위해 위해 튜닝 Job을 describe 할 수있어야 합니다. SageMaker의 콘솔->Jobs으로 이동하여 하이퍼파라미터의 튜닝 Job의 진행상태를 확인할 수 있습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "하이퍼파라미터 튜닝 Job을 간단히 체크해하여 성공적으로 시작했는지 확인하시기 바랍니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'InProgress'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 튜닝 Job 완료 후, 결과 분석하기\n",
    "튜닝 Job 결과를 분석하기 위해 \"HPO_Analyze_TuningJob_Results.ipynb\"  예제를 참조하십시오.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최상의 모델 배포하기\n",
    "이제 최상의 모델을 얻었으며, endpoint에서 배포할 수 있습니다. 모델을 배포하는 방법은 SageMaker sample notebook이나 SageMaker documentation을 참고하시기 바랍니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
