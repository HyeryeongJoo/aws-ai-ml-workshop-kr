{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e56fed",
   "metadata": {},
   "source": [
    "# SageMaker Endpoint (Single Model Endpoint)\n",
    "---\n",
    "\n",
    "이제 **SageMaker 모델 호스팅 서비스인 SageMaker 엔드포인트**에 모델을 배포할 준비가 되었습니다. \n",
    "\n",
    "SageMaker 엔드포인트는 REST API를 통해 실시간 추론을 수행할 수 있는 완전 관리형 서비스입니다. 기본적으로 분산 컨테이너로 고가용성, 다중 모델 로딩, A/B 테스트를 위한 인프라 환경(EC2, 로드밸런서, 오토스케일링, 모델 아티팩트 로딩 등)이 사전 구축되어 있기에 몇 줄의 코드만으로 Endpoint가 자동으로 생성되기에, 모델을 프로덕션에 빠르게 배포할 수 있습니다.\n",
    "\n",
    "SageMaker 빌트인 XGBoost를 사용하면 별도의 훈련/추론 스크립트 작성 없이 쉽게 모델을 훈련하고 엔드포인트로 배포할 수 있습니다. 하지만, 여러 가지 요인들로 인해 (예: SHAP 계산을 위한 피쳐 기여값 리턴, 추론값 및 추론 스코어 동시 리턴 등) 커스텀 추론 로직이 필요한 경우, SageMaker 빌트인 XGBoost 대신 SageMaker XGBoost 컨테이너를 사용할 수 있습니다.\n",
    "\n",
    "이 노트북은 SageMaker XGBoost 컨테이너 상에서, 기본적인 추론 스크립트로 모델을 배포하는 법을 아래와 같은 목차로 진행합니다. \n",
    "완료 시간은 **20분** 정도 소요됩니다.\n",
    "\n",
    "### 목차\n",
    "- [1. Create Model Serving Script](#1.-Create-Model-Serving-Script)\n",
    "- [2. Deploy a trained model from Amazon S3](#2.-Deploy-a-trained-model-from-Amazon-S3)\n",
    "    - [2.1. Deploy to Local Environment: XGBoostModel class](#2.1.-Deploy-to-Local-Environment:-XGBoostModel-class)\n",
    "    - [2.2. Deploy to Local Environment: Model class](#2.2.-Deploy-to-Local-Environment:-Model-class)\n",
    "    - [2.3. Deploy to Hosting Instance](#2.3.-Deploy-to-Hosting-Instance)\n",
    "\n",
    "유사한 예제로 실습해 보실 분들은 아래 링크의 샘플 노트북을 참조해 주세요.\n",
    "- https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone_dist_script_mode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc24946",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%store -r\n",
    "XGB_FRAMEWORK_VERSION = '1.3-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a18fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\r\n",
      "Requirement already satisfied: xgboost==1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.3.1)\r\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost==1.3.1) (1.19.5)\r\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost==1.3.1) (1.5.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==1.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57e71d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 0. (Optional) Prepare Your Model\n",
    "---\n",
    "\n",
    "혹시 이전 과정을 다 끝내지 못했다면, 아래 코드 셀의 주석을 해제하여 코드 셀을 실행해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4ab3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import xgboost as xgb\n",
    "# data = pd.read_csv('featureset/train.csv')\n",
    "# train = data.drop('fraud', axis=1)\n",
    "# label = pd.DataFrame(data['fraud'])\n",
    "# dtrain = xgb.DMatrix(train, label=label)\n",
    "\n",
    "# params = {'max_depth': 3, 'eta': 0.2, 'objective': \"binary:logistic\", 'scale_pos_weight': 29}\n",
    "# num_boost_round = 100\n",
    "# nfold = 5\n",
    "# early_stopping_rounds = 10\n",
    "\n",
    "# cv_results = xgb.cv(\n",
    "#     params = params,\n",
    "#     dtrain = dtrain,\n",
    "#     num_boost_round = num_boost_round,\n",
    "#     nfold = nfold,\n",
    "#     early_stopping_rounds = early_stopping_rounds,\n",
    "#     metrics = ('auc'),\n",
    "#     stratified = True, # 레이블 (0,1) 의 분포에 따라 훈련 , 검증 세트 분리\n",
    "#     seed = 0\n",
    "# )\n",
    "\n",
    "# print(\"cv_results: \", cv_results)\n",
    "\n",
    "# # Select the best score\n",
    "# print(f\"[0]#011train-auc:{cv_results.iloc[-1]['train-auc-mean']}\")\n",
    "# print(f\"[1]#011validation-auc:{cv_results.iloc[-1]['test-auc-mean']}\")\n",
    "\n",
    "# metrics_data = {\n",
    "#     'classification_metrics': {\n",
    "#         'validation:auc': { 'value': cv_results.iloc[-1]['test-auc-mean']},\n",
    "#         'train:auc': {'value': cv_results.iloc[-1]['train-auc-mean']}\n",
    "#     }\n",
    "# }\n",
    "      \n",
    "# model = xgb.train(params=params, dtrain=dtrain, num_boost_round=len(cv_results))\n",
    "# model.save_model(\"xgboost-model\")\n",
    "# !tar -czvf model.tar.gz xgboost-model && rm xgboost-model   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ff464",
   "metadata": {},
   "source": [
    "`Session()`은 AWS 환경에 접속하는 접속 정보와 SageMaker에서 사용하는 리소스를 관리하기 위한 편리한 방법을 제공합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6d2307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sagemaker-us-east-1-143656149352', 'arn:aws:iam::143656149352:role/service-role/SageMakerExecRole-team-clouddev')\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sm_session = sagemaker.session.Session()\n",
    "boto_session = boto3.session.Session()\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "bucket = sm_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto_session.region_name\n",
    "\n",
    "print((bucket, role))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fb13b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 1. Create Model Serving Script\n",
    "\n",
    "---\n",
    "\n",
    "아래 코드 셀은 src 디렉토리에 SageMaker 추론 스크립트를 저장합니다.\n",
    "\n",
    "#### Option 1.\n",
    "- `model_fn(model_dir)`: S3의 `model_dir`에 저장된 모델 아티팩트를 로드합니다.\n",
    "- `input_fn(request_body, content_type)`: 입력 데이터를 전처리합니다. `content_type`은 입력 데이터 종류에 따라 다양하게 처리 가능합니다. (예: `application/x-npy`, `application/json`, `application/csv`등)\n",
    "- `predict_fn(input_object, model)`: `input_fn(...)`을 통해 들어온 데이터에 대해 추론을 수행합니다.\n",
    "- `output_fn(prediction, accept_type)`: `predict_fn(...)`에서 받은 추론 결과를 후처리를 거쳐 프론트엔드로 전송합니다.\n",
    "\n",
    "#### Option 2.\n",
    "- `model_fn(model_dir)`: S3의 model_dir에 저장된 모델 아티팩트를 로드합니다.\n",
    "- `transform_fn(model, request_body, content_type, accept_type)`: `input_fn(...), predict_fn(...), output_fn(...)`을 `transform_fn(...)`으로 통합할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a118df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/inference.py\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import xgboost as xgb\n",
    "import sagemaker_xgboost_container.encoder as xgb_encoders\n",
    "NUM_FEATURES = 58\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserialize and return fitted model.\n",
    "    \"\"\"\n",
    "    model_file = \"xgboost-model\"\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(os.path.join(model_dir, model_file))\n",
    "    return model\n",
    "                     \n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    The SageMaker XGBoost model server receives the request data body and the content type,\n",
    "    and invokes the `input_fn`.\n",
    "    Return a DMatrix (an object that can be passed to predict_fn).\n",
    "    \"\"\"\n",
    "    print(\"Content type: \", request_content_type)\n",
    "    if request_content_type == \"application/x-npy\":        \n",
    "        stream = BytesIO(request_body)\n",
    "        array = np.frombuffer(stream.getvalue())\n",
    "        array = array.reshape(int(len(array)/NUM_FEATURES), NUM_FEATURES)\n",
    "        return xgb.DMatrix(array)\n",
    "    elif request_content_type == \"text/csv\":\n",
    "        return xgb_encoders.csv_to_dmatrix(request_body.rstrip(\"\\n\"))\n",
    "    elif request_content_type == \"text/libsvm\":\n",
    "        return xgb_encoders.libsvm_to_dmatrix(request_body)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Content type {} is not supported.\".format(request_content_type)\n",
    "        )\n",
    "        \n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    SageMaker XGBoost model server invokes `predict_fn` on the return value of `input_fn`.\n",
    "\n",
    "    Return a two-dimensional NumPy array (predictions and scores)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    y_probs = model.predict(input_data)\n",
    "    print(\"--- Inference time: %s secs ---\" % (time.time() - start_time))    \n",
    "    y_preds = [1 if e >= 0.5 else 0 for e in y_probs] \n",
    "    #feature_contribs = model.predict(input_data, pred_contribs=True, validate_features=False)\n",
    "    return np.vstack((y_preds, y_probs))\n",
    "\n",
    "\n",
    "def output_fn(predictions, content_type=\"application/json\"):\n",
    "    \"\"\"\n",
    "    After invoking predict_fn, the model server invokes `output_fn`.\n",
    "    \"\"\"\n",
    "    if content_type == \"text/csv\":\n",
    "        return ','.join(str(x) for x in outputs)\n",
    "    elif content_type == \"application/json\":\n",
    "        outputs = json.dumps({\n",
    "            'pred': predictions[0,:].tolist(),\n",
    "            'prob': predictions[1,:].tolist()\n",
    "        })        \n",
    "        \n",
    "        return outputs\n",
    "    else:\n",
    "        raise ValueError(\"Content type {} is not supported.\".format(content_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79811a6",
   "metadata": {},
   "source": [
    "### Check Inference Results & Debugging\n",
    "\n",
    "로컬 엔드포인트나 호스팅 엔드포인트 배포 전, 로컬 환경 상에서 직접 추론을 수행하여 결과를 확인합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d4d901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost-model\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf model && mkdir model && tar -xzvf model.tar.gz -C model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "293c1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model = xgb.Booster()\n",
    "model.load_model(\"model/xgboost-model\")\n",
    "\n",
    "test_df = pd.read_csv('featureset/test.csv')\n",
    "y_test = test_df.iloc[:, 0].astype('int')\n",
    "test_df = test_df.drop('fraud', axis=1)\n",
    "dtest = xgb.DMatrix(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81e1ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10092484, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob = model.predict(dtest)\n",
    "y_pred = np.array([1 if e >= 0.5 else 0 for e in y_prob])\n",
    "y_prob[0], y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5767b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 2. Deploy a trained model from Amazon S3\n",
    "---\n",
    "\n",
    "SageMaker API의 `Model` 클래스는 훈련한 모델을 서빙하기 위한 모델 아티팩트와 도커 이미지를 정의합니다. \n",
    "`Model` 클래스 인스턴스 호출 시 AWS에서 사전 빌드한 도커 이미지 URL을 직접 가져올 수도 있지만, Model의 자식 클래스로(예: `XGBoostModel`, `TensorFlowModel`) 초기화하면 파라메터에 버전만 지정하는 것만으로 편리하게 추론을 수행하는 환경을 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5a33d",
   "metadata": {},
   "source": [
    "### Upload model artifacts to S3\n",
    "압축한 모델 아티팩트를 Amazon S3로 복사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0213c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 4.2 KiB/4.2 KiB (46.8 KiB/s) with 1 file(s) remaining\r",
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-143656149352/sm-special-webinar/deploy/model.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "prefix = 'sm-special-webinar/deploy'\n",
    "s3_path = f's3://{bucket}/{prefix}/model.tar.gz'\n",
    "!aws s3 cp model.tar.gz {s3_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9130c1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2.1. Deploy to Local Environment: XGBoostModel class\n",
    "\n",
    "SageMaker 호스팅 엔드포인트로 배포하기 전에 로컬 모드 엔드포인트로 배포할 수 있습니다. 로컬 모드는 현재 개발 중인 환경에서 도커 컨테이너를 실행하여 SageMaker 프로세싱/훈련/추론 작업을 에뮬레이트할 수 있습니다. 추론 작업의 경우는 Amazon ECR의 딥러닝 프레임워크 기반 추론 컨테이너를 로컬로 가져오고(docker pull) 컨테이너를 실행하여(docker run) 모델 서버를 시작합니다.\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n",
    "\n",
    "```python\n",
    "local_model_path = f'{os.getcwd()}/model'\n",
    "ecr_uri = xgb_image_uri\n",
    "\n",
    "# 도커 컨테이너 구동\n",
    "!docker run --name xgb -itd -p 8080:8080 -v {local_model_path}:/opt/ml/model {ecr_uri} serve\n",
    "\n",
    "# 실시간 호출 테스트 \n",
    "!curl -X POST -H 'Content-Type: application/json' localhost:8080/invocations -d ...\n",
    "\n",
    "# 도커 컨테이너 중지 및 삭제    \n",
    "!docker stop xgb\n",
    "!docker rm xgb\n",
    "```\n",
    "\n",
    "참고로 SageMaker SDK에서 `deploy(...)` 메소드로 엔드포인트 배포 시, 인스턴스 타입을 local 이나 local_gpu로 지정하면 위의 과정을 자동으로 수행할 수 있습니다.\n",
    "\n",
    "```python\n",
    "# 로컬 엔드포인트 배포\n",
    "local_predictor = local_model.deploy(initial_instance_count=1, instance_type=\"local\")\n",
    "\n",
    "# 실시간 호출 테스트 \n",
    "local_predictor.predict(...)\n",
    "\n",
    "# 로컬 엔드포인트 삭제 (도커 컨테이너 중지 및 삭제)\n",
    "local_predictor.delete_endpoint()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23165f5",
   "metadata": {},
   "source": [
    "아래 코드를 보시면 아시겠지만, 지속적으로 업데이트되는 파이썬 버전&프레임워크 버전&트랜스포머 버전에 쉽게 대응할 수 있습니다. AWS에서 관리하고 있는 딥러닝 컨테이너(DLC) 목록을 아래 주소에서 확인해 보세요.\n",
    "- https://github.com/aws/deep-learning-containers/blob/master/available_images.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a59a9b",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09c9a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "xgb_model = XGBoostModel(\n",
    "    model_data=s3_path,\n",
    "    role=role,\n",
    "    entry_point=\"src/inference.py\",\n",
    "    framework_version=XGB_FRAMEWORK_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6d725",
   "metadata": {},
   "source": [
    "### Create Endpoint\n",
    "\n",
    "SageMaker SDK는 `deploy(...)` 메소드를 호출 시, `create-endpoint-config`와 `create-endpoint`를 같이 수행합니다. 좀 더 세분화된 파라메터 조정을 원하면 AWS CLI나 boto3 SDK client 활용을 권장 드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "536ad8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to 8gv7dd94gg-algo-1-39snu\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:50:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:50:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:50:INFO] nginx config: \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m worker_processes auto;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m daemon off;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m error_log  /dev/stderr;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m events {\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m }\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m http {\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   default_type application/octet-stream;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   upstream gunicorn {\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     server unix:/tmp/gunicorn.sock;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   }\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   server {\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     location ~ ^/(ping|invocations|execution-parameters) {\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m       proxy_set_header Host $http_host;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m       proxy_redirect off;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m       proxy_read_timeout 60s;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m       proxy_pass http://gunicorn;\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     }\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     location / {\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m       return 404 \"{}\";\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     }\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   }\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m }\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:50:INFO] Module inference does not provide a setup.py. \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Generating setup.py\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:50:INFO] Generating setup.cfg\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:50:INFO] Generating MANIFEST.in\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:50:INFO] Installing module with the following command:\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m /miniconda3/bin/python3 -m pip install . \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[?25hBuilding wheels for collected packages: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Building wheel for inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[?25h  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=4631 sha256=475abc787a48f9c1e8bf9132a19837e0bf323c161ba6484e2a285e3d0cefbeca\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-8o28gjbg/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Successfully built inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Installing collected packages: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Successfully installed inference-1.0.0\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m You should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[0m[2022-03-13 12:10:52 +0000] [38] [INFO] Starting gunicorn 19.10.0\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13 12:10:52 +0000] [38] [INFO] Listening at: unix:/tmp/gunicorn.sock (38)\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13 12:10:52 +0000] [38] [INFO] Using worker: gevent\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13 12:10:52 +0000] [41] [INFO] Booting worker with pid: 41\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13 12:10:52 +0000] [42] [INFO] Booting worker with pid: 42\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13 12:10:52 +0000] [43] [INFO] Booting worker with pid: 43\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13 12:10:52 +0000] [44] [INFO] Booting worker with pid: 44\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:53:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:53:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:53:INFO] Installing module with the following command:\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m /miniconda3/bin/python3 -m pip install . \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:54:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:54:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:54:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[?25hBuilding wheels for collected packages: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Building wheel for inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[?25h  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=7201 sha256=7f556b3e967fc36911376e3f776940e816ba4b5bb011ce82db51256523f2f72d\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-u11_m92u/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Successfully built inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Installing collected packages: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Attempting uninstall: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     Found existing installation: inference 1.0.0\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     Can't uninstall 'inference'. No files were found to uninstall.\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Successfully installed inference-1.0.0\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m You should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[0m172.18.0.1 - - [13/Mar/2022:12:10:55 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"python-urllib3/1.26.8\"\n",
      "!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='local'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6429bf3c",
   "metadata": {},
   "source": [
    "### Check Docker\n",
    "\n",
    "모델 서빙을 위한 도커 컨테이너가 구동되고 있음을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b2c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                                  COMMAND   CREATED         STATUS         PORTS                                       NAMES\r\n",
      "2ec1f0265a38   683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1   \"serve\"   7 seconds ago   Up 5 seconds   0.0.0.0:8080->8080/tcp, :::8080->8080/tcp   8gv7dd94gg-algo-1-39snu\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929aaee2",
   "metadata": {},
   "source": [
    "### Prediction - SageMaker SDK & text/csv\n",
    "샘플 데이터에 대해 추론을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b00143eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:58:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:10:58:INFO] Installing module with the following command:\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m /miniconda3/bin/python3 -m pip install . \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[?25hBuilding wheels for collected packages: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Building wheel for inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[?25h  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=9945 sha256=6101e0d2e25dcad5afce8cff4fdc1685a4b5fb3b4bd1acd195aa60b0c735d099\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-l_kiil2h/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Successfully built inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Installing collected packages: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Attempting uninstall: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     Found existing installation: inference 1.0.0\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     Can't uninstall 'inference'. No files were found to uninstall.\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Successfully installed inference-1.0.0\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m You should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[0mContent type:  text/csv\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:11:00:INFO] Determined delimiter of CSV input is ','\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m --- Inference time: 0.0015077590942382812 secs ---\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m 172.18.0.1 - - [13/Mar/2022:12:11:00 +0000] \"POST /invocations HTTP/1.1\" 200 123 \"-\" \"python-urllib3/1.26.8\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0]),\n",
       " [0.0, 0.0, 0.0, 0.0],\n",
       " [0.10092484205961227,\n",
       "  0.08251918852329254,\n",
       "  0.4293206036090851,\n",
       "  0.23539546132087708])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer, NumpySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "xgb_predictor.serializer = CSVSerializer()\n",
    "xgb_predictor.deserializer = JSONDeserializer() \n",
    "\n",
    "outputs = xgb_predictor.predict(test_df.values[0:4,:])\n",
    "y_test_sample = y_test[0:4].values\n",
    "y_pred_sample = outputs['pred']; y_prob_sample = outputs['prob']\n",
    "y_test_sample, y_pred_sample, y_prob_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4a02e",
   "metadata": {},
   "source": [
    "### Prediction - boto3 SDK & application/x-npy\n",
    "\n",
    "위의 코드 셀처럼 SageMaker SDK의 `predict(...)` 메소드로 추론을 수행할 수도 있지만, 이번에는 boto3의 `invoke_endpoint(...)` 메소드로 추론을 수행해 보겠습니다.\n",
    "Boto3는 서비스 레벨의 저수준(low-level) SDK로, ML 실험에 초점을 맞춰 일부 기능들이 추상화된 고수준(high-level) SDK인 SageMaker SDK와 달리 SageMaker API를 완벽하게 제어할 수 있습으며, 프로덕션 및 자동화 작업에 적합합니다.\n",
    "\n",
    "[Note] `invoke_endpoint(...)` 호출을 위한 런타임 클라이언트 인스턴스 생성 시, 로컬 배포 모드에서는`sagemaker.local.LocalSagemakerRuntimeClient(...)`를 호출해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bafb036f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:11:01:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:11:01:INFO] Installing module with the following command:\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m /miniconda3/bin/python3 -m pip install . \n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[?25hBuilding wheels for collected packages: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Building wheel for inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[?25h  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=12868 sha256=cadf9e8ae4f186ead65af19199061e2a93868c32c63f3e5ea3d097601564cc98\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-1ul0_ic3/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Successfully built inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Installing collected packages: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m   Attempting uninstall: inference\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     Found existing installation: inference 1.0.0\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m     Can't uninstall 'inference'. No files were found to uninstall.\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Successfully installed inference-1.0.0\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m You should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m \u001b[0mContent type:  application/x-npy\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m --- Inference time: 0.0014278888702392578 secs ---\n",
      "{'pred': [0.0, 0.0, 0.0, 0.0], 'prob': [0.10092484205961227, 0.08251918852329254, 0.4293206036090851, 0.23539546132087708]}\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m 172.18.0.1 - - [13/Mar/2022:12:11:03 +0000] \"POST /invocations HTTP/1.1\" 200 123 \"-\" \"python-urllib3/1.26.8\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = xgb_model.endpoint_name\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-npy',\n",
    "    Accept='application/json',\n",
    "    Body=test_df.values[0:4,:].tobytes()\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd0099",
   "metadata": {},
   "source": [
    "### Prediction - boto3 SDK & text/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c225483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m Content type:  text/csv\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m [2022-03-13:12:11:12:INFO] Determined delimiter of CSV input is ','\n",
      "{'pred': [0.0, 0.0, 0.0, 0.0], 'prob': [0.10092484205961227, 0.08251918852329254, 0.4293206036090851, 0.23539546132087708]}\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m --- Inference time: 0.00011134147644042969 secs ---\n",
      "\u001b[36m8gv7dd94gg-algo-1-39snu |\u001b[0m 172.18.0.1 - - [13/Mar/2022:12:11:12 +0000] \"POST /invocations HTTP/1.1\" 200 123 \"-\" \"python-urllib3/1.26.8\"\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from io import StringIO\n",
    "csv_file = io.StringIO()\n",
    "test_df[0:4].to_csv(csv_file, sep=\",\", header=False, index=False)\n",
    "payload = csv_file.getvalue()\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='text/csv',\n",
    "    Accept='application/json',\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c281291",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0be20e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "xgb_predictor.delete_endpoint()\n",
    "xgb_model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf4805",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2.2. Deploy to Local Environment: Model class\n",
    "\n",
    "이번에는 `Model` 클래스로 로컬 환경에서 모델 서빙을 수행합니다. 여러분의 추론 환경을 커스터마이징하여 private ECR에 등록할 때 유용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8124b8",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4336989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1\n"
     ]
    }
   ],
   "source": [
    "# If you need to create a sagemaker.model.Model, rather than sagemaker.xgboost.model.XGBoostModel\n",
    "image_uri = sagemaker.image_uris.retrieve(\"xgboost\", region, XGB_FRAMEWORK_VERSION)\n",
    "print(image_uri)\n",
    "\n",
    "xgb_model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=s3_path,\n",
    "    role=role,\n",
    "    entry_point=\"src/inference.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e6bc71",
   "metadata": {},
   "source": [
    "### Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3923b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to mym4yp1h67-algo-1-i5mw1\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:25:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:25:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:25:INFO] nginx config: \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m worker_processes auto;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m daemon off;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m error_log  /dev/stderr;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m events {\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m }\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m http {\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   default_type application/octet-stream;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   upstream gunicorn {\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     server unix:/tmp/gunicorn.sock;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   }\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   server {\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     location ~ ^/(ping|invocations|execution-parameters) {\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m       proxy_set_header Host $http_host;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m       proxy_redirect off;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m       proxy_read_timeout 60s;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m       proxy_pass http://gunicorn;\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     }\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     location / {\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m       return 404 \"{}\";\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     }\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   }\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m }\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:26:INFO] Module inference does not provide a setup.py. \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Generating setup.py\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:26:INFO] Generating setup.cfg\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:26:INFO] Generating MANIFEST.in\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:26:INFO] Installing module with the following command:\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m /miniconda3/bin/python3 -m pip install . \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[?25hBuilding wheels for collected packages: inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Building wheel for inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[?25h  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=4631 sha256=f68e5d84412141728628fb9d897d5650e021257802021f3b5ceac58bc1a8e1ce\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-s95_ilrq/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Successfully built inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Installing collected packages: inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Successfully installed inference-1.0.0\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m You should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[0m[2022-03-13 12:11:27 +0000] [38] [INFO] Starting gunicorn 19.10.0\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13 12:11:27 +0000] [38] [INFO] Listening at: unix:/tmp/gunicorn.sock (38)\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13 12:11:27 +0000] [38] [INFO] Using worker: gevent\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13 12:11:27 +0000] [41] [INFO] Booting worker with pid: 41\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13 12:11:27 +0000] [42] [INFO] Booting worker with pid: 42\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13 12:11:27 +0000] [46] [INFO] Booting worker with pid: 46\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13 12:11:27 +0000] [47] [INFO] Booting worker with pid: 47\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:28:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:29:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:29:INFO] Installing module with the following command:\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m /miniconda3/bin/python3 -m pip install . \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:29:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:29:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:29:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[?25hBuilding wheels for collected packages: inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Building wheel for inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[?25h  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=7201 sha256=1d988fce6975e382249f66504c7f69727d79ab7ad64deda29b8561ee54d76452\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-mgp74oc7/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Successfully built inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Installing collected packages: inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Attempting uninstall: inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     Found existing installation: inference 1.0.0\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     Can't uninstall 'inference'. No files were found to uninstall.\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Successfully installed inference-1.0.0\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m You should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "!\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[0m172.18.0.1 - - [13/Mar/2022:12:11:30 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"python-urllib3/1.26.8\"\n"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='local'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02621b5a",
   "metadata": {},
   "source": [
    "### Create Predictor\n",
    "\n",
    "`Model` 클래스로 모델 생성 시, `Predictor` 클래스를 생성하고 직렬화 및 역직렬화 포맷을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1992780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "endpoint_name = xgb_model.endpoint_name\n",
    "local_sess = LocalSession()\n",
    "\n",
    "xgb_predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, \n",
    "    sagemaker_session=local_sess,\n",
    "    serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd37c79",
   "metadata": {},
   "source": [
    "### Prediction - SageMaker SDK & text/csv\n",
    "샘플 데이터에 대해 추론을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e65d7b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:36:INFO] No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:36:INFO] Installing module with the following command:\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m /miniconda3/bin/python3 -m pip install . \n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[?25hBuilding wheels for collected packages: inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Building wheel for inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[?25h  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=9945 sha256=9557fb5c12dd918caef29bba8180d1295f436eee28965d9febd52739baf534ad\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-utym2irb/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Successfully built inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Installing collected packages: inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m   Attempting uninstall: inference\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     Found existing installation: inference 1.0.0\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m     Can't uninstall 'inference'. No files were found to uninstall.\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m Successfully installed inference-1.0.0\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m You should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m \u001b[0mContent type:  text/csv\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m [2022-03-13:12:11:37:INFO] Determined delimiter of CSV input is ','\n",
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m --- Inference time: 0.0015187263488769531 secs ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0]),\n",
       " [0.0, 0.0, 0.0, 0.0],\n",
       " [0.10092484205961227,\n",
       "  0.08251918852329254,\n",
       "  0.4293206036090851,\n",
       "  0.23539546132087708])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mmym4yp1h67-algo-1-i5mw1 |\u001b[0m 172.18.0.1 - - [13/Mar/2022:12:11:37 +0000] \"POST /invocations HTTP/1.1\" 200 123 \"-\" \"python-urllib3/1.26.8\"\r\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "outputs = json.loads(xgb_predictor.predict(test_df.values[0:4,:]))\n",
    "y_test_sample = y_test[0:4].values\n",
    "y_pred_sample = outputs['pred']; y_prob_sample = outputs['prob']\n",
    "y_test_sample, y_pred_sample, y_prob_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a38ce6e",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db274366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "xgb_predictor.delete_model()\n",
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f9f97",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2.3. Deploy to Hosting Instance\n",
    "\n",
    "로컬 모드에서 충분히 디버깅했으면 실제 호스팅 인스턴스로 배포할 차례입니다. 코드는 거의 동일하며, `instance_type`만 다르다는 점을 주목해 주세요! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b9983",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20b3cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "\n",
    "xgb_model = XGBoostModel(\n",
    "    model_data=s3_path,\n",
    "    role=role,\n",
    "    entry_point=\"src/inference.py\",\n",
    "    framework_version=XGB_FRAMEWORK_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8f197",
   "metadata": {},
   "source": [
    "### Create Endpoint\n",
    "\n",
    "SageMaker SDK는 `deploy(...)` 메소드를 호출 시, `create-endpoint-config`와 `create-endpoint`를 같이 수행합니다. 좀 더 세분화된 파라메터 조정을 원하면 AWS CLI나 boto3 SDK client 활용을 권장 드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a0ca3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge', \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2888f87e",
   "metadata": {},
   "source": [
    "### Wait for the endpoint jobs to complete\n",
    "\n",
    "엔드포인트가 생성될 때까지 기다립니다. 엔드포인트가 가리키는 호스팅 리소스를 프로비저닝하는 데에 몇 분의 시간이 소요됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd441de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b><a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints/sagemaker-xgboost-2022-03-13-12-11-49-886\">[Deploy model from S3] Review Endpoint</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "def make_endpoint_link(region, endpoint_name, endpoint_task):\n",
    "    endpoint_link = f'<b><a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">{endpoint_task} Review Endpoint</a></b>'   \n",
    "    return endpoint_link \n",
    "        \n",
    "endpoint_link = make_endpoint_link(region, xgb_predictor.endpoint_name, '[Deploy model from S3]')\n",
    "display(HTML(endpoint_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2787c385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'sagemaker-xgboost-2022-03-13-12-11-49-886',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:143656149352:endpoint/sagemaker-xgboost-2022-03-13-12-11-49-886',\n",
       " 'EndpointConfigName': 'sagemaker-xgboost-2022-03-13-12-11-49-886',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1',\n",
       "     'ResolvedImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost@sha256:a054dd7910b920f54d1b65728306858543601a1c4ae49866cda181ca4069e3d9',\n",
       "     'ResolutionTime': datetime.datetime(2022, 3, 13, 12, 11, 50, 714000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2022, 3, 13, 12, 11, 50, 88000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 3, 13, 12, 14, 15, 98000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'dc6b99e4-0de4-4d65-9f87-7e426f44cfdd',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'dc6b99e4-0de4-4d65-9f87-7e426f44cfdd',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '768',\n",
       "   'date': 'Sun, 13 Mar 2022 12:14:19 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.wait_for_endpoint(xgb_predictor.endpoint_name, poll=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7396b3b1",
   "metadata": {},
   "source": [
    "### Prediction - SageMaker SDK & text/csv\n",
    "샘플 데이터에 대해 추론을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f70ae60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0]),\n",
       " [0.0, 0.0, 0.0, 0.0],\n",
       " [0.10092484205961227,\n",
       "  0.08251918852329254,\n",
       "  0.4293206036090851,\n",
       "  0.23539546132087708])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer, NumpySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "xgb_predictor.serializer = CSVSerializer()\n",
    "xgb_predictor.deserializer = JSONDeserializer() \n",
    "\n",
    "outputs = xgb_predictor.predict(test_df.values[0:4,:])\n",
    "y_test_sample = y_test[0:4].values\n",
    "y_pred_sample = outputs['pred']; y_prob_sample = outputs['prob']\n",
    "y_test_sample, y_pred_sample, y_prob_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a82a01",
   "metadata": {},
   "source": [
    "### Prediction - boto3 SDK & application/x-npy\n",
    "\n",
    "위의 코드 셀처럼 SageMaker SDK의 `predict(...)` 메소드로 추론을 수행할 수도 있지만, 이번에는 boto3의 `invoke_endpoint(...)` 메소드로 추론을 수행해 보겠습니다.\n",
    "Boto3는 서비스 레벨의 저수준(low-level) SDK로, ML 실험에 초점을 맞춰 일부 기능들이 추상화된 고수준(high-level) SDK인 SageMaker SDK와 달리 SageMaker API를 완벽하게 제어할 수 있습으며, 프로덕션 및 자동화 작업에 적합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e21d1eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred': [0.0, 0.0, 0.0, 0.0], 'prob': [0.10092484205961227, 0.08251918852329254, 0.4293206036090851, 0.23539546132087708]}\n"
     ]
    }
   ],
   "source": [
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "endpoint_name = xgb_model.endpoint_name\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-npy',\n",
    "    Accept='application/json',\n",
    "    Body=test_df.values[0:4,:].tobytes()\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f8c86",
   "metadata": {},
   "source": [
    "### Prediction - boto3 SDK & text/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d6787ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred': [0.0, 0.0, 0.0, 0.0], 'prob': [0.10092484205961227, 0.08251918852329254, 0.4293206036090851, 0.23539546132087708]}\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from io import StringIO\n",
    "csv_file = io.StringIO()\n",
    "test_df[0:4].to_csv(csv_file, sep=\",\", header=False, index=False)\n",
    "payload = csv_file.getvalue()\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='text/csv',\n",
    "    Accept='application/json',\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac22b3",
   "metadata": {},
   "source": [
    "### Evaluation (Not Required)\n",
    "\n",
    "테스트셋에 대해 성능 평가를 수행합니다. 다만, 일반적인 경우 테스트셋은 정답 데이터가 포함되어 있지 않다는 점 유념해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c5205f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = xgb_predictor.predict(test_df.values)\n",
    "y_pred = outputs['pred']; y_prob = outputs['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0cf4ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.72      0.83       967\n",
      "           1       0.09      0.79      0.16        33\n",
      "\n",
      "    accuracy                           0.72      1000\n",
      "   macro avg       0.54      0.75      0.50      1000\n",
      "weighted avg       0.96      0.72      0.81      1000\n",
      "\n",
      "[[697 270]\n",
      " [  7  26]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'regression_metrics': {'mse': {'value': 0.277,\n",
       "   'standard_deviation': 0.4558848538830812}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(f\"{classification_report(y_true=y_test, y_pred=y_pred)}\")\n",
    "\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)     \n",
    "print(cm)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "std = np.std(y_test - y_pred)\n",
    "report_dict = {\n",
    "    \"regression_metrics\": {\n",
    "        \"mse\": {\n",
    "            \"value\": mse,\n",
    "            \"standard_deviation\": std\n",
    "        },\n",
    "    },\n",
    "}\n",
    "report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1b1aa2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "pyplot.plot(fpr, tpr, linestyle='--', label='Fraud')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.title(f'AUC={auc:.4f}')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58656dbd",
   "metadata": {},
   "source": [
    "다음 모듈에서 재사용할 변수들을 저장합니다. 만약 다음 모듈로 진행하지 않는다면 아래 섹션의 코드 셀을 주석 해제 후 실행해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fec30b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n",
      "Stored 'test_df' (DataFrame)\n",
      "Stored 's3_path' (str)\n"
     ]
    }
   ],
   "source": [
    "%store endpoint_name test_df s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560e4a2",
   "metadata": {},
   "source": [
    "### (Optional) Endpoint Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_predictor.delete_endpoint()\n",
    "# xgb_model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
