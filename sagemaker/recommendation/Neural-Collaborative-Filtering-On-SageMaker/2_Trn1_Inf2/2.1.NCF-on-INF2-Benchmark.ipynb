{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 2.1] Inference NCF on INF2 - Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 torch_neuronx 를 로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 훈련된 모델 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 모델 아티펙트 확인\n",
    "\n",
    "- 이미 훈련된 파이토치로 훈련된 모델 아티텍트의 경로를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifact is assigend from :  models/NeuMF-end.pth\n"
     ]
    }
   ],
   "source": [
    "artifact_path = 'models/NeuMF-end.pth'\n",
    "print(\"model artifact is assigend from : \", artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 로딩에 필요한 설정 파일 생성\n",
    "\n",
    "- 모델 로딩시에 필요한 파라미터 사용 (기존의 값을 사용 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of num_layers:  3\n",
      "user_num:  6040  item_num:  3706\n",
      "src/model_config.json is saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'src/model_config.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import config\n",
    "from common_utils import save_json, load_json\n",
    "\n",
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.factor_num = 32\n",
    "        self.num_layers = 3\n",
    "        self.dropout = 0.0\n",
    "                        \n",
    "args = Params()\n",
    "print(\"# of num_layers: \", args.num_layers)\n",
    "\n",
    "\n",
    "# 모델 훈련시에 결정된 user, item 의 숫자\n",
    "user_num = 6040  \n",
    "item_num = 3706\n",
    "print(\"user_num: \", user_num, \" item_num: \", item_num)\n",
    "\n",
    "model_config_dict = {\n",
    "    'user_num': str(user_num),\n",
    "    'item_num': str(item_num),\n",
    "    'factor_num' : str(args.factor_num),\n",
    "    'num_layers' : str(args.num_layers),\n",
    "    'dropout' : str(args.dropout),\n",
    "    'model_type': config.model\n",
    "}\n",
    "\n",
    "model_config_file = 'model_config.json'\n",
    "model_config_file_path = os.path.join('src', model_config_file)\n",
    "\n",
    "save_json(model_config_file_path, model_config_dict)\n",
    "# model_config_dict = load_json(model_config_file_path)    \n",
    "# model_config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 모델 로딩\n",
    "- 모델 로딩 함수 model_fn() 를 통하여 모델 로딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Staring model_fn() ###############\n",
      "device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from inference import model_fn\n",
    "\n",
    "ncf_model = model_fn(config.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 컴파일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 샘플 입력 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:  <class 'tuple'>\n",
      "len:  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def create_dummy_input(batch_size):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # print(\"Using {} device\".format(device))\n",
    "\n",
    "    user_np = np.zeros((1,100)).astype(np.int32)\n",
    "    item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "    return (\n",
    "        torch.repeat_interleave(torch.from_numpy(user_np), batch_size, 0),\n",
    "        torch.repeat_interleave(torch.from_numpy(item_np), batch_size, 0),\n",
    "    )\n",
    "\n",
    "dummy_inputs = create_dummy_input(batch_size=1)\n",
    "\n",
    "print(\"type: \", type(dummy_inputs))\n",
    "print(\"len: \", len(dummy_inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Script 으로 변환 (컴파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-1.1848],\n",
      "         [-2.9062],\n",
      "         [-2.9215],\n",
      "         [-0.2536],\n",
      "         [-2.1842],\n",
      "         [-0.9262],\n",
      "         [ 1.8365],\n",
      "         [-1.9201],\n",
      "         [-3.1589],\n",
      "         [-1.3983],\n",
      "         [-0.9339],\n",
      "         [-2.1668],\n",
      "         [ 0.6848],\n",
      "         [-3.3775],\n",
      "         [ 0.6939],\n",
      "         [-2.4077],\n",
      "         [ 1.1072],\n",
      "         [-3.1092],\n",
      "         [-3.4677],\n",
      "         [-3.0932],\n",
      "         [-3.1637],\n",
      "         [ 0.8479],\n",
      "         [ 3.2808],\n",
      "         [-2.6363],\n",
      "         [-1.3871],\n",
      "         [-4.2089],\n",
      "         [-0.5071],\n",
      "         [-0.6448],\n",
      "         [-2.0258],\n",
      "         [-1.0498],\n",
      "         [-4.3857],\n",
      "         [ 0.5838],\n",
      "         [-2.1706],\n",
      "         [-3.4535],\n",
      "         [ 1.3635],\n",
      "         [-3.5763],\n",
      "         [-1.6100],\n",
      "         [-1.7371],\n",
      "         [-1.5105],\n",
      "         [ 1.6561],\n",
      "         [-1.6870],\n",
      "         [-3.5930],\n",
      "         [-3.4934],\n",
      "         [-3.4258],\n",
      "         [-1.9256],\n",
      "         [-2.4687],\n",
      "         [-4.3822],\n",
      "         [-2.5416],\n",
      "         [-1.7941],\n",
      "         [-3.4284],\n",
      "         [-2.0919],\n",
      "         [-1.5155],\n",
      "         [-0.5078],\n",
      "         [-0.9661],\n",
      "         [-3.8006],\n",
      "         [-3.3326],\n",
      "         [-2.7731],\n",
      "         [-1.5982],\n",
      "         [-1.1956],\n",
      "         [-2.7635],\n",
      "         [-0.5612],\n",
      "         [-0.6162],\n",
      "         [-2.6363],\n",
      "         [ 0.4321],\n",
      "         [ 0.3080],\n",
      "         [-0.4631],\n",
      "         [-2.9188],\n",
      "         [-2.6322],\n",
      "         [-0.3596],\n",
      "         [ 0.2046],\n",
      "         [ 1.8739],\n",
      "         [-0.8160],\n",
      "         [-2.9205],\n",
      "         [-2.0215],\n",
      "         [-0.3109],\n",
      "         [-0.3199],\n",
      "         [-1.0715],\n",
      "         [-0.1325],\n",
      "         [ 0.7063],\n",
      "         [-0.1362],\n",
      "         [-2.0493],\n",
      "         [-1.4876],\n",
      "         [-3.2506],\n",
      "         [-0.7274],\n",
      "         [-2.7665],\n",
      "         [-0.9661],\n",
      "         [-1.7371],\n",
      "         [ 2.4899],\n",
      "         [-3.3320],\n",
      "         [-2.7894],\n",
      "         [-4.0188],\n",
      "         [-0.8477],\n",
      "         [ 1.0905],\n",
      "         [-2.2822],\n",
      "         [-2.1706],\n",
      "         [-1.0498],\n",
      "         [-1.7654],\n",
      "         [-2.0109],\n",
      "         [-0.4649],\n",
      "         [-1.6792]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([1, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/12/2023 06:40:17 AM WARNING 36012 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:19 AM WARNING 36012 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:19 AM WARNING 36012 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:19 AM WARNING 36012 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:19 AM WARNING 36012 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:19 AM WARNING 36012 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:19 AM WARNING 36012 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:19 AM WARNING 36012 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:19 AM WARNING 36012 [WalrusDriver]: DRAM allocation successful\n"
     ]
    }
   ],
   "source": [
    "def convert_torch_script(model, dummy_inputs):\n",
    "    # Compile the model for Neuron\n",
    "    model_neuron = torch_neuronx.trace(model, dummy_inputs)\n",
    "    \n",
    "    return model_neuron\n",
    "\n",
    "model_neuron = convert_torch_script(ncf_model, dummy_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:prediction  <class 'tuple'>\n",
      "type:prediction[0]  <class 'torch.Tensor'>\n",
      "recommended_item_index:  \n",
      " tensor([22, 87, 70,  6, 39, 34, 16, 92, 21, 78])\n"
     ]
    }
   ],
   "source": [
    "def extract_top_k(prediction, top_k = 10):\n",
    "    prediction = torch.squeeze(prediction) # remove dimension\n",
    "    _, indices = torch.topk(prediction, top_k)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "prediction = model_neuron(dummy_inputs[0],dummy_inputs[1])\n",
    "print(\"type:prediction \", type(prediction))\n",
    "print(\"type:prediction[0] \", type(prediction[0]))\n",
    "\n",
    "recommended_item_index = extract_top_k(prediction[0], top_k = 10)\n",
    "print(\"recommended_item_index:  \\n\", recommended_item_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 모델 저장 및 로딩 후 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TorchScript for inference deployment\n",
    "filename = 'models/model.pt'\n",
    "torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:prediction  <class 'tuple'>\n",
      "type:prediction[0]  <class 'torch.Tensor'>\n",
      "recommended_item_index:  \n",
      " tensor([22, 87, 70,  6, 39, 34, 16, 92, 21, 78])\n"
     ]
    }
   ],
   "source": [
    "# Load the TorchScript compiled model\n",
    "load_model_neuron = torch.jit.load(filename)\n",
    "\n",
    "prediction = load_model_neuron(dummy_inputs[0],dummy_inputs[1])\n",
    "print(\"type:prediction \", type(prediction))\n",
    "print(\"type:prediction[0] \", type(prediction[0]))\n",
    "\n",
    "recommended_item_index = extract_top_k(prediction[0], top_k = 10)\n",
    "print(\"recommended_item_index:  \\n\", recommended_item_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 벤치 마킹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일부 샘플로 추론 시간 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latencies:  [0.48184, 0.12088, 0.11539, 0.11706, 0.11206]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "latencies = []\n",
    "num_test = 5\n",
    "for _ in range(num_test):\n",
    "    start = time.time()\n",
    "    prediction = load_model_neuron(dummy_inputs[0],dummy_inputs[1])    \n",
    "    finish = time.time()\n",
    "    elapse_time = round((finish - start) * 1000, 5)\n",
    "    # print(prediction)\n",
    "    latencies.append(elapse_time)\n",
    "\n",
    "print(\"latencies: \", latencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벤치 마킹, 모델 수 및 Thread 수 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inf2_util import benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename:    models/model.pt\n",
      "Batch Size:  1\n",
      "Batches:     2000\n",
      "Inferences:  2000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    0.115\n",
      "Throughput:  17406.568\n",
      "Latency P50: 0.111\n",
      "Latency P95: 0.125\n",
      "Latency P99: 0.135\n"
     ]
    }
   ],
   "source": [
    "# Benchmark BERT on Neuron\n",
    "# benchmark(filename, example, n_models=2, n_threads=2, batches_per_thread=1000)\n",
    "benchmark(filename, dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename:    models/model.pt\n",
      "Batch Size:  1\n",
      "Batches:     4000\n",
      "Inferences:  4000\n",
      "Threads:     4\n",
      "Models:      2\n",
      "Duration:    0.142\n",
      "Throughput:  28144.308\n",
      "Latency P50: 0.136\n",
      "Latency P95: 0.161\n",
      "Latency P99: 0.206\n"
     ]
    }
   ],
   "source": [
    "benchmark(filename, dummy_inputs, n_models=2, n_threads=4, batches_per_thread=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적의 배치 사이즈 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-1.9389],\n",
      "         [-1.9872],\n",
      "         [-1.2366],\n",
      "         [ 0.6196],\n",
      "         [-2.4418],\n",
      "         [-3.1095],\n",
      "         [-2.8314],\n",
      "         [-1.5271],\n",
      "         [-2.1842],\n",
      "         [-1.4032],\n",
      "         [ 0.2330],\n",
      "         [-0.2536],\n",
      "         [ 0.6151],\n",
      "         [-0.7603],\n",
      "         [-1.0449],\n",
      "         [-0.7522],\n",
      "         [-0.8561],\n",
      "         [-4.0481],\n",
      "         [-2.9439],\n",
      "         [-2.3725],\n",
      "         [ 0.1770],\n",
      "         [-2.3422],\n",
      "         [-2.0962],\n",
      "         [-2.0046],\n",
      "         [ 1.3567],\n",
      "         [-3.0413],\n",
      "         [ 1.1072],\n",
      "         [-0.4168],\n",
      "         [-3.5763],\n",
      "         [ 0.9679],\n",
      "         [ 1.8365],\n",
      "         [-1.1425],\n",
      "         [-2.1949],\n",
      "         [-3.2080],\n",
      "         [-2.5564],\n",
      "         [-0.1525],\n",
      "         [-3.4258],\n",
      "         [-2.5250],\n",
      "         [-0.1535],\n",
      "         [-3.1450],\n",
      "         [-3.6925],\n",
      "         [-2.5250],\n",
      "         [-1.1053],\n",
      "         [-1.5729],\n",
      "         [-1.6448],\n",
      "         [-1.3120],\n",
      "         [-1.4801],\n",
      "         [-1.6100],\n",
      "         [-3.0187],\n",
      "         [ 0.2330],\n",
      "         [-3.4050],\n",
      "         [ 1.3431],\n",
      "         [-4.9187],\n",
      "         [-1.4078],\n",
      "         [-3.8251],\n",
      "         [-2.0488],\n",
      "         [ 1.6681],\n",
      "         [-2.3725],\n",
      "         [ 0.1876],\n",
      "         [-3.3279],\n",
      "         [-4.6169],\n",
      "         [-1.6136],\n",
      "         [-2.8862],\n",
      "         [-2.7445],\n",
      "         [ 1.4246],\n",
      "         [-0.8978],\n",
      "         [-1.1806],\n",
      "         [-1.3871],\n",
      "         [-1.2659],\n",
      "         [ 1.6681],\n",
      "         [-4.6059],\n",
      "         [-1.1898],\n",
      "         [ 0.5289],\n",
      "         [-2.1833],\n",
      "         [-2.7658],\n",
      "         [-3.9294],\n",
      "         [-2.5136],\n",
      "         [-3.0826],\n",
      "         [ 1.4683],\n",
      "         [-2.9205],\n",
      "         [-0.1325],\n",
      "         [-1.4751],\n",
      "         [-1.6964],\n",
      "         [-2.4979],\n",
      "         [-1.1890],\n",
      "         [-0.4864],\n",
      "         [-3.6985],\n",
      "         [-1.5105],\n",
      "         [-0.3410],\n",
      "         [ 0.8121],\n",
      "         [-2.4952],\n",
      "         [ 0.2645],\n",
      "         [ 2.5198],\n",
      "         [-2.7998],\n",
      "         [-2.6322],\n",
      "         [ 0.3384],\n",
      "         [-3.1095],\n",
      "         [-3.3304],\n",
      "         [-2.7072],\n",
      "         [-1.5982]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2023 06:40:23 AM WARNING 36081 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:25 AM WARNING 36081 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:25 AM WARNING 36081 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:25 AM WARNING 36081 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:25 AM WARNING 36081 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:25 AM WARNING 36081 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:25 AM WARNING 36081 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:25 AM WARNING 36081 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:25 AM WARNING 36081 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-1.0365],\n",
      "         [-0.6960],\n",
      "         [-5.3186],\n",
      "         [-2.5546],\n",
      "         [-1.9959],\n",
      "         [-1.6822],\n",
      "         [-2.6483],\n",
      "         [-2.0274],\n",
      "         [-0.5445],\n",
      "         [-3.1637],\n",
      "         [-1.0943],\n",
      "         [-3.0403],\n",
      "         [-1.6977],\n",
      "         [-0.2422],\n",
      "         [-1.7862],\n",
      "         [-0.4970],\n",
      "         [-2.5605],\n",
      "         [-1.3185],\n",
      "         [-4.0969],\n",
      "         [-2.1618],\n",
      "         [-2.4952],\n",
      "         [-3.4677],\n",
      "         [-2.5342],\n",
      "         [-3.0927],\n",
      "         [ 0.4353],\n",
      "         [-2.5310],\n",
      "         [-3.4284],\n",
      "         [ 0.3642],\n",
      "         [-2.0267],\n",
      "         [-0.7425],\n",
      "         [-3.5763],\n",
      "         [-0.5675],\n",
      "         [-0.9022],\n",
      "         [-1.5686],\n",
      "         [-0.6397],\n",
      "         [-2.1833],\n",
      "         [ 2.1892],\n",
      "         [-2.0686],\n",
      "         [-1.0002],\n",
      "         [-4.4377],\n",
      "         [ 2.2862],\n",
      "         [-1.8735],\n",
      "         [-0.9339],\n",
      "         [-2.0422],\n",
      "         [-1.0282],\n",
      "         [-0.6058],\n",
      "         [-2.7894],\n",
      "         [ 0.6939],\n",
      "         [-3.1095],\n",
      "         [-1.1757],\n",
      "         [-1.1131],\n",
      "         [-0.1884],\n",
      "         [-1.6018],\n",
      "         [-2.5250],\n",
      "         [ 0.8017],\n",
      "         [-1.8881],\n",
      "         [-3.6449],\n",
      "         [-2.1842],\n",
      "         [ 1.3175],\n",
      "         [-3.8593],\n",
      "         [-3.2872],\n",
      "         [-1.6182],\n",
      "         [-0.4972],\n",
      "         [ 0.2158],\n",
      "         [-1.2405],\n",
      "         [-2.1591],\n",
      "         [-1.1425],\n",
      "         [ 2.1892],\n",
      "         [-3.4501],\n",
      "         [-2.9962],\n",
      "         [-2.0686],\n",
      "         [ 0.1901],\n",
      "         [-2.7648],\n",
      "         [-2.7237],\n",
      "         [-2.2822],\n",
      "         [-2.2837],\n",
      "         [-2.0686],\n",
      "         [-3.2271],\n",
      "         [-2.0919],\n",
      "         [-0.5751],\n",
      "         [-3.2411],\n",
      "         [ 0.5500],\n",
      "         [-1.2366],\n",
      "         [-0.2246],\n",
      "         [-0.9255],\n",
      "         [-0.8315],\n",
      "         [-1.3093],\n",
      "         [-0.9750],\n",
      "         [-0.8575],\n",
      "         [-1.1990],\n",
      "         [-0.4675],\n",
      "         [-1.0002],\n",
      "         [-1.5928],\n",
      "         [-1.1343],\n",
      "         [ 0.8861],\n",
      "         [-3.9965],\n",
      "         [-0.3487],\n",
      "         [-1.5944],\n",
      "         [-3.8313],\n",
      "         [-4.5885]],\n",
      "\n",
      "        [[-1.0365],\n",
      "         [-0.6960],\n",
      "         [-5.3186],\n",
      "         [-2.5546],\n",
      "         [-1.9959],\n",
      "         [-1.6822],\n",
      "         [-2.6483],\n",
      "         [-2.0274],\n",
      "         [-0.5445],\n",
      "         [-3.1637],\n",
      "         [-1.0943],\n",
      "         [-3.0403],\n",
      "         [-1.6977],\n",
      "         [-0.2422],\n",
      "         [-1.7862],\n",
      "         [-0.4970],\n",
      "         [-2.5605],\n",
      "         [-1.3185],\n",
      "         [-4.0969],\n",
      "         [-2.1618],\n",
      "         [-2.4952],\n",
      "         [-3.4677],\n",
      "         [-2.5342],\n",
      "         [-3.0927],\n",
      "         [ 0.4353],\n",
      "         [-2.5310],\n",
      "         [-3.4284],\n",
      "         [ 0.3642],\n",
      "         [-2.0267],\n",
      "         [-0.7425],\n",
      "         [-3.5763],\n",
      "         [-0.5675],\n",
      "         [-0.9022],\n",
      "         [-1.5686],\n",
      "         [-0.6397],\n",
      "         [-2.1833],\n",
      "         [ 2.1892],\n",
      "         [-2.0686],\n",
      "         [-1.0002],\n",
      "         [-4.4377],\n",
      "         [ 2.2862],\n",
      "         [-1.8735],\n",
      "         [-0.9339],\n",
      "         [-2.0422],\n",
      "         [-1.0282],\n",
      "         [-0.6058],\n",
      "         [-2.7894],\n",
      "         [ 0.6939],\n",
      "         [-3.1095],\n",
      "         [-1.1757],\n",
      "         [-1.1131],\n",
      "         [-0.1884],\n",
      "         [-1.6018],\n",
      "         [-2.5250],\n",
      "         [ 0.8017],\n",
      "         [-1.8881],\n",
      "         [-3.6449],\n",
      "         [-2.1842],\n",
      "         [ 1.3175],\n",
      "         [-3.8593],\n",
      "         [-3.2872],\n",
      "         [-1.6182],\n",
      "         [-0.4972],\n",
      "         [ 0.2158],\n",
      "         [-1.2405],\n",
      "         [-2.1591],\n",
      "         [-1.1425],\n",
      "         [ 2.1892],\n",
      "         [-3.4501],\n",
      "         [-2.9962],\n",
      "         [-2.0686],\n",
      "         [ 0.1901],\n",
      "         [-2.7648],\n",
      "         [-2.7237],\n",
      "         [-2.2822],\n",
      "         [-2.2837],\n",
      "         [-2.0686],\n",
      "         [-3.2271],\n",
      "         [-2.0919],\n",
      "         [-0.5751],\n",
      "         [-3.2411],\n",
      "         [ 0.5500],\n",
      "         [-1.2366],\n",
      "         [-0.2246],\n",
      "         [-0.9255],\n",
      "         [-0.8315],\n",
      "         [-1.3093],\n",
      "         [-0.9750],\n",
      "         [-0.8575],\n",
      "         [-1.1990],\n",
      "         [-0.4675],\n",
      "         [-1.0002],\n",
      "         [-1.5928],\n",
      "         [-1.1343],\n",
      "         [ 0.8861],\n",
      "         [-3.9965],\n",
      "         [-0.3487],\n",
      "         [-1.5944],\n",
      "         [-3.8313],\n",
      "         [-4.5885]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([2, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([2, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/12/2023 06:40:26 AM WARNING 36126 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:28 AM WARNING 36126 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:28 AM WARNING 36126 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:28 AM WARNING 36126 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:28 AM WARNING 36126 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:28 AM WARNING 36126 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:28 AM WARNING 36126 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:28 AM WARNING 36126 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:28 AM WARNING 36126 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-2.5242],\n",
      "         [-0.4542],\n",
      "         [-2.5238],\n",
      "         [ 0.1247],\n",
      "         [-1.4890],\n",
      "         [-1.0365],\n",
      "         [-2.7635],\n",
      "         [-3.4934],\n",
      "         [ 0.2207],\n",
      "         [-2.1106],\n",
      "         [-2.1096],\n",
      "         [-3.7962],\n",
      "         [-3.0040],\n",
      "         [-0.7806],\n",
      "         [-0.9262],\n",
      "         [-3.7332],\n",
      "         [-3.4024],\n",
      "         [-0.2246],\n",
      "         [-3.0406],\n",
      "         [-0.2422],\n",
      "         [ 2.2073],\n",
      "         [-0.5709],\n",
      "         [-3.2271],\n",
      "         [-0.0759],\n",
      "         [-0.0950],\n",
      "         [ 0.5289],\n",
      "         [-5.0949],\n",
      "         [ 1.1824],\n",
      "         [-2.8489],\n",
      "         [-0.1097],\n",
      "         [ 1.0631],\n",
      "         [-1.4877],\n",
      "         [-1.5194],\n",
      "         [-1.3754],\n",
      "         [-1.0986],\n",
      "         [-2.6928],\n",
      "         [ 0.8927],\n",
      "         [-2.2033],\n",
      "         [-0.4616],\n",
      "         [-1.3871],\n",
      "         [-2.2334],\n",
      "         [-3.9233],\n",
      "         [-1.7654],\n",
      "         [ 0.3776],\n",
      "         [-2.0860],\n",
      "         [-2.4765],\n",
      "         [-0.9894],\n",
      "         [-2.6483],\n",
      "         [-2.5493],\n",
      "         [-3.7642],\n",
      "         [-0.3521],\n",
      "         [-0.3755],\n",
      "         [-1.9414],\n",
      "         [-3.1589],\n",
      "         [-3.7945],\n",
      "         [-1.9903],\n",
      "         [ 1.8238],\n",
      "         [-0.5715],\n",
      "         [-2.1591],\n",
      "         [-1.7941],\n",
      "         [-2.0686],\n",
      "         [-0.5715],\n",
      "         [-4.6059],\n",
      "         [-3.0761],\n",
      "         [-0.4972],\n",
      "         [-3.5200],\n",
      "         [-4.1222],\n",
      "         [ 1.1168],\n",
      "         [-4.1247],\n",
      "         [-0.3431],\n",
      "         [-2.4081],\n",
      "         [ 1.7616],\n",
      "         [-3.1209],\n",
      "         [-1.9852],\n",
      "         [-0.9339],\n",
      "         [ 2.5423],\n",
      "         [-3.1821],\n",
      "         [-4.5842],\n",
      "         [-0.9155],\n",
      "         [-0.7002],\n",
      "         [-1.7533],\n",
      "         [-1.3937],\n",
      "         [-1.7131],\n",
      "         [-2.5785],\n",
      "         [-0.2246],\n",
      "         [-2.4068],\n",
      "         [-2.5169],\n",
      "         [-1.1446],\n",
      "         [-1.0111],\n",
      "         [ 1.0296],\n",
      "         [-2.7894],\n",
      "         [-2.1427],\n",
      "         [-2.0686],\n",
      "         [ 1.6003],\n",
      "         [-4.5188],\n",
      "         [ 0.7087],\n",
      "         [-2.7894],\n",
      "         [-0.5283],\n",
      "         [-0.8369],\n",
      "         [-0.8993]],\n",
      "\n",
      "        [[-2.5242],\n",
      "         [-0.4542],\n",
      "         [-2.5238],\n",
      "         [ 0.1247],\n",
      "         [-1.4890],\n",
      "         [-1.0365],\n",
      "         [-2.7635],\n",
      "         [-3.4934],\n",
      "         [ 0.2207],\n",
      "         [-2.1106],\n",
      "         [-2.1096],\n",
      "         [-3.7962],\n",
      "         [-3.0040],\n",
      "         [-0.7806],\n",
      "         [-0.9262],\n",
      "         [-3.7332],\n",
      "         [-3.4024],\n",
      "         [-0.2246],\n",
      "         [-3.0406],\n",
      "         [-0.2422],\n",
      "         [ 2.2073],\n",
      "         [-0.5709],\n",
      "         [-3.2271],\n",
      "         [-0.0759],\n",
      "         [-0.0950],\n",
      "         [ 0.5289],\n",
      "         [-5.0949],\n",
      "         [ 1.1824],\n",
      "         [-2.8489],\n",
      "         [-0.1097],\n",
      "         [ 1.0631],\n",
      "         [-1.4877],\n",
      "         [-1.5194],\n",
      "         [-1.3754],\n",
      "         [-1.0986],\n",
      "         [-2.6928],\n",
      "         [ 0.8927],\n",
      "         [-2.2033],\n",
      "         [-0.4616],\n",
      "         [-1.3871],\n",
      "         [-2.2334],\n",
      "         [-3.9233],\n",
      "         [-1.7654],\n",
      "         [ 0.3776],\n",
      "         [-2.0860],\n",
      "         [-2.4765],\n",
      "         [-0.9894],\n",
      "         [-2.6483],\n",
      "         [-2.5493],\n",
      "         [-3.7642],\n",
      "         [-0.3521],\n",
      "         [-0.3755],\n",
      "         [-1.9414],\n",
      "         [-3.1589],\n",
      "         [-3.7945],\n",
      "         [-1.9903],\n",
      "         [ 1.8238],\n",
      "         [-0.5715],\n",
      "         [-2.1591],\n",
      "         [-1.7941],\n",
      "         [-2.0686],\n",
      "         [-0.5715],\n",
      "         [-4.6059],\n",
      "         [-3.0761],\n",
      "         [-0.4972],\n",
      "         [-3.5200],\n",
      "         [-4.1222],\n",
      "         [ 1.1168],\n",
      "         [-4.1247],\n",
      "         [-0.3431],\n",
      "         [-2.4081],\n",
      "         [ 1.7616],\n",
      "         [-3.1209],\n",
      "         [-1.9852],\n",
      "         [-0.9339],\n",
      "         [ 2.5423],\n",
      "         [-3.1821],\n",
      "         [-4.5842],\n",
      "         [-0.9155],\n",
      "         [-0.7002],\n",
      "         [-1.7533],\n",
      "         [-1.3937],\n",
      "         [-1.7131],\n",
      "         [-2.5785],\n",
      "         [-0.2246],\n",
      "         [-2.4068],\n",
      "         [-2.5169],\n",
      "         [-1.1446],\n",
      "         [-1.0111],\n",
      "         [ 1.0296],\n",
      "         [-2.7894],\n",
      "         [-2.1427],\n",
      "         [-2.0686],\n",
      "         [ 1.6003],\n",
      "         [-4.5188],\n",
      "         [ 0.7087],\n",
      "         [-2.7894],\n",
      "         [-0.5283],\n",
      "         [-0.8369],\n",
      "         [-0.8993]],\n",
      "\n",
      "        [[-2.5242],\n",
      "         [-0.4542],\n",
      "         [-2.5238],\n",
      "         [ 0.1247],\n",
      "         [-1.4890],\n",
      "         [-1.0365],\n",
      "         [-2.7635],\n",
      "         [-3.4934],\n",
      "         [ 0.2207],\n",
      "         [-2.1106],\n",
      "         [-2.1096],\n",
      "         [-3.7962],\n",
      "         [-3.0040],\n",
      "         [-0.7806],\n",
      "         [-0.9262],\n",
      "         [-3.7332],\n",
      "         [-3.4024],\n",
      "         [-0.2246],\n",
      "         [-3.0406],\n",
      "         [-0.2422],\n",
      "         [ 2.2073],\n",
      "         [-0.5709],\n",
      "         [-3.2271],\n",
      "         [-0.0759],\n",
      "         [-0.0950],\n",
      "         [ 0.5289],\n",
      "         [-5.0949],\n",
      "         [ 1.1824],\n",
      "         [-2.8489],\n",
      "         [-0.1097],\n",
      "         [ 1.0631],\n",
      "         [-1.4877],\n",
      "         [-1.5194],\n",
      "         [-1.3754],\n",
      "         [-1.0986],\n",
      "         [-2.6928],\n",
      "         [ 0.8927],\n",
      "         [-2.2033],\n",
      "         [-0.4616],\n",
      "         [-1.3871],\n",
      "         [-2.2334],\n",
      "         [-3.9233],\n",
      "         [-1.7654],\n",
      "         [ 0.3776],\n",
      "         [-2.0860],\n",
      "         [-2.4765],\n",
      "         [-0.9894],\n",
      "         [-2.6483],\n",
      "         [-2.5493],\n",
      "         [-3.7642],\n",
      "         [-0.3521],\n",
      "         [-0.3755],\n",
      "         [-1.9414],\n",
      "         [-3.1589],\n",
      "         [-3.7945],\n",
      "         [-1.9903],\n",
      "         [ 1.8238],\n",
      "         [-0.5715],\n",
      "         [-2.1591],\n",
      "         [-1.7941],\n",
      "         [-2.0686],\n",
      "         [-0.5715],\n",
      "         [-4.6059],\n",
      "         [-3.0761],\n",
      "         [-0.4972],\n",
      "         [-3.5200],\n",
      "         [-4.1222],\n",
      "         [ 1.1168],\n",
      "         [-4.1247],\n",
      "         [-0.3431],\n",
      "         [-2.4081],\n",
      "         [ 1.7616],\n",
      "         [-3.1209],\n",
      "         [-1.9852],\n",
      "         [-0.9339],\n",
      "         [ 2.5423],\n",
      "         [-3.1821],\n",
      "         [-4.5842],\n",
      "         [-0.9155],\n",
      "         [-0.7002],\n",
      "         [-1.7533],\n",
      "         [-1.3937],\n",
      "         [-1.7131],\n",
      "         [-2.5785],\n",
      "         [-0.2246],\n",
      "         [-2.4068],\n",
      "         [-2.5169],\n",
      "         [-1.1446],\n",
      "         [-1.0111],\n",
      "         [ 1.0296],\n",
      "         [-2.7894],\n",
      "         [-2.1427],\n",
      "         [-2.0686],\n",
      "         [ 1.6003],\n",
      "         [-4.5188],\n",
      "         [ 0.7087],\n",
      "         [-2.7894],\n",
      "         [-0.5283],\n",
      "         [-0.8369],\n",
      "         [-0.8993]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([3, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([3, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/12/2023 06:40:29 AM WARNING 36172 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:32 AM WARNING 36172 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:32 AM WARNING 36172 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:32 AM WARNING 36172 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:32 AM WARNING 36172 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:32 AM WARNING 36172 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:32 AM WARNING 36172 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:32 AM WARNING 36172 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:32 AM WARNING 36172 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-2.4651],\n",
      "         [-3.9864],\n",
      "         [-2.7717],\n",
      "         [-1.0700],\n",
      "         [-1.8002],\n",
      "         [-1.3093],\n",
      "         [ 0.4360],\n",
      "         [-0.3335],\n",
      "         [-2.9313],\n",
      "         [-3.0761],\n",
      "         [-2.7164],\n",
      "         [-1.5686],\n",
      "         [-3.5692],\n",
      "         [-0.3487],\n",
      "         [-3.6566],\n",
      "         [-1.5556],\n",
      "         [-2.5493],\n",
      "         [-1.8651],\n",
      "         [-0.3431],\n",
      "         [-2.4802],\n",
      "         [-0.3307],\n",
      "         [-3.5692],\n",
      "         [-2.7270],\n",
      "         [-0.6204],\n",
      "         [ 1.9901],\n",
      "         [-1.7840],\n",
      "         [-2.7092],\n",
      "         [-0.0950],\n",
      "         [-0.3431],\n",
      "         [-5.0949],\n",
      "         [ 0.9245],\n",
      "         [-3.5272],\n",
      "         [-2.1949],\n",
      "         [-2.6515],\n",
      "         [ 0.2207],\n",
      "         [-0.9661],\n",
      "         [-2.5144],\n",
      "         [-2.6322],\n",
      "         [-2.5310],\n",
      "         [-3.8482],\n",
      "         [ 0.9245],\n",
      "         [-1.1125],\n",
      "         [-1.4872],\n",
      "         [-3.8313],\n",
      "         [-5.0949],\n",
      "         [ 1.8238],\n",
      "         [-1.9528],\n",
      "         [-1.2386],\n",
      "         [-2.0258],\n",
      "         [-0.2868],\n",
      "         [ 0.8017],\n",
      "         [-3.4501],\n",
      "         [-1.1587],\n",
      "         [ 3.2312],\n",
      "         [ 1.9215],\n",
      "         [-2.2493],\n",
      "         [-5.4541],\n",
      "         [-2.0689],\n",
      "         [-2.1618],\n",
      "         [ 1.0591],\n",
      "         [ 0.4353],\n",
      "         [-3.8873],\n",
      "         [-1.1891],\n",
      "         [-0.0657],\n",
      "         [ 1.0591],\n",
      "         [-0.9339],\n",
      "         [-1.9685],\n",
      "         [-1.1956],\n",
      "         [-1.4985],\n",
      "         [ 0.8242],\n",
      "         [ 0.6848],\n",
      "         [-2.0950],\n",
      "         [-4.6059],\n",
      "         [-1.4657],\n",
      "         [-1.3663],\n",
      "         [-1.7728],\n",
      "         [-3.1032],\n",
      "         [-0.4373],\n",
      "         [-3.5130],\n",
      "         [-1.1848],\n",
      "         [-0.2422],\n",
      "         [-2.7665],\n",
      "         [ 0.0058],\n",
      "         [-4.2089],\n",
      "         [-3.3304],\n",
      "         [ 1.4627],\n",
      "         [-1.6824],\n",
      "         [-0.5546],\n",
      "         [-1.9710],\n",
      "         [-1.0943],\n",
      "         [-2.8369],\n",
      "         [-0.1721],\n",
      "         [-4.4662],\n",
      "         [-1.1398],\n",
      "         [ 0.8876],\n",
      "         [-3.8251],\n",
      "         [-1.0159],\n",
      "         [-0.0230],\n",
      "         [-2.2033],\n",
      "         [ 1.4246]],\n",
      "\n",
      "        [[-2.4651],\n",
      "         [-3.9864],\n",
      "         [-2.7717],\n",
      "         [-1.0700],\n",
      "         [-1.8002],\n",
      "         [-1.3093],\n",
      "         [ 0.4360],\n",
      "         [-0.3335],\n",
      "         [-2.9313],\n",
      "         [-3.0761],\n",
      "         [-2.7164],\n",
      "         [-1.5686],\n",
      "         [-3.5692],\n",
      "         [-0.3487],\n",
      "         [-3.6566],\n",
      "         [-1.5556],\n",
      "         [-2.5493],\n",
      "         [-1.8651],\n",
      "         [-0.3431],\n",
      "         [-2.4802],\n",
      "         [-0.3307],\n",
      "         [-3.5692],\n",
      "         [-2.7270],\n",
      "         [-0.6204],\n",
      "         [ 1.9901],\n",
      "         [-1.7840],\n",
      "         [-2.7092],\n",
      "         [-0.0950],\n",
      "         [-0.3431],\n",
      "         [-5.0949],\n",
      "         [ 0.9245],\n",
      "         [-3.5272],\n",
      "         [-2.1949],\n",
      "         [-2.6515],\n",
      "         [ 0.2207],\n",
      "         [-0.9661],\n",
      "         [-2.5144],\n",
      "         [-2.6322],\n",
      "         [-2.5310],\n",
      "         [-3.8482],\n",
      "         [ 0.9245],\n",
      "         [-1.1125],\n",
      "         [-1.4872],\n",
      "         [-3.8313],\n",
      "         [-5.0949],\n",
      "         [ 1.8238],\n",
      "         [-1.9528],\n",
      "         [-1.2386],\n",
      "         [-2.0258],\n",
      "         [-0.2868],\n",
      "         [ 0.8017],\n",
      "         [-3.4501],\n",
      "         [-1.1587],\n",
      "         [ 3.2312],\n",
      "         [ 1.9215],\n",
      "         [-2.2493],\n",
      "         [-5.4541],\n",
      "         [-2.0689],\n",
      "         [-2.1618],\n",
      "         [ 1.0591],\n",
      "         [ 0.4353],\n",
      "         [-3.8873],\n",
      "         [-1.1891],\n",
      "         [-0.0657],\n",
      "         [ 1.0591],\n",
      "         [-0.9339],\n",
      "         [-1.9685],\n",
      "         [-1.1956],\n",
      "         [-1.4985],\n",
      "         [ 0.8242],\n",
      "         [ 0.6848],\n",
      "         [-2.0950],\n",
      "         [-4.6059],\n",
      "         [-1.4657],\n",
      "         [-1.3663],\n",
      "         [-1.7728],\n",
      "         [-3.1032],\n",
      "         [-0.4373],\n",
      "         [-3.5130],\n",
      "         [-1.1848],\n",
      "         [-0.2422],\n",
      "         [-2.7665],\n",
      "         [ 0.0058],\n",
      "         [-4.2089],\n",
      "         [-3.3304],\n",
      "         [ 1.4627],\n",
      "         [-1.6824],\n",
      "         [-0.5546],\n",
      "         [-1.9710],\n",
      "         [-1.0943],\n",
      "         [-2.8369],\n",
      "         [-0.1721],\n",
      "         [-4.4662],\n",
      "         [-1.1398],\n",
      "         [ 0.8876],\n",
      "         [-3.8251],\n",
      "         [-1.0159],\n",
      "         [-0.0230],\n",
      "         [-2.2033],\n",
      "         [ 1.4246]],\n",
      "\n",
      "        [[-2.4651],\n",
      "         [-3.9864],\n",
      "         [-2.7717],\n",
      "         [-1.0700],\n",
      "         [-1.8002],\n",
      "         [-1.3093],\n",
      "         [ 0.4360],\n",
      "         [-0.3335],\n",
      "         [-2.9313],\n",
      "         [-3.0761],\n",
      "         [-2.7164],\n",
      "         [-1.5686],\n",
      "         [-3.5692],\n",
      "         [-0.3487],\n",
      "         [-3.6566],\n",
      "         [-1.5556],\n",
      "         [-2.5493],\n",
      "         [-1.8651],\n",
      "         [-0.3431],\n",
      "         [-2.4802],\n",
      "         [-0.3307],\n",
      "         [-3.5692],\n",
      "         [-2.7270],\n",
      "         [-0.6204],\n",
      "         [ 1.9901],\n",
      "         [-1.7840],\n",
      "         [-2.7092],\n",
      "         [-0.0950],\n",
      "         [-0.3431],\n",
      "         [-5.0949],\n",
      "         [ 0.9245],\n",
      "         [-3.5272],\n",
      "         [-2.1949],\n",
      "         [-2.6515],\n",
      "         [ 0.2207],\n",
      "         [-0.9661],\n",
      "         [-2.5144],\n",
      "         [-2.6322],\n",
      "         [-2.5310],\n",
      "         [-3.8482],\n",
      "         [ 0.9245],\n",
      "         [-1.1125],\n",
      "         [-1.4872],\n",
      "         [-3.8313],\n",
      "         [-5.0949],\n",
      "         [ 1.8238],\n",
      "         [-1.9528],\n",
      "         [-1.2386],\n",
      "         [-2.0258],\n",
      "         [-0.2868],\n",
      "         [ 0.8017],\n",
      "         [-3.4501],\n",
      "         [-1.1587],\n",
      "         [ 3.2312],\n",
      "         [ 1.9215],\n",
      "         [-2.2493],\n",
      "         [-5.4541],\n",
      "         [-2.0689],\n",
      "         [-2.1618],\n",
      "         [ 1.0591],\n",
      "         [ 0.4353],\n",
      "         [-3.8873],\n",
      "         [-1.1891],\n",
      "         [-0.0657],\n",
      "         [ 1.0591],\n",
      "         [-0.9339],\n",
      "         [-1.9685],\n",
      "         [-1.1956],\n",
      "         [-1.4985],\n",
      "         [ 0.8242],\n",
      "         [ 0.6848],\n",
      "         [-2.0950],\n",
      "         [-4.6059],\n",
      "         [-1.4657],\n",
      "         [-1.3663],\n",
      "         [-1.7728],\n",
      "         [-3.1032],\n",
      "         [-0.4373],\n",
      "         [-3.5130],\n",
      "         [-1.1848],\n",
      "         [-0.2422],\n",
      "         [-2.7665],\n",
      "         [ 0.0058],\n",
      "         [-4.2089],\n",
      "         [-3.3304],\n",
      "         [ 1.4627],\n",
      "         [-1.6824],\n",
      "         [-0.5546],\n",
      "         [-1.9710],\n",
      "         [-1.0943],\n",
      "         [-2.8369],\n",
      "         [-0.1721],\n",
      "         [-4.4662],\n",
      "         [-1.1398],\n",
      "         [ 0.8876],\n",
      "         [-3.8251],\n",
      "         [-1.0159],\n",
      "         [-0.0230],\n",
      "         [-2.2033],\n",
      "         [ 1.4246]],\n",
      "\n",
      "        [[-2.4651],\n",
      "         [-3.9864],\n",
      "         [-2.7717],\n",
      "         [-1.0700],\n",
      "         [-1.8002],\n",
      "         [-1.3093],\n",
      "         [ 0.4360],\n",
      "         [-0.3335],\n",
      "         [-2.9313],\n",
      "         [-3.0761],\n",
      "         [-2.7164],\n",
      "         [-1.5686],\n",
      "         [-3.5692],\n",
      "         [-0.3487],\n",
      "         [-3.6566],\n",
      "         [-1.5556],\n",
      "         [-2.5493],\n",
      "         [-1.8651],\n",
      "         [-0.3431],\n",
      "         [-2.4802],\n",
      "         [-0.3307],\n",
      "         [-3.5692],\n",
      "         [-2.7270],\n",
      "         [-0.6204],\n",
      "         [ 1.9901],\n",
      "         [-1.7840],\n",
      "         [-2.7092],\n",
      "         [-0.0950],\n",
      "         [-0.3431],\n",
      "         [-5.0949],\n",
      "         [ 0.9245],\n",
      "         [-3.5272],\n",
      "         [-2.1949],\n",
      "         [-2.6515],\n",
      "         [ 0.2207],\n",
      "         [-0.9661],\n",
      "         [-2.5144],\n",
      "         [-2.6322],\n",
      "         [-2.5310],\n",
      "         [-3.8482],\n",
      "         [ 0.9245],\n",
      "         [-1.1125],\n",
      "         [-1.4872],\n",
      "         [-3.8313],\n",
      "         [-5.0949],\n",
      "         [ 1.8238],\n",
      "         [-1.9528],\n",
      "         [-1.2386],\n",
      "         [-2.0258],\n",
      "         [-0.2868],\n",
      "         [ 0.8017],\n",
      "         [-3.4501],\n",
      "         [-1.1587],\n",
      "         [ 3.2312],\n",
      "         [ 1.9215],\n",
      "         [-2.2493],\n",
      "         [-5.4541],\n",
      "         [-2.0689],\n",
      "         [-2.1618],\n",
      "         [ 1.0591],\n",
      "         [ 0.4353],\n",
      "         [-3.8873],\n",
      "         [-1.1891],\n",
      "         [-0.0657],\n",
      "         [ 1.0591],\n",
      "         [-0.9339],\n",
      "         [-1.9685],\n",
      "         [-1.1956],\n",
      "         [-1.4985],\n",
      "         [ 0.8242],\n",
      "         [ 0.6848],\n",
      "         [-2.0950],\n",
      "         [-4.6059],\n",
      "         [-1.4657],\n",
      "         [-1.3663],\n",
      "         [-1.7728],\n",
      "         [-3.1032],\n",
      "         [-0.4373],\n",
      "         [-3.5130],\n",
      "         [-1.1848],\n",
      "         [-0.2422],\n",
      "         [-2.7665],\n",
      "         [ 0.0058],\n",
      "         [-4.2089],\n",
      "         [-3.3304],\n",
      "         [ 1.4627],\n",
      "         [-1.6824],\n",
      "         [-0.5546],\n",
      "         [-1.9710],\n",
      "         [-1.0943],\n",
      "         [-2.8369],\n",
      "         [-0.1721],\n",
      "         [-4.4662],\n",
      "         [-1.1398],\n",
      "         [ 0.8876],\n",
      "         [-3.8251],\n",
      "         [-1.0159],\n",
      "         [-0.0230],\n",
      "         [-2.2033],\n",
      "         [ 1.4246]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([4, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([4, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/12/2023 06:40:33 AM WARNING 36214 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:35 AM WARNING 36214 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:35 AM WARNING 36214 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:35 AM WARNING 36214 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:35 AM WARNING 36214 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:35 AM WARNING 36214 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:35 AM WARNING 36214 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:35 AM WARNING 36214 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:35 AM WARNING 36214 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-2.9262],\n",
      "         [-2.5564],\n",
      "         [-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.9255],\n",
      "         [-2.9161],\n",
      "         [-0.6204],\n",
      "         [ 3.2808],\n",
      "         [-3.4663],\n",
      "         [-1.2973],\n",
      "         [-3.3636],\n",
      "         [-1.9852],\n",
      "         [-4.3534],\n",
      "         [-1.4890],\n",
      "         [-1.8576],\n",
      "         [-1.0943],\n",
      "         [-1.2366],\n",
      "         [ 1.9565],\n",
      "         [ 0.3384],\n",
      "         [-2.4267],\n",
      "         [-1.6304],\n",
      "         [-1.4078],\n",
      "         [-1.1806],\n",
      "         [-3.4934],\n",
      "         [-4.9023],\n",
      "         [-2.4418],\n",
      "         [-1.7464],\n",
      "         [-2.0274],\n",
      "         [ 1.6681],\n",
      "         [-1.2119],\n",
      "         [-0.7380],\n",
      "         [ 0.1770],\n",
      "         [-1.0162],\n",
      "         [-0.6204],\n",
      "         [-0.1439],\n",
      "         [ 0.0108],\n",
      "         [-1.9465],\n",
      "         [-3.4284],\n",
      "         [-2.4765],\n",
      "         [-1.4032],\n",
      "         [-3.1637],\n",
      "         [-4.0834],\n",
      "         [-1.0849],\n",
      "         [-0.6370],\n",
      "         [-2.2999],\n",
      "         [-1.0925],\n",
      "         [-0.9116],\n",
      "         [ 0.6848],\n",
      "         [-1.5105],\n",
      "         [-2.0838],\n",
      "         [-4.3857],\n",
      "         [-2.8314],\n",
      "         [ 0.1505],\n",
      "         [ 0.6848],\n",
      "         [-2.3553],\n",
      "         [-1.5513],\n",
      "         [-1.1446],\n",
      "         [-1.6018],\n",
      "         [-2.6363],\n",
      "         [-2.6756],\n",
      "         [-1.2973],\n",
      "         [-0.5071],\n",
      "         [-0.7489],\n",
      "         [-2.1843],\n",
      "         [ 0.4706],\n",
      "         [-2.7648],\n",
      "         [-1.0715],\n",
      "         [ 2.0705],\n",
      "         [-3.5890],\n",
      "         [ 0.8560],\n",
      "         [-1.8651],\n",
      "         [-2.5564],\n",
      "         [-1.6136],\n",
      "         [ 0.1505],\n",
      "         [-2.7731],\n",
      "         [-1.8719],\n",
      "         [-3.2350],\n",
      "         [-1.1739],\n",
      "         [-2.8287],\n",
      "         [-1.7941],\n",
      "         [-0.1535],\n",
      "         [ 0.8242],\n",
      "         [-0.4139],\n",
      "         [-0.9975],\n",
      "         [-1.3185],\n",
      "         [-3.4050],\n",
      "         [ 1.0379],\n",
      "         [-3.9864],\n",
      "         [-0.5574],\n",
      "         [-0.3410],\n",
      "         [-3.0364],\n",
      "         [-2.6161],\n",
      "         [ 1.4627],\n",
      "         [-1.6964],\n",
      "         [-3.5637],\n",
      "         [-3.6553],\n",
      "         [-4.4159],\n",
      "         [-0.4649],\n",
      "         [-2.1523],\n",
      "         [-1.8441]],\n",
      "\n",
      "        [[-2.9262],\n",
      "         [-2.5564],\n",
      "         [-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.9255],\n",
      "         [-2.9161],\n",
      "         [-0.6204],\n",
      "         [ 3.2808],\n",
      "         [-3.4663],\n",
      "         [-1.2973],\n",
      "         [-3.3636],\n",
      "         [-1.9852],\n",
      "         [-4.3534],\n",
      "         [-1.4890],\n",
      "         [-1.8576],\n",
      "         [-1.0943],\n",
      "         [-1.2366],\n",
      "         [ 1.9565],\n",
      "         [ 0.3384],\n",
      "         [-2.4267],\n",
      "         [-1.6304],\n",
      "         [-1.4078],\n",
      "         [-1.1806],\n",
      "         [-3.4934],\n",
      "         [-4.9023],\n",
      "         [-2.4418],\n",
      "         [-1.7464],\n",
      "         [-2.0274],\n",
      "         [ 1.6681],\n",
      "         [-1.2119],\n",
      "         [-0.7380],\n",
      "         [ 0.1770],\n",
      "         [-1.0162],\n",
      "         [-0.6204],\n",
      "         [-0.1439],\n",
      "         [ 0.0108],\n",
      "         [-1.9465],\n",
      "         [-3.4284],\n",
      "         [-2.4765],\n",
      "         [-1.4032],\n",
      "         [-3.1637],\n",
      "         [-4.0834],\n",
      "         [-1.0849],\n",
      "         [-0.6370],\n",
      "         [-2.2999],\n",
      "         [-1.0925],\n",
      "         [-0.9116],\n",
      "         [ 0.6848],\n",
      "         [-1.5105],\n",
      "         [-2.0838],\n",
      "         [-4.3857],\n",
      "         [-2.8314],\n",
      "         [ 0.1505],\n",
      "         [ 0.6848],\n",
      "         [-2.3553],\n",
      "         [-1.5513],\n",
      "         [-1.1446],\n",
      "         [-1.6018],\n",
      "         [-2.6363],\n",
      "         [-2.6756],\n",
      "         [-1.2973],\n",
      "         [-0.5071],\n",
      "         [-0.7489],\n",
      "         [-2.1843],\n",
      "         [ 0.4706],\n",
      "         [-2.7648],\n",
      "         [-1.0715],\n",
      "         [ 2.0705],\n",
      "         [-3.5890],\n",
      "         [ 0.8560],\n",
      "         [-1.8651],\n",
      "         [-2.5564],\n",
      "         [-1.6136],\n",
      "         [ 0.1505],\n",
      "         [-2.7731],\n",
      "         [-1.8719],\n",
      "         [-3.2350],\n",
      "         [-1.1739],\n",
      "         [-2.8287],\n",
      "         [-1.7941],\n",
      "         [-0.1535],\n",
      "         [ 0.8242],\n",
      "         [-0.4139],\n",
      "         [-0.9975],\n",
      "         [-1.3185],\n",
      "         [-3.4050],\n",
      "         [ 1.0379],\n",
      "         [-3.9864],\n",
      "         [-0.5574],\n",
      "         [-0.3410],\n",
      "         [-3.0364],\n",
      "         [-2.6161],\n",
      "         [ 1.4627],\n",
      "         [-1.6964],\n",
      "         [-3.5637],\n",
      "         [-3.6553],\n",
      "         [-4.4159],\n",
      "         [-0.4649],\n",
      "         [-2.1523],\n",
      "         [-1.8441]],\n",
      "\n",
      "        [[-2.9262],\n",
      "         [-2.5564],\n",
      "         [-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.9255],\n",
      "         [-2.9161],\n",
      "         [-0.6204],\n",
      "         [ 3.2808],\n",
      "         [-3.4663],\n",
      "         [-1.2973],\n",
      "         [-3.3636],\n",
      "         [-1.9852],\n",
      "         [-4.3534],\n",
      "         [-1.4890],\n",
      "         [-1.8576],\n",
      "         [-1.0943],\n",
      "         [-1.2366],\n",
      "         [ 1.9565],\n",
      "         [ 0.3384],\n",
      "         [-2.4267],\n",
      "         [-1.6304],\n",
      "         [-1.4078],\n",
      "         [-1.1806],\n",
      "         [-3.4934],\n",
      "         [-4.9023],\n",
      "         [-2.4418],\n",
      "         [-1.7464],\n",
      "         [-2.0274],\n",
      "         [ 1.6681],\n",
      "         [-1.2119],\n",
      "         [-0.7380],\n",
      "         [ 0.1770],\n",
      "         [-1.0162],\n",
      "         [-0.6204],\n",
      "         [-0.1439],\n",
      "         [ 0.0108],\n",
      "         [-1.9465],\n",
      "         [-3.4284],\n",
      "         [-2.4765],\n",
      "         [-1.4032],\n",
      "         [-3.1637],\n",
      "         [-4.0834],\n",
      "         [-1.0849],\n",
      "         [-0.6370],\n",
      "         [-2.2999],\n",
      "         [-1.0925],\n",
      "         [-0.9116],\n",
      "         [ 0.6848],\n",
      "         [-1.5105],\n",
      "         [-2.0838],\n",
      "         [-4.3857],\n",
      "         [-2.8314],\n",
      "         [ 0.1505],\n",
      "         [ 0.6848],\n",
      "         [-2.3553],\n",
      "         [-1.5513],\n",
      "         [-1.1446],\n",
      "         [-1.6018],\n",
      "         [-2.6363],\n",
      "         [-2.6756],\n",
      "         [-1.2973],\n",
      "         [-0.5071],\n",
      "         [-0.7489],\n",
      "         [-2.1843],\n",
      "         [ 0.4706],\n",
      "         [-2.7648],\n",
      "         [-1.0715],\n",
      "         [ 2.0705],\n",
      "         [-3.5890],\n",
      "         [ 0.8560],\n",
      "         [-1.8651],\n",
      "         [-2.5564],\n",
      "         [-1.6136],\n",
      "         [ 0.1505],\n",
      "         [-2.7731],\n",
      "         [-1.8719],\n",
      "         [-3.2350],\n",
      "         [-1.1739],\n",
      "         [-2.8287],\n",
      "         [-1.7941],\n",
      "         [-0.1535],\n",
      "         [ 0.8242],\n",
      "         [-0.4139],\n",
      "         [-0.9975],\n",
      "         [-1.3185],\n",
      "         [-3.4050],\n",
      "         [ 1.0379],\n",
      "         [-3.9864],\n",
      "         [-0.5574],\n",
      "         [-0.3410],\n",
      "         [-3.0364],\n",
      "         [-2.6161],\n",
      "         [ 1.4627],\n",
      "         [-1.6964],\n",
      "         [-3.5637],\n",
      "         [-3.6553],\n",
      "         [-4.4159],\n",
      "         [-0.4649],\n",
      "         [-2.1523],\n",
      "         [-1.8441]],\n",
      "\n",
      "        [[-2.9262],\n",
      "         [-2.5564],\n",
      "         [-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.9255],\n",
      "         [-2.9161],\n",
      "         [-0.6204],\n",
      "         [ 3.2808],\n",
      "         [-3.4663],\n",
      "         [-1.2973],\n",
      "         [-3.3636],\n",
      "         [-1.9852],\n",
      "         [-4.3534],\n",
      "         [-1.4890],\n",
      "         [-1.8576],\n",
      "         [-1.0943],\n",
      "         [-1.2366],\n",
      "         [ 1.9565],\n",
      "         [ 0.3384],\n",
      "         [-2.4267],\n",
      "         [-1.6304],\n",
      "         [-1.4078],\n",
      "         [-1.1806],\n",
      "         [-3.4934],\n",
      "         [-4.9023],\n",
      "         [-2.4418],\n",
      "         [-1.7464],\n",
      "         [-2.0274],\n",
      "         [ 1.6681],\n",
      "         [-1.2119],\n",
      "         [-0.7380],\n",
      "         [ 0.1770],\n",
      "         [-1.0162],\n",
      "         [-0.6204],\n",
      "         [-0.1439],\n",
      "         [ 0.0108],\n",
      "         [-1.9465],\n",
      "         [-3.4284],\n",
      "         [-2.4765],\n",
      "         [-1.4032],\n",
      "         [-3.1637],\n",
      "         [-4.0834],\n",
      "         [-1.0849],\n",
      "         [-0.6370],\n",
      "         [-2.2999],\n",
      "         [-1.0925],\n",
      "         [-0.9116],\n",
      "         [ 0.6848],\n",
      "         [-1.5105],\n",
      "         [-2.0838],\n",
      "         [-4.3857],\n",
      "         [-2.8314],\n",
      "         [ 0.1505],\n",
      "         [ 0.6848],\n",
      "         [-2.3553],\n",
      "         [-1.5513],\n",
      "         [-1.1446],\n",
      "         [-1.6018],\n",
      "         [-2.6363],\n",
      "         [-2.6756],\n",
      "         [-1.2973],\n",
      "         [-0.5071],\n",
      "         [-0.7489],\n",
      "         [-2.1843],\n",
      "         [ 0.4706],\n",
      "         [-2.7648],\n",
      "         [-1.0715],\n",
      "         [ 2.0705],\n",
      "         [-3.5890],\n",
      "         [ 0.8560],\n",
      "         [-1.8651],\n",
      "         [-2.5564],\n",
      "         [-1.6136],\n",
      "         [ 0.1505],\n",
      "         [-2.7731],\n",
      "         [-1.8719],\n",
      "         [-3.2350],\n",
      "         [-1.1739],\n",
      "         [-2.8287],\n",
      "         [-1.7941],\n",
      "         [-0.1535],\n",
      "         [ 0.8242],\n",
      "         [-0.4139],\n",
      "         [-0.9975],\n",
      "         [-1.3185],\n",
      "         [-3.4050],\n",
      "         [ 1.0379],\n",
      "         [-3.9864],\n",
      "         [-0.5574],\n",
      "         [-0.3410],\n",
      "         [-3.0364],\n",
      "         [-2.6161],\n",
      "         [ 1.4627],\n",
      "         [-1.6964],\n",
      "         [-3.5637],\n",
      "         [-3.6553],\n",
      "         [-4.4159],\n",
      "         [-0.4649],\n",
      "         [-2.1523],\n",
      "         [-1.8441]],\n",
      "\n",
      "        [[-2.9262],\n",
      "         [-2.5564],\n",
      "         [-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.9255],\n",
      "         [-2.9161],\n",
      "         [-0.6204],\n",
      "         [ 3.2808],\n",
      "         [-3.4663],\n",
      "         [-1.2973],\n",
      "         [-3.3636],\n",
      "         [-1.9852],\n",
      "         [-4.3534],\n",
      "         [-1.4890],\n",
      "         [-1.8576],\n",
      "         [-1.0943],\n",
      "         [-1.2366],\n",
      "         [ 1.9565],\n",
      "         [ 0.3384],\n",
      "         [-2.4267],\n",
      "         [-1.6304],\n",
      "         [-1.4078],\n",
      "         [-1.1806],\n",
      "         [-3.4934],\n",
      "         [-4.9023],\n",
      "         [-2.4418],\n",
      "         [-1.7464],\n",
      "         [-2.0274],\n",
      "         [ 1.6681],\n",
      "         [-1.2119],\n",
      "         [-0.7380],\n",
      "         [ 0.1770],\n",
      "         [-1.0162],\n",
      "         [-0.6204],\n",
      "         [-0.1439],\n",
      "         [ 0.0108],\n",
      "         [-1.9465],\n",
      "         [-3.4284],\n",
      "         [-2.4765],\n",
      "         [-1.4032],\n",
      "         [-3.1637],\n",
      "         [-4.0834],\n",
      "         [-1.0849],\n",
      "         [-0.6370],\n",
      "         [-2.2999],\n",
      "         [-1.0925],\n",
      "         [-0.9116],\n",
      "         [ 0.6848],\n",
      "         [-1.5105],\n",
      "         [-2.0838],\n",
      "         [-4.3857],\n",
      "         [-2.8314],\n",
      "         [ 0.1505],\n",
      "         [ 0.6848],\n",
      "         [-2.3553],\n",
      "         [-1.5513],\n",
      "         [-1.1446],\n",
      "         [-1.6018],\n",
      "         [-2.6363],\n",
      "         [-2.6756],\n",
      "         [-1.2973],\n",
      "         [-0.5071],\n",
      "         [-0.7489],\n",
      "         [-2.1843],\n",
      "         [ 0.4706],\n",
      "         [-2.7648],\n",
      "         [-1.0715],\n",
      "         [ 2.0705],\n",
      "         [-3.5890],\n",
      "         [ 0.8560],\n",
      "         [-1.8651],\n",
      "         [-2.5564],\n",
      "         [-1.6136],\n",
      "         [ 0.1505],\n",
      "         [-2.7731],\n",
      "         [-1.8719],\n",
      "         [-3.2350],\n",
      "         [-1.1739],\n",
      "         [-2.8287],\n",
      "         [-1.7941],\n",
      "         [-0.1535],\n",
      "         [ 0.8242],\n",
      "         [-0.4139],\n",
      "         [-0.9975],\n",
      "         [-1.3185],\n",
      "         [-3.4050],\n",
      "         [ 1.0379],\n",
      "         [-3.9864],\n",
      "         [-0.5574],\n",
      "         [-0.3410],\n",
      "         [-3.0364],\n",
      "         [-2.6161],\n",
      "         [ 1.4627],\n",
      "         [-1.6964],\n",
      "         [-3.5637],\n",
      "         [-3.6553],\n",
      "         [-4.4159],\n",
      "         [-0.4649],\n",
      "         [-2.1523],\n",
      "         [-1.8441]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([5, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([5, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/12/2023 06:40:36 AM WARNING 36262 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:38 AM WARNING 36262 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:38 AM WARNING 36262 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:38 AM WARNING 36262 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:38 AM WARNING 36262 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:38 AM WARNING 36262 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:38 AM WARNING 36262 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:38 AM WARNING 36262 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:38 AM WARNING 36262 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-2.2837],\n",
      "         [-3.8251],\n",
      "         [-0.1429],\n",
      "         [-0.7893],\n",
      "         [-2.8314],\n",
      "         [-3.1015],\n",
      "         [-1.3289],\n",
      "         [-3.4464],\n",
      "         [-0.6162],\n",
      "         [ 1.1168],\n",
      "         [ 1.0710],\n",
      "         [-1.9685],\n",
      "         [-1.3301],\n",
      "         [-3.0180],\n",
      "         [ 0.0058],\n",
      "         [-4.5206],\n",
      "         [-1.2386],\n",
      "         [-1.5513],\n",
      "         [-2.8821],\n",
      "         [-1.5194],\n",
      "         [-3.1637],\n",
      "         [-0.3611],\n",
      "         [-0.0109],\n",
      "         [-1.4093],\n",
      "         [-2.3167],\n",
      "         [-1.6684],\n",
      "         [-3.6449],\n",
      "         [ 0.1876],\n",
      "         [ 0.5289],\n",
      "         [-2.7455],\n",
      "         [ 2.6148],\n",
      "         [-0.0950],\n",
      "         [-1.1848],\n",
      "         [-1.0449],\n",
      "         [-1.1328],\n",
      "         [-2.0258],\n",
      "         [-2.2999],\n",
      "         [-1.4074],\n",
      "         [-2.5814],\n",
      "         [-4.3822],\n",
      "         [-4.6175],\n",
      "         [-1.6870],\n",
      "         [-1.8048],\n",
      "         [-0.4814],\n",
      "         [-0.2935],\n",
      "         [-0.5546],\n",
      "         [-2.7455],\n",
      "         [-1.7464],\n",
      "         [-1.6136],\n",
      "         [-3.0666],\n",
      "         [-1.8576],\n",
      "         [-4.1222],\n",
      "         [-3.5130],\n",
      "         [-1.2659],\n",
      "         [ 0.9076],\n",
      "         [-1.5650],\n",
      "         [-0.2536],\n",
      "         [-1.5823],\n",
      "         [-2.0921],\n",
      "         [-1.9465],\n",
      "         [-1.4890],\n",
      "         [-3.2315],\n",
      "         [-1.2067],\n",
      "         [-3.5567],\n",
      "         [ 0.2126],\n",
      "         [-3.5272],\n",
      "         [-3.3200],\n",
      "         [-0.1439],\n",
      "         [-2.4068],\n",
      "         [-2.1842],\n",
      "         [-1.8622],\n",
      "         [-0.1259],\n",
      "         [-1.5729],\n",
      "         [-0.9661],\n",
      "         [-2.6161],\n",
      "         [-4.4754],\n",
      "         [ 1.1168],\n",
      "         [-3.5272],\n",
      "         [-3.4501],\n",
      "         [-1.8815],\n",
      "         [-0.6960],\n",
      "         [-0.2092],\n",
      "         [ 2.5423],\n",
      "         [-0.5200],\n",
      "         [-0.8315],\n",
      "         [-1.7862],\n",
      "         [ 1.1652],\n",
      "         [-4.2089],\n",
      "         [-2.4802],\n",
      "         [-2.0109],\n",
      "         [-1.6822],\n",
      "         [-1.7840],\n",
      "         [-0.8993],\n",
      "         [-1.5729],\n",
      "         [ 1.3070],\n",
      "         [ 0.2207],\n",
      "         [-4.0969],\n",
      "         [ 1.0905],\n",
      "         [ 1.0379],\n",
      "         [-1.1757]],\n",
      "\n",
      "        [[-2.2837],\n",
      "         [-3.8251],\n",
      "         [-0.1429],\n",
      "         [-0.7893],\n",
      "         [-2.8314],\n",
      "         [-3.1015],\n",
      "         [-1.3289],\n",
      "         [-3.4464],\n",
      "         [-0.6162],\n",
      "         [ 1.1168],\n",
      "         [ 1.0710],\n",
      "         [-1.9685],\n",
      "         [-1.3301],\n",
      "         [-3.0180],\n",
      "         [ 0.0058],\n",
      "         [-4.5206],\n",
      "         [-1.2386],\n",
      "         [-1.5513],\n",
      "         [-2.8821],\n",
      "         [-1.5194],\n",
      "         [-3.1637],\n",
      "         [-0.3611],\n",
      "         [-0.0109],\n",
      "         [-1.4093],\n",
      "         [-2.3167],\n",
      "         [-1.6684],\n",
      "         [-3.6449],\n",
      "         [ 0.1876],\n",
      "         [ 0.5289],\n",
      "         [-2.7455],\n",
      "         [ 2.6148],\n",
      "         [-0.0950],\n",
      "         [-1.1848],\n",
      "         [-1.0449],\n",
      "         [-1.1328],\n",
      "         [-2.0258],\n",
      "         [-2.2999],\n",
      "         [-1.4074],\n",
      "         [-2.5814],\n",
      "         [-4.3822],\n",
      "         [-4.6175],\n",
      "         [-1.6870],\n",
      "         [-1.8048],\n",
      "         [-0.4814],\n",
      "         [-0.2935],\n",
      "         [-0.5546],\n",
      "         [-2.7455],\n",
      "         [-1.7464],\n",
      "         [-1.6136],\n",
      "         [-3.0666],\n",
      "         [-1.8576],\n",
      "         [-4.1222],\n",
      "         [-3.5130],\n",
      "         [-1.2659],\n",
      "         [ 0.9076],\n",
      "         [-1.5650],\n",
      "         [-0.2536],\n",
      "         [-1.5823],\n",
      "         [-2.0921],\n",
      "         [-1.9465],\n",
      "         [-1.4890],\n",
      "         [-3.2315],\n",
      "         [-1.2067],\n",
      "         [-3.5567],\n",
      "         [ 0.2126],\n",
      "         [-3.5272],\n",
      "         [-3.3200],\n",
      "         [-0.1439],\n",
      "         [-2.4068],\n",
      "         [-2.1842],\n",
      "         [-1.8622],\n",
      "         [-0.1259],\n",
      "         [-1.5729],\n",
      "         [-0.9661],\n",
      "         [-2.6161],\n",
      "         [-4.4754],\n",
      "         [ 1.1168],\n",
      "         [-3.5272],\n",
      "         [-3.4501],\n",
      "         [-1.8815],\n",
      "         [-0.6960],\n",
      "         [-0.2092],\n",
      "         [ 2.5423],\n",
      "         [-0.5200],\n",
      "         [-0.8315],\n",
      "         [-1.7862],\n",
      "         [ 1.1652],\n",
      "         [-4.2089],\n",
      "         [-2.4802],\n",
      "         [-2.0109],\n",
      "         [-1.6822],\n",
      "         [-1.7840],\n",
      "         [-0.8993],\n",
      "         [-1.5729],\n",
      "         [ 1.3070],\n",
      "         [ 0.2207],\n",
      "         [-4.0969],\n",
      "         [ 1.0905],\n",
      "         [ 1.0379],\n",
      "         [-1.1757]],\n",
      "\n",
      "        [[-2.2837],\n",
      "         [-3.8251],\n",
      "         [-0.1429],\n",
      "         [-0.7893],\n",
      "         [-2.8314],\n",
      "         [-3.1015],\n",
      "         [-1.3289],\n",
      "         [-3.4464],\n",
      "         [-0.6162],\n",
      "         [ 1.1168],\n",
      "         [ 1.0710],\n",
      "         [-1.9685],\n",
      "         [-1.3301],\n",
      "         [-3.0180],\n",
      "         [ 0.0058],\n",
      "         [-4.5206],\n",
      "         [-1.2386],\n",
      "         [-1.5513],\n",
      "         [-2.8821],\n",
      "         [-1.5194],\n",
      "         [-3.1637],\n",
      "         [-0.3611],\n",
      "         [-0.0109],\n",
      "         [-1.4093],\n",
      "         [-2.3167],\n",
      "         [-1.6684],\n",
      "         [-3.6449],\n",
      "         [ 0.1876],\n",
      "         [ 0.5289],\n",
      "         [-2.7455],\n",
      "         [ 2.6148],\n",
      "         [-0.0950],\n",
      "         [-1.1848],\n",
      "         [-1.0449],\n",
      "         [-1.1328],\n",
      "         [-2.0258],\n",
      "         [-2.2999],\n",
      "         [-1.4074],\n",
      "         [-2.5814],\n",
      "         [-4.3822],\n",
      "         [-4.6175],\n",
      "         [-1.6870],\n",
      "         [-1.8048],\n",
      "         [-0.4814],\n",
      "         [-0.2935],\n",
      "         [-0.5546],\n",
      "         [-2.7455],\n",
      "         [-1.7464],\n",
      "         [-1.6136],\n",
      "         [-3.0666],\n",
      "         [-1.8576],\n",
      "         [-4.1222],\n",
      "         [-3.5130],\n",
      "         [-1.2659],\n",
      "         [ 0.9076],\n",
      "         [-1.5650],\n",
      "         [-0.2536],\n",
      "         [-1.5823],\n",
      "         [-2.0921],\n",
      "         [-1.9465],\n",
      "         [-1.4890],\n",
      "         [-3.2315],\n",
      "         [-1.2067],\n",
      "         [-3.5567],\n",
      "         [ 0.2126],\n",
      "         [-3.5272],\n",
      "         [-3.3200],\n",
      "         [-0.1439],\n",
      "         [-2.4068],\n",
      "         [-2.1842],\n",
      "         [-1.8622],\n",
      "         [-0.1259],\n",
      "         [-1.5729],\n",
      "         [-0.9661],\n",
      "         [-2.6161],\n",
      "         [-4.4754],\n",
      "         [ 1.1168],\n",
      "         [-3.5272],\n",
      "         [-3.4501],\n",
      "         [-1.8815],\n",
      "         [-0.6960],\n",
      "         [-0.2092],\n",
      "         [ 2.5423],\n",
      "         [-0.5200],\n",
      "         [-0.8315],\n",
      "         [-1.7862],\n",
      "         [ 1.1652],\n",
      "         [-4.2089],\n",
      "         [-2.4802],\n",
      "         [-2.0109],\n",
      "         [-1.6822],\n",
      "         [-1.7840],\n",
      "         [-0.8993],\n",
      "         [-1.5729],\n",
      "         [ 1.3070],\n",
      "         [ 0.2207],\n",
      "         [-4.0969],\n",
      "         [ 1.0905],\n",
      "         [ 1.0379],\n",
      "         [-1.1757]],\n",
      "\n",
      "        [[-2.2837],\n",
      "         [-3.8251],\n",
      "         [-0.1429],\n",
      "         [-0.7893],\n",
      "         [-2.8314],\n",
      "         [-3.1015],\n",
      "         [-1.3289],\n",
      "         [-3.4464],\n",
      "         [-0.6162],\n",
      "         [ 1.1168],\n",
      "         [ 1.0710],\n",
      "         [-1.9685],\n",
      "         [-1.3301],\n",
      "         [-3.0180],\n",
      "         [ 0.0058],\n",
      "         [-4.5206],\n",
      "         [-1.2386],\n",
      "         [-1.5513],\n",
      "         [-2.8821],\n",
      "         [-1.5194],\n",
      "         [-3.1637],\n",
      "         [-0.3611],\n",
      "         [-0.0109],\n",
      "         [-1.4093],\n",
      "         [-2.3167],\n",
      "         [-1.6684],\n",
      "         [-3.6449],\n",
      "         [ 0.1876],\n",
      "         [ 0.5289],\n",
      "         [-2.7455],\n",
      "         [ 2.6148],\n",
      "         [-0.0950],\n",
      "         [-1.1848],\n",
      "         [-1.0449],\n",
      "         [-1.1328],\n",
      "         [-2.0258],\n",
      "         [-2.2999],\n",
      "         [-1.4074],\n",
      "         [-2.5814],\n",
      "         [-4.3822],\n",
      "         [-4.6175],\n",
      "         [-1.6870],\n",
      "         [-1.8048],\n",
      "         [-0.4814],\n",
      "         [-0.2935],\n",
      "         [-0.5546],\n",
      "         [-2.7455],\n",
      "         [-1.7464],\n",
      "         [-1.6136],\n",
      "         [-3.0666],\n",
      "         [-1.8576],\n",
      "         [-4.1222],\n",
      "         [-3.5130],\n",
      "         [-1.2659],\n",
      "         [ 0.9076],\n",
      "         [-1.5650],\n",
      "         [-0.2536],\n",
      "         [-1.5823],\n",
      "         [-2.0921],\n",
      "         [-1.9465],\n",
      "         [-1.4890],\n",
      "         [-3.2315],\n",
      "         [-1.2067],\n",
      "         [-3.5567],\n",
      "         [ 0.2126],\n",
      "         [-3.5272],\n",
      "         [-3.3200],\n",
      "         [-0.1439],\n",
      "         [-2.4068],\n",
      "         [-2.1842],\n",
      "         [-1.8622],\n",
      "         [-0.1259],\n",
      "         [-1.5729],\n",
      "         [-0.9661],\n",
      "         [-2.6161],\n",
      "         [-4.4754],\n",
      "         [ 1.1168],\n",
      "         [-3.5272],\n",
      "         [-3.4501],\n",
      "         [-1.8815],\n",
      "         [-0.6960],\n",
      "         [-0.2092],\n",
      "         [ 2.5423],\n",
      "         [-0.5200],\n",
      "         [-0.8315],\n",
      "         [-1.7862],\n",
      "         [ 1.1652],\n",
      "         [-4.2089],\n",
      "         [-2.4802],\n",
      "         [-2.0109],\n",
      "         [-1.6822],\n",
      "         [-1.7840],\n",
      "         [-0.8993],\n",
      "         [-1.5729],\n",
      "         [ 1.3070],\n",
      "         [ 0.2207],\n",
      "         [-4.0969],\n",
      "         [ 1.0905],\n",
      "         [ 1.0379],\n",
      "         [-1.1757]],\n",
      "\n",
      "        [[-2.2837],\n",
      "         [-3.8251],\n",
      "         [-0.1429],\n",
      "         [-0.7893],\n",
      "         [-2.8314],\n",
      "         [-3.1015],\n",
      "         [-1.3289],\n",
      "         [-3.4464],\n",
      "         [-0.6162],\n",
      "         [ 1.1168],\n",
      "         [ 1.0710],\n",
      "         [-1.9685],\n",
      "         [-1.3301],\n",
      "         [-3.0180],\n",
      "         [ 0.0058],\n",
      "         [-4.5206],\n",
      "         [-1.2386],\n",
      "         [-1.5513],\n",
      "         [-2.8821],\n",
      "         [-1.5194],\n",
      "         [-3.1637],\n",
      "         [-0.3611],\n",
      "         [-0.0109],\n",
      "         [-1.4093],\n",
      "         [-2.3167],\n",
      "         [-1.6684],\n",
      "         [-3.6449],\n",
      "         [ 0.1876],\n",
      "         [ 0.5289],\n",
      "         [-2.7455],\n",
      "         [ 2.6148],\n",
      "         [-0.0950],\n",
      "         [-1.1848],\n",
      "         [-1.0449],\n",
      "         [-1.1328],\n",
      "         [-2.0258],\n",
      "         [-2.2999],\n",
      "         [-1.4074],\n",
      "         [-2.5814],\n",
      "         [-4.3822],\n",
      "         [-4.6175],\n",
      "         [-1.6870],\n",
      "         [-1.8048],\n",
      "         [-0.4814],\n",
      "         [-0.2935],\n",
      "         [-0.5546],\n",
      "         [-2.7455],\n",
      "         [-1.7464],\n",
      "         [-1.6136],\n",
      "         [-3.0666],\n",
      "         [-1.8576],\n",
      "         [-4.1222],\n",
      "         [-3.5130],\n",
      "         [-1.2659],\n",
      "         [ 0.9076],\n",
      "         [-1.5650],\n",
      "         [-0.2536],\n",
      "         [-1.5823],\n",
      "         [-2.0921],\n",
      "         [-1.9465],\n",
      "         [-1.4890],\n",
      "         [-3.2315],\n",
      "         [-1.2067],\n",
      "         [-3.5567],\n",
      "         [ 0.2126],\n",
      "         [-3.5272],\n",
      "         [-3.3200],\n",
      "         [-0.1439],\n",
      "         [-2.4068],\n",
      "         [-2.1842],\n",
      "         [-1.8622],\n",
      "         [-0.1259],\n",
      "         [-1.5729],\n",
      "         [-0.9661],\n",
      "         [-2.6161],\n",
      "         [-4.4754],\n",
      "         [ 1.1168],\n",
      "         [-3.5272],\n",
      "         [-3.4501],\n",
      "         [-1.8815],\n",
      "         [-0.6960],\n",
      "         [-0.2092],\n",
      "         [ 2.5423],\n",
      "         [-0.5200],\n",
      "         [-0.8315],\n",
      "         [-1.7862],\n",
      "         [ 1.1652],\n",
      "         [-4.2089],\n",
      "         [-2.4802],\n",
      "         [-2.0109],\n",
      "         [-1.6822],\n",
      "         [-1.7840],\n",
      "         [-0.8993],\n",
      "         [-1.5729],\n",
      "         [ 1.3070],\n",
      "         [ 0.2207],\n",
      "         [-4.0969],\n",
      "         [ 1.0905],\n",
      "         [ 1.0379],\n",
      "         [-1.1757]],\n",
      "\n",
      "        [[-2.2837],\n",
      "         [-3.8251],\n",
      "         [-0.1429],\n",
      "         [-0.7893],\n",
      "         [-2.8314],\n",
      "         [-3.1015],\n",
      "         [-1.3289],\n",
      "         [-3.4464],\n",
      "         [-0.6162],\n",
      "         [ 1.1168],\n",
      "         [ 1.0710],\n",
      "         [-1.9685],\n",
      "         [-1.3301],\n",
      "         [-3.0180],\n",
      "         [ 0.0058],\n",
      "         [-4.5206],\n",
      "         [-1.2386],\n",
      "         [-1.5513],\n",
      "         [-2.8821],\n",
      "         [-1.5194],\n",
      "         [-3.1637],\n",
      "         [-0.3611],\n",
      "         [-0.0109],\n",
      "         [-1.4093],\n",
      "         [-2.3167],\n",
      "         [-1.6684],\n",
      "         [-3.6449],\n",
      "         [ 0.1876],\n",
      "         [ 0.5289],\n",
      "         [-2.7455],\n",
      "         [ 2.6148],\n",
      "         [-0.0950],\n",
      "         [-1.1848],\n",
      "         [-1.0449],\n",
      "         [-1.1328],\n",
      "         [-2.0258],\n",
      "         [-2.2999],\n",
      "         [-1.4074],\n",
      "         [-2.5814],\n",
      "         [-4.3822],\n",
      "         [-4.6175],\n",
      "         [-1.6870],\n",
      "         [-1.8048],\n",
      "         [-0.4814],\n",
      "         [-0.2935],\n",
      "         [-0.5546],\n",
      "         [-2.7455],\n",
      "         [-1.7464],\n",
      "         [-1.6136],\n",
      "         [-3.0666],\n",
      "         [-1.8576],\n",
      "         [-4.1222],\n",
      "         [-3.5130],\n",
      "         [-1.2659],\n",
      "         [ 0.9076],\n",
      "         [-1.5650],\n",
      "         [-0.2536],\n",
      "         [-1.5823],\n",
      "         [-2.0921],\n",
      "         [-1.9465],\n",
      "         [-1.4890],\n",
      "         [-3.2315],\n",
      "         [-1.2067],\n",
      "         [-3.5567],\n",
      "         [ 0.2126],\n",
      "         [-3.5272],\n",
      "         [-3.3200],\n",
      "         [-0.1439],\n",
      "         [-2.4068],\n",
      "         [-2.1842],\n",
      "         [-1.8622],\n",
      "         [-0.1259],\n",
      "         [-1.5729],\n",
      "         [-0.9661],\n",
      "         [-2.6161],\n",
      "         [-4.4754],\n",
      "         [ 1.1168],\n",
      "         [-3.5272],\n",
      "         [-3.4501],\n",
      "         [-1.8815],\n",
      "         [-0.6960],\n",
      "         [-0.2092],\n",
      "         [ 2.5423],\n",
      "         [-0.5200],\n",
      "         [-0.8315],\n",
      "         [-1.7862],\n",
      "         [ 1.1652],\n",
      "         [-4.2089],\n",
      "         [-2.4802],\n",
      "         [-2.0109],\n",
      "         [-1.6822],\n",
      "         [-1.7840],\n",
      "         [-0.8993],\n",
      "         [-1.5729],\n",
      "         [ 1.3070],\n",
      "         [ 0.2207],\n",
      "         [-4.0969],\n",
      "         [ 1.0905],\n",
      "         [ 1.0379],\n",
      "         [-1.1757]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([6, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([6, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/12/2023 06:40:39 AM WARNING 36309 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:41 AM WARNING 36309 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:41 AM WARNING 36309 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:41 AM WARNING 36309 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:41 AM WARNING 36309 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:41 AM WARNING 36309 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:41 AM WARNING 36309 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:41 AM WARNING 36309 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:41 AM WARNING 36309 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-0.4649],\n",
      "         [-0.7524],\n",
      "         [-2.0258],\n",
      "         [-2.5242],\n",
      "         [-4.0481],\n",
      "         [-1.6468],\n",
      "         [-1.0343],\n",
      "         [-0.3199],\n",
      "         [ 0.2117],\n",
      "         [-2.0258],\n",
      "         [ 0.2480],\n",
      "         [ 1.2056],\n",
      "         [-4.5188],\n",
      "         [-2.7717],\n",
      "         [-1.8002],\n",
      "         [-3.8006],\n",
      "         [-0.4579],\n",
      "         [-1.1680],\n",
      "         [ 0.2480],\n",
      "         [-2.5238],\n",
      "         [-1.6329],\n",
      "         [-3.3623],\n",
      "         [-1.8048],\n",
      "         [-1.2444],\n",
      "         [-1.3983],\n",
      "         [ 1.4683],\n",
      "         [-1.7759],\n",
      "         [-2.0097],\n",
      "         [-1.7759],\n",
      "         [-3.2315],\n",
      "         [-1.1433],\n",
      "         [-2.5568],\n",
      "         [-3.8251],\n",
      "         [-2.7657],\n",
      "         [ 0.4360],\n",
      "         [ 0.7063],\n",
      "         [-2.6161],\n",
      "         [ 0.8839],\n",
      "         [-0.9022],\n",
      "         [-2.2677],\n",
      "         [-2.5080],\n",
      "         [-0.8369],\n",
      "         [ 0.8017],\n",
      "         [-2.0283],\n",
      "         [-1.3185],\n",
      "         [-3.9659],\n",
      "         [-0.7299],\n",
      "         [ 1.1294],\n",
      "         [-2.0481],\n",
      "         [-2.5579],\n",
      "         [-3.4464],\n",
      "         [-4.7241],\n",
      "         [-1.9668],\n",
      "         [-1.8741],\n",
      "         [-3.5440],\n",
      "         [-2.1427],\n",
      "         [-2.5416],\n",
      "         [-0.5715],\n",
      "         [-0.6099],\n",
      "         [-0.6052],\n",
      "         [ 1.3635],\n",
      "         [ 0.3270],\n",
      "         [-3.0932],\n",
      "         [-2.0759],\n",
      "         [-0.5751],\n",
      "         [-1.4297],\n",
      "         [-2.2873],\n",
      "         [-2.2873],\n",
      "         [-4.1070],\n",
      "         [-2.8947],\n",
      "         [-2.7245],\n",
      "         [-2.5568],\n",
      "         [ 0.3270],\n",
      "         [ 3.0670],\n",
      "         [-0.2246],\n",
      "         [ 2.0705],\n",
      "         [-2.0439],\n",
      "         [-0.2422],\n",
      "         [-0.9255],\n",
      "         [-0.0759],\n",
      "         [ 1.8238],\n",
      "         [-0.4814],\n",
      "         [-0.5283],\n",
      "         [-2.3266],\n",
      "         [-1.0282],\n",
      "         [-0.7328],\n",
      "         [-1.8651],\n",
      "         [-1.9685],\n",
      "         [-3.1450],\n",
      "         [-2.8408],\n",
      "         [-1.0925],\n",
      "         [-0.5587],\n",
      "         [ 0.3931],\n",
      "         [-1.8859],\n",
      "         [-2.3266],\n",
      "         [-0.8993],\n",
      "         [-1.7941],\n",
      "         [-3.0413],\n",
      "         [-1.0946],\n",
      "         [-1.7649]],\n",
      "\n",
      "        [[-0.4649],\n",
      "         [-0.7524],\n",
      "         [-2.0258],\n",
      "         [-2.5242],\n",
      "         [-4.0481],\n",
      "         [-1.6468],\n",
      "         [-1.0343],\n",
      "         [-0.3199],\n",
      "         [ 0.2117],\n",
      "         [-2.0258],\n",
      "         [ 0.2480],\n",
      "         [ 1.2056],\n",
      "         [-4.5188],\n",
      "         [-2.7717],\n",
      "         [-1.8002],\n",
      "         [-3.8006],\n",
      "         [-0.4579],\n",
      "         [-1.1680],\n",
      "         [ 0.2480],\n",
      "         [-2.5238],\n",
      "         [-1.6329],\n",
      "         [-3.3623],\n",
      "         [-1.8048],\n",
      "         [-1.2444],\n",
      "         [-1.3983],\n",
      "         [ 1.4683],\n",
      "         [-1.7759],\n",
      "         [-2.0097],\n",
      "         [-1.7759],\n",
      "         [-3.2315],\n",
      "         [-1.1433],\n",
      "         [-2.5568],\n",
      "         [-3.8251],\n",
      "         [-2.7657],\n",
      "         [ 0.4360],\n",
      "         [ 0.7063],\n",
      "         [-2.6161],\n",
      "         [ 0.8839],\n",
      "         [-0.9022],\n",
      "         [-2.2677],\n",
      "         [-2.5080],\n",
      "         [-0.8369],\n",
      "         [ 0.8017],\n",
      "         [-2.0283],\n",
      "         [-1.3185],\n",
      "         [-3.9659],\n",
      "         [-0.7299],\n",
      "         [ 1.1294],\n",
      "         [-2.0481],\n",
      "         [-2.5579],\n",
      "         [-3.4464],\n",
      "         [-4.7241],\n",
      "         [-1.9668],\n",
      "         [-1.8741],\n",
      "         [-3.5440],\n",
      "         [-2.1427],\n",
      "         [-2.5416],\n",
      "         [-0.5715],\n",
      "         [-0.6099],\n",
      "         [-0.6052],\n",
      "         [ 1.3635],\n",
      "         [ 0.3270],\n",
      "         [-3.0932],\n",
      "         [-2.0759],\n",
      "         [-0.5751],\n",
      "         [-1.4297],\n",
      "         [-2.2873],\n",
      "         [-2.2873],\n",
      "         [-4.1070],\n",
      "         [-2.8947],\n",
      "         [-2.7245],\n",
      "         [-2.5568],\n",
      "         [ 0.3270],\n",
      "         [ 3.0670],\n",
      "         [-0.2246],\n",
      "         [ 2.0705],\n",
      "         [-2.0439],\n",
      "         [-0.2422],\n",
      "         [-0.9255],\n",
      "         [-0.0759],\n",
      "         [ 1.8238],\n",
      "         [-0.4814],\n",
      "         [-0.5283],\n",
      "         [-2.3266],\n",
      "         [-1.0282],\n",
      "         [-0.7328],\n",
      "         [-1.8651],\n",
      "         [-1.9685],\n",
      "         [-3.1450],\n",
      "         [-2.8408],\n",
      "         [-1.0925],\n",
      "         [-0.5587],\n",
      "         [ 0.3931],\n",
      "         [-1.8859],\n",
      "         [-2.3266],\n",
      "         [-0.8993],\n",
      "         [-1.7941],\n",
      "         [-3.0413],\n",
      "         [-1.0946],\n",
      "         [-1.7649]],\n",
      "\n",
      "        [[-0.4649],\n",
      "         [-0.7524],\n",
      "         [-2.0258],\n",
      "         [-2.5242],\n",
      "         [-4.0481],\n",
      "         [-1.6468],\n",
      "         [-1.0343],\n",
      "         [-0.3199],\n",
      "         [ 0.2117],\n",
      "         [-2.0258],\n",
      "         [ 0.2480],\n",
      "         [ 1.2056],\n",
      "         [-4.5188],\n",
      "         [-2.7717],\n",
      "         [-1.8002],\n",
      "         [-3.8006],\n",
      "         [-0.4579],\n",
      "         [-1.1680],\n",
      "         [ 0.2480],\n",
      "         [-2.5238],\n",
      "         [-1.6329],\n",
      "         [-3.3623],\n",
      "         [-1.8048],\n",
      "         [-1.2444],\n",
      "         [-1.3983],\n",
      "         [ 1.4683],\n",
      "         [-1.7759],\n",
      "         [-2.0097],\n",
      "         [-1.7759],\n",
      "         [-3.2315],\n",
      "         [-1.1433],\n",
      "         [-2.5568],\n",
      "         [-3.8251],\n",
      "         [-2.7657],\n",
      "         [ 0.4360],\n",
      "         [ 0.7063],\n",
      "         [-2.6161],\n",
      "         [ 0.8839],\n",
      "         [-0.9022],\n",
      "         [-2.2677],\n",
      "         [-2.5080],\n",
      "         [-0.8369],\n",
      "         [ 0.8017],\n",
      "         [-2.0283],\n",
      "         [-1.3185],\n",
      "         [-3.9659],\n",
      "         [-0.7299],\n",
      "         [ 1.1294],\n",
      "         [-2.0481],\n",
      "         [-2.5579],\n",
      "         [-3.4464],\n",
      "         [-4.7241],\n",
      "         [-1.9668],\n",
      "         [-1.8741],\n",
      "         [-3.5440],\n",
      "         [-2.1427],\n",
      "         [-2.5416],\n",
      "         [-0.5715],\n",
      "         [-0.6099],\n",
      "         [-0.6052],\n",
      "         [ 1.3635],\n",
      "         [ 0.3270],\n",
      "         [-3.0932],\n",
      "         [-2.0759],\n",
      "         [-0.5751],\n",
      "         [-1.4297],\n",
      "         [-2.2873],\n",
      "         [-2.2873],\n",
      "         [-4.1070],\n",
      "         [-2.8947],\n",
      "         [-2.7245],\n",
      "         [-2.5568],\n",
      "         [ 0.3270],\n",
      "         [ 3.0670],\n",
      "         [-0.2246],\n",
      "         [ 2.0705],\n",
      "         [-2.0439],\n",
      "         [-0.2422],\n",
      "         [-0.9255],\n",
      "         [-0.0759],\n",
      "         [ 1.8238],\n",
      "         [-0.4814],\n",
      "         [-0.5283],\n",
      "         [-2.3266],\n",
      "         [-1.0282],\n",
      "         [-0.7328],\n",
      "         [-1.8651],\n",
      "         [-1.9685],\n",
      "         [-3.1450],\n",
      "         [-2.8408],\n",
      "         [-1.0925],\n",
      "         [-0.5587],\n",
      "         [ 0.3931],\n",
      "         [-1.8859],\n",
      "         [-2.3266],\n",
      "         [-0.8993],\n",
      "         [-1.7941],\n",
      "         [-3.0413],\n",
      "         [-1.0946],\n",
      "         [-1.7649]],\n",
      "\n",
      "        [[-0.4649],\n",
      "         [-0.7524],\n",
      "         [-2.0258],\n",
      "         [-2.5242],\n",
      "         [-4.0481],\n",
      "         [-1.6468],\n",
      "         [-1.0343],\n",
      "         [-0.3199],\n",
      "         [ 0.2117],\n",
      "         [-2.0258],\n",
      "         [ 0.2480],\n",
      "         [ 1.2056],\n",
      "         [-4.5188],\n",
      "         [-2.7717],\n",
      "         [-1.8002],\n",
      "         [-3.8006],\n",
      "         [-0.4579],\n",
      "         [-1.1680],\n",
      "         [ 0.2480],\n",
      "         [-2.5238],\n",
      "         [-1.6329],\n",
      "         [-3.3623],\n",
      "         [-1.8048],\n",
      "         [-1.2444],\n",
      "         [-1.3983],\n",
      "         [ 1.4683],\n",
      "         [-1.7759],\n",
      "         [-2.0097],\n",
      "         [-1.7759],\n",
      "         [-3.2315],\n",
      "         [-1.1433],\n",
      "         [-2.5568],\n",
      "         [-3.8251],\n",
      "         [-2.7657],\n",
      "         [ 0.4360],\n",
      "         [ 0.7063],\n",
      "         [-2.6161],\n",
      "         [ 0.8839],\n",
      "         [-0.9022],\n",
      "         [-2.2677],\n",
      "         [-2.5080],\n",
      "         [-0.8369],\n",
      "         [ 0.8017],\n",
      "         [-2.0283],\n",
      "         [-1.3185],\n",
      "         [-3.9659],\n",
      "         [-0.7299],\n",
      "         [ 1.1294],\n",
      "         [-2.0481],\n",
      "         [-2.5579],\n",
      "         [-3.4464],\n",
      "         [-4.7241],\n",
      "         [-1.9668],\n",
      "         [-1.8741],\n",
      "         [-3.5440],\n",
      "         [-2.1427],\n",
      "         [-2.5416],\n",
      "         [-0.5715],\n",
      "         [-0.6099],\n",
      "         [-0.6052],\n",
      "         [ 1.3635],\n",
      "         [ 0.3270],\n",
      "         [-3.0932],\n",
      "         [-2.0759],\n",
      "         [-0.5751],\n",
      "         [-1.4297],\n",
      "         [-2.2873],\n",
      "         [-2.2873],\n",
      "         [-4.1070],\n",
      "         [-2.8947],\n",
      "         [-2.7245],\n",
      "         [-2.5568],\n",
      "         [ 0.3270],\n",
      "         [ 3.0670],\n",
      "         [-0.2246],\n",
      "         [ 2.0705],\n",
      "         [-2.0439],\n",
      "         [-0.2422],\n",
      "         [-0.9255],\n",
      "         [-0.0759],\n",
      "         [ 1.8238],\n",
      "         [-0.4814],\n",
      "         [-0.5283],\n",
      "         [-2.3266],\n",
      "         [-1.0282],\n",
      "         [-0.7328],\n",
      "         [-1.8651],\n",
      "         [-1.9685],\n",
      "         [-3.1450],\n",
      "         [-2.8408],\n",
      "         [-1.0925],\n",
      "         [-0.5587],\n",
      "         [ 0.3931],\n",
      "         [-1.8859],\n",
      "         [-2.3266],\n",
      "         [-0.8993],\n",
      "         [-1.7941],\n",
      "         [-3.0413],\n",
      "         [-1.0946],\n",
      "         [-1.7649]],\n",
      "\n",
      "        [[-0.4649],\n",
      "         [-0.7524],\n",
      "         [-2.0258],\n",
      "         [-2.5242],\n",
      "         [-4.0481],\n",
      "         [-1.6468],\n",
      "         [-1.0343],\n",
      "         [-0.3199],\n",
      "         [ 0.2117],\n",
      "         [-2.0258],\n",
      "         [ 0.2480],\n",
      "         [ 1.2056],\n",
      "         [-4.5188],\n",
      "         [-2.7717],\n",
      "         [-1.8002],\n",
      "         [-3.8006],\n",
      "         [-0.4579],\n",
      "         [-1.1680],\n",
      "         [ 0.2480],\n",
      "         [-2.5238],\n",
      "         [-1.6329],\n",
      "         [-3.3623],\n",
      "         [-1.8048],\n",
      "         [-1.2444],\n",
      "         [-1.3983],\n",
      "         [ 1.4683],\n",
      "         [-1.7759],\n",
      "         [-2.0097],\n",
      "         [-1.7759],\n",
      "         [-3.2315],\n",
      "         [-1.1433],\n",
      "         [-2.5568],\n",
      "         [-3.8251],\n",
      "         [-2.7657],\n",
      "         [ 0.4360],\n",
      "         [ 0.7063],\n",
      "         [-2.6161],\n",
      "         [ 0.8839],\n",
      "         [-0.9022],\n",
      "         [-2.2677],\n",
      "         [-2.5080],\n",
      "         [-0.8369],\n",
      "         [ 0.8017],\n",
      "         [-2.0283],\n",
      "         [-1.3185],\n",
      "         [-3.9659],\n",
      "         [-0.7299],\n",
      "         [ 1.1294],\n",
      "         [-2.0481],\n",
      "         [-2.5579],\n",
      "         [-3.4464],\n",
      "         [-4.7241],\n",
      "         [-1.9668],\n",
      "         [-1.8741],\n",
      "         [-3.5440],\n",
      "         [-2.1427],\n",
      "         [-2.5416],\n",
      "         [-0.5715],\n",
      "         [-0.6099],\n",
      "         [-0.6052],\n",
      "         [ 1.3635],\n",
      "         [ 0.3270],\n",
      "         [-3.0932],\n",
      "         [-2.0759],\n",
      "         [-0.5751],\n",
      "         [-1.4297],\n",
      "         [-2.2873],\n",
      "         [-2.2873],\n",
      "         [-4.1070],\n",
      "         [-2.8947],\n",
      "         [-2.7245],\n",
      "         [-2.5568],\n",
      "         [ 0.3270],\n",
      "         [ 3.0670],\n",
      "         [-0.2246],\n",
      "         [ 2.0705],\n",
      "         [-2.0439],\n",
      "         [-0.2422],\n",
      "         [-0.9255],\n",
      "         [-0.0759],\n",
      "         [ 1.8238],\n",
      "         [-0.4814],\n",
      "         [-0.5283],\n",
      "         [-2.3266],\n",
      "         [-1.0282],\n",
      "         [-0.7328],\n",
      "         [-1.8651],\n",
      "         [-1.9685],\n",
      "         [-3.1450],\n",
      "         [-2.8408],\n",
      "         [-1.0925],\n",
      "         [-0.5587],\n",
      "         [ 0.3931],\n",
      "         [-1.8859],\n",
      "         [-2.3266],\n",
      "         [-0.8993],\n",
      "         [-1.7941],\n",
      "         [-3.0413],\n",
      "         [-1.0946],\n",
      "         [-1.7649]],\n",
      "\n",
      "        [[-0.4649],\n",
      "         [-0.7524],\n",
      "         [-2.0258],\n",
      "         [-2.5242],\n",
      "         [-4.0481],\n",
      "         [-1.6468],\n",
      "         [-1.0343],\n",
      "         [-0.3199],\n",
      "         [ 0.2117],\n",
      "         [-2.0258],\n",
      "         [ 0.2480],\n",
      "         [ 1.2056],\n",
      "         [-4.5188],\n",
      "         [-2.7717],\n",
      "         [-1.8002],\n",
      "         [-3.8006],\n",
      "         [-0.4579],\n",
      "         [-1.1680],\n",
      "         [ 0.2480],\n",
      "         [-2.5238],\n",
      "         [-1.6329],\n",
      "         [-3.3623],\n",
      "         [-1.8048],\n",
      "         [-1.2444],\n",
      "         [-1.3983],\n",
      "         [ 1.4683],\n",
      "         [-1.7759],\n",
      "         [-2.0097],\n",
      "         [-1.7759],\n",
      "         [-3.2315],\n",
      "         [-1.1433],\n",
      "         [-2.5568],\n",
      "         [-3.8251],\n",
      "         [-2.7657],\n",
      "         [ 0.4360],\n",
      "         [ 0.7063],\n",
      "         [-2.6161],\n",
      "         [ 0.8839],\n",
      "         [-0.9022],\n",
      "         [-2.2677],\n",
      "         [-2.5080],\n",
      "         [-0.8369],\n",
      "         [ 0.8017],\n",
      "         [-2.0283],\n",
      "         [-1.3185],\n",
      "         [-3.9659],\n",
      "         [-0.7299],\n",
      "         [ 1.1294],\n",
      "         [-2.0481],\n",
      "         [-2.5579],\n",
      "         [-3.4464],\n",
      "         [-4.7241],\n",
      "         [-1.9668],\n",
      "         [-1.8741],\n",
      "         [-3.5440],\n",
      "         [-2.1427],\n",
      "         [-2.5416],\n",
      "         [-0.5715],\n",
      "         [-0.6099],\n",
      "         [-0.6052],\n",
      "         [ 1.3635],\n",
      "         [ 0.3270],\n",
      "         [-3.0932],\n",
      "         [-2.0759],\n",
      "         [-0.5751],\n",
      "         [-1.4297],\n",
      "         [-2.2873],\n",
      "         [-2.2873],\n",
      "         [-4.1070],\n",
      "         [-2.8947],\n",
      "         [-2.7245],\n",
      "         [-2.5568],\n",
      "         [ 0.3270],\n",
      "         [ 3.0670],\n",
      "         [-0.2246],\n",
      "         [ 2.0705],\n",
      "         [-2.0439],\n",
      "         [-0.2422],\n",
      "         [-0.9255],\n",
      "         [-0.0759],\n",
      "         [ 1.8238],\n",
      "         [-0.4814],\n",
      "         [-0.5283],\n",
      "         [-2.3266],\n",
      "         [-1.0282],\n",
      "         [-0.7328],\n",
      "         [-1.8651],\n",
      "         [-1.9685],\n",
      "         [-3.1450],\n",
      "         [-2.8408],\n",
      "         [-1.0925],\n",
      "         [-0.5587],\n",
      "         [ 0.3931],\n",
      "         [-1.8859],\n",
      "         [-2.3266],\n",
      "         [-0.8993],\n",
      "         [-1.7941],\n",
      "         [-3.0413],\n",
      "         [-1.0946],\n",
      "         [-1.7649]],\n",
      "\n",
      "        [[-0.4649],\n",
      "         [-0.7524],\n",
      "         [-2.0258],\n",
      "         [-2.5242],\n",
      "         [-4.0481],\n",
      "         [-1.6468],\n",
      "         [-1.0343],\n",
      "         [-0.3199],\n",
      "         [ 0.2117],\n",
      "         [-2.0258],\n",
      "         [ 0.2480],\n",
      "         [ 1.2056],\n",
      "         [-4.5188],\n",
      "         [-2.7717],\n",
      "         [-1.8002],\n",
      "         [-3.8006],\n",
      "         [-0.4579],\n",
      "         [-1.1680],\n",
      "         [ 0.2480],\n",
      "         [-2.5238],\n",
      "         [-1.6329],\n",
      "         [-3.3623],\n",
      "         [-1.8048],\n",
      "         [-1.2444],\n",
      "         [-1.3983],\n",
      "         [ 1.4683],\n",
      "         [-1.7759],\n",
      "         [-2.0097],\n",
      "         [-1.7759],\n",
      "         [-3.2315],\n",
      "         [-1.1433],\n",
      "         [-2.5568],\n",
      "         [-3.8251],\n",
      "         [-2.7657],\n",
      "         [ 0.4360],\n",
      "         [ 0.7063],\n",
      "         [-2.6161],\n",
      "         [ 0.8839],\n",
      "         [-0.9022],\n",
      "         [-2.2677],\n",
      "         [-2.5080],\n",
      "         [-0.8369],\n",
      "         [ 0.8017],\n",
      "         [-2.0283],\n",
      "         [-1.3185],\n",
      "         [-3.9659],\n",
      "         [-0.7299],\n",
      "         [ 1.1294],\n",
      "         [-2.0481],\n",
      "         [-2.5579],\n",
      "         [-3.4464],\n",
      "         [-4.7241],\n",
      "         [-1.9668],\n",
      "         [-1.8741],\n",
      "         [-3.5440],\n",
      "         [-2.1427],\n",
      "         [-2.5416],\n",
      "         [-0.5715],\n",
      "         [-0.6099],\n",
      "         [-0.6052],\n",
      "         [ 1.3635],\n",
      "         [ 0.3270],\n",
      "         [-3.0932],\n",
      "         [-2.0759],\n",
      "         [-0.5751],\n",
      "         [-1.4297],\n",
      "         [-2.2873],\n",
      "         [-2.2873],\n",
      "         [-4.1070],\n",
      "         [-2.8947],\n",
      "         [-2.7245],\n",
      "         [-2.5568],\n",
      "         [ 0.3270],\n",
      "         [ 3.0670],\n",
      "         [-0.2246],\n",
      "         [ 2.0705],\n",
      "         [-2.0439],\n",
      "         [-0.2422],\n",
      "         [-0.9255],\n",
      "         [-0.0759],\n",
      "         [ 1.8238],\n",
      "         [-0.4814],\n",
      "         [-0.5283],\n",
      "         [-2.3266],\n",
      "         [-1.0282],\n",
      "         [-0.7328],\n",
      "         [-1.8651],\n",
      "         [-1.9685],\n",
      "         [-3.1450],\n",
      "         [-2.8408],\n",
      "         [-1.0925],\n",
      "         [-0.5587],\n",
      "         [ 0.3931],\n",
      "         [-1.8859],\n",
      "         [-2.3266],\n",
      "         [-0.8993],\n",
      "         [-1.7941],\n",
      "         [-3.0413],\n",
      "         [-1.0946],\n",
      "         [-1.7649]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([7, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([7, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/12/2023 06:40:42 AM WARNING 36352 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:44 AM WARNING 36352 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:44 AM WARNING 36352 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:44 AM WARNING 36352 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:44 AM WARNING 36352 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:44 AM WARNING 36352 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:44 AM WARNING 36352 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:44 AM WARNING 36352 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:44 AM WARNING 36352 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.7489],\n",
      "         [-2.7994],\n",
      "         [-2.7237],\n",
      "         [-2.8820],\n",
      "         [-2.1427],\n",
      "         [-3.0403],\n",
      "         [-1.4093],\n",
      "         [ 0.2046],\n",
      "         [-1.8449],\n",
      "         [-1.5513],\n",
      "         [-0.4168],\n",
      "         [-2.8309],\n",
      "         [ 1.4381],\n",
      "         [-3.1450],\n",
      "         [ 1.0379],\n",
      "         [-2.0528],\n",
      "         [-0.5071],\n",
      "         [-3.5637],\n",
      "         [-1.7862],\n",
      "         [-2.5238],\n",
      "         [ 2.1306],\n",
      "         [-2.0689],\n",
      "         [-0.1535],\n",
      "         [-0.6929],\n",
      "         [-0.5413],\n",
      "         [-1.5514],\n",
      "         [-3.8593],\n",
      "         [ 1.7544],\n",
      "         [-0.9155],\n",
      "         [-0.5445],\n",
      "         [-0.2166],\n",
      "         [ 1.8134],\n",
      "         [-3.0761],\n",
      "         [-1.7678],\n",
      "         [-1.9391],\n",
      "         [-3.1007],\n",
      "         [-3.3775],\n",
      "         [ 1.3435],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.3871],\n",
      "         [ 0.9175],\n",
      "         [-1.1990],\n",
      "         [-2.8309],\n",
      "         [-0.8025],\n",
      "         [-3.6159],\n",
      "         [ 1.8365],\n",
      "         [-1.4074],\n",
      "         [-2.5488],\n",
      "         [-0.2283],\n",
      "         [-4.0969],\n",
      "         [-4.2499],\n",
      "         [-3.2872],\n",
      "         [ 0.9648],\n",
      "         [-1.5105],\n",
      "         [ 0.6151],\n",
      "         [-1.8992],\n",
      "         [-0.7299],\n",
      "         [ 0.2480],\n",
      "         [ 1.4381],\n",
      "         [-3.6449],\n",
      "         [-2.7894],\n",
      "         [-1.0120],\n",
      "         [-2.7657],\n",
      "         [-0.5709],\n",
      "         [ 3.0670],\n",
      "         [-0.5709],\n",
      "         [-1.8622],\n",
      "         [-2.6794],\n",
      "         [-3.2381],\n",
      "         [-3.5964],\n",
      "         [-0.5459],\n",
      "         [-2.2822],\n",
      "         [-4.3857],\n",
      "         [-1.5271],\n",
      "         [ 0.2207],\n",
      "         [ 0.2126],\n",
      "         [ 1.1888],\n",
      "         [-3.1112],\n",
      "         [-0.6861],\n",
      "         [ 0.2158],\n",
      "         [ 0.0266],\n",
      "         [-1.9391],\n",
      "         [-3.3304],\n",
      "         [-1.0162],\n",
      "         [-0.9339],\n",
      "         [-0.9661],\n",
      "         [-3.2328],\n",
      "         [-2.6869],\n",
      "         [-1.6182],\n",
      "         [-3.3304],\n",
      "         [-0.8699],\n",
      "         [ 0.5838],\n",
      "         [-1.2374],\n",
      "         [-1.5786],\n",
      "         [-0.0900],\n",
      "         [-1.7941],\n",
      "         [-2.4915]],\n",
      "\n",
      "        [[-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.7489],\n",
      "         [-2.7994],\n",
      "         [-2.7237],\n",
      "         [-2.8820],\n",
      "         [-2.1427],\n",
      "         [-3.0403],\n",
      "         [-1.4093],\n",
      "         [ 0.2046],\n",
      "         [-1.8449],\n",
      "         [-1.5513],\n",
      "         [-0.4168],\n",
      "         [-2.8309],\n",
      "         [ 1.4381],\n",
      "         [-3.1450],\n",
      "         [ 1.0379],\n",
      "         [-2.0528],\n",
      "         [-0.5071],\n",
      "         [-3.5637],\n",
      "         [-1.7862],\n",
      "         [-2.5238],\n",
      "         [ 2.1306],\n",
      "         [-2.0689],\n",
      "         [-0.1535],\n",
      "         [-0.6929],\n",
      "         [-0.5413],\n",
      "         [-1.5514],\n",
      "         [-3.8593],\n",
      "         [ 1.7544],\n",
      "         [-0.9155],\n",
      "         [-0.5445],\n",
      "         [-0.2166],\n",
      "         [ 1.8134],\n",
      "         [-3.0761],\n",
      "         [-1.7678],\n",
      "         [-1.9391],\n",
      "         [-3.1007],\n",
      "         [-3.3775],\n",
      "         [ 1.3435],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.3871],\n",
      "         [ 0.9175],\n",
      "         [-1.1990],\n",
      "         [-2.8309],\n",
      "         [-0.8025],\n",
      "         [-3.6159],\n",
      "         [ 1.8365],\n",
      "         [-1.4074],\n",
      "         [-2.5488],\n",
      "         [-0.2283],\n",
      "         [-4.0969],\n",
      "         [-4.2499],\n",
      "         [-3.2872],\n",
      "         [ 0.9648],\n",
      "         [-1.5105],\n",
      "         [ 0.6151],\n",
      "         [-1.8992],\n",
      "         [-0.7299],\n",
      "         [ 0.2480],\n",
      "         [ 1.4381],\n",
      "         [-3.6449],\n",
      "         [-2.7894],\n",
      "         [-1.0120],\n",
      "         [-2.7657],\n",
      "         [-0.5709],\n",
      "         [ 3.0670],\n",
      "         [-0.5709],\n",
      "         [-1.8622],\n",
      "         [-2.6794],\n",
      "         [-3.2381],\n",
      "         [-3.5964],\n",
      "         [-0.5459],\n",
      "         [-2.2822],\n",
      "         [-4.3857],\n",
      "         [-1.5271],\n",
      "         [ 0.2207],\n",
      "         [ 0.2126],\n",
      "         [ 1.1888],\n",
      "         [-3.1112],\n",
      "         [-0.6861],\n",
      "         [ 0.2158],\n",
      "         [ 0.0266],\n",
      "         [-1.9391],\n",
      "         [-3.3304],\n",
      "         [-1.0162],\n",
      "         [-0.9339],\n",
      "         [-0.9661],\n",
      "         [-3.2328],\n",
      "         [-2.6869],\n",
      "         [-1.6182],\n",
      "         [-3.3304],\n",
      "         [-0.8699],\n",
      "         [ 0.5838],\n",
      "         [-1.2374],\n",
      "         [-1.5786],\n",
      "         [-0.0900],\n",
      "         [-1.7941],\n",
      "         [-2.4915]],\n",
      "\n",
      "        [[-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.7489],\n",
      "         [-2.7994],\n",
      "         [-2.7237],\n",
      "         [-2.8820],\n",
      "         [-2.1427],\n",
      "         [-3.0403],\n",
      "         [-1.4093],\n",
      "         [ 0.2046],\n",
      "         [-1.8449],\n",
      "         [-1.5513],\n",
      "         [-0.4168],\n",
      "         [-2.8309],\n",
      "         [ 1.4381],\n",
      "         [-3.1450],\n",
      "         [ 1.0379],\n",
      "         [-2.0528],\n",
      "         [-0.5071],\n",
      "         [-3.5637],\n",
      "         [-1.7862],\n",
      "         [-2.5238],\n",
      "         [ 2.1306],\n",
      "         [-2.0689],\n",
      "         [-0.1535],\n",
      "         [-0.6929],\n",
      "         [-0.5413],\n",
      "         [-1.5514],\n",
      "         [-3.8593],\n",
      "         [ 1.7544],\n",
      "         [-0.9155],\n",
      "         [-0.5445],\n",
      "         [-0.2166],\n",
      "         [ 1.8134],\n",
      "         [-3.0761],\n",
      "         [-1.7678],\n",
      "         [-1.9391],\n",
      "         [-3.1007],\n",
      "         [-3.3775],\n",
      "         [ 1.3435],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.3871],\n",
      "         [ 0.9175],\n",
      "         [-1.1990],\n",
      "         [-2.8309],\n",
      "         [-0.8025],\n",
      "         [-3.6159],\n",
      "         [ 1.8365],\n",
      "         [-1.4074],\n",
      "         [-2.5488],\n",
      "         [-0.2283],\n",
      "         [-4.0969],\n",
      "         [-4.2499],\n",
      "         [-3.2872],\n",
      "         [ 0.9648],\n",
      "         [-1.5105],\n",
      "         [ 0.6151],\n",
      "         [-1.8992],\n",
      "         [-0.7299],\n",
      "         [ 0.2480],\n",
      "         [ 1.4381],\n",
      "         [-3.6449],\n",
      "         [-2.7894],\n",
      "         [-1.0120],\n",
      "         [-2.7657],\n",
      "         [-0.5709],\n",
      "         [ 3.0670],\n",
      "         [-0.5709],\n",
      "         [-1.8622],\n",
      "         [-2.6794],\n",
      "         [-3.2381],\n",
      "         [-3.5964],\n",
      "         [-0.5459],\n",
      "         [-2.2822],\n",
      "         [-4.3857],\n",
      "         [-1.5271],\n",
      "         [ 0.2207],\n",
      "         [ 0.2126],\n",
      "         [ 1.1888],\n",
      "         [-3.1112],\n",
      "         [-0.6861],\n",
      "         [ 0.2158],\n",
      "         [ 0.0266],\n",
      "         [-1.9391],\n",
      "         [-3.3304],\n",
      "         [-1.0162],\n",
      "         [-0.9339],\n",
      "         [-0.9661],\n",
      "         [-3.2328],\n",
      "         [-2.6869],\n",
      "         [-1.6182],\n",
      "         [-3.3304],\n",
      "         [-0.8699],\n",
      "         [ 0.5838],\n",
      "         [-1.2374],\n",
      "         [-1.5786],\n",
      "         [-0.0900],\n",
      "         [-1.7941],\n",
      "         [-2.4915]],\n",
      "\n",
      "        [[-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.7489],\n",
      "         [-2.7994],\n",
      "         [-2.7237],\n",
      "         [-2.8820],\n",
      "         [-2.1427],\n",
      "         [-3.0403],\n",
      "         [-1.4093],\n",
      "         [ 0.2046],\n",
      "         [-1.8449],\n",
      "         [-1.5513],\n",
      "         [-0.4168],\n",
      "         [-2.8309],\n",
      "         [ 1.4381],\n",
      "         [-3.1450],\n",
      "         [ 1.0379],\n",
      "         [-2.0528],\n",
      "         [-0.5071],\n",
      "         [-3.5637],\n",
      "         [-1.7862],\n",
      "         [-2.5238],\n",
      "         [ 2.1306],\n",
      "         [-2.0689],\n",
      "         [-0.1535],\n",
      "         [-0.6929],\n",
      "         [-0.5413],\n",
      "         [-1.5514],\n",
      "         [-3.8593],\n",
      "         [ 1.7544],\n",
      "         [-0.9155],\n",
      "         [-0.5445],\n",
      "         [-0.2166],\n",
      "         [ 1.8134],\n",
      "         [-3.0761],\n",
      "         [-1.7678],\n",
      "         [-1.9391],\n",
      "         [-3.1007],\n",
      "         [-3.3775],\n",
      "         [ 1.3435],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.3871],\n",
      "         [ 0.9175],\n",
      "         [-1.1990],\n",
      "         [-2.8309],\n",
      "         [-0.8025],\n",
      "         [-3.6159],\n",
      "         [ 1.8365],\n",
      "         [-1.4074],\n",
      "         [-2.5488],\n",
      "         [-0.2283],\n",
      "         [-4.0969],\n",
      "         [-4.2499],\n",
      "         [-3.2872],\n",
      "         [ 0.9648],\n",
      "         [-1.5105],\n",
      "         [ 0.6151],\n",
      "         [-1.8992],\n",
      "         [-0.7299],\n",
      "         [ 0.2480],\n",
      "         [ 1.4381],\n",
      "         [-3.6449],\n",
      "         [-2.7894],\n",
      "         [-1.0120],\n",
      "         [-2.7657],\n",
      "         [-0.5709],\n",
      "         [ 3.0670],\n",
      "         [-0.5709],\n",
      "         [-1.8622],\n",
      "         [-2.6794],\n",
      "         [-3.2381],\n",
      "         [-3.5964],\n",
      "         [-0.5459],\n",
      "         [-2.2822],\n",
      "         [-4.3857],\n",
      "         [-1.5271],\n",
      "         [ 0.2207],\n",
      "         [ 0.2126],\n",
      "         [ 1.1888],\n",
      "         [-3.1112],\n",
      "         [-0.6861],\n",
      "         [ 0.2158],\n",
      "         [ 0.0266],\n",
      "         [-1.9391],\n",
      "         [-3.3304],\n",
      "         [-1.0162],\n",
      "         [-0.9339],\n",
      "         [-0.9661],\n",
      "         [-3.2328],\n",
      "         [-2.6869],\n",
      "         [-1.6182],\n",
      "         [-3.3304],\n",
      "         [-0.8699],\n",
      "         [ 0.5838],\n",
      "         [-1.2374],\n",
      "         [-1.5786],\n",
      "         [-0.0900],\n",
      "         [-1.7941],\n",
      "         [-2.4915]],\n",
      "\n",
      "        [[-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.7489],\n",
      "         [-2.7994],\n",
      "         [-2.7237],\n",
      "         [-2.8820],\n",
      "         [-2.1427],\n",
      "         [-3.0403],\n",
      "         [-1.4093],\n",
      "         [ 0.2046],\n",
      "         [-1.8449],\n",
      "         [-1.5513],\n",
      "         [-0.4168],\n",
      "         [-2.8309],\n",
      "         [ 1.4381],\n",
      "         [-3.1450],\n",
      "         [ 1.0379],\n",
      "         [-2.0528],\n",
      "         [-0.5071],\n",
      "         [-3.5637],\n",
      "         [-1.7862],\n",
      "         [-2.5238],\n",
      "         [ 2.1306],\n",
      "         [-2.0689],\n",
      "         [-0.1535],\n",
      "         [-0.6929],\n",
      "         [-0.5413],\n",
      "         [-1.5514],\n",
      "         [-3.8593],\n",
      "         [ 1.7544],\n",
      "         [-0.9155],\n",
      "         [-0.5445],\n",
      "         [-0.2166],\n",
      "         [ 1.8134],\n",
      "         [-3.0761],\n",
      "         [-1.7678],\n",
      "         [-1.9391],\n",
      "         [-3.1007],\n",
      "         [-3.3775],\n",
      "         [ 1.3435],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.3871],\n",
      "         [ 0.9175],\n",
      "         [-1.1990],\n",
      "         [-2.8309],\n",
      "         [-0.8025],\n",
      "         [-3.6159],\n",
      "         [ 1.8365],\n",
      "         [-1.4074],\n",
      "         [-2.5488],\n",
      "         [-0.2283],\n",
      "         [-4.0969],\n",
      "         [-4.2499],\n",
      "         [-3.2872],\n",
      "         [ 0.9648],\n",
      "         [-1.5105],\n",
      "         [ 0.6151],\n",
      "         [-1.8992],\n",
      "         [-0.7299],\n",
      "         [ 0.2480],\n",
      "         [ 1.4381],\n",
      "         [-3.6449],\n",
      "         [-2.7894],\n",
      "         [-1.0120],\n",
      "         [-2.7657],\n",
      "         [-0.5709],\n",
      "         [ 3.0670],\n",
      "         [-0.5709],\n",
      "         [-1.8622],\n",
      "         [-2.6794],\n",
      "         [-3.2381],\n",
      "         [-3.5964],\n",
      "         [-0.5459],\n",
      "         [-2.2822],\n",
      "         [-4.3857],\n",
      "         [-1.5271],\n",
      "         [ 0.2207],\n",
      "         [ 0.2126],\n",
      "         [ 1.1888],\n",
      "         [-3.1112],\n",
      "         [-0.6861],\n",
      "         [ 0.2158],\n",
      "         [ 0.0266],\n",
      "         [-1.9391],\n",
      "         [-3.3304],\n",
      "         [-1.0162],\n",
      "         [-0.9339],\n",
      "         [-0.9661],\n",
      "         [-3.2328],\n",
      "         [-2.6869],\n",
      "         [-1.6182],\n",
      "         [-3.3304],\n",
      "         [-0.8699],\n",
      "         [ 0.5838],\n",
      "         [-1.2374],\n",
      "         [-1.5786],\n",
      "         [-0.0900],\n",
      "         [-1.7941],\n",
      "         [-2.4915]],\n",
      "\n",
      "        [[-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.7489],\n",
      "         [-2.7994],\n",
      "         [-2.7237],\n",
      "         [-2.8820],\n",
      "         [-2.1427],\n",
      "         [-3.0403],\n",
      "         [-1.4093],\n",
      "         [ 0.2046],\n",
      "         [-1.8449],\n",
      "         [-1.5513],\n",
      "         [-0.4168],\n",
      "         [-2.8309],\n",
      "         [ 1.4381],\n",
      "         [-3.1450],\n",
      "         [ 1.0379],\n",
      "         [-2.0528],\n",
      "         [-0.5071],\n",
      "         [-3.5637],\n",
      "         [-1.7862],\n",
      "         [-2.5238],\n",
      "         [ 2.1306],\n",
      "         [-2.0689],\n",
      "         [-0.1535],\n",
      "         [-0.6929],\n",
      "         [-0.5413],\n",
      "         [-1.5514],\n",
      "         [-3.8593],\n",
      "         [ 1.7544],\n",
      "         [-0.9155],\n",
      "         [-0.5445],\n",
      "         [-0.2166],\n",
      "         [ 1.8134],\n",
      "         [-3.0761],\n",
      "         [-1.7678],\n",
      "         [-1.9391],\n",
      "         [-3.1007],\n",
      "         [-3.3775],\n",
      "         [ 1.3435],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.3871],\n",
      "         [ 0.9175],\n",
      "         [-1.1990],\n",
      "         [-2.8309],\n",
      "         [-0.8025],\n",
      "         [-3.6159],\n",
      "         [ 1.8365],\n",
      "         [-1.4074],\n",
      "         [-2.5488],\n",
      "         [-0.2283],\n",
      "         [-4.0969],\n",
      "         [-4.2499],\n",
      "         [-3.2872],\n",
      "         [ 0.9648],\n",
      "         [-1.5105],\n",
      "         [ 0.6151],\n",
      "         [-1.8992],\n",
      "         [-0.7299],\n",
      "         [ 0.2480],\n",
      "         [ 1.4381],\n",
      "         [-3.6449],\n",
      "         [-2.7894],\n",
      "         [-1.0120],\n",
      "         [-2.7657],\n",
      "         [-0.5709],\n",
      "         [ 3.0670],\n",
      "         [-0.5709],\n",
      "         [-1.8622],\n",
      "         [-2.6794],\n",
      "         [-3.2381],\n",
      "         [-3.5964],\n",
      "         [-0.5459],\n",
      "         [-2.2822],\n",
      "         [-4.3857],\n",
      "         [-1.5271],\n",
      "         [ 0.2207],\n",
      "         [ 0.2126],\n",
      "         [ 1.1888],\n",
      "         [-3.1112],\n",
      "         [-0.6861],\n",
      "         [ 0.2158],\n",
      "         [ 0.0266],\n",
      "         [-1.9391],\n",
      "         [-3.3304],\n",
      "         [-1.0162],\n",
      "         [-0.9339],\n",
      "         [-0.9661],\n",
      "         [-3.2328],\n",
      "         [-2.6869],\n",
      "         [-1.6182],\n",
      "         [-3.3304],\n",
      "         [-0.8699],\n",
      "         [ 0.5838],\n",
      "         [-1.2374],\n",
      "         [-1.5786],\n",
      "         [-0.0900],\n",
      "         [-1.7941],\n",
      "         [-2.4915]],\n",
      "\n",
      "        [[-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.7489],\n",
      "         [-2.7994],\n",
      "         [-2.7237],\n",
      "         [-2.8820],\n",
      "         [-2.1427],\n",
      "         [-3.0403],\n",
      "         [-1.4093],\n",
      "         [ 0.2046],\n",
      "         [-1.8449],\n",
      "         [-1.5513],\n",
      "         [-0.4168],\n",
      "         [-2.8309],\n",
      "         [ 1.4381],\n",
      "         [-3.1450],\n",
      "         [ 1.0379],\n",
      "         [-2.0528],\n",
      "         [-0.5071],\n",
      "         [-3.5637],\n",
      "         [-1.7862],\n",
      "         [-2.5238],\n",
      "         [ 2.1306],\n",
      "         [-2.0689],\n",
      "         [-0.1535],\n",
      "         [-0.6929],\n",
      "         [-0.5413],\n",
      "         [-1.5514],\n",
      "         [-3.8593],\n",
      "         [ 1.7544],\n",
      "         [-0.9155],\n",
      "         [-0.5445],\n",
      "         [-0.2166],\n",
      "         [ 1.8134],\n",
      "         [-3.0761],\n",
      "         [-1.7678],\n",
      "         [-1.9391],\n",
      "         [-3.1007],\n",
      "         [-3.3775],\n",
      "         [ 1.3435],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.3871],\n",
      "         [ 0.9175],\n",
      "         [-1.1990],\n",
      "         [-2.8309],\n",
      "         [-0.8025],\n",
      "         [-3.6159],\n",
      "         [ 1.8365],\n",
      "         [-1.4074],\n",
      "         [-2.5488],\n",
      "         [-0.2283],\n",
      "         [-4.0969],\n",
      "         [-4.2499],\n",
      "         [-3.2872],\n",
      "         [ 0.9648],\n",
      "         [-1.5105],\n",
      "         [ 0.6151],\n",
      "         [-1.8992],\n",
      "         [-0.7299],\n",
      "         [ 0.2480],\n",
      "         [ 1.4381],\n",
      "         [-3.6449],\n",
      "         [-2.7894],\n",
      "         [-1.0120],\n",
      "         [-2.7657],\n",
      "         [-0.5709],\n",
      "         [ 3.0670],\n",
      "         [-0.5709],\n",
      "         [-1.8622],\n",
      "         [-2.6794],\n",
      "         [-3.2381],\n",
      "         [-3.5964],\n",
      "         [-0.5459],\n",
      "         [-2.2822],\n",
      "         [-4.3857],\n",
      "         [-1.5271],\n",
      "         [ 0.2207],\n",
      "         [ 0.2126],\n",
      "         [ 1.1888],\n",
      "         [-3.1112],\n",
      "         [-0.6861],\n",
      "         [ 0.2158],\n",
      "         [ 0.0266],\n",
      "         [-1.9391],\n",
      "         [-3.3304],\n",
      "         [-1.0162],\n",
      "         [-0.9339],\n",
      "         [-0.9661],\n",
      "         [-3.2328],\n",
      "         [-2.6869],\n",
      "         [-1.6182],\n",
      "         [-3.3304],\n",
      "         [-0.8699],\n",
      "         [ 0.5838],\n",
      "         [-1.2374],\n",
      "         [-1.5786],\n",
      "         [-0.0900],\n",
      "         [-1.7941],\n",
      "         [-2.4915]],\n",
      "\n",
      "        [[-0.5231],\n",
      "         [-1.5513],\n",
      "         [-0.7489],\n",
      "         [-2.7994],\n",
      "         [-2.7237],\n",
      "         [-2.8820],\n",
      "         [-2.1427],\n",
      "         [-3.0403],\n",
      "         [-1.4093],\n",
      "         [ 0.2046],\n",
      "         [-1.8449],\n",
      "         [-1.5513],\n",
      "         [-0.4168],\n",
      "         [-2.8309],\n",
      "         [ 1.4381],\n",
      "         [-3.1450],\n",
      "         [ 1.0379],\n",
      "         [-2.0528],\n",
      "         [-0.5071],\n",
      "         [-3.5637],\n",
      "         [-1.7862],\n",
      "         [-2.5238],\n",
      "         [ 2.1306],\n",
      "         [-2.0689],\n",
      "         [-0.1535],\n",
      "         [-0.6929],\n",
      "         [-0.5413],\n",
      "         [-1.5514],\n",
      "         [-3.8593],\n",
      "         [ 1.7544],\n",
      "         [-0.9155],\n",
      "         [-0.5445],\n",
      "         [-0.2166],\n",
      "         [ 1.8134],\n",
      "         [-3.0761],\n",
      "         [-1.7678],\n",
      "         [-1.9391],\n",
      "         [-3.1007],\n",
      "         [-3.3775],\n",
      "         [ 1.3435],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.3871],\n",
      "         [ 0.9175],\n",
      "         [-1.1990],\n",
      "         [-2.8309],\n",
      "         [-0.8025],\n",
      "         [-3.6159],\n",
      "         [ 1.8365],\n",
      "         [-1.4074],\n",
      "         [-2.5488],\n",
      "         [-0.2283],\n",
      "         [-4.0969],\n",
      "         [-4.2499],\n",
      "         [-3.2872],\n",
      "         [ 0.9648],\n",
      "         [-1.5105],\n",
      "         [ 0.6151],\n",
      "         [-1.8992],\n",
      "         [-0.7299],\n",
      "         [ 0.2480],\n",
      "         [ 1.4381],\n",
      "         [-3.6449],\n",
      "         [-2.7894],\n",
      "         [-1.0120],\n",
      "         [-2.7657],\n",
      "         [-0.5709],\n",
      "         [ 3.0670],\n",
      "         [-0.5709],\n",
      "         [-1.8622],\n",
      "         [-2.6794],\n",
      "         [-3.2381],\n",
      "         [-3.5964],\n",
      "         [-0.5459],\n",
      "         [-2.2822],\n",
      "         [-4.3857],\n",
      "         [-1.5271],\n",
      "         [ 0.2207],\n",
      "         [ 0.2126],\n",
      "         [ 1.1888],\n",
      "         [-3.1112],\n",
      "         [-0.6861],\n",
      "         [ 0.2158],\n",
      "         [ 0.0266],\n",
      "         [-1.9391],\n",
      "         [-3.3304],\n",
      "         [-1.0162],\n",
      "         [-0.9339],\n",
      "         [-0.9661],\n",
      "         [-3.2328],\n",
      "         [-2.6869],\n",
      "         [-1.6182],\n",
      "         [-3.3304],\n",
      "         [-0.8699],\n",
      "         [ 0.5838],\n",
      "         [-1.2374],\n",
      "         [-1.5786],\n",
      "         [-0.0900],\n",
      "         [-1.7941],\n",
      "         [-2.4915]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([8, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([8, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/12/2023 06:40:45 AM WARNING 36398 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:47 AM WARNING 36398 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:47 AM WARNING 36398 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:47 AM WARNING 36398 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:47 AM WARNING 36398 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:47 AM WARNING 36398 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:47 AM WARNING 36398 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:47 AM WARNING 36398 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:47 AM WARNING 36398 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-2.1843],\n",
      "         [ 0.2960],\n",
      "         [-1.2366],\n",
      "         [-3.8482],\n",
      "         [-0.9900],\n",
      "         [-0.5445],\n",
      "         [-3.1589],\n",
      "         [ 1.0348],\n",
      "         [-3.5930],\n",
      "         [-0.2443],\n",
      "         [-1.4032],\n",
      "         [-0.4616],\n",
      "         [ 0.5075],\n",
      "         [-2.7533],\n",
      "         [-2.0439],\n",
      "         [-2.0175],\n",
      "         [-2.4687],\n",
      "         [-1.4890],\n",
      "         [-1.7728],\n",
      "         [-2.7092],\n",
      "         [-1.1757],\n",
      "         [-3.5692],\n",
      "         [-2.4353],\n",
      "         [-2.4068],\n",
      "         [-0.3075],\n",
      "         [-0.3596],\n",
      "         [-1.7533],\n",
      "         [ 0.6629],\n",
      "         [-4.0188],\n",
      "         [ 2.1306],\n",
      "         [ 0.2117],\n",
      "         [-2.1862],\n",
      "         [-2.3422],\n",
      "         [-2.6509],\n",
      "         [-1.1435],\n",
      "         [-0.8315],\n",
      "         [-2.6332],\n",
      "         [-1.7654],\n",
      "         [ 0.9255],\n",
      "         [ 0.3642],\n",
      "         [-1.1435],\n",
      "         [-2.5310],\n",
      "         [-1.2659],\n",
      "         [-1.6448],\n",
      "         [-2.7542],\n",
      "         [-4.7241],\n",
      "         [-1.2374],\n",
      "         [ 2.4899],\n",
      "         [-1.6553],\n",
      "         [-1.0715],\n",
      "         [-4.5842],\n",
      "         [-2.0830],\n",
      "         [-0.7247],\n",
      "         [-0.1525],\n",
      "         [ 0.2034],\n",
      "         [-2.2677],\n",
      "         [-1.7816],\n",
      "         [ 0.8861],\n",
      "         [-3.4242],\n",
      "         [-1.9372],\n",
      "         [ 1.3635],\n",
      "         [-2.4056],\n",
      "         [-2.4386],\n",
      "         [-3.3392],\n",
      "         [-2.5238],\n",
      "         [-1.2374],\n",
      "         [-2.7648],\n",
      "         [-1.5686],\n",
      "         [-0.4616],\n",
      "         [-1.5982],\n",
      "         [-1.8003],\n",
      "         [ 0.4321],\n",
      "         [-0.7045],\n",
      "         [-3.3636],\n",
      "         [ 1.0710],\n",
      "         [-2.1862],\n",
      "         [-3.1821],\n",
      "         [ 0.4706],\n",
      "         [ 1.0591],\n",
      "         [-3.5763],\n",
      "         [-0.1721],\n",
      "         [-0.6039],\n",
      "         [-4.1203],\n",
      "         [-2.9439],\n",
      "         [-1.5113],\n",
      "         [-0.8315],\n",
      "         [-2.7648],\n",
      "         [-0.4814],\n",
      "         [ 0.6204],\n",
      "         [-3.2080],\n",
      "         [-1.5928],\n",
      "         [ 2.5423],\n",
      "         [-1.7489],\n",
      "         [-0.4675],\n",
      "         [-1.7962],\n",
      "         [ 0.4360],\n",
      "         [-1.7221],\n",
      "         [ 1.3754],\n",
      "         [-2.0689],\n",
      "         [-2.7092]],\n",
      "\n",
      "        [[-2.1843],\n",
      "         [ 0.2960],\n",
      "         [-1.2366],\n",
      "         [-3.8482],\n",
      "         [-0.9900],\n",
      "         [-0.5445],\n",
      "         [-3.1589],\n",
      "         [ 1.0348],\n",
      "         [-3.5930],\n",
      "         [-0.2443],\n",
      "         [-1.4032],\n",
      "         [-0.4616],\n",
      "         [ 0.5075],\n",
      "         [-2.7533],\n",
      "         [-2.0439],\n",
      "         [-2.0175],\n",
      "         [-2.4687],\n",
      "         [-1.4890],\n",
      "         [-1.7728],\n",
      "         [-2.7092],\n",
      "         [-1.1757],\n",
      "         [-3.5692],\n",
      "         [-2.4353],\n",
      "         [-2.4068],\n",
      "         [-0.3075],\n",
      "         [-0.3596],\n",
      "         [-1.7533],\n",
      "         [ 0.6629],\n",
      "         [-4.0188],\n",
      "         [ 2.1306],\n",
      "         [ 0.2117],\n",
      "         [-2.1862],\n",
      "         [-2.3422],\n",
      "         [-2.6509],\n",
      "         [-1.1435],\n",
      "         [-0.8315],\n",
      "         [-2.6332],\n",
      "         [-1.7654],\n",
      "         [ 0.9255],\n",
      "         [ 0.3642],\n",
      "         [-1.1435],\n",
      "         [-2.5310],\n",
      "         [-1.2659],\n",
      "         [-1.6448],\n",
      "         [-2.7542],\n",
      "         [-4.7241],\n",
      "         [-1.2374],\n",
      "         [ 2.4899],\n",
      "         [-1.6553],\n",
      "         [-1.0715],\n",
      "         [-4.5842],\n",
      "         [-2.0830],\n",
      "         [-0.7247],\n",
      "         [-0.1525],\n",
      "         [ 0.2034],\n",
      "         [-2.2677],\n",
      "         [-1.7816],\n",
      "         [ 0.8861],\n",
      "         [-3.4242],\n",
      "         [-1.9372],\n",
      "         [ 1.3635],\n",
      "         [-2.4056],\n",
      "         [-2.4386],\n",
      "         [-3.3392],\n",
      "         [-2.5238],\n",
      "         [-1.2374],\n",
      "         [-2.7648],\n",
      "         [-1.5686],\n",
      "         [-0.4616],\n",
      "         [-1.5982],\n",
      "         [-1.8003],\n",
      "         [ 0.4321],\n",
      "         [-0.7045],\n",
      "         [-3.3636],\n",
      "         [ 1.0710],\n",
      "         [-2.1862],\n",
      "         [-3.1821],\n",
      "         [ 0.4706],\n",
      "         [ 1.0591],\n",
      "         [-3.5763],\n",
      "         [-0.1721],\n",
      "         [-0.6039],\n",
      "         [-4.1203],\n",
      "         [-2.9439],\n",
      "         [-1.5113],\n",
      "         [-0.8315],\n",
      "         [-2.7648],\n",
      "         [-0.4814],\n",
      "         [ 0.6204],\n",
      "         [-3.2080],\n",
      "         [-1.5928],\n",
      "         [ 2.5423],\n",
      "         [-1.7489],\n",
      "         [-0.4675],\n",
      "         [-1.7962],\n",
      "         [ 0.4360],\n",
      "         [-1.7221],\n",
      "         [ 1.3754],\n",
      "         [-2.0689],\n",
      "         [-2.7092]],\n",
      "\n",
      "        [[-2.1843],\n",
      "         [ 0.2960],\n",
      "         [-1.2366],\n",
      "         [-3.8482],\n",
      "         [-0.9900],\n",
      "         [-0.5445],\n",
      "         [-3.1589],\n",
      "         [ 1.0348],\n",
      "         [-3.5930],\n",
      "         [-0.2443],\n",
      "         [-1.4032],\n",
      "         [-0.4616],\n",
      "         [ 0.5075],\n",
      "         [-2.7533],\n",
      "         [-2.0439],\n",
      "         [-2.0175],\n",
      "         [-2.4687],\n",
      "         [-1.4890],\n",
      "         [-1.7728],\n",
      "         [-2.7092],\n",
      "         [-1.1757],\n",
      "         [-3.5692],\n",
      "         [-2.4353],\n",
      "         [-2.4068],\n",
      "         [-0.3075],\n",
      "         [-0.3596],\n",
      "         [-1.7533],\n",
      "         [ 0.6629],\n",
      "         [-4.0188],\n",
      "         [ 2.1306],\n",
      "         [ 0.2117],\n",
      "         [-2.1862],\n",
      "         [-2.3422],\n",
      "         [-2.6509],\n",
      "         [-1.1435],\n",
      "         [-0.8315],\n",
      "         [-2.6332],\n",
      "         [-1.7654],\n",
      "         [ 0.9255],\n",
      "         [ 0.3642],\n",
      "         [-1.1435],\n",
      "         [-2.5310],\n",
      "         [-1.2659],\n",
      "         [-1.6448],\n",
      "         [-2.7542],\n",
      "         [-4.7241],\n",
      "         [-1.2374],\n",
      "         [ 2.4899],\n",
      "         [-1.6553],\n",
      "         [-1.0715],\n",
      "         [-4.5842],\n",
      "         [-2.0830],\n",
      "         [-0.7247],\n",
      "         [-0.1525],\n",
      "         [ 0.2034],\n",
      "         [-2.2677],\n",
      "         [-1.7816],\n",
      "         [ 0.8861],\n",
      "         [-3.4242],\n",
      "         [-1.9372],\n",
      "         [ 1.3635],\n",
      "         [-2.4056],\n",
      "         [-2.4386],\n",
      "         [-3.3392],\n",
      "         [-2.5238],\n",
      "         [-1.2374],\n",
      "         [-2.7648],\n",
      "         [-1.5686],\n",
      "         [-0.4616],\n",
      "         [-1.5982],\n",
      "         [-1.8003],\n",
      "         [ 0.4321],\n",
      "         [-0.7045],\n",
      "         [-3.3636],\n",
      "         [ 1.0710],\n",
      "         [-2.1862],\n",
      "         [-3.1821],\n",
      "         [ 0.4706],\n",
      "         [ 1.0591],\n",
      "         [-3.5763],\n",
      "         [-0.1721],\n",
      "         [-0.6039],\n",
      "         [-4.1203],\n",
      "         [-2.9439],\n",
      "         [-1.5113],\n",
      "         [-0.8315],\n",
      "         [-2.7648],\n",
      "         [-0.4814],\n",
      "         [ 0.6204],\n",
      "         [-3.2080],\n",
      "         [-1.5928],\n",
      "         [ 2.5423],\n",
      "         [-1.7489],\n",
      "         [-0.4675],\n",
      "         [-1.7962],\n",
      "         [ 0.4360],\n",
      "         [-1.7221],\n",
      "         [ 1.3754],\n",
      "         [-2.0689],\n",
      "         [-2.7092]],\n",
      "\n",
      "        [[-2.1843],\n",
      "         [ 0.2960],\n",
      "         [-1.2366],\n",
      "         [-3.8482],\n",
      "         [-0.9900],\n",
      "         [-0.5445],\n",
      "         [-3.1589],\n",
      "         [ 1.0348],\n",
      "         [-3.5930],\n",
      "         [-0.2443],\n",
      "         [-1.4032],\n",
      "         [-0.4616],\n",
      "         [ 0.5075],\n",
      "         [-2.7533],\n",
      "         [-2.0439],\n",
      "         [-2.0175],\n",
      "         [-2.4687],\n",
      "         [-1.4890],\n",
      "         [-1.7728],\n",
      "         [-2.7092],\n",
      "         [-1.1757],\n",
      "         [-3.5692],\n",
      "         [-2.4353],\n",
      "         [-2.4068],\n",
      "         [-0.3075],\n",
      "         [-0.3596],\n",
      "         [-1.7533],\n",
      "         [ 0.6629],\n",
      "         [-4.0188],\n",
      "         [ 2.1306],\n",
      "         [ 0.2117],\n",
      "         [-2.1862],\n",
      "         [-2.3422],\n",
      "         [-2.6509],\n",
      "         [-1.1435],\n",
      "         [-0.8315],\n",
      "         [-2.6332],\n",
      "         [-1.7654],\n",
      "         [ 0.9255],\n",
      "         [ 0.3642],\n",
      "         [-1.1435],\n",
      "         [-2.5310],\n",
      "         [-1.2659],\n",
      "         [-1.6448],\n",
      "         [-2.7542],\n",
      "         [-4.7241],\n",
      "         [-1.2374],\n",
      "         [ 2.4899],\n",
      "         [-1.6553],\n",
      "         [-1.0715],\n",
      "         [-4.5842],\n",
      "         [-2.0830],\n",
      "         [-0.7247],\n",
      "         [-0.1525],\n",
      "         [ 0.2034],\n",
      "         [-2.2677],\n",
      "         [-1.7816],\n",
      "         [ 0.8861],\n",
      "         [-3.4242],\n",
      "         [-1.9372],\n",
      "         [ 1.3635],\n",
      "         [-2.4056],\n",
      "         [-2.4386],\n",
      "         [-3.3392],\n",
      "         [-2.5238],\n",
      "         [-1.2374],\n",
      "         [-2.7648],\n",
      "         [-1.5686],\n",
      "         [-0.4616],\n",
      "         [-1.5982],\n",
      "         [-1.8003],\n",
      "         [ 0.4321],\n",
      "         [-0.7045],\n",
      "         [-3.3636],\n",
      "         [ 1.0710],\n",
      "         [-2.1862],\n",
      "         [-3.1821],\n",
      "         [ 0.4706],\n",
      "         [ 1.0591],\n",
      "         [-3.5763],\n",
      "         [-0.1721],\n",
      "         [-0.6039],\n",
      "         [-4.1203],\n",
      "         [-2.9439],\n",
      "         [-1.5113],\n",
      "         [-0.8315],\n",
      "         [-2.7648],\n",
      "         [-0.4814],\n",
      "         [ 0.6204],\n",
      "         [-3.2080],\n",
      "         [-1.5928],\n",
      "         [ 2.5423],\n",
      "         [-1.7489],\n",
      "         [-0.4675],\n",
      "         [-1.7962],\n",
      "         [ 0.4360],\n",
      "         [-1.7221],\n",
      "         [ 1.3754],\n",
      "         [-2.0689],\n",
      "         [-2.7092]],\n",
      "\n",
      "        [[-2.1843],\n",
      "         [ 0.2960],\n",
      "         [-1.2366],\n",
      "         [-3.8482],\n",
      "         [-0.9900],\n",
      "         [-0.5445],\n",
      "         [-3.1589],\n",
      "         [ 1.0348],\n",
      "         [-3.5930],\n",
      "         [-0.2443],\n",
      "         [-1.4032],\n",
      "         [-0.4616],\n",
      "         [ 0.5075],\n",
      "         [-2.7533],\n",
      "         [-2.0439],\n",
      "         [-2.0175],\n",
      "         [-2.4687],\n",
      "         [-1.4890],\n",
      "         [-1.7728],\n",
      "         [-2.7092],\n",
      "         [-1.1757],\n",
      "         [-3.5692],\n",
      "         [-2.4353],\n",
      "         [-2.4068],\n",
      "         [-0.3075],\n",
      "         [-0.3596],\n",
      "         [-1.7533],\n",
      "         [ 0.6629],\n",
      "         [-4.0188],\n",
      "         [ 2.1306],\n",
      "         [ 0.2117],\n",
      "         [-2.1862],\n",
      "         [-2.3422],\n",
      "         [-2.6509],\n",
      "         [-1.1435],\n",
      "         [-0.8315],\n",
      "         [-2.6332],\n",
      "         [-1.7654],\n",
      "         [ 0.9255],\n",
      "         [ 0.3642],\n",
      "         [-1.1435],\n",
      "         [-2.5310],\n",
      "         [-1.2659],\n",
      "         [-1.6448],\n",
      "         [-2.7542],\n",
      "         [-4.7241],\n",
      "         [-1.2374],\n",
      "         [ 2.4899],\n",
      "         [-1.6553],\n",
      "         [-1.0715],\n",
      "         [-4.5842],\n",
      "         [-2.0830],\n",
      "         [-0.7247],\n",
      "         [-0.1525],\n",
      "         [ 0.2034],\n",
      "         [-2.2677],\n",
      "         [-1.7816],\n",
      "         [ 0.8861],\n",
      "         [-3.4242],\n",
      "         [-1.9372],\n",
      "         [ 1.3635],\n",
      "         [-2.4056],\n",
      "         [-2.4386],\n",
      "         [-3.3392],\n",
      "         [-2.5238],\n",
      "         [-1.2374],\n",
      "         [-2.7648],\n",
      "         [-1.5686],\n",
      "         [-0.4616],\n",
      "         [-1.5982],\n",
      "         [-1.8003],\n",
      "         [ 0.4321],\n",
      "         [-0.7045],\n",
      "         [-3.3636],\n",
      "         [ 1.0710],\n",
      "         [-2.1862],\n",
      "         [-3.1821],\n",
      "         [ 0.4706],\n",
      "         [ 1.0591],\n",
      "         [-3.5763],\n",
      "         [-0.1721],\n",
      "         [-0.6039],\n",
      "         [-4.1203],\n",
      "         [-2.9439],\n",
      "         [-1.5113],\n",
      "         [-0.8315],\n",
      "         [-2.7648],\n",
      "         [-0.4814],\n",
      "         [ 0.6204],\n",
      "         [-3.2080],\n",
      "         [-1.5928],\n",
      "         [ 2.5423],\n",
      "         [-1.7489],\n",
      "         [-0.4675],\n",
      "         [-1.7962],\n",
      "         [ 0.4360],\n",
      "         [-1.7221],\n",
      "         [ 1.3754],\n",
      "         [-2.0689],\n",
      "         [-2.7092]],\n",
      "\n",
      "        [[-2.1843],\n",
      "         [ 0.2960],\n",
      "         [-1.2366],\n",
      "         [-3.8482],\n",
      "         [-0.9900],\n",
      "         [-0.5445],\n",
      "         [-3.1589],\n",
      "         [ 1.0348],\n",
      "         [-3.5930],\n",
      "         [-0.2443],\n",
      "         [-1.4032],\n",
      "         [-0.4616],\n",
      "         [ 0.5075],\n",
      "         [-2.7533],\n",
      "         [-2.0439],\n",
      "         [-2.0175],\n",
      "         [-2.4687],\n",
      "         [-1.4890],\n",
      "         [-1.7728],\n",
      "         [-2.7092],\n",
      "         [-1.1757],\n",
      "         [-3.5692],\n",
      "         [-2.4353],\n",
      "         [-2.4068],\n",
      "         [-0.3075],\n",
      "         [-0.3596],\n",
      "         [-1.7533],\n",
      "         [ 0.6629],\n",
      "         [-4.0188],\n",
      "         [ 2.1306],\n",
      "         [ 0.2117],\n",
      "         [-2.1862],\n",
      "         [-2.3422],\n",
      "         [-2.6509],\n",
      "         [-1.1435],\n",
      "         [-0.8315],\n",
      "         [-2.6332],\n",
      "         [-1.7654],\n",
      "         [ 0.9255],\n",
      "         [ 0.3642],\n",
      "         [-1.1435],\n",
      "         [-2.5310],\n",
      "         [-1.2659],\n",
      "         [-1.6448],\n",
      "         [-2.7542],\n",
      "         [-4.7241],\n",
      "         [-1.2374],\n",
      "         [ 2.4899],\n",
      "         [-1.6553],\n",
      "         [-1.0715],\n",
      "         [-4.5842],\n",
      "         [-2.0830],\n",
      "         [-0.7247],\n",
      "         [-0.1525],\n",
      "         [ 0.2034],\n",
      "         [-2.2677],\n",
      "         [-1.7816],\n",
      "         [ 0.8861],\n",
      "         [-3.4242],\n",
      "         [-1.9372],\n",
      "         [ 1.3635],\n",
      "         [-2.4056],\n",
      "         [-2.4386],\n",
      "         [-3.3392],\n",
      "         [-2.5238],\n",
      "         [-1.2374],\n",
      "         [-2.7648],\n",
      "         [-1.5686],\n",
      "         [-0.4616],\n",
      "         [-1.5982],\n",
      "         [-1.8003],\n",
      "         [ 0.4321],\n",
      "         [-0.7045],\n",
      "         [-3.3636],\n",
      "         [ 1.0710],\n",
      "         [-2.1862],\n",
      "         [-3.1821],\n",
      "         [ 0.4706],\n",
      "         [ 1.0591],\n",
      "         [-3.5763],\n",
      "         [-0.1721],\n",
      "         [-0.6039],\n",
      "         [-4.1203],\n",
      "         [-2.9439],\n",
      "         [-1.5113],\n",
      "         [-0.8315],\n",
      "         [-2.7648],\n",
      "         [-0.4814],\n",
      "         [ 0.6204],\n",
      "         [-3.2080],\n",
      "         [-1.5928],\n",
      "         [ 2.5423],\n",
      "         [-1.7489],\n",
      "         [-0.4675],\n",
      "         [-1.7962],\n",
      "         [ 0.4360],\n",
      "         [-1.7221],\n",
      "         [ 1.3754],\n",
      "         [-2.0689],\n",
      "         [-2.7092]],\n",
      "\n",
      "        [[-2.1843],\n",
      "         [ 0.2960],\n",
      "         [-1.2366],\n",
      "         [-3.8482],\n",
      "         [-0.9900],\n",
      "         [-0.5445],\n",
      "         [-3.1589],\n",
      "         [ 1.0348],\n",
      "         [-3.5930],\n",
      "         [-0.2443],\n",
      "         [-1.4032],\n",
      "         [-0.4616],\n",
      "         [ 0.5075],\n",
      "         [-2.7533],\n",
      "         [-2.0439],\n",
      "         [-2.0175],\n",
      "         [-2.4687],\n",
      "         [-1.4890],\n",
      "         [-1.7728],\n",
      "         [-2.7092],\n",
      "         [-1.1757],\n",
      "         [-3.5692],\n",
      "         [-2.4353],\n",
      "         [-2.4068],\n",
      "         [-0.3075],\n",
      "         [-0.3596],\n",
      "         [-1.7533],\n",
      "         [ 0.6629],\n",
      "         [-4.0188],\n",
      "         [ 2.1306],\n",
      "         [ 0.2117],\n",
      "         [-2.1862],\n",
      "         [-2.3422],\n",
      "         [-2.6509],\n",
      "         [-1.1435],\n",
      "         [-0.8315],\n",
      "         [-2.6332],\n",
      "         [-1.7654],\n",
      "         [ 0.9255],\n",
      "         [ 0.3642],\n",
      "         [-1.1435],\n",
      "         [-2.5310],\n",
      "         [-1.2659],\n",
      "         [-1.6448],\n",
      "         [-2.7542],\n",
      "         [-4.7241],\n",
      "         [-1.2374],\n",
      "         [ 2.4899],\n",
      "         [-1.6553],\n",
      "         [-1.0715],\n",
      "         [-4.5842],\n",
      "         [-2.0830],\n",
      "         [-0.7247],\n",
      "         [-0.1525],\n",
      "         [ 0.2034],\n",
      "         [-2.2677],\n",
      "         [-1.7816],\n",
      "         [ 0.8861],\n",
      "         [-3.4242],\n",
      "         [-1.9372],\n",
      "         [ 1.3635],\n",
      "         [-2.4056],\n",
      "         [-2.4386],\n",
      "         [-3.3392],\n",
      "         [-2.5238],\n",
      "         [-1.2374],\n",
      "         [-2.7648],\n",
      "         [-1.5686],\n",
      "         [-0.4616],\n",
      "         [-1.5982],\n",
      "         [-1.8003],\n",
      "         [ 0.4321],\n",
      "         [-0.7045],\n",
      "         [-3.3636],\n",
      "         [ 1.0710],\n",
      "         [-2.1862],\n",
      "         [-3.1821],\n",
      "         [ 0.4706],\n",
      "         [ 1.0591],\n",
      "         [-3.5763],\n",
      "         [-0.1721],\n",
      "         [-0.6039],\n",
      "         [-4.1203],\n",
      "         [-2.9439],\n",
      "         [-1.5113],\n",
      "         [-0.8315],\n",
      "         [-2.7648],\n",
      "         [-0.4814],\n",
      "         [ 0.6204],\n",
      "         [-3.2080],\n",
      "         [-1.5928],\n",
      "         [ 2.5423],\n",
      "         [-1.7489],\n",
      "         [-0.4675],\n",
      "         [-1.7962],\n",
      "         [ 0.4360],\n",
      "         [-1.7221],\n",
      "         [ 1.3754],\n",
      "         [-2.0689],\n",
      "         [-2.7092]],\n",
      "\n",
      "        [[-2.1843],\n",
      "         [ 0.2960],\n",
      "         [-1.2366],\n",
      "         [-3.8482],\n",
      "         [-0.9900],\n",
      "         [-0.5445],\n",
      "         [-3.1589],\n",
      "         [ 1.0348],\n",
      "         [-3.5930],\n",
      "         [-0.2443],\n",
      "         [-1.4032],\n",
      "         [-0.4616],\n",
      "         [ 0.5075],\n",
      "         [-2.7533],\n",
      "         [-2.0439],\n",
      "         [-2.0175],\n",
      "         [-2.4687],\n",
      "         [-1.4890],\n",
      "         [-1.7728],\n",
      "         [-2.7092],\n",
      "         [-1.1757],\n",
      "         [-3.5692],\n",
      "         [-2.4353],\n",
      "         [-2.4068],\n",
      "         [-0.3075],\n",
      "         [-0.3596],\n",
      "         [-1.7533],\n",
      "         [ 0.6629],\n",
      "         [-4.0188],\n",
      "         [ 2.1306],\n",
      "         [ 0.2117],\n",
      "         [-2.1862],\n",
      "         [-2.3422],\n",
      "         [-2.6509],\n",
      "         [-1.1435],\n",
      "         [-0.8315],\n",
      "         [-2.6332],\n",
      "         [-1.7654],\n",
      "         [ 0.9255],\n",
      "         [ 0.3642],\n",
      "         [-1.1435],\n",
      "         [-2.5310],\n",
      "         [-1.2659],\n",
      "         [-1.6448],\n",
      "         [-2.7542],\n",
      "         [-4.7241],\n",
      "         [-1.2374],\n",
      "         [ 2.4899],\n",
      "         [-1.6553],\n",
      "         [-1.0715],\n",
      "         [-4.5842],\n",
      "         [-2.0830],\n",
      "         [-0.7247],\n",
      "         [-0.1525],\n",
      "         [ 0.2034],\n",
      "         [-2.2677],\n",
      "         [-1.7816],\n",
      "         [ 0.8861],\n",
      "         [-3.4242],\n",
      "         [-1.9372],\n",
      "         [ 1.3635],\n",
      "         [-2.4056],\n",
      "         [-2.4386],\n",
      "         [-3.3392],\n",
      "         [-2.5238],\n",
      "         [-1.2374],\n",
      "         [-2.7648],\n",
      "         [-1.5686],\n",
      "         [-0.4616],\n",
      "         [-1.5982],\n",
      "         [-1.8003],\n",
      "         [ 0.4321],\n",
      "         [-0.7045],\n",
      "         [-3.3636],\n",
      "         [ 1.0710],\n",
      "         [-2.1862],\n",
      "         [-3.1821],\n",
      "         [ 0.4706],\n",
      "         [ 1.0591],\n",
      "         [-3.5763],\n",
      "         [-0.1721],\n",
      "         [-0.6039],\n",
      "         [-4.1203],\n",
      "         [-2.9439],\n",
      "         [-1.5113],\n",
      "         [-0.8315],\n",
      "         [-2.7648],\n",
      "         [-0.4814],\n",
      "         [ 0.6204],\n",
      "         [-3.2080],\n",
      "         [-1.5928],\n",
      "         [ 2.5423],\n",
      "         [-1.7489],\n",
      "         [-0.4675],\n",
      "         [-1.7962],\n",
      "         [ 0.4360],\n",
      "         [-1.7221],\n",
      "         [ 1.3754],\n",
      "         [-2.0689],\n",
      "         [-2.7092]],\n",
      "\n",
      "        [[-2.1843],\n",
      "         [ 0.2960],\n",
      "         [-1.2366],\n",
      "         [-3.8482],\n",
      "         [-0.9900],\n",
      "         [-0.5445],\n",
      "         [-3.1589],\n",
      "         [ 1.0348],\n",
      "         [-3.5930],\n",
      "         [-0.2443],\n",
      "         [-1.4032],\n",
      "         [-0.4616],\n",
      "         [ 0.5075],\n",
      "         [-2.7533],\n",
      "         [-2.0439],\n",
      "         [-2.0175],\n",
      "         [-2.4687],\n",
      "         [-1.4890],\n",
      "         [-1.7728],\n",
      "         [-2.7092],\n",
      "         [-1.1757],\n",
      "         [-3.5692],\n",
      "         [-2.4353],\n",
      "         [-2.4068],\n",
      "         [-0.3075],\n",
      "         [-0.3596],\n",
      "         [-1.7533],\n",
      "         [ 0.6629],\n",
      "         [-4.0188],\n",
      "         [ 2.1306],\n",
      "         [ 0.2117],\n",
      "         [-2.1862],\n",
      "         [-2.3422],\n",
      "         [-2.6509],\n",
      "         [-1.1435],\n",
      "         [-0.8315],\n",
      "         [-2.6332],\n",
      "         [-1.7654],\n",
      "         [ 0.9255],\n",
      "         [ 0.3642],\n",
      "         [-1.1435],\n",
      "         [-2.5310],\n",
      "         [-1.2659],\n",
      "         [-1.6448],\n",
      "         [-2.7542],\n",
      "         [-4.7241],\n",
      "         [-1.2374],\n",
      "         [ 2.4899],\n",
      "         [-1.6553],\n",
      "         [-1.0715],\n",
      "         [-4.5842],\n",
      "         [-2.0830],\n",
      "         [-0.7247],\n",
      "         [-0.1525],\n",
      "         [ 0.2034],\n",
      "         [-2.2677],\n",
      "         [-1.7816],\n",
      "         [ 0.8861],\n",
      "         [-3.4242],\n",
      "         [-1.9372],\n",
      "         [ 1.3635],\n",
      "         [-2.4056],\n",
      "         [-2.4386],\n",
      "         [-3.3392],\n",
      "         [-2.5238],\n",
      "         [-1.2374],\n",
      "         [-2.7648],\n",
      "         [-1.5686],\n",
      "         [-0.4616],\n",
      "         [-1.5982],\n",
      "         [-1.8003],\n",
      "         [ 0.4321],\n",
      "         [-0.7045],\n",
      "         [-3.3636],\n",
      "         [ 1.0710],\n",
      "         [-2.1862],\n",
      "         [-3.1821],\n",
      "         [ 0.4706],\n",
      "         [ 1.0591],\n",
      "         [-3.5763],\n",
      "         [-0.1721],\n",
      "         [-0.6039],\n",
      "         [-4.1203],\n",
      "         [-2.9439],\n",
      "         [-1.5113],\n",
      "         [-0.8315],\n",
      "         [-2.7648],\n",
      "         [-0.4814],\n",
      "         [ 0.6204],\n",
      "         [-3.2080],\n",
      "         [-1.5928],\n",
      "         [ 2.5423],\n",
      "         [-1.7489],\n",
      "         [-0.4675],\n",
      "         [-1.7962],\n",
      "         [ 0.4360],\n",
      "         [-1.7221],\n",
      "         [ 1.3754],\n",
      "         [-2.0689],\n",
      "         [-2.7092]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([9, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([9, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/12/2023 06:40:48 AM WARNING 36444 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:51 AM WARNING 36444 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:51 AM WARNING 36444 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:51 AM WARNING 36444 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:51 AM WARNING 36444 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:51 AM WARNING 36444 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:51 AM WARNING 36444 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:51 AM WARNING 36444 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:51 AM WARNING 36444 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[ 1.4956],\n",
      "         [-3.1032],\n",
      "         [-3.7962],\n",
      "         [-1.7941],\n",
      "         [-2.6515],\n",
      "         [-0.7275],\n",
      "         [-0.5078],\n",
      "         [-2.7542],\n",
      "         [ 0.6848],\n",
      "         [-0.5196],\n",
      "         [-1.2194],\n",
      "         [-0.6477],\n",
      "         [-4.0309],\n",
      "         [-3.2231],\n",
      "         [ 0.3195],\n",
      "         [-2.9161],\n",
      "         [-1.2807],\n",
      "         [-3.0826],\n",
      "         [-1.7549],\n",
      "         [-3.4934],\n",
      "         [-4.5206],\n",
      "         [-2.7994],\n",
      "         [-2.1908],\n",
      "         [ 0.4353],\n",
      "         [-0.5231],\n",
      "         [-1.3301],\n",
      "         [-2.0933],\n",
      "         [-0.2389],\n",
      "         [-1.8741],\n",
      "         [ 2.2073],\n",
      "         [-1.1898],\n",
      "         [-4.5842],\n",
      "         [-0.0635],\n",
      "         [-2.5310],\n",
      "         [-0.5940],\n",
      "         [-2.5539],\n",
      "         [-3.2271],\n",
      "         [-1.5556],\n",
      "         [-1.8003],\n",
      "         [ 1.3003],\n",
      "         [-1.3663],\n",
      "         [-1.8576],\n",
      "         [ 2.2073],\n",
      "         [ 1.0348],\n",
      "         [-1.0946],\n",
      "         [-2.7866],\n",
      "         [ 1.3635],\n",
      "         [-2.7542],\n",
      "         [-3.3392],\n",
      "         [-1.7221],\n",
      "         [-3.5930],\n",
      "         [-3.2165],\n",
      "         [-1.8002],\n",
      "         [-1.8651],\n",
      "         [ 0.9648],\n",
      "         [ 1.3635],\n",
      "         [-1.2729],\n",
      "         [-3.5272],\n",
      "         [-1.2119],\n",
      "         [-0.8120],\n",
      "         [-2.4056],\n",
      "         [-2.7608],\n",
      "         [-1.1328],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.9201],\n",
      "         [-2.1106],\n",
      "         [-3.0158],\n",
      "         [-0.7956],\n",
      "         [ 1.3635],\n",
      "         [-1.0002],\n",
      "         [-0.5078],\n",
      "         [-1.4877],\n",
      "         [-2.1908],\n",
      "         [-2.3055],\n",
      "         [-0.0657],\n",
      "         [ 1.3407],\n",
      "         [-0.5333],\n",
      "         [-1.8741],\n",
      "         [-1.1287],\n",
      "         [-1.4741],\n",
      "         [ 0.9255],\n",
      "         [ 1.0635],\n",
      "         [-0.0950],\n",
      "         [-2.7245],\n",
      "         [-3.8251],\n",
      "         [ 0.2480],\n",
      "         [-1.7962],\n",
      "         [-0.4970],\n",
      "         [-2.2033],\n",
      "         [-1.4297],\n",
      "         [-0.4616],\n",
      "         [-1.4192],\n",
      "         [-0.9740],\n",
      "         [-3.5567],\n",
      "         [ 1.1168],\n",
      "         [-1.4985],\n",
      "         [-2.4721],\n",
      "         [-0.7425],\n",
      "         [-2.7658]],\n",
      "\n",
      "        [[ 1.4956],\n",
      "         [-3.1032],\n",
      "         [-3.7962],\n",
      "         [-1.7941],\n",
      "         [-2.6515],\n",
      "         [-0.7275],\n",
      "         [-0.5078],\n",
      "         [-2.7542],\n",
      "         [ 0.6848],\n",
      "         [-0.5196],\n",
      "         [-1.2194],\n",
      "         [-0.6477],\n",
      "         [-4.0309],\n",
      "         [-3.2231],\n",
      "         [ 0.3195],\n",
      "         [-2.9161],\n",
      "         [-1.2807],\n",
      "         [-3.0826],\n",
      "         [-1.7549],\n",
      "         [-3.4934],\n",
      "         [-4.5206],\n",
      "         [-2.7994],\n",
      "         [-2.1908],\n",
      "         [ 0.4353],\n",
      "         [-0.5231],\n",
      "         [-1.3301],\n",
      "         [-2.0933],\n",
      "         [-0.2389],\n",
      "         [-1.8741],\n",
      "         [ 2.2073],\n",
      "         [-1.1898],\n",
      "         [-4.5842],\n",
      "         [-0.0635],\n",
      "         [-2.5310],\n",
      "         [-0.5940],\n",
      "         [-2.5539],\n",
      "         [-3.2271],\n",
      "         [-1.5556],\n",
      "         [-1.8003],\n",
      "         [ 1.3003],\n",
      "         [-1.3663],\n",
      "         [-1.8576],\n",
      "         [ 2.2073],\n",
      "         [ 1.0348],\n",
      "         [-1.0946],\n",
      "         [-2.7866],\n",
      "         [ 1.3635],\n",
      "         [-2.7542],\n",
      "         [-3.3392],\n",
      "         [-1.7221],\n",
      "         [-3.5930],\n",
      "         [-3.2165],\n",
      "         [-1.8002],\n",
      "         [-1.8651],\n",
      "         [ 0.9648],\n",
      "         [ 1.3635],\n",
      "         [-1.2729],\n",
      "         [-3.5272],\n",
      "         [-1.2119],\n",
      "         [-0.8120],\n",
      "         [-2.4056],\n",
      "         [-2.7608],\n",
      "         [-1.1328],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.9201],\n",
      "         [-2.1106],\n",
      "         [-3.0158],\n",
      "         [-0.7956],\n",
      "         [ 1.3635],\n",
      "         [-1.0002],\n",
      "         [-0.5078],\n",
      "         [-1.4877],\n",
      "         [-2.1908],\n",
      "         [-2.3055],\n",
      "         [-0.0657],\n",
      "         [ 1.3407],\n",
      "         [-0.5333],\n",
      "         [-1.8741],\n",
      "         [-1.1287],\n",
      "         [-1.4741],\n",
      "         [ 0.9255],\n",
      "         [ 1.0635],\n",
      "         [-0.0950],\n",
      "         [-2.7245],\n",
      "         [-3.8251],\n",
      "         [ 0.2480],\n",
      "         [-1.7962],\n",
      "         [-0.4970],\n",
      "         [-2.2033],\n",
      "         [-1.4297],\n",
      "         [-0.4616],\n",
      "         [-1.4192],\n",
      "         [-0.9740],\n",
      "         [-3.5567],\n",
      "         [ 1.1168],\n",
      "         [-1.4985],\n",
      "         [-2.4721],\n",
      "         [-0.7425],\n",
      "         [-2.7658]],\n",
      "\n",
      "        [[ 1.4956],\n",
      "         [-3.1032],\n",
      "         [-3.7962],\n",
      "         [-1.7941],\n",
      "         [-2.6515],\n",
      "         [-0.7275],\n",
      "         [-0.5078],\n",
      "         [-2.7542],\n",
      "         [ 0.6848],\n",
      "         [-0.5196],\n",
      "         [-1.2194],\n",
      "         [-0.6477],\n",
      "         [-4.0309],\n",
      "         [-3.2231],\n",
      "         [ 0.3195],\n",
      "         [-2.9161],\n",
      "         [-1.2807],\n",
      "         [-3.0826],\n",
      "         [-1.7549],\n",
      "         [-3.4934],\n",
      "         [-4.5206],\n",
      "         [-2.7994],\n",
      "         [-2.1908],\n",
      "         [ 0.4353],\n",
      "         [-0.5231],\n",
      "         [-1.3301],\n",
      "         [-2.0933],\n",
      "         [-0.2389],\n",
      "         [-1.8741],\n",
      "         [ 2.2073],\n",
      "         [-1.1898],\n",
      "         [-4.5842],\n",
      "         [-0.0635],\n",
      "         [-2.5310],\n",
      "         [-0.5940],\n",
      "         [-2.5539],\n",
      "         [-3.2271],\n",
      "         [-1.5556],\n",
      "         [-1.8003],\n",
      "         [ 1.3003],\n",
      "         [-1.3663],\n",
      "         [-1.8576],\n",
      "         [ 2.2073],\n",
      "         [ 1.0348],\n",
      "         [-1.0946],\n",
      "         [-2.7866],\n",
      "         [ 1.3635],\n",
      "         [-2.7542],\n",
      "         [-3.3392],\n",
      "         [-1.7221],\n",
      "         [-3.5930],\n",
      "         [-3.2165],\n",
      "         [-1.8002],\n",
      "         [-1.8651],\n",
      "         [ 0.9648],\n",
      "         [ 1.3635],\n",
      "         [-1.2729],\n",
      "         [-3.5272],\n",
      "         [-1.2119],\n",
      "         [-0.8120],\n",
      "         [-2.4056],\n",
      "         [-2.7608],\n",
      "         [-1.1328],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.9201],\n",
      "         [-2.1106],\n",
      "         [-3.0158],\n",
      "         [-0.7956],\n",
      "         [ 1.3635],\n",
      "         [-1.0002],\n",
      "         [-0.5078],\n",
      "         [-1.4877],\n",
      "         [-2.1908],\n",
      "         [-2.3055],\n",
      "         [-0.0657],\n",
      "         [ 1.3407],\n",
      "         [-0.5333],\n",
      "         [-1.8741],\n",
      "         [-1.1287],\n",
      "         [-1.4741],\n",
      "         [ 0.9255],\n",
      "         [ 1.0635],\n",
      "         [-0.0950],\n",
      "         [-2.7245],\n",
      "         [-3.8251],\n",
      "         [ 0.2480],\n",
      "         [-1.7962],\n",
      "         [-0.4970],\n",
      "         [-2.2033],\n",
      "         [-1.4297],\n",
      "         [-0.4616],\n",
      "         [-1.4192],\n",
      "         [-0.9740],\n",
      "         [-3.5567],\n",
      "         [ 1.1168],\n",
      "         [-1.4985],\n",
      "         [-2.4721],\n",
      "         [-0.7425],\n",
      "         [-2.7658]],\n",
      "\n",
      "        [[ 1.4956],\n",
      "         [-3.1032],\n",
      "         [-3.7962],\n",
      "         [-1.7941],\n",
      "         [-2.6515],\n",
      "         [-0.7275],\n",
      "         [-0.5078],\n",
      "         [-2.7542],\n",
      "         [ 0.6848],\n",
      "         [-0.5196],\n",
      "         [-1.2194],\n",
      "         [-0.6477],\n",
      "         [-4.0309],\n",
      "         [-3.2231],\n",
      "         [ 0.3195],\n",
      "         [-2.9161],\n",
      "         [-1.2807],\n",
      "         [-3.0826],\n",
      "         [-1.7549],\n",
      "         [-3.4934],\n",
      "         [-4.5206],\n",
      "         [-2.7994],\n",
      "         [-2.1908],\n",
      "         [ 0.4353],\n",
      "         [-0.5231],\n",
      "         [-1.3301],\n",
      "         [-2.0933],\n",
      "         [-0.2389],\n",
      "         [-1.8741],\n",
      "         [ 2.2073],\n",
      "         [-1.1898],\n",
      "         [-4.5842],\n",
      "         [-0.0635],\n",
      "         [-2.5310],\n",
      "         [-0.5940],\n",
      "         [-2.5539],\n",
      "         [-3.2271],\n",
      "         [-1.5556],\n",
      "         [-1.8003],\n",
      "         [ 1.3003],\n",
      "         [-1.3663],\n",
      "         [-1.8576],\n",
      "         [ 2.2073],\n",
      "         [ 1.0348],\n",
      "         [-1.0946],\n",
      "         [-2.7866],\n",
      "         [ 1.3635],\n",
      "         [-2.7542],\n",
      "         [-3.3392],\n",
      "         [-1.7221],\n",
      "         [-3.5930],\n",
      "         [-3.2165],\n",
      "         [-1.8002],\n",
      "         [-1.8651],\n",
      "         [ 0.9648],\n",
      "         [ 1.3635],\n",
      "         [-1.2729],\n",
      "         [-3.5272],\n",
      "         [-1.2119],\n",
      "         [-0.8120],\n",
      "         [-2.4056],\n",
      "         [-2.7608],\n",
      "         [-1.1328],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.9201],\n",
      "         [-2.1106],\n",
      "         [-3.0158],\n",
      "         [-0.7956],\n",
      "         [ 1.3635],\n",
      "         [-1.0002],\n",
      "         [-0.5078],\n",
      "         [-1.4877],\n",
      "         [-2.1908],\n",
      "         [-2.3055],\n",
      "         [-0.0657],\n",
      "         [ 1.3407],\n",
      "         [-0.5333],\n",
      "         [-1.8741],\n",
      "         [-1.1287],\n",
      "         [-1.4741],\n",
      "         [ 0.9255],\n",
      "         [ 1.0635],\n",
      "         [-0.0950],\n",
      "         [-2.7245],\n",
      "         [-3.8251],\n",
      "         [ 0.2480],\n",
      "         [-1.7962],\n",
      "         [-0.4970],\n",
      "         [-2.2033],\n",
      "         [-1.4297],\n",
      "         [-0.4616],\n",
      "         [-1.4192],\n",
      "         [-0.9740],\n",
      "         [-3.5567],\n",
      "         [ 1.1168],\n",
      "         [-1.4985],\n",
      "         [-2.4721],\n",
      "         [-0.7425],\n",
      "         [-2.7658]],\n",
      "\n",
      "        [[ 1.4956],\n",
      "         [-3.1032],\n",
      "         [-3.7962],\n",
      "         [-1.7941],\n",
      "         [-2.6515],\n",
      "         [-0.7275],\n",
      "         [-0.5078],\n",
      "         [-2.7542],\n",
      "         [ 0.6848],\n",
      "         [-0.5196],\n",
      "         [-1.2194],\n",
      "         [-0.6477],\n",
      "         [-4.0309],\n",
      "         [-3.2231],\n",
      "         [ 0.3195],\n",
      "         [-2.9161],\n",
      "         [-1.2807],\n",
      "         [-3.0826],\n",
      "         [-1.7549],\n",
      "         [-3.4934],\n",
      "         [-4.5206],\n",
      "         [-2.7994],\n",
      "         [-2.1908],\n",
      "         [ 0.4353],\n",
      "         [-0.5231],\n",
      "         [-1.3301],\n",
      "         [-2.0933],\n",
      "         [-0.2389],\n",
      "         [-1.8741],\n",
      "         [ 2.2073],\n",
      "         [-1.1898],\n",
      "         [-4.5842],\n",
      "         [-0.0635],\n",
      "         [-2.5310],\n",
      "         [-0.5940],\n",
      "         [-2.5539],\n",
      "         [-3.2271],\n",
      "         [-1.5556],\n",
      "         [-1.8003],\n",
      "         [ 1.3003],\n",
      "         [-1.3663],\n",
      "         [-1.8576],\n",
      "         [ 2.2073],\n",
      "         [ 1.0348],\n",
      "         [-1.0946],\n",
      "         [-2.7866],\n",
      "         [ 1.3635],\n",
      "         [-2.7542],\n",
      "         [-3.3392],\n",
      "         [-1.7221],\n",
      "         [-3.5930],\n",
      "         [-3.2165],\n",
      "         [-1.8002],\n",
      "         [-1.8651],\n",
      "         [ 0.9648],\n",
      "         [ 1.3635],\n",
      "         [-1.2729],\n",
      "         [-3.5272],\n",
      "         [-1.2119],\n",
      "         [-0.8120],\n",
      "         [-2.4056],\n",
      "         [-2.7608],\n",
      "         [-1.1328],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.9201],\n",
      "         [-2.1106],\n",
      "         [-3.0158],\n",
      "         [-0.7956],\n",
      "         [ 1.3635],\n",
      "         [-1.0002],\n",
      "         [-0.5078],\n",
      "         [-1.4877],\n",
      "         [-2.1908],\n",
      "         [-2.3055],\n",
      "         [-0.0657],\n",
      "         [ 1.3407],\n",
      "         [-0.5333],\n",
      "         [-1.8741],\n",
      "         [-1.1287],\n",
      "         [-1.4741],\n",
      "         [ 0.9255],\n",
      "         [ 1.0635],\n",
      "         [-0.0950],\n",
      "         [-2.7245],\n",
      "         [-3.8251],\n",
      "         [ 0.2480],\n",
      "         [-1.7962],\n",
      "         [-0.4970],\n",
      "         [-2.2033],\n",
      "         [-1.4297],\n",
      "         [-0.4616],\n",
      "         [-1.4192],\n",
      "         [-0.9740],\n",
      "         [-3.5567],\n",
      "         [ 1.1168],\n",
      "         [-1.4985],\n",
      "         [-2.4721],\n",
      "         [-0.7425],\n",
      "         [-2.7658]],\n",
      "\n",
      "        [[ 1.4956],\n",
      "         [-3.1032],\n",
      "         [-3.7962],\n",
      "         [-1.7941],\n",
      "         [-2.6515],\n",
      "         [-0.7275],\n",
      "         [-0.5078],\n",
      "         [-2.7542],\n",
      "         [ 0.6848],\n",
      "         [-0.5196],\n",
      "         [-1.2194],\n",
      "         [-0.6477],\n",
      "         [-4.0309],\n",
      "         [-3.2231],\n",
      "         [ 0.3195],\n",
      "         [-2.9161],\n",
      "         [-1.2807],\n",
      "         [-3.0826],\n",
      "         [-1.7549],\n",
      "         [-3.4934],\n",
      "         [-4.5206],\n",
      "         [-2.7994],\n",
      "         [-2.1908],\n",
      "         [ 0.4353],\n",
      "         [-0.5231],\n",
      "         [-1.3301],\n",
      "         [-2.0933],\n",
      "         [-0.2389],\n",
      "         [-1.8741],\n",
      "         [ 2.2073],\n",
      "         [-1.1898],\n",
      "         [-4.5842],\n",
      "         [-0.0635],\n",
      "         [-2.5310],\n",
      "         [-0.5940],\n",
      "         [-2.5539],\n",
      "         [-3.2271],\n",
      "         [-1.5556],\n",
      "         [-1.8003],\n",
      "         [ 1.3003],\n",
      "         [-1.3663],\n",
      "         [-1.8576],\n",
      "         [ 2.2073],\n",
      "         [ 1.0348],\n",
      "         [-1.0946],\n",
      "         [-2.7866],\n",
      "         [ 1.3635],\n",
      "         [-2.7542],\n",
      "         [-3.3392],\n",
      "         [-1.7221],\n",
      "         [-3.5930],\n",
      "         [-3.2165],\n",
      "         [-1.8002],\n",
      "         [-1.8651],\n",
      "         [ 0.9648],\n",
      "         [ 1.3635],\n",
      "         [-1.2729],\n",
      "         [-3.5272],\n",
      "         [-1.2119],\n",
      "         [-0.8120],\n",
      "         [-2.4056],\n",
      "         [-2.7608],\n",
      "         [-1.1328],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.9201],\n",
      "         [-2.1106],\n",
      "         [-3.0158],\n",
      "         [-0.7956],\n",
      "         [ 1.3635],\n",
      "         [-1.0002],\n",
      "         [-0.5078],\n",
      "         [-1.4877],\n",
      "         [-2.1908],\n",
      "         [-2.3055],\n",
      "         [-0.0657],\n",
      "         [ 1.3407],\n",
      "         [-0.5333],\n",
      "         [-1.8741],\n",
      "         [-1.1287],\n",
      "         [-1.4741],\n",
      "         [ 0.9255],\n",
      "         [ 1.0635],\n",
      "         [-0.0950],\n",
      "         [-2.7245],\n",
      "         [-3.8251],\n",
      "         [ 0.2480],\n",
      "         [-1.7962],\n",
      "         [-0.4970],\n",
      "         [-2.2033],\n",
      "         [-1.4297],\n",
      "         [-0.4616],\n",
      "         [-1.4192],\n",
      "         [-0.9740],\n",
      "         [-3.5567],\n",
      "         [ 1.1168],\n",
      "         [-1.4985],\n",
      "         [-2.4721],\n",
      "         [-0.7425],\n",
      "         [-2.7658]],\n",
      "\n",
      "        [[ 1.4956],\n",
      "         [-3.1032],\n",
      "         [-3.7962],\n",
      "         [-1.7941],\n",
      "         [-2.6515],\n",
      "         [-0.7275],\n",
      "         [-0.5078],\n",
      "         [-2.7542],\n",
      "         [ 0.6848],\n",
      "         [-0.5196],\n",
      "         [-1.2194],\n",
      "         [-0.6477],\n",
      "         [-4.0309],\n",
      "         [-3.2231],\n",
      "         [ 0.3195],\n",
      "         [-2.9161],\n",
      "         [-1.2807],\n",
      "         [-3.0826],\n",
      "         [-1.7549],\n",
      "         [-3.4934],\n",
      "         [-4.5206],\n",
      "         [-2.7994],\n",
      "         [-2.1908],\n",
      "         [ 0.4353],\n",
      "         [-0.5231],\n",
      "         [-1.3301],\n",
      "         [-2.0933],\n",
      "         [-0.2389],\n",
      "         [-1.8741],\n",
      "         [ 2.2073],\n",
      "         [-1.1898],\n",
      "         [-4.5842],\n",
      "         [-0.0635],\n",
      "         [-2.5310],\n",
      "         [-0.5940],\n",
      "         [-2.5539],\n",
      "         [-3.2271],\n",
      "         [-1.5556],\n",
      "         [-1.8003],\n",
      "         [ 1.3003],\n",
      "         [-1.3663],\n",
      "         [-1.8576],\n",
      "         [ 2.2073],\n",
      "         [ 1.0348],\n",
      "         [-1.0946],\n",
      "         [-2.7866],\n",
      "         [ 1.3635],\n",
      "         [-2.7542],\n",
      "         [-3.3392],\n",
      "         [-1.7221],\n",
      "         [-3.5930],\n",
      "         [-3.2165],\n",
      "         [-1.8002],\n",
      "         [-1.8651],\n",
      "         [ 0.9648],\n",
      "         [ 1.3635],\n",
      "         [-1.2729],\n",
      "         [-3.5272],\n",
      "         [-1.2119],\n",
      "         [-0.8120],\n",
      "         [-2.4056],\n",
      "         [-2.7608],\n",
      "         [-1.1328],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.9201],\n",
      "         [-2.1106],\n",
      "         [-3.0158],\n",
      "         [-0.7956],\n",
      "         [ 1.3635],\n",
      "         [-1.0002],\n",
      "         [-0.5078],\n",
      "         [-1.4877],\n",
      "         [-2.1908],\n",
      "         [-2.3055],\n",
      "         [-0.0657],\n",
      "         [ 1.3407],\n",
      "         [-0.5333],\n",
      "         [-1.8741],\n",
      "         [-1.1287],\n",
      "         [-1.4741],\n",
      "         [ 0.9255],\n",
      "         [ 1.0635],\n",
      "         [-0.0950],\n",
      "         [-2.7245],\n",
      "         [-3.8251],\n",
      "         [ 0.2480],\n",
      "         [-1.7962],\n",
      "         [-0.4970],\n",
      "         [-2.2033],\n",
      "         [-1.4297],\n",
      "         [-0.4616],\n",
      "         [-1.4192],\n",
      "         [-0.9740],\n",
      "         [-3.5567],\n",
      "         [ 1.1168],\n",
      "         [-1.4985],\n",
      "         [-2.4721],\n",
      "         [-0.7425],\n",
      "         [-2.7658]],\n",
      "\n",
      "        [[ 1.4956],\n",
      "         [-3.1032],\n",
      "         [-3.7962],\n",
      "         [-1.7941],\n",
      "         [-2.6515],\n",
      "         [-0.7275],\n",
      "         [-0.5078],\n",
      "         [-2.7542],\n",
      "         [ 0.6848],\n",
      "         [-0.5196],\n",
      "         [-1.2194],\n",
      "         [-0.6477],\n",
      "         [-4.0309],\n",
      "         [-3.2231],\n",
      "         [ 0.3195],\n",
      "         [-2.9161],\n",
      "         [-1.2807],\n",
      "         [-3.0826],\n",
      "         [-1.7549],\n",
      "         [-3.4934],\n",
      "         [-4.5206],\n",
      "         [-2.7994],\n",
      "         [-2.1908],\n",
      "         [ 0.4353],\n",
      "         [-0.5231],\n",
      "         [-1.3301],\n",
      "         [-2.0933],\n",
      "         [-0.2389],\n",
      "         [-1.8741],\n",
      "         [ 2.2073],\n",
      "         [-1.1898],\n",
      "         [-4.5842],\n",
      "         [-0.0635],\n",
      "         [-2.5310],\n",
      "         [-0.5940],\n",
      "         [-2.5539],\n",
      "         [-3.2271],\n",
      "         [-1.5556],\n",
      "         [-1.8003],\n",
      "         [ 1.3003],\n",
      "         [-1.3663],\n",
      "         [-1.8576],\n",
      "         [ 2.2073],\n",
      "         [ 1.0348],\n",
      "         [-1.0946],\n",
      "         [-2.7866],\n",
      "         [ 1.3635],\n",
      "         [-2.7542],\n",
      "         [-3.3392],\n",
      "         [-1.7221],\n",
      "         [-3.5930],\n",
      "         [-3.2165],\n",
      "         [-1.8002],\n",
      "         [-1.8651],\n",
      "         [ 0.9648],\n",
      "         [ 1.3635],\n",
      "         [-1.2729],\n",
      "         [-3.5272],\n",
      "         [-1.2119],\n",
      "         [-0.8120],\n",
      "         [-2.4056],\n",
      "         [-2.7608],\n",
      "         [-1.1328],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.9201],\n",
      "         [-2.1106],\n",
      "         [-3.0158],\n",
      "         [-0.7956],\n",
      "         [ 1.3635],\n",
      "         [-1.0002],\n",
      "         [-0.5078],\n",
      "         [-1.4877],\n",
      "         [-2.1908],\n",
      "         [-2.3055],\n",
      "         [-0.0657],\n",
      "         [ 1.3407],\n",
      "         [-0.5333],\n",
      "         [-1.8741],\n",
      "         [-1.1287],\n",
      "         [-1.4741],\n",
      "         [ 0.9255],\n",
      "         [ 1.0635],\n",
      "         [-0.0950],\n",
      "         [-2.7245],\n",
      "         [-3.8251],\n",
      "         [ 0.2480],\n",
      "         [-1.7962],\n",
      "         [-0.4970],\n",
      "         [-2.2033],\n",
      "         [-1.4297],\n",
      "         [-0.4616],\n",
      "         [-1.4192],\n",
      "         [-0.9740],\n",
      "         [-3.5567],\n",
      "         [ 1.1168],\n",
      "         [-1.4985],\n",
      "         [-2.4721],\n",
      "         [-0.7425],\n",
      "         [-2.7658]],\n",
      "\n",
      "        [[ 1.4956],\n",
      "         [-3.1032],\n",
      "         [-3.7962],\n",
      "         [-1.7941],\n",
      "         [-2.6515],\n",
      "         [-0.7275],\n",
      "         [-0.5078],\n",
      "         [-2.7542],\n",
      "         [ 0.6848],\n",
      "         [-0.5196],\n",
      "         [-1.2194],\n",
      "         [-0.6477],\n",
      "         [-4.0309],\n",
      "         [-3.2231],\n",
      "         [ 0.3195],\n",
      "         [-2.9161],\n",
      "         [-1.2807],\n",
      "         [-3.0826],\n",
      "         [-1.7549],\n",
      "         [-3.4934],\n",
      "         [-4.5206],\n",
      "         [-2.7994],\n",
      "         [-2.1908],\n",
      "         [ 0.4353],\n",
      "         [-0.5231],\n",
      "         [-1.3301],\n",
      "         [-2.0933],\n",
      "         [-0.2389],\n",
      "         [-1.8741],\n",
      "         [ 2.2073],\n",
      "         [-1.1898],\n",
      "         [-4.5842],\n",
      "         [-0.0635],\n",
      "         [-2.5310],\n",
      "         [-0.5940],\n",
      "         [-2.5539],\n",
      "         [-3.2271],\n",
      "         [-1.5556],\n",
      "         [-1.8003],\n",
      "         [ 1.3003],\n",
      "         [-1.3663],\n",
      "         [-1.8576],\n",
      "         [ 2.2073],\n",
      "         [ 1.0348],\n",
      "         [-1.0946],\n",
      "         [-2.7866],\n",
      "         [ 1.3635],\n",
      "         [-2.7542],\n",
      "         [-3.3392],\n",
      "         [-1.7221],\n",
      "         [-3.5930],\n",
      "         [-3.2165],\n",
      "         [-1.8002],\n",
      "         [-1.8651],\n",
      "         [ 0.9648],\n",
      "         [ 1.3635],\n",
      "         [-1.2729],\n",
      "         [-3.5272],\n",
      "         [-1.2119],\n",
      "         [-0.8120],\n",
      "         [-2.4056],\n",
      "         [-2.7608],\n",
      "         [-1.1328],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.9201],\n",
      "         [-2.1106],\n",
      "         [-3.0158],\n",
      "         [-0.7956],\n",
      "         [ 1.3635],\n",
      "         [-1.0002],\n",
      "         [-0.5078],\n",
      "         [-1.4877],\n",
      "         [-2.1908],\n",
      "         [-2.3055],\n",
      "         [-0.0657],\n",
      "         [ 1.3407],\n",
      "         [-0.5333],\n",
      "         [-1.8741],\n",
      "         [-1.1287],\n",
      "         [-1.4741],\n",
      "         [ 0.9255],\n",
      "         [ 1.0635],\n",
      "         [-0.0950],\n",
      "         [-2.7245],\n",
      "         [-3.8251],\n",
      "         [ 0.2480],\n",
      "         [-1.7962],\n",
      "         [-0.4970],\n",
      "         [-2.2033],\n",
      "         [-1.4297],\n",
      "         [-0.4616],\n",
      "         [-1.4192],\n",
      "         [-0.9740],\n",
      "         [-3.5567],\n",
      "         [ 1.1168],\n",
      "         [-1.4985],\n",
      "         [-2.4721],\n",
      "         [-0.7425],\n",
      "         [-2.7658]],\n",
      "\n",
      "        [[ 1.4956],\n",
      "         [-3.1032],\n",
      "         [-3.7962],\n",
      "         [-1.7941],\n",
      "         [-2.6515],\n",
      "         [-0.7275],\n",
      "         [-0.5078],\n",
      "         [-2.7542],\n",
      "         [ 0.6848],\n",
      "         [-0.5196],\n",
      "         [-1.2194],\n",
      "         [-0.6477],\n",
      "         [-4.0309],\n",
      "         [-3.2231],\n",
      "         [ 0.3195],\n",
      "         [-2.9161],\n",
      "         [-1.2807],\n",
      "         [-3.0826],\n",
      "         [-1.7549],\n",
      "         [-3.4934],\n",
      "         [-4.5206],\n",
      "         [-2.7994],\n",
      "         [-2.1908],\n",
      "         [ 0.4353],\n",
      "         [-0.5231],\n",
      "         [-1.3301],\n",
      "         [-2.0933],\n",
      "         [-0.2389],\n",
      "         [-1.8741],\n",
      "         [ 2.2073],\n",
      "         [-1.1898],\n",
      "         [-4.5842],\n",
      "         [-0.0635],\n",
      "         [-2.5310],\n",
      "         [-0.5940],\n",
      "         [-2.5539],\n",
      "         [-3.2271],\n",
      "         [-1.5556],\n",
      "         [-1.8003],\n",
      "         [ 1.3003],\n",
      "         [-1.3663],\n",
      "         [-1.8576],\n",
      "         [ 2.2073],\n",
      "         [ 1.0348],\n",
      "         [-1.0946],\n",
      "         [-2.7866],\n",
      "         [ 1.3635],\n",
      "         [-2.7542],\n",
      "         [-3.3392],\n",
      "         [-1.7221],\n",
      "         [-3.5930],\n",
      "         [-3.2165],\n",
      "         [-1.8002],\n",
      "         [-1.8651],\n",
      "         [ 0.9648],\n",
      "         [ 1.3635],\n",
      "         [-1.2729],\n",
      "         [-3.5272],\n",
      "         [-1.2119],\n",
      "         [-0.8120],\n",
      "         [-2.4056],\n",
      "         [-2.7608],\n",
      "         [-1.1328],\n",
      "         [-0.5231],\n",
      "         [-1.6136],\n",
      "         [-1.9201],\n",
      "         [-2.1106],\n",
      "         [-3.0158],\n",
      "         [-0.7956],\n",
      "         [ 1.3635],\n",
      "         [-1.0002],\n",
      "         [-0.5078],\n",
      "         [-1.4877],\n",
      "         [-2.1908],\n",
      "         [-2.3055],\n",
      "         [-0.0657],\n",
      "         [ 1.3407],\n",
      "         [-0.5333],\n",
      "         [-1.8741],\n",
      "         [-1.1287],\n",
      "         [-1.4741],\n",
      "         [ 0.9255],\n",
      "         [ 1.0635],\n",
      "         [-0.0950],\n",
      "         [-2.7245],\n",
      "         [-3.8251],\n",
      "         [ 0.2480],\n",
      "         [-1.7962],\n",
      "         [-0.4970],\n",
      "         [-2.2033],\n",
      "         [-1.4297],\n",
      "         [-0.4616],\n",
      "         [-1.4192],\n",
      "         [-0.9740],\n",
      "         [-3.5567],\n",
      "         [ 1.1168],\n",
      "         [-1.4985],\n",
      "         [-2.4721],\n",
      "         [-0.7425],\n",
      "         [-2.7658]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([10, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([10, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/12/2023 06:40:52 AM WARNING 36492 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/12/2023 06:40:54 AM WARNING 36492 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/12/2023 06:40:54 AM WARNING 36492 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/12/2023 06:40:54 AM WARNING 36492 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/12/2023 06:40:54 AM WARNING 36492 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/12/2023 06:40:54 AM WARNING 36492 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/12/2023 06:40:54 AM WARNING 36492 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/12/2023 06:40:54 AM WARNING 36492 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/12/2023 06:40:54 AM WARNING 36492 [WalrusDriver]: DRAM allocation successful\n"
     ]
    }
   ],
   "source": [
    "# Compile BERT for different batch sizes\n",
    "for batch_size in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "# for batch_size in [1, 2, 4, 8, 16, 32, 64, 128, 256]:\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(name, torchscript=True)\n",
    "    dummy_inputs = create_dummy_input(batch_size= batch_size)\n",
    "    # example = encode(tokenizer, sequence_0, sequence_2, batch_size=batch_size)\n",
    "    model_neuron = convert_torch_script(ncf_model, dummy_inputs)    \n",
    "    # ㅠmodel_neuron = torch_neuronx.trace(model, example)\n",
    "    filename = f'model_batch_size_{batch_size}.pt'\n",
    "    torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_1.pt\n",
      "Batch Size:  1\n",
      "Batches:     2000\n",
      "Inferences:  2000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    0.731\n",
      "Throughput:  2735.084\n",
      "Latency P50: 0.738\n",
      "Latency P95: 0.891\n",
      "Latency P99: 1.335\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_2.pt\n",
      "Batch Size:  2\n",
      "Batches:     2000\n",
      "Inferences:  4000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    0.986\n",
      "Throughput:  4056.616\n",
      "Latency P50: 0.936\n",
      "Latency P95: 1.147\n",
      "Latency P99: 1.579\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_3.pt\n",
      "Batch Size:  3\n",
      "Batches:     2000\n",
      "Inferences:  6000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.119\n",
      "Throughput:  5361.024\n",
      "Latency P50: 1.119\n",
      "Latency P95: 1.241\n",
      "Latency P99: 1.965\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_4.pt\n",
      "Batch Size:  4\n",
      "Batches:     2000\n",
      "Inferences:  8000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    0.947\n",
      "Throughput:  8450.953\n",
      "Latency P50: 0.944\n",
      "Latency P95: 1.007\n",
      "Latency P99: 1.064\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_5.pt\n",
      "Batch Size:  5\n",
      "Batches:     2000\n",
      "Inferences:  10000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.026\n",
      "Throughput:  9743.362\n",
      "Latency P50: 0.972\n",
      "Latency P95: 1.189\n",
      "Latency P99: 1.610\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_6.pt\n",
      "Batch Size:  6\n",
      "Batches:     2000\n",
      "Inferences:  12000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.046\n",
      "Throughput:  11477.047\n",
      "Latency P50: 0.988\n",
      "Latency P95: 1.188\n",
      "Latency P99: 1.715\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_7.pt\n",
      "Batch Size:  7\n",
      "Batches:     2000\n",
      "Inferences:  14000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.055\n",
      "Throughput:  13268.609\n",
      "Latency P50: 0.998\n",
      "Latency P95: 1.227\n",
      "Latency P99: 1.680\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_8.pt\n",
      "Batch Size:  8\n",
      "Batches:     2000\n",
      "Inferences:  16000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.011\n",
      "Throughput:  15833.223\n",
      "Latency P50: 0.986\n",
      "Latency P95: 1.167\n",
      "Latency P99: 1.371\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_9.pt\n",
      "Batch Size:  9\n",
      "Batches:     2000\n",
      "Inferences:  18000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.032\n",
      "Throughput:  17433.655\n",
      "Latency P50: 1.000\n",
      "Latency P95: 1.199\n",
      "Latency P99: 1.327\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_10.pt\n",
      "Batch Size:  10\n",
      "Batches:     2000\n",
      "Inferences:  20000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.074\n",
      "Throughput:  18616.017\n",
      "Latency P50: 1.020\n",
      "Latency P95: 1.232\n",
      "Latency P99: 1.471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Benchmark BERT for different batch sizes\n",
    "for batch_size in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "# for batch_size in [1, 2, 4, 8, 16, 32, 64, 128, 256]:\n",
    "    print('-'*50)\n",
    "    dummy_inputs = create_dummy_input(batch_size= batch_size)\n",
    "    filename = f'model_batch_size_{batch_size}.pt'\n",
    "    benchmark(filename, dummy_inputs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-neuronx)",
   "language": "python",
   "name": "aws_neuron_venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
