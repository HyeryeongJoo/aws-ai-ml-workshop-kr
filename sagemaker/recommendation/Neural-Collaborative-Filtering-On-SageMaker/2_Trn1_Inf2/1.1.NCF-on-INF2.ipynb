{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 1.1] Inference NCF on INF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전 노트북에서 훈련 후의 아티펙트를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas==1.2.0\n",
    "# ! pip install numpy==1.20.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip list | grep pandas\n",
    "# ! pip list | grep numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 배포 준비\n",
    "\n",
    "### 이전 노트북에서 훈련된 모델의 S3 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_path = 'models/NeuMF-end.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifact is assigend from :  models/NeuMF-end.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"model artifact is assigend from : \", artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론을 위한  데이터 세트 로딩\n",
    "- 전부 데이터를 로딩할 필요가 없지만, 여기서는 기존에 사용한 함수를 이용하기 위해서 전체 데이터를 로드 합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils \n",
    "# train_data, test_data, user_num ,item_num, train_mat = data_utils.load_all(test_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 생성\n",
    "- 모델 로딩시에 아라 파라미터 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batch_size:  256\n"
     ]
    }
   ],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        # self.epochs = 1        \n",
    "        self.num_ng = 4\n",
    "        self.batch_size = 256\n",
    "        self.test_num_ng = 99\n",
    "        self.factor_num = 32\n",
    "        self.num_layers = 3\n",
    "        self.dropout = 0.0\n",
    "        # self.lr = 0.001\n",
    "        self.top_k = 10\n",
    "        self.out = True\n",
    "        # self.gpu = \"0\"\n",
    "                        \n",
    "args = Params()\n",
    "print(\"# of batch_size: \", args.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 훈련된 모델 아티펙트 다운로드 및 압축해제\n",
    "- 모델 아티펙트를 다운로드 합니다.\n",
    "- 다운로드 받은 모델 아티펙트의 압축을 해제하고 모델 가중치인 models/model.pth 파일을 얻습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 훈련된 모델 로딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 모델 네트워크 설정 저장\n",
    "- 모델 네트워크를 생성시에 사용할 설정값을 model_config.json 로 저장함.\n",
    "- model_fn() 함수에서 모델 네트워크를 생성시에 사용 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_data_dir:  ./models/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import config\n",
    "\n",
    "model_data_dir = config.model_path\n",
    "os.makedirs(model_data_dir, exist_ok=True)\n",
    "print(\"model_data_dir: \", model_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_num:  6040  item_num:  3706\n"
     ]
    }
   ],
   "source": [
    "user_num = 6040  \n",
    "item_num = 3706\n",
    "print(\"user_num: \", user_num, \" item_num: \", item_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/model_config.json is saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'src/model_config.json'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from common_utils import save_json, load_json\n",
    "\n",
    "model_config_dict = {\n",
    "    'user_num': str(user_num),\n",
    "    'item_num': str(item_num),\n",
    "    'factor_num' : str(args.factor_num),\n",
    "    'num_layers' : str(args.num_layers),\n",
    "    'dropout' : str(args.dropout),\n",
    "    'model_type': config.model\n",
    "}\n",
    "\n",
    "model_config_file = 'model_config.json'\n",
    "model_config_file_path = os.path.join('src', model_config_file)\n",
    "\n",
    "save_json(model_config_file_path, model_config_dict)\n",
    "# model_config_dict = load_json(model_config_file_path)    \n",
    "# model_config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2. 모델 로딩\n",
    "- 복수개의 모델로 진행하기 위해서, 편의상 동일한 모델에서 생성 함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Staring model_fn() ###############\n",
      "device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from inference import model_fn\n",
    "\n",
    "ncf_model = model_fn(config.model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Trition 서빙 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 샘플 입력 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "type:  <class 'tuple'>\n",
      "len:  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def create_dummy_input(batch_size):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using {} device\".format(device))\n",
    "\n",
    "    user_np = np.zeros((1,100)).astype(np.int32)\n",
    "    item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "\n",
    "    return (\n",
    "        torch.repeat_interleave(torch.from_numpy(user_np), batch_size, 0),\n",
    "        torch.repeat_interleave(torch.from_numpy(item_np), batch_size, 0),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "dummy_inputs = create_dummy_input(batch_size=1)\n",
    "\n",
    "print(\"type: \", type(dummy_inputs))\n",
    "print(\"len: \", len(dummy_inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. 샘플 입력으로 모델 추론 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tes\n"
     ]
    }
   ],
   "source": [
    "print(\"tes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Torch Script 으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### test 1 #####\n",
      "#### prediction size: \n",
      " torch.Size([1, 100, 1])\n",
      "#### prediction: \n",
      " tensor([[[-1.9414],\n",
      "         [-1.8735],\n",
      "         [ 1.4246],\n",
      "         [-0.7328],\n",
      "         [-2.1308],\n",
      "         [-0.7274],\n",
      "         [ 1.4956],\n",
      "         [-3.1558],\n",
      "         [-0.6960],\n",
      "         [-0.8369],\n",
      "         [-2.6756],\n",
      "         [-2.5667],\n",
      "         [-1.5944],\n",
      "         [ 1.8739],\n",
      "         [-3.2315],\n",
      "         [-2.7633],\n",
      "         [-1.8741],\n",
      "         [-0.8376],\n",
      "         [-1.0394],\n",
      "         [-1.6824],\n",
      "         [-1.3185],\n",
      "         [-0.3596],\n",
      "         [-2.1842],\n",
      "         [-3.5964],\n",
      "         [ 0.8121],\n",
      "         [ 0.9245],\n",
      "         [-4.9187],\n",
      "         [-3.6179],\n",
      "         [ 2.5198],\n",
      "         [-0.6039],\n",
      "         [-0.4386],\n",
      "         [-1.0476],\n",
      "         [-1.7840],\n",
      "         [-1.1848],\n",
      "         [-1.5194],\n",
      "         [-3.2381],\n",
      "         [-4.3907],\n",
      "         [-4.3857],\n",
      "         [-3.8251],\n",
      "         [-1.7962],\n",
      "         [-2.0258],\n",
      "         [-3.0666],\n",
      "         [-4.5885],\n",
      "         [-2.0000],\n",
      "         [-0.6448],\n",
      "         [-2.6794],\n",
      "         [-2.3167],\n",
      "         [-1.4741],\n",
      "         [-0.1550],\n",
      "         [-3.6985],\n",
      "         [-2.0493],\n",
      "         [-2.4721],\n",
      "         [ 0.3195],\n",
      "         [-1.3547],\n",
      "         [ 3.2312],\n",
      "         [ 1.7544],\n",
      "         [-3.5692],\n",
      "         [-2.9215],\n",
      "         [-1.0120],\n",
      "         [-0.9255],\n",
      "         [-0.7830],\n",
      "         [-1.1956],\n",
      "         [-1.4078],\n",
      "         [-3.0403],\n",
      "         [-1.4890],\n",
      "         [ 1.4956],\n",
      "         [-0.5196],\n",
      "         [-3.7962],\n",
      "         [-1.0943],\n",
      "         [-3.4050],\n",
      "         [ 1.6561],\n",
      "         [-3.6985],\n",
      "         [-1.0715],\n",
      "         [ 0.1505],\n",
      "         [-2.6509],\n",
      "         [-0.9339],\n",
      "         [-3.5272],\n",
      "         [-2.5785],\n",
      "         [ 1.4683],\n",
      "         [-1.0343],\n",
      "         [-1.3547],\n",
      "         [-2.1940],\n",
      "         [-1.2194],\n",
      "         [-2.4979],\n",
      "         [-0.4168],\n",
      "         [-1.3967],\n",
      "         [ 0.8017],\n",
      "         [-0.8277],\n",
      "         [-1.7816],\n",
      "         [-3.5637],\n",
      "         [-0.8025],\n",
      "         [-3.2165],\n",
      "         [-2.4068],\n",
      "         [-2.0962],\n",
      "         [ 0.1344],\n",
      "         [-2.9346],\n",
      "         [-0.0810],\n",
      "         [-0.9894],\n",
      "         [ 0.9175],\n",
      "         [-1.0700]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/07/2023 02:09:06 PM WARNING 100723 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/07/2023 02:09:08 PM WARNING 100723 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/07/2023 02:09:08 PM WARNING 100723 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/07/2023 02:09:08 PM WARNING 100723 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/07/2023 02:09:08 PM WARNING 100723 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/07/2023 02:09:08 PM WARNING 100723 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/07/2023 02:09:08 PM WARNING 100723 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/07/2023 02:09:08 PM WARNING 100723 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/07/2023 02:09:08 PM WARNING 100723 [WalrusDriver]: DRAM allocation successful\n"
     ]
    }
   ],
   "source": [
    "def convert_torch_script(model, dummy_inputs):\n",
    "    # Compile the model for Neuron\n",
    "    model_neuron = torch_neuronx.trace(model, dummy_inputs)\n",
    "    \n",
    "    return model_neuron\n",
    "\n",
    "model_neuron = convert_torch_script(ncf_model, dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0, 125, 834,  39, 770, 899, 464, 531, 791, 768, 790, 675, 727,\n",
       "         844,  14, 855, 230, 698, 732, 884, 605, 686, 756,  99, 296, 213,  29,\n",
       "         813, 497, 171, 129, 350, 625, 399, 249, 982, 197, 980, 962, 441, 455,\n",
       "         761, 990, 367, 819, 236, 944, 851, 860,  43, 250, 560, 979, 435, 152,\n",
       "          40, 420, 659, 348, 363, 679, 512, 447, 749, 841, 631, 531, 290, 913,\n",
       "         696, 889,  51, 250, 685, 264, 753, 511, 888,  94, 189, 359, 152, 922,\n",
       "         595, 759, 205, 832,   1, 155, 118, 882,  75, 984, 165, 911, 419, 883,\n",
       "         166, 728,  30, 537]], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_neuron(dummy_inputs[0],dummy_inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-neuronx)",
   "language": "python",
   "name": "aws_neuron_venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
