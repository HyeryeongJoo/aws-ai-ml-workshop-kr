{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 1.1] Inference NCF on INF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 torch_neuronx 를 로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 훈련된 모델 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 모델 아티펙트 확인\n",
    "\n",
    "- 이미 훈련된 파이토치로 훈련된 모델 아티텍트의 경로를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifact is assigend from :  models/NeuMF-end.pth\n"
     ]
    }
   ],
   "source": [
    "artifact_path = 'models/NeuMF-end.pth'\n",
    "print(\"model artifact is assigend from : \", artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 로딩에 필요한 설정 파일 생성\n",
    "\n",
    "- 모델 로딩시에 필요한 파라미터 사용 (기존의 값을 사용 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of num_layers:  3\n",
      "user_num:  6040  item_num:  3706\n",
      "src/model_config.json is saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'src/model_config.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import config\n",
    "from common_utils import save_json, load_json\n",
    "\n",
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.factor_num = 32\n",
    "        self.num_layers = 3\n",
    "        self.dropout = 0.0\n",
    "                        \n",
    "args = Params()\n",
    "print(\"# of num_layers: \", args.num_layers)\n",
    "\n",
    "\n",
    "# 모델 훈련시에 결정된 user, item 의 숫자\n",
    "user_num = 6040  \n",
    "item_num = 3706\n",
    "print(\"user_num: \", user_num, \" item_num: \", item_num)\n",
    "\n",
    "model_config_dict = {\n",
    "    'user_num': str(user_num),\n",
    "    'item_num': str(item_num),\n",
    "    'factor_num' : str(args.factor_num),\n",
    "    'num_layers' : str(args.num_layers),\n",
    "    'dropout' : str(args.dropout),\n",
    "    'model_type': config.model\n",
    "}\n",
    "\n",
    "model_config_file = 'model_config.json'\n",
    "model_config_file_path = os.path.join('src', model_config_file)\n",
    "\n",
    "save_json(model_config_file_path, model_config_dict)\n",
    "# model_config_dict = load_json(model_config_file_path)    \n",
    "# model_config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 모델 로딩\n",
    "- 모델 로딩 함수 model_fn() 를 통하여 모델 로딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Staring model_fn() ###############\n",
      "device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from inference import model_fn\n",
    "\n",
    "ncf_model = model_fn(config.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 컴파일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 샘플 입력 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "type:  <class 'tuple'>\n",
      "len:  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def create_dummy_input(batch_size):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using {} device\".format(device))\n",
    "\n",
    "    user_np = np.zeros((1,100)).astype(np.int32)\n",
    "    item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "    return (\n",
    "        torch.repeat_interleave(torch.from_numpy(user_np), batch_size, 0),\n",
    "        torch.repeat_interleave(torch.from_numpy(item_np), batch_size, 0),\n",
    "    )\n",
    "\n",
    "dummy_inputs = create_dummy_input(batch_size=1)\n",
    "\n",
    "print(\"type: \", type(dummy_inputs))\n",
    "print(\"len: \", len(dummy_inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Script 으로 변환 (컴파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### prediction: \n",
      " tensor([[[-2.8306],\n",
      "         [-1.6553],\n",
      "         [ 2.5198],\n",
      "         [ 0.1828],\n",
      "         [-1.6887],\n",
      "         [-1.7654],\n",
      "         [-0.2166],\n",
      "         [-2.7551],\n",
      "         [-2.4077],\n",
      "         [-3.2328],\n",
      "         [-1.1433],\n",
      "         [-2.6161],\n",
      "         [-3.3304],\n",
      "         [ 0.2665],\n",
      "         [-0.8025],\n",
      "         [-1.6792],\n",
      "         [-0.3755],\n",
      "         [-0.1721],\n",
      "         [-1.1990],\n",
      "         [ 3.0670],\n",
      "         [-1.9710],\n",
      "         [ 1.1168],\n",
      "         [-1.0715],\n",
      "         [-2.0493],\n",
      "         [-2.0830],\n",
      "         [-1.3871],\n",
      "         [-2.8947],\n",
      "         [ 0.2034],\n",
      "         [-1.7649],\n",
      "         [ 1.8365],\n",
      "         [-3.6234],\n",
      "         [-2.7542],\n",
      "         [-1.3663],\n",
      "         [-0.1977],\n",
      "         [-1.7464],\n",
      "         [-3.9345],\n",
      "         [-2.0422],\n",
      "         [-0.5732],\n",
      "         [-1.8859],\n",
      "         [-1.6468],\n",
      "         [-3.0406],\n",
      "         [ 0.3931],\n",
      "         [-2.0422],\n",
      "         [ 0.4182],\n",
      "         [-2.5493],\n",
      "         [ 1.4564],\n",
      "         [-4.1203],\n",
      "         [ 2.5553],\n",
      "         [-0.6861],\n",
      "         [ 2.1306],\n",
      "         [-2.3055],\n",
      "         [-2.7905],\n",
      "         [ 1.8238],\n",
      "         [ 0.5838],\n",
      "         [-3.2080],\n",
      "         [-2.0109],\n",
      "         [-1.5514],\n",
      "         [-3.4663],\n",
      "         [ 1.1852],\n",
      "         [-1.8815],\n",
      "         [-1.9661],\n",
      "         [-0.5732],\n",
      "         [ 1.4246],\n",
      "         [-2.4418],\n",
      "         [-1.1053],\n",
      "         [-1.0365],\n",
      "         [-1.3983],\n",
      "         [-1.8441],\n",
      "         [-3.6553],\n",
      "         [-0.6370],\n",
      "         [-0.3335],\n",
      "         [-2.1523],\n",
      "         [-3.2328],\n",
      "         [-2.5546],\n",
      "         [ 0.3776],\n",
      "         [-0.5551],\n",
      "         [-0.6099],\n",
      "         [-0.1642],\n",
      "         [ 1.8222],\n",
      "         [-0.2868],\n",
      "         [-1.6870],\n",
      "         [-2.5539],\n",
      "         [-3.9436],\n",
      "         [-1.2973],\n",
      "         [ 0.2465],\n",
      "         [ 0.5398],\n",
      "         [-0.8561],\n",
      "         [-2.0721],\n",
      "         [-2.6322],\n",
      "         [-2.7582],\n",
      "         [-0.6099],\n",
      "         [-1.6182],\n",
      "         [-1.0531],\n",
      "         [ 0.3931],\n",
      "         [ 0.8560],\n",
      "         [ 1.2475],\n",
      "         [-2.9978],\n",
      "         [-1.7128],\n",
      "         [-2.1078],\n",
      "         [-2.5169]]], device='xla:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=0, shape=torch.Size([1, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:117: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 100]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "03/10/2023 01:47:18 PM WARNING 23724 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/10/2023 01:47:20 PM WARNING 23724 [WalrusDriver]: 0% PSUM demand before spilling\n",
      "03/10/2023 01:47:20 PM WARNING 23724 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/10/2023 01:47:20 PM WARNING 23724 [WalrusDriver]: 0% PSUM utilization after allocation\n",
      "03/10/2023 01:47:20 PM WARNING 23724 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/10/2023 01:47:20 PM WARNING 23724 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/10/2023 01:47:20 PM WARNING 23724 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/10/2023 01:47:20 PM WARNING 23724 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/10/2023 01:47:20 PM WARNING 23724 [WalrusDriver]: DRAM allocation successful\n"
     ]
    }
   ],
   "source": [
    "def convert_torch_script(model, dummy_inputs):\n",
    "    # Compile the model for Neuron\n",
    "    model_neuron = torch_neuronx.trace(model, dummy_inputs)\n",
    "    \n",
    "    return model_neuron\n",
    "\n",
    "model_neuron = convert_torch_script(ncf_model, dummy_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:prediction  <class 'tuple'>\n",
      "type:prediction[0]  <class 'torch.Tensor'>\n",
      "recommended_item_index: \n",
      " tensor([19, 47,  2, 49, 29, 52, 78, 45, 62, 95])\n"
     ]
    }
   ],
   "source": [
    "def extract_top_k(prediction, top_k = 10):\n",
    "    prediction = torch.squeeze(prediction) # remove dimension\n",
    "    _, indices = torch.topk(prediction, top_k)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "prediction = model_neuron(dummy_inputs[0],dummy_inputs[1])\n",
    "print(\"type:prediction \", type(prediction))\n",
    "print(\"type:prediction[0] \", type(prediction[0]))\n",
    "\n",
    "recommended_item_index = extract_top_k(prediction[0], top_k = 10)\n",
    "print(\"recommended_item_index: \\n\", recommended_item_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-neuronx)",
   "language": "python",
   "name": "aws_neuron_venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
