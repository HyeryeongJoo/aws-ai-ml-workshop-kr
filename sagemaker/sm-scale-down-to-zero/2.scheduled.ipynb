{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660292bc-cea8-4bb9-965c-684ef3368679",
   "metadata": {},
   "source": [
    "# Scheduled your SageMaker Inference Endpoints to scale in to Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09668c9e-4794-4aa0-8a12-6fb134a66f8a",
   "metadata": {},
   "source": [
    "In some scenario, you might observe consistent weekly traffic patterns: steady workload Monday through Friday, and no traffic on weekends. You can optimize costs and performance by configuring scheduled actions that align with these patterns:\n",
    "\n",
    "* **Weekend Scale-in (Friday Evening)**: Configure a scheduled action to reduce the number of model copies to zero. This will instruct SageMaker to scale the number instance behind the endpoint to zero, completely eliminating costs during weekend no usage period. \n",
    "* **Workweek Scale-out (Monday Morning)**: Set up a complementary scheduled action to restore the required model capacity, for the Inference component on Monday morning, ensuring your application is ready for weekday operations.\n",
    "\n",
    "This demo notebook demonstrates how you can schedule the scale in of your SageMaker endpoint to zero instances during idle periods, eliminating the previous requirement of maintaining at least one running instance. \n",
    "\n",
    "**Note:** Scale-to-zero is only supported when using inference components. for more information on Inference Components see “[Reduce model deployment costs by 50% on average using the latest features of Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/reduce-model-deployment-costs-by-50-on-average-using-sagemakers-latest-features/)” blog.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c926cc-d7b0-4eb7-8ac7-6d6d84c57c7d",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27b126-cf7f-4e87-bb6b-c9c328212b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir sagemaker==2.235.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.session.Session(boto_session) # sagemaker session for interacting with different AWS APIs\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "model_bucket = sagemaker_session.default_bucket()  # bucket to house model artifacts\n",
    "\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "print(f\"prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1718a-1f73-45f0-bb79-e3d0d7761f5b",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4a700-336b-4bc6-acfd-374fddf614a0",
   "metadata": {},
   "source": [
    "### Create a SageMaker endpoint configuration\n",
    "\n",
    "We begin by creating the endpoint configuration and set MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an unique name for our endpoint config\n",
    "endpoint_config_name = f\"{prefix}-llama3-8b-scale-to-zero-sc-config\"\n",
    "print(f\"Endpoint config name: {endpoint_config_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e65d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure variant name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 3\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18901bc5-7df0-4675-aedf-a624792a00f6",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint\n",
    "Next, we create our endpoint using the endpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-llama3-8b-scale-to-zero-sc-endpoint\"\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac73b99-c239-4b3d-b5c4-ac7fa548db40",
   "metadata": {},
   "source": [
    "#### We wait for our endpoint to go InService. This step can take ~3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d6b67-7dae-4c02-890e-c3c405b6521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Let's see how much it takes\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = desc[\"EndpointStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e89259-9e8f-4a45-8dab-74f2c04f4a2a",
   "metadata": {},
   "source": [
    "## Create Model Builder\n",
    "We use Amazon SageMaker Fast Model Loader. The feature works by streaming model weights directly from Amazon S3 to GPU accelerators, bypassing the typical sequential loading steps that contribute to deployment latency. In internal testing, this approach has shown to load large models up to 15 times faster compared to traditional methods. For more information on this feature, please refer to our example [notebook on GitHub](https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/Llama3.1/Llama3.1-70B-SageMaker-Fast-Model-Loader.ipynb)\n",
    "\n",
    "We'll make use of the ModelBuilder class to prepare and package the model inference components. In this example, we're using the Meta-Llama-3-8B-Instruct SageMaker JumpStart.\n",
    "\n",
    "Key configurations:\n",
    "- Model: Meta-Llama-3-8B-Instruct\n",
    "- Schema Builder: Defines input/output format\n",
    "- Model_metadata: `CUSTOM_MODEL_PATH` Here we reuse the shards from our previous model optimization run in the autoscaling example notebook\n",
    "- Instance_type: ml.g5.12xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2998320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "import logging\n",
    "\n",
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "response = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the east coast.\"\n",
    "\n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\"\n",
    "output_path = f\"s3://{model_bucket}/llama3-8b/sharding\"\n",
    "\n",
    "model_builder = ModelBuilder(\n",
    "    model=model_id,\n",
    "    model_metadata={\n",
    "                \"CUSTOM_MODEL_PATH\": output_path,\n",
    "            },\n",
    "    role_arn=role,\n",
    "    schema_builder=SchemaBuilder(sample_input=prompt, sample_output=response),\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    log_level=logging.WARN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79050306-89da-4572-bff7-64404fdab5fd",
   "metadata": {},
   "source": [
    "## Build and Deploy Model\n",
    "After optimization, we'll build the final model artifacts and deploy them to a SageMaker endpoint. \n",
    "\n",
    "Key configurations:\n",
    "- Instance Type: ml.g5.12xlarge\n",
    "- Memory Request: 104096 MB\n",
    "- Number of Accelerators: 4 (for tensor parallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e4d6a-6a47-40e4-bd09-51099f873edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = model_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d534d8e-91b9-4e40-b80f-4d294a57ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our model is sharded\n",
    "if not final_model._is_sharded_model:\n",
    "    final_model._is_sharded_model = True\n",
    "final_model._is_sharded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa5672-827f-4cbf-ac68-d17fa4c0a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnableNetworkIsolation cannot be set to True since SageMaker Fast Model Loading of model requires network access.\n",
    "if final_model._enable_network_isolation:\n",
    "    final_model._enable_network_isolation = False\n",
    "    \n",
    "final_model._enable_network_isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ddf72c-fc39-42c4-9e06-f4f8389872f6",
   "metadata": {},
   "source": [
    "#### Select the container image to use\n",
    "Use the latest LMI image to take advantage of caching feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabd824-7728-4507-99b5-dd54dee0e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124\"\n",
    "print(f\"Image going to be used is ---- > {final_model.image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772261e-de5d-4213-967b-cb10c6d1f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "resources_required = ResourceRequirements(\n",
    "    requests={\n",
    "        \"memory\" : 104096,\n",
    "        \"num_accelerators\": 4,\n",
    "        \"copies\": 1, # specify the number of initial copies (default is 1)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe454e-1d62-462b-bcc4-0aa7b0800cc2",
   "metadata": {},
   "source": [
    "#### Deploy your model to the endpoint\n",
    "\n",
    "Deploy your model with the model’s existing deploy method. We specify the name of our existing Real-time endpoint SageMaker will host the model on our existing endpoint, so it can starts making predictions on incoming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7189d-3d40-47f4-8571-d6b6dedbab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.deploy(\n",
    "    instance_type=\"ml.g5.12xlarge\", \n",
    "    accept_eula=True, \n",
    "    endpoint_name=endpoint_name,\n",
    "    # endpoint_logging=False, \n",
    "    resources=resources_required,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fad11c-57d7-4c27-bcdb-57a8699136ca",
   "metadata": {},
   "source": [
    "### Test the endpoint with a sample prompt\n",
    "Now we can invoke our endpoint with sample text to test its functionality and see the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b1ea2-78d9-4757-904b-037681cc4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import retrieve_default \n",
    "\n",
    "endpoint_name = final_model.endpoint_name \n",
    "inference_component_name = final_model.inference_component_name\n",
    "\n",
    "predictor = retrieve_default(endpoint_name, sagemaker_session=sagemaker_session, \n",
    "                             inference_component_name = inference_component_name, \n",
    "                             model_id=model_id) \n",
    "\n",
    "payload = { \"inputs\": \"What is deep learning a?\", \n",
    "            \"parameters\": { \n",
    "                \"max_new_tokens\": 64, \n",
    "                \"top_p\": 0.7, \n",
    "                \"temperature\": 0.9 \n",
    "            } \n",
    "        }\n",
    "response = predictor.predict(payload) \n",
    "print(response['generated_text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac2748-56a6-456d-afe4-f7af03149efc",
   "metadata": {},
   "source": [
    "## Schedules using (UpdateInferenceComponentRuntimeConfig API)\n",
    "\n",
    "You can scale your endpoint to zero in two ways. The first method is to set the number of model copies to zero in your Inference component using [UpdateInferenceComponentRuntimeConfigAPI](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateInferenceComponentRuntimeConfig.html). This approach maintains your endpoint configuration while eliminating compute costs during periods of inactivity. \n",
    "```\n",
    "sagemaker_client.update_inference_component_runtime_config(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    DesiredRuntimeConfig={\n",
    "        'CopyCount': 0\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c210d-84f5-4c66-a7b5-52d66a1f70cd",
   "metadata": {},
   "source": [
    "### Create a schedule to shutdown the endpoint on friday and brings it back on Monday\n",
    "\n",
    "[Amazon EventBridge Scheduler](https://docs.aws.amazon.com/eventbridge/latest/userguide/using-eventbridge-scheduler.html) can automate SageMaker API calls using cron/rate expressions for recurring schedules or one-time invocations. To function, EventBridge Scheduler requires an execution role with appropriate permissions to invoke the target API operations on your behalf, please refer to the [documentation](https://docs.aws.amazon.com/scheduler/latest/UserGuide/setting-up.html#setting-up-execution-role) on how to create this role. The specific permissions needed depend on the target API being called.\n",
    "\n",
    "The code below creates two scheduled actions for the Inference component during 2024-2025. The first schedule scales in the CopyCount to zero every Friday at 18:00 UTC+1, while the second schedule restores model capacity every Monday at 07:00 UTC+1. The schedule swill start on November 29, 2024, end on December 31, 2025, and will be deleted after completion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2eef9-e0ff-4c89-8bd8-3845ac25174c",
   "metadata": {},
   "source": [
    "#### Weekend Scale-in (Friday Evening)\n",
    "We start with creating a schedule to scale in our endpoint to 0 every friday at 18:00 UTC+1, starting on November 29, 2024 and ending on December 31, 2025. We need to specify the target API to call [UpdateInferenceComponentRuntimeConfigAPI](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateInferenceComponentRuntimeConfig.html) in this case , and the correct parameter for that API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59c9a7-6fb5-46bf-91d4-5af177d16d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_component_name = final_model.inference_component_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a23b2-3ac2-45d6-b98e-351a706c1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "scheduler = boto3.client('scheduler')\n",
    "\n",
    "flex_window = {\n",
    "    \"Mode\": \"OFF\"\n",
    "}\n",
    "\n",
    "# We specify the SageMaker target API for the scale in schedule\n",
    "scale_in_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:updateInferenceComponentRuntimeConfig\",\n",
    "    \"Input\": json.dumps({ \"DesiredRuntimeConfig\": {\"CopyCount\": 0}, \"InferenceComponentName\": inference_component_name })\n",
    "}\n",
    "\n",
    "# Scale in our endpoint to 0 every friday at 18:00 UTC+1, starting on November 29, 2024\n",
    "update_IC_scale_in_schedule = \"scale-to-zero-schedule\"\n",
    "scheduler.create_schedule(\n",
    "    Name=update_IC_scale_in_schedule,\n",
    "    ScheduleExpression=\"cron(00 18 ? * 6 2024-2025)\",\n",
    "    ScheduleExpressionTimezone=\"UTC+1\", # Set the correct timezone for your application\n",
    "    Target=scale_in_target,\n",
    "    FlexibleTimeWindow=flex_window,\n",
    "    ActionAfterCompletion=\"DELETE\",\n",
    "    StartDate=\"2024-11-29T17:00:00\",\n",
    "    EndDate=\"2025-12-31T23:59:59\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb578b58-7d33-4811-922e-1036e633119f",
   "metadata": {},
   "source": [
    "#### Workweek Scale-out (Monday Morning):\n",
    "Set up a complementary scheduled action to restore the required model capacity, for the Inference component on Monday morning 07:00 UTC+1, ensuring your application is ready for weekday operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the SageMaker target API for the scale out schedule\n",
    "scale_out_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:updateInferenceComponentRuntimeConfig\",\n",
    "    \"Input\": json.dumps({ \"DesiredRuntimeConfig\": {\"CopyCount\": 2}, \"InferenceComponentName\": inference_component_name })\n",
    "}\n",
    "\n",
    "# Scale out our endpoint every Monday at 07:00 UTC+1\n",
    "update_IC_scale_out_schedule = \"scale-out-schedule\"\n",
    "scheduler.create_schedule(\n",
    "    Name=update_IC_scale_out_schedule,\n",
    "    ScheduleExpression=\"cron(00 07 ? * 2 2024-2025)\",\n",
    "    ScheduleExpressionTimezone=\"UTC+1\", # Set the correct timezone for your application\n",
    "    Target=scale_out_target,\n",
    "    FlexibleTimeWindow=flex_window,\n",
    "    ActionAfterCompletion=\"DELETE\",\n",
    "    StartDate=\"2024-11-29T17:00:00\",\n",
    "    EndDate=\"2025-12-31T23:59:59\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338ce87-0777-4026-baf4-20803fb5cfec",
   "metadata": {},
   "source": [
    "## Schedules using (DeleteInferenceComponen API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09b8ab-cdf1-4309-814a-f8f1d39bed6e",
   "metadata": {},
   "source": [
    "The second method is to delete the Inference components by calling the [DeleteInferenceComponent API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DeleteInferenceComponent.html). This alternative approach achieves the same cost-saving benefit while completely removing the components from your configuration. The following code creates a scheduled action that automatically delete the IC every Friday at 18:00 UTC during 2024-2025. it also create a complementary scheduled action that recreate the IC every Monday at 07:00 UTC+1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c40e3b4-0215-497a-86df-31c2c62cd311",
   "metadata": {},
   "source": [
    "We Fetch the model_name to be able to recreate our inference component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4609448-601a-400e-80b7-cf909bc8fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "model_name = res ['Specification']['ModelName']\n",
    "print(model_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6829b-4cf6-4a78-a720-b585c8e702a3",
   "metadata": {},
   "source": [
    "#### Weekend Scale-in (Friday Evening)\n",
    "The following code creates a scheduled action that automatically delete the IC every Friday at 18:00 UTC during 2024-2025. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c5d8f-274e-4629-86a9-87189911b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "scheduler = boto3.client('scheduler')\n",
    "\n",
    "flex_window = {\n",
    "    \"Mode\": \"OFF\"\n",
    "}\n",
    "\n",
    "# We specify the SageMaker target API for the scale in schedule\n",
    "scale_in_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:deleteInferenceComponent\",\n",
    "    \"Input\": json.dumps({\"InferenceComponentName\": inference_component_name })\n",
    "}\n",
    "\n",
    "# Scale in our endpoint by deleting the IC every friday at 18:00 UTC+1\n",
    "delete_IC_scale_in_schedule = \"scale-to-zero-schedule-1\"\n",
    "scheduler.create_schedule(\n",
    "    Name=delete_IC_scale_in_schedule,\n",
    "    ScheduleExpression=\"cron(00 18 ? * 6 2024-2025)\",\n",
    "    ScheduleExpressionTimezone=\"UTC+1\", # Set the correct timezone for your application\n",
    "    Target=scale_in_target,\n",
    "    FlexibleTimeWindow=flex_window,\n",
    "    ActionAfterCompletion=\"DELETE\",\n",
    "    StartDate=\"2024-11-29T17:00:00\",\n",
    "    EndDate=\"2025-12-31T23:59:59\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00800cf7-1f82-40e4-8c24-18f47449721d",
   "metadata": {},
   "source": [
    "#### Workweek Scale-out (Monday Morning):\n",
    "create a complementary scheduled action that recreate the IC every Monday at 07:00 UTC+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec751d-a157-4ef7-8363-7759a6ba6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the SageMaker target API for the scale up schedule\n",
    "input_config = {\n",
    "  \"EndpointName\": endpoint_name,\n",
    "  \"InferenceComponentName\": inference_component_name,\n",
    "  \"RuntimeConfig\": {\n",
    "    \"CopyCount\": 2\n",
    "  },\n",
    "  \"Specification\": {\n",
    "    \"ModelName\": model_name,\n",
    "    \"StartupParameters\": {\n",
    "        \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "    },\n",
    "    \"ComputeResourceRequirements\": {\n",
    "      \"MinMemoryRequiredInMb\": 1024,\n",
    "      \"NumberOfAcceleratorDevicesRequired\": 1\n",
    "    }\n",
    "  },\n",
    "  \"VariantName\": variant_name\n",
    "}\n",
    "\n",
    "scale_out_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:createInferenceComponent\",\n",
    "    \"Input\": json.dumps(input_config)\n",
    "}\n",
    "\n",
    "# Scale out our endpoint by recreating the IC every Monday at 07:00 UTC+1\n",
    "delete_IC_scale_out_schedule = \"scale-out-schedule-1\"\n",
    "scheduler.create_schedule(\n",
    "    Name=delete_IC_scale_out_schedule,\n",
    "    ScheduleExpression=\"cron(00 07 ? * 2 2024-2025)\",\n",
    "    ScheduleExpressionTimezone=\"UTC+1\", # Set the correct timezone for your application\n",
    "    Target=scale_out_target,\n",
    "    FlexibleTimeWindow=flex_window,\n",
    "    ActionAfterCompletion=\"DELETE\",\n",
    "    StartDate=\"2024-11-29T17:00:00\",\n",
    "    EndDate=\"2025-12-31T23:59:59\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6624eb-278e-4035-925e-dc0c69f7d654",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "To schedule the scale to Zero on an endpoint with multiple inference components (IC), all ICs must be either set to 0 or deleted. You can also automate this process by using EventBridge Scheduler to trigger a Lambda function that handles either deletion or zero-setting of all ICs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ffab4-db77-467f-b472-a621c229adee",
   "metadata": {},
   "source": [
    "## Optionally clean up the environment\n",
    "\n",
    "- Delete schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79145a5-03d6-407b-85f6-78796235b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all the schedule created above\n",
    "schedules = [delete_IC_scale_out_schedule, delete_IC_scale_in_schedule, update_IC_scale_out_schedule, update_IC_scale_in_schedule]\n",
    "\n",
    "for schedule in schedules:\n",
    "    try:\n",
    "        scheduler.delete_schedule(Name=schedule)\n",
    "        print(f\"Deleted schedule [b]{schedule} ✅\")\n",
    "    except scheduler.exceptions.ResourceNotFoundException:\n",
    "        print(f\"Schedule [b]{schedule}[/b] not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf611d6-e15b-4a0b-8c05-284ae1b8a07d",
   "metadata": {},
   "source": [
    "Delete inference component\n",
    "\n",
    "Delete endpoint\n",
    "\n",
    "delete endpoint-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11e610-c8c1-4fb2-bd4d-5c71e76a1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name)\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
