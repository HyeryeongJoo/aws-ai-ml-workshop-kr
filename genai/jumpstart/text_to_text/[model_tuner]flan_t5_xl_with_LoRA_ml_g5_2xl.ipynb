{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [model_tuner]flan_t5_xl_with_LoRA_ml_g5_2xl\n",
    "\n",
    "이 sagemaker 예제에서는 [Low-Rank Adaptation of Large Language Models (LoRA)](https://arxiv.org/abs/2106.09685)를 적용하여 단일 GPU에서 flan-t5-xl를 fine-tun하는 방법에 대해 알아볼 것입니다. \n",
    "Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), [PEFT](https://github.com/huggingface/peft)를 활용할 예정입니다.\n",
    "\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune flan-t5-xl with LoRA and bnb int-8 on Amazon SageMaker\n",
    "4. Deploy the model to Amazon SageMaker Endpoint\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning은 Hugging Face의 신규 오픈소스 라이브러리로, 모든 모델의 파라미터를 fine-tuning없이 다양한 downstream application에 대한 pre-trained language models (PLMs)을 효과적으로 적용할 수 있게 합니다. PEFT는 현재 다음 techniques을 포함하고 있습니다.\n",
    "\n",
    "- LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "- AdaLoRA: [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.10512.pdf)\n",
    "\n",
    "[](https://github.com/huggingface/notebooks/blob/main/sagemaker/24_train_bloom_peft_lora/sagemaker-notebook.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# #!/bin/bash\n",
    "\n",
    "# DAEMON_PATH=\"/etc/docker\"\n",
    "# MEMORY_SIZE=10G\n",
    "\n",
    "# FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# # echo $FLAG\n",
    "\n",
    "# if [ \"$FLAG\" == true ]; then\n",
    "#     echo \"Already revised\"\n",
    "# else\n",
    "#     echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "#     sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "#     sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "#     sudo service docker restart\n",
    "#     echo \"Docker Restart\"\n",
    "# fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets[s3] transformers sagemaker py7zr --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install \"transformers==4.26.0\" \"datasets[s3]==2.9.0\" sagemaker py7zr --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로컬 환경에서 SageMaker를 사용하려는 경우. SageMaker에 필요한 권한이 있는 IAM 역할에 대한 액세스 권한이 필요합니다. 자세한 내용은 [여기](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::322537213286:role/service-role/AmazonSageMaker-ExecutionRole-20230528T120509\n",
      "sagemaker bucket: sagemaker-us-west-2-322537213286\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "여기서는 약 16k개의 메신저 대화 모음과 요약이 포함된 [samsum](https://huggingface.co/datasets/samsum) 데이터셋을 사용하겠습니다. 대화는 영어에 능통한 언어학자들이 작성하고 기록하였습니다.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load the `samsum` 데이터셋을 로드하기 위해, 🤗 Datasets 라이브러리에서 `load_dataset()` 메소드를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a64ecd4b7e458ab41b4c10c3b93f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "# Train dataset size: 14732"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습하기 위해, 🤗 Transformers Tokenizer를 이용하여 inputs (text)를 token IDs로 변환하게 됩니다. 이것이 의미하는 것을 모르신다면, the Hugging Face Course의 **[chapter 6](https://huggingface.co/course/chapter6/1?fw=tf)** 을 확인하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id='google/flan-t5-xl'\n",
    "\n",
    "# Load tokenizer of flan-t5-xl\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.model_max_length = 2048 # overwrite wrong value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 시작하기 전에 데이터를 전처리해야 합니다. Abstractive Summarization가 텍스트 생성 task 입니다. 모델은 텍스트를 입력으로 받아 summary를 출력으로 생성합니다. 데이터를 효율적으로 일괄 처리하기 위해 입력과 출력에 걸리는 시간을 파악하고자 합니다.\n",
    "\n",
    "모델의 성능을 개선하기 위해 instruct prompt를 구성하는 데 사용할 `prompt_template`를 정의하였습니다.`prompt_template`은 시작과 끝이 \"고정\"되어 있고, 문서가 중간에 있습니다. 즉, \"고정된\" 템플릿 부분 + 문서가 모델의 최대 길이를 초과하지 않도록 해야 합니다. \n",
    "학습 전에 데이터 집합을 전처리하고 디스크에 저장한 다음 S3에 업로드합니다. 이 단계는 로컬 컴퓨터 또는 CPU에서 실행하고 [Hugging Face Hub](https://huggingface.co/docs/hub/datasets-overview)에 업로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-ccf0f9a49ab83084.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 50\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = dataset[\"train\"].map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "input_lengths = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lengths, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = dataset[\"train\"].map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "target_lengths = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lengths, 90))\n",
    "print(f\"Max target length: {max_target_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-e4c84224a7b3f37d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-9fb0724aee9b2408.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "dataset to: s3://sagemaker-us-west-2-322537213286/processed/samsum-sagemaker/data\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/samsum-sagemaker/data'\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(f\"{training_input_path}/tokenized_train\")\n",
    "\n",
    "## save test dataset without preprocessing to evaluate in the training script\n",
    "tokenized_dataset[\"test\"].save_to_disk(f\"{training_input_path}/tokenized_test\")\n",
    "dataset[\"test\"].save_to_disk(f\"{training_input_path}/test\")\n",
    "\n",
    "# save datasets to disk for local debugging\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"./data/tokenized_train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"./data/tokenized_test\")\n",
    "dataset[\"test\"].save_to_disk(\"./data/test\")\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋을 처리한 후 새로운 [FileSystem integration](https://huggingface.co/docs/datasets/filesystems)을 사용하여 데이터셋을 S3에 업로드할 것입니다. 여기서는 `sess.default_bucket()`을 사용하고 있으며, 데이터셋을 다른 S3 버킷에 저장하려면 이 값을 조정합니다. 이후 학습 스크립트에서 S3 경로를 사용할 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Fine-Tune flan-t5-xl with LoRA and bnb int-8 on Amazon SageMaker\n",
    "\n",
    "LoRA 기법 외에도 [bitsandbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration)을 사용하여 frozen된 LLM을 int8로 quantize합니다. 이를 통해 flan-t5-xl에 필요한 메모리를 최대 4배까지 줄일 수 있습니다. \n",
    "\n",
    "PEFT를 사용하여 모델을 학습하는 [run_clm.py](./scripts/run_clm.py)를 준비했습니다. 어떻게 작동하는지 궁금하다면 [Efficient Large Language Model training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft) 블로그에서 학습 스크립트에 대해 자세히 설명되어 있습니다.\n",
    "\n",
    "SageMaker 학습 작업을 생성하기 위해 `HuggingFace` Estimator가 필요합니다. Estimator는 end-to-end Amazon SageMaker 학습과 배포 작업을 처리합니다. Estimator는 인프라 사용을 관리합니다. \n",
    "SageMaker는 필요한 모든 ec2 인스턴스를 시작하고 관리하며, 올바른 huggingface 컨테이너를 제공하고, 제공된 스크립트를 업로드하고, `/opt/ml/input/data`의 컨테이너에 S3 버킷에서 데이터를 다운로드합니다. 그런 다음 실행을 통해 학습 작업을 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type='ml.g5.2xlarge'\n",
    "# instance_type='local_gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if instance_type in ['local', 'local_gpu']:\n",
    "    from sagemaker.local import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    data_path=f'file://{Path.cwd()}/data'\n",
    "else:\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    data_path=training_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-peft-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                                 # pre-trained model\n",
    "  # 'model_id': \"philschmid/flan-t5-xxl-sharded-fp16\",\n",
    "  'epochs': 1,                                          # number of training epochs\n",
    "  'per_device_train_batch_size': 100, #50, #15,         # batch size for training\n",
    "  'eval_sample': 50,                                    # batch size for evaluation\n",
    "  'lr': 2e-4,                                           # learning rate used during training\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point          = 'run_clm.py',               # train script\n",
    "    source_dir           = f'{Path.cwd()}/flan_t5_xl_with_LoRA',  # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type,              # instances type used for the training job\n",
    "    instance_count       = 1,                          # the number of instances used for training\n",
    "    base_job_name        = job_name,                   # the name of the training job\n",
    "    role                 = role,                       # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,                        # the size of the EBS volume in GB\n",
    "    framework_version    = '2.0',                      # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',                    # the python version used in the training job\n",
    "    sagemaker_session    = sagemaker_session,\n",
    "    hyperparameters      = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `.fit()` 메서드가 학습 스크립트에 S3 경로를 전달하여 학습 작업을 시작할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'tokenized_train': data_path+'/tokenized_train', \n",
    "    'tokenized_test': data_path+'/tokenized_test',\n",
    "    'test': data_path+'/test',\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(data, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-26 01:45:31 Starting - Starting the training job...\n",
      "2023-06-26 01:45:48 Starting - Preparing the instances for training......\n",
      "2023-06-26 01:46:45 Downloading - Downloading input data...\n",
      "2023-06-26 01:47:06 Training - Downloading the training image...........................\n",
      "2023-06-26 01:51:42 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:19,282 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:19,294 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:19,303 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:19,304 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:20,617 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting peft==0.2.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.2.0-py3-none-any.whl (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.3/40.3 kB 4.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.27.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 75.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.17.1 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.17.1-py3-none-any.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.8/212.8 kB 53.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.37.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.37.1-py3-none-any.whl (76.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.3/76.3 MB 31.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 79.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 27.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting rouge_score (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.27.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 60.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.27.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 97.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 111.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 49.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 92.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py (from rouge_score->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 32.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk (from rouge_score->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 110.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 28.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 45.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 35.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.1->-r requirements.txt (line 2)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0->-r requirements.txt (line 1)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0->-r requirements.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0->-r requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score->-r requirements.txt (line 7)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score->-r requirements.txt (line 7)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.2.0->-r requirements.txt (line 1)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.2.0->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f4d53622f0d6ef6154871e013fecb107add6d41c1e8b1013b1500988a945308e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\u001b[0m\n",
      "\u001b[34mSuccessfully built rouge_score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, bitsandbytes, xxhash, regex, multidict, frozenlist, async-timeout, absl-py, yarl, responses, nltk, huggingface-hub, aiosignal, transformers, rouge_score, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.4.0 accelerate-0.17.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bitsandbytes-0.37.1 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 nltk-3.8.1 peft-0.2.0 regex-2023.6.3 responses-0.18.0 rouge_score-0.1.2 tokenizers-0.13.3 transformers-4.27.1 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,871 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,871 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,884 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,905 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,926 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,936 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"tokenized_test\": \"/opt/ml/input/data/tokenized_test\",\n",
      "        \"tokenized_train\": \"/opt/ml/input/data/tokenized_train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_sample\": 50,\n",
      "        \"lr\": 0.0002,\n",
      "        \"model_id\": \"google/flan-t5-xl\",\n",
      "        \"per_device_train_batch_size\": 100\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"tokenized_test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"tokenized_train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-322537213286/huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_sample\":50,\"lr\":0.0002,\"model_id\":\"google/flan-t5-xl\",\"per_device_train_batch_size\":100}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_clm.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"tokenized_test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"tokenized_train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"tokenized_test\",\"tokenized_train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_clm\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-322537213286/huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"tokenized_test\":\"/opt/ml/input/data/tokenized_test\",\"tokenized_train\":\"/opt/ml/input/data/tokenized_train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_sample\":50,\"lr\":0.0002,\"model_id\":\"google/flan-t5-xl\",\"per_device_train_batch_size\":100},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"tokenized_test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"tokenized_train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_sample\",\"50\",\"--lr\",\"0.0002\",\"--model_id\",\"google/flan-t5-xl\",\"--per_device_train_batch_size\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TOKENIZED_TEST=/opt/ml/input/data/tokenized_test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TOKENIZED_TRAIN=/opt/ml/input/data/tokenized_train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_SAMPLE=50\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=google/flan-t5-xl\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=100\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_clm.py --epochs 1 --eval_sample 50 --lr 0.0002 --model_id google/flan-t5-xl --per_device_train_batch_size 100\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,962 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mTrain_sagemaker\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 1.44kB [00:00, 7.36MB/s]\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)model.bin.index.json: 50.8kB [00:00, 148MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.45G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   0%|          | 21.0M/9.45G [00:00<01:01, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   1%|          | 52.4M/9.45G [00:00<00:40, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   1%|          | 105M/9.45G [00:00<00:27, 334MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   2%|▏         | 157M/9.45G [00:00<00:24, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   2%|▏         | 210M/9.45G [00:00<00:22, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   3%|▎         | 262M/9.45G [00:00<00:21, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   3%|▎         | 315M/9.45G [00:00<00:21, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   4%|▍         | 367M/9.45G [00:00<00:20, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   4%|▍         | 419M/9.45G [00:01<00:20, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   5%|▍         | 472M/9.45G [00:01<00:19, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   6%|▌         | 524M/9.45G [00:01<00:19, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   6%|▌         | 577M/9.45G [00:01<00:19, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   7%|▋         | 629M/9.45G [00:01<00:19, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   7%|▋         | 682M/9.45G [00:01<00:19, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   8%|▊         | 734M/9.45G [00:01<00:19, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   8%|▊         | 786M/9.45G [00:01<00:20, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   9%|▉         | 839M/9.45G [00:01<00:20, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:   9%|▉         | 891M/9.45G [00:02<00:20, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  10%|▉         | 944M/9.45G [00:02<00:20, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  10%|█         | 986M/9.45G [00:02<00:20, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  11%|█         | 1.04G/9.45G [00:02<00:19, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.09G/9.45G [00:02<00:19, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.14G/9.45G [00:02<00:18, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.20G/9.45G [00:02<00:18, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.25G/9.45G [00:02<00:18, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.30G/9.45G [00:03<00:18, 448MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.35G/9.45G [00:03<00:17, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.41G/9.45G [00:03<00:17, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  15%|█▌        | 1.46G/9.45G [00:03<00:17, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.51G/9.45G [00:03<00:17, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.56G/9.45G [00:03<00:17, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.61G/9.45G [00:03<00:17, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.67G/9.45G [00:03<00:17, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.72G/9.45G [00:03<00:17, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.77G/9.45G [00:04<00:16, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.82G/9.45G [00:04<00:16, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.88G/9.45G [00:04<00:16, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  20%|██        | 1.93G/9.45G [00:04<00:16, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  21%|██        | 1.98G/9.45G [00:04<00:16, 455MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.03G/9.45G [00:04<00:16, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.09G/9.45G [00:04<00:16, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.14G/9.45G [00:04<00:16, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.19G/9.45G [00:05<00:16, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  24%|██▎       | 2.24G/9.45G [00:05<00:16, 444MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.30G/9.45G [00:05<00:15, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  25%|██▍       | 2.35G/9.45G [00:05<00:15, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  25%|██▌       | 2.40G/9.45G [00:05<00:15, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.45G/9.45G [00:05<00:15, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.51G/9.45G [00:05<00:15, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.56G/9.45G [00:05<00:15, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  28%|██▊       | 2.61G/9.45G [00:05<00:15, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  28%|██▊       | 2.66G/9.45G [00:06<00:14, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  29%|██▊       | 2.72G/9.45G [00:06<00:14, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  29%|██▉       | 2.77G/9.45G [00:06<00:14, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  30%|██▉       | 2.82G/9.45G [00:06<00:15, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  30%|███       | 2.87G/9.45G [00:06<00:15, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  31%|███       | 2.93G/9.45G [00:06<00:15, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  32%|███▏      | 2.98G/9.45G [00:06<00:14, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  32%|███▏      | 3.03G/9.45G [00:06<00:14, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  33%|███▎      | 3.08G/9.45G [00:07<00:14, 448MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  33%|███▎      | 3.14G/9.45G [00:07<00:14, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  34%|███▎      | 3.19G/9.45G [00:07<00:15, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  34%|███▍      | 3.24G/9.45G [00:07<00:14, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  35%|███▍      | 3.29G/9.45G [00:07<00:14, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  35%|███▌      | 3.34G/9.45G [00:07<00:13, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  36%|███▌      | 3.40G/9.45G [00:07<00:13, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  37%|███▋      | 3.45G/9.45G [00:07<00:14, 412MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  37%|███▋      | 3.50G/9.45G [00:08<00:14, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  38%|███▊      | 3.55G/9.45G [00:08<00:13, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  38%|███▊      | 3.61G/9.45G [00:08<00:13, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  39%|███▊      | 3.66G/9.45G [00:08<00:13, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  39%|███▉      | 3.71G/9.45G [00:08<00:12, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  40%|███▉      | 3.76G/9.45G [00:08<00:12, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  40%|████      | 3.82G/9.45G [00:08<00:12, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  41%|████      | 3.87G/9.45G [00:08<00:12, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  42%|████▏     | 3.92G/9.45G [00:08<00:12, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  42%|████▏     | 3.97G/9.45G [00:09<00:11, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  43%|████▎     | 4.03G/9.45G [00:09<00:11, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  43%|████▎     | 4.08G/9.45G [00:09<00:11, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  44%|████▎     | 4.13G/9.45G [00:09<00:11, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  44%|████▍     | 4.18G/9.45G [00:09<00:11, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  45%|████▍     | 4.24G/9.45G [00:09<00:11, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  45%|████▌     | 4.29G/9.45G [00:09<00:11, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  46%|████▌     | 4.34G/9.45G [00:09<00:11, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  46%|████▋     | 4.39G/9.45G [00:09<00:11, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  47%|████▋     | 4.45G/9.45G [00:10<00:10, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  48%|████▊     | 4.50G/9.45G [00:10<00:10, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  48%|████▊     | 4.55G/9.45G [00:10<00:13, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  49%|████▊     | 4.59G/9.45G [00:10<00:12, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  49%|████▉     | 4.65G/9.45G [00:10<00:12, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  50%|████▉     | 4.70G/9.45G [00:10<00:11, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  50%|█████     | 4.75G/9.45G [00:10<00:11, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  51%|█████     | 4.80G/9.45G [00:10<00:10, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  51%|█████▏    | 4.85G/9.45G [00:11<00:10, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 4.91G/9.45G [00:11<00:10, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 4.96G/9.45G [00:11<00:09, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  53%|█████▎    | 5.01G/9.45G [00:11<00:09, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  54%|█████▎    | 5.06G/9.45G [00:11<00:09, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  54%|█████▍    | 5.12G/9.45G [00:11<00:09, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  55%|█████▍    | 5.17G/9.45G [00:11<00:09, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  55%|█████▌    | 5.22G/9.45G [00:11<00:09, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  56%|█████▌    | 5.27G/9.45G [00:11<00:09, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  56%|█████▋    | 5.33G/9.45G [00:12<00:09, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  57%|█████▋    | 5.38G/9.45G [00:12<00:08, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  57%|█████▋    | 5.43G/9.45G [00:12<00:08, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  58%|█████▊    | 5.48G/9.45G [00:12<00:08, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  59%|█████▊    | 5.54G/9.45G [00:12<00:08, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  59%|█████▉    | 5.59G/9.45G [00:12<00:08, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  60%|█████▉    | 5.64G/9.45G [00:12<00:08, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  60%|██████    | 5.69G/9.45G [00:12<00:08, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  61%|██████    | 5.75G/9.45G [00:13<00:08, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  61%|██████▏   | 5.80G/9.45G [00:13<00:09, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  62%|██████▏   | 5.84G/9.45G [00:13<00:11, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  62%|██████▏   | 5.88G/9.45G [00:13<00:12, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  63%|██████▎   | 5.91G/9.45G [00:13<00:12, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  63%|██████▎   | 5.95G/9.45G [00:13<00:12, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  63%|██████▎   | 5.98G/9.45G [00:13<00:13, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  64%|██████▎   | 6.01G/9.45G [00:14<00:13, 258MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  64%|██████▍   | 6.04G/9.45G [00:14<00:13, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  64%|██████▍   | 6.07G/9.45G [00:14<00:13, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  65%|██████▍   | 6.10G/9.45G [00:14<00:13, 248MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  65%|██████▍   | 6.13G/9.45G [00:14<00:13, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  65%|██████▌   | 6.17G/9.45G [00:14<00:13, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.20G/9.45G [00:14<00:13, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.23G/9.45G [00:15<00:13, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.26G/9.45G [00:15<00:13, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  67%|██████▋   | 6.29G/9.45G [00:15<00:13, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  67%|██████▋   | 6.32G/9.45G [00:15<00:12, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  67%|██████▋   | 6.35G/9.45G [00:15<00:12, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  68%|██████▊   | 6.39G/9.45G [00:15<00:12, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  68%|██████▊   | 6.42G/9.45G [00:15<00:12, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  68%|██████▊   | 6.45G/9.45G [00:15<00:12, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  69%|██████▊   | 6.48G/9.45G [00:16<00:12, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  69%|██████▉   | 6.51G/9.45G [00:16<00:13, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  69%|██████▉   | 6.54G/9.45G [00:16<00:14, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  69%|██████▉   | 6.56G/9.45G [00:16<00:14, 193MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  70%|██████▉   | 6.59G/9.45G [00:16<00:16, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  70%|██████▉   | 6.61G/9.45G [00:16<00:17, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  70%|███████   | 6.63G/9.45G [00:16<00:17, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  70%|███████   | 6.65G/9.45G [00:17<00:18, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  71%|███████   | 6.67G/9.45G [00:17<00:18, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  71%|███████   | 6.69G/9.45G [00:17<00:18, 152MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  71%|███████   | 6.71G/9.45G [00:17<00:19, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  71%|███████   | 6.73G/9.45G [00:17<00:19, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  71%|███████▏  | 6.75G/9.45G [00:17<00:18, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 6.77G/9.45G [00:18<00:19, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 6.79G/9.45G [00:18<00:19, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 6.82G/9.45G [00:18<00:18, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 6.84G/9.45G [00:18<00:19, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 6.86G/9.45G [00:18<00:19, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 6.88G/9.45G [00:18<00:18, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 6.90G/9.45G [00:18<00:19, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 6.92G/9.45G [00:19<00:19, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 6.94G/9.45G [00:19<00:17, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  74%|███████▎  | 6.96G/9.45G [00:19<00:18, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  74%|███████▍  | 6.98G/9.45G [00:19<00:18, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  74%|███████▍  | 7.00G/9.45G [00:19<00:18, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  74%|███████▍  | 7.03G/9.45G [00:19<00:18, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  75%|███████▍  | 7.05G/9.45G [00:20<00:18, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  75%|███████▍  | 7.07G/9.45G [00:20<00:17, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  75%|███████▌  | 7.09G/9.45G [00:20<00:17, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  75%|███████▌  | 7.11G/9.45G [00:20<00:18, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  75%|███████▌  | 7.13G/9.45G [00:20<00:17, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  76%|███████▌  | 7.15G/9.45G [00:20<00:16, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  76%|███████▌  | 7.17G/9.45G [00:21<00:17, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  76%|███████▌  | 7.19G/9.45G [00:21<00:17, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  76%|███████▋  | 7.21G/9.45G [00:21<00:16, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.24G/9.45G [00:21<00:16, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.26G/9.45G [00:21<00:17, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.28G/9.45G [00:21<00:16, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.30G/9.45G [00:22<00:16, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.32G/9.45G [00:22<00:16, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.34G/9.45G [00:22<00:14, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.36G/9.45G [00:22<00:15, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.38G/9.45G [00:22<00:15, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.40G/9.45G [00:22<00:15, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  79%|███████▊  | 7.42G/9.45G [00:22<00:15, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  79%|███████▉  | 7.44G/9.45G [00:23<00:18, 107MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  79%|███████▉  | 7.49G/9.45G [00:23<00:12, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  79%|███████▉  | 7.51G/9.45G [00:23<00:13, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  80%|███████▉  | 7.53G/9.45G [00:23<00:13, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  80%|███████▉  | 7.55G/9.45G [00:23<00:13, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  80%|████████  | 7.57G/9.45G [00:24<00:14, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  80%|████████  | 7.59G/9.45G [00:24<00:14, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  81%|████████  | 7.61G/9.45G [00:24<00:14, 126MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  81%|████████  | 7.63G/9.45G [00:24<00:15, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  81%|████████  | 7.65G/9.45G [00:24<00:15, 119MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  81%|████████  | 7.68G/9.45G [00:24<00:14, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  81%|████████▏ | 7.70G/9.45G [00:25<00:14, 119MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 7.72G/9.45G [00:25<00:14, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 7.74G/9.45G [00:25<00:14, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 7.76G/9.45G [00:25<00:14, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 7.78G/9.45G [00:25<00:13, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 7.80G/9.45G [00:25<00:13, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 7.82G/9.45G [00:26<00:13, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 7.84G/9.45G [00:26<00:13, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 7.86G/9.45G [00:26<00:13, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 7.89G/9.45G [00:26<00:13, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  84%|████████▎ | 7.91G/9.45G [00:26<00:13, 116MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  84%|████████▍ | 7.93G/9.45G [00:26<00:12, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  84%|████████▍ | 7.95G/9.45G [00:27<00:12, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  84%|████████▍ | 7.97G/9.45G [00:27<00:12, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  85%|████████▍ | 7.99G/9.45G [00:27<00:12, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  85%|████████▍ | 8.01G/9.45G [00:27<00:11, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  85%|████████▍ | 8.03G/9.45G [00:27<00:11, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  85%|████████▌ | 8.05G/9.45G [00:28<00:11, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  85%|████████▌ | 8.07G/9.45G [00:28<00:11, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  86%|████████▌ | 8.10G/9.45G [00:28<00:10, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  86%|████████▌ | 8.12G/9.45G [00:28<00:10, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  86%|████████▌ | 8.14G/9.45G [00:28<00:10, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  86%|████████▋ | 8.16G/9.45G [00:28<00:09, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.18G/9.45G [00:28<00:09, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.20G/9.45G [00:29<00:09, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.22G/9.45G [00:29<00:08, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.24G/9.45G [00:29<00:08, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.26G/9.45G [00:29<00:08, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.28G/9.45G [00:29<00:08, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.30G/9.45G [00:29<00:07, 152MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.33G/9.45G [00:29<00:08, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.35G/9.45G [00:30<00:08, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  89%|████████▊ | 8.38G/9.45G [00:30<00:06, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  89%|████████▉ | 8.40G/9.45G [00:30<00:07, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  89%|████████▉ | 8.42G/9.45G [00:30<00:07, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  89%|████████▉ | 8.44G/9.45G [00:30<00:06, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  90%|████████▉ | 8.46G/9.45G [00:30<00:06, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  90%|████████▉ | 8.48G/9.45G [00:31<00:07, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  90%|████████▉ | 8.50G/9.45G [00:31<00:06, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  90%|█████████ | 8.52G/9.45G [00:31<00:06, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  90%|█████████ | 8.55G/9.45G [00:31<00:06, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  91%|█████████ | 8.57G/9.45G [00:31<00:06, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  91%|█████████ | 8.59G/9.45G [00:31<00:05, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  91%|█████████ | 8.61G/9.45G [00:31<00:06, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  91%|█████████▏| 8.63G/9.45G [00:32<00:06, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 8.65G/9.45G [00:32<00:05, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 8.67G/9.45G [00:32<00:05, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 8.69G/9.45G [00:32<00:05, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 8.71G/9.45G [00:32<00:05, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 8.73G/9.45G [00:32<00:04, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 8.76G/9.45G [00:33<00:05, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 8.78G/9.45G [00:33<00:04, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 8.80G/9.45G [00:33<00:04, 150MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 8.82G/9.45G [00:33<00:04, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  94%|█████████▎| 8.84G/9.45G [00:33<00:04, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 8.86G/9.45G [00:33<00:04, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 8.88G/9.45G [00:33<00:04, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 8.90G/9.45G [00:34<00:04, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 8.92G/9.45G [00:34<00:03, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  95%|█████████▍| 8.94G/9.45G [00:34<00:03, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  95%|█████████▍| 8.97G/9.45G [00:34<00:03, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  95%|█████████▌| 8.99G/9.45G [00:34<00:03, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  95%|█████████▌| 9.01G/9.45G [00:34<00:03, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  96%|█████████▌| 9.03G/9.45G [00:35<00:03, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  96%|█████████▌| 9.05G/9.45G [00:35<00:02, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  96%|█████████▌| 9.07G/9.45G [00:35<00:02, 147MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  96%|█████████▌| 9.09G/9.45G [00:35<00:02, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  96%|█████████▋| 9.11G/9.45G [00:35<00:02, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.13G/9.45G [00:35<00:02, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.15G/9.45G [00:35<00:02, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.18G/9.45G [00:36<00:02, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.20G/9.45G [00:36<00:01, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.22G/9.45G [00:36<00:01, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.24G/9.45G [00:36<00:01, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.26G/9.45G [00:36<00:01, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.28G/9.45G [00:36<00:01, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.30G/9.45G [00:37<00:01, 112MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  99%|█████████▉| 9.33G/9.45G [00:37<00:00, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  99%|█████████▉| 9.35G/9.45G [00:37<00:00, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  99%|█████████▉| 9.37G/9.45G [00:37<00:00, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin:  99%|█████████▉| 9.40G/9.45G [00:37<00:00, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin: 100%|█████████▉| 9.42G/9.45G [00:37<00:00, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin: 100%|█████████▉| 9.44G/9.45G [00:38<00:00, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00001-of-00002.bin: 100%|██████████| 9.45G/9.45G [00:38<00:00, 248MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:38<00:38, 38.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:   2%|▏         | 41.9M/1.95G [00:00<00:05, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:   4%|▍         | 83.9M/1.95G [00:00<00:10, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:   6%|▌         | 115M/1.95G [00:00<00:11, 163MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:   7%|▋         | 136M/1.95G [00:00<00:12, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:   8%|▊         | 157M/1.95G [00:00<00:11, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:   9%|▉         | 178M/1.95G [00:01<00:12, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  10%|█         | 199M/1.95G [00:01<00:12, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  11%|█▏        | 220M/1.95G [00:01<00:12, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  12%|█▏        | 241M/1.95G [00:01<00:12, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  13%|█▎        | 262M/1.95G [00:01<00:11, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  15%|█▍        | 283M/1.95G [00:01<00:12, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  16%|█▌        | 304M/1.95G [00:02<00:11, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  17%|█▋        | 325M/1.95G [00:02<00:11, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  18%|█▊        | 346M/1.95G [00:02<00:12, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  19%|█▉        | 367M/1.95G [00:02<00:11, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  20%|█▉        | 388M/1.95G [00:02<00:11, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  21%|██        | 409M/1.95G [00:02<00:11, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  22%|██▏       | 430M/1.95G [00:02<00:11, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  23%|██▎       | 451M/1.95G [00:03<00:11, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  24%|██▍       | 472M/1.95G [00:03<00:11, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  25%|██▌       | 493M/1.95G [00:03<00:10, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  26%|██▋       | 514M/1.95G [00:03<00:10, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  27%|██▋       | 535M/1.95G [00:03<00:10, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  29%|██▊       | 556M/1.95G [00:03<00:10, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  30%|██▉       | 577M/1.95G [00:04<00:09, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  31%|███       | 598M/1.95G [00:04<00:10, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  32%|███▏      | 619M/1.95G [00:04<00:09, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  33%|███▎      | 640M/1.95G [00:04<00:09, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  34%|███▍      | 661M/1.95G [00:04<00:09, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  35%|███▍      | 682M/1.95G [00:04<00:09, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  36%|███▌      | 703M/1.95G [00:04<00:08, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  37%|███▋      | 724M/1.95G [00:05<00:09, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  38%|███▊      | 744M/1.95G [00:05<00:09, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  39%|███▉      | 765M/1.95G [00:05<00:08, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  40%|████      | 786M/1.95G [00:05<00:08, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  41%|████▏     | 807M/1.95G [00:05<00:08, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  42%|████▏     | 828M/1.95G [00:05<00:08, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  44%|████▎     | 849M/1.95G [00:06<00:08, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  45%|████▍     | 870M/1.95G [00:06<00:08, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  46%|████▌     | 891M/1.95G [00:06<00:08, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  47%|████▋     | 912M/1.95G [00:06<00:07, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  48%|████▊     | 933M/1.95G [00:06<00:07, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  49%|████▉     | 954M/1.95G [00:06<00:07, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  50%|█████     | 975M/1.95G [00:07<00:06, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  51%|█████     | 996M/1.95G [00:07<00:07, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  52%|█████▏    | 1.02G/1.95G [00:07<00:07, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  53%|█████▎    | 1.04G/1.95G [00:07<00:06, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  54%|█████▍    | 1.06G/1.95G [00:07<00:06, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  55%|█████▌    | 1.08G/1.95G [00:07<00:06, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  56%|█████▋    | 1.10G/1.95G [00:07<00:06, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  58%|█████▊    | 1.12G/1.95G [00:08<00:06, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  59%|█████▊    | 1.14G/1.95G [00:08<00:06, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  60%|█████▉    | 1.16G/1.95G [00:08<00:05, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  61%|██████    | 1.18G/1.95G [00:08<00:05, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  62%|██████▏   | 1.21G/1.95G [00:08<00:05, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  63%|██████▎   | 1.23G/1.95G [00:08<00:05, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  64%|██████▍   | 1.25G/1.95G [00:09<00:06, 108MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  66%|██████▌   | 1.28G/1.95G [00:09<00:04, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  67%|██████▋   | 1.30G/1.95G [00:09<00:04, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  68%|██████▊   | 1.32G/1.95G [00:09<00:04, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  69%|██████▉   | 1.34G/1.95G [00:09<00:04, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  70%|██████▉   | 1.36G/1.95G [00:09<00:04, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  71%|███████   | 1.38G/1.95G [00:10<00:04, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  72%|███████▏  | 1.41G/1.95G [00:10<00:04, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  73%|███████▎  | 1.43G/1.95G [00:10<00:03, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  74%|███████▍  | 1.45G/1.95G [00:10<00:03, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  75%|███████▌  | 1.47G/1.95G [00:10<00:03, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  76%|███████▋  | 1.49G/1.95G [00:10<00:03, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  77%|███████▋  | 1.51G/1.95G [00:11<00:03, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  79%|███████▊  | 1.53G/1.95G [00:11<00:03, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  80%|███████▉  | 1.55G/1.95G [00:11<00:03, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  81%|████████  | 1.57G/1.95G [00:11<00:02, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  82%|████████▏ | 1.59G/1.95G [00:11<00:02, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  83%|████████▎ | 1.61G/1.95G [00:11<00:02, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  84%|████████▍ | 1.64G/1.95G [00:11<00:02, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  85%|████████▍ | 1.66G/1.95G [00:12<00:02, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  86%|████████▌ | 1.68G/1.95G [00:12<00:02, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  87%|████████▋ | 1.70G/1.95G [00:12<00:01, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  88%|████████▊ | 1.72G/1.95G [00:12<00:01, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  89%|████████▉ | 1.74G/1.95G [00:12<00:01, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  90%|█████████ | 1.76G/1.95G [00:12<00:01, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  91%|█████████▏| 1.78G/1.95G [00:13<00:01, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  93%|█████████▎| 1.80G/1.95G [00:13<00:01, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  94%|█████████▎| 1.82G/1.95G [00:13<00:00, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  95%|█████████▍| 1.85G/1.95G [00:13<00:00, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  96%|█████████▌| 1.87G/1.95G [00:13<00:00, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  97%|█████████▋| 1.89G/1.95G [00:13<00:00, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  98%|█████████▊| 1.91G/1.95G [00:13<00:00, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin:  99%|█████████▉| 1.93G/1.95G [00:14<00:00, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin: 100%|██████████| 1.95G/1.95G [00:14<00:00, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)l-00002-of-00002.bin: 100%|██████████| 1.95G/1.95G [00:14<00:00, 136MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:52<00:00, 24.29s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:52<00:00, 26.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [01:17<01:17, 77.41s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 37.22s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 43.25s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 1.70MB/s]\u001b[0m\n",
      "\u001b[34mtrainable params: 9437184 || all params: 2859194368 || trainable%: 0.33006444422319176\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 2.54kB [00:00, 16.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 401MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 2.42MB [00:00, 33.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json: 2.20kB [00:00, 15.8MB/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/74 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\u001b[0m\n",
      "\u001b[34m1%|▏         | 1/74 [00:40<49:03, 40.33s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 2/74 [01:12<42:26, 35.36s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 3/74 [01:44<39:57, 33.77s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/74 [02:15<38:31, 33.02s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 5/74 [02:47<37:29, 32.61s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 6/74 [03:19<36:41, 32.37s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 7/74 [03:51<35:58, 32.21s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 8/74 [04:23<35:19, 32.11s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 9/74 [04:55<34:43, 32.05s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 10/74 [05:27<34:07, 31.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2028, 'learning_rate': 0.000172972972972973, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 10/74 [05:27<34:07, 31.99s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 11/74 [05:59<33:33, 31.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 12/74 [06:31<32:59, 31.93s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 13/74 [07:02<32:26, 31.92s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 14/74 [07:34<31:54, 31.91s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 15/74 [08:06<31:22, 31.91s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 16/74 [08:38<30:49, 31.89s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 17/74 [09:10<30:17, 31.89s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 18/74 [09:42<29:45, 31.88s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 19/74 [10:14<29:13, 31.88s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 20/74 [10:46<28:41, 31.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1626, 'learning_rate': 0.00014594594594594595, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 20/74 [10:46<28:41, 31.89s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 21/74 [11:17<28:09, 31.88s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 22/74 [11:49<27:38, 31.88s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 23/74 [12:21<27:06, 31.89s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 24/74 [12:53<26:34, 31.90s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 25/74 [13:25<26:02, 31.89s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 26/74 [13:57<25:30, 31.88s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 27/74 [14:29<24:58, 31.89s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 28/74 [15:01<24:27, 31.89s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 29/74 [15:33<23:55, 31.90s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 30/74 [16:05<23:23, 31.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1571, 'learning_rate': 0.00011891891891891893, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 30/74 [16:05<23:23, 31.89s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 31/74 [16:36<22:51, 31.89s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 32/74 [17:08<22:19, 31.89s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 33/74 [17:40<21:47, 31.89s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 34/74 [18:12<21:15, 31.89s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 35/74 [18:44<20:43, 31.89s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 36/74 [19:16<20:11, 31.89s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 37/74 [19:48<19:39, 31.88s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 38/74 [20:20<19:07, 31.89s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 39/74 [20:51<18:35, 31.88s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 40/74 [21:23<18:04, 31.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1438, 'learning_rate': 9.18918918918919e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 40/74 [21:23<18:04, 31.89s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 41/74 [21:55<17:31, 31.87s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 42/74 [22:27<16:59, 31.87s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 43/74 [22:59<16:28, 31.88s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 44/74 [23:31<15:56, 31.88s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 45/74 [24:03<15:24, 31.88s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 46/74 [24:35<14:52, 31.88s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 47/74 [25:07<14:20, 31.89s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 48/74 [25:38<13:49, 31.89s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 49/74 [26:10<13:17, 31.89s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 50/74 [26:42<12:45, 31.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1257, 'learning_rate': 6.486486486486487e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 50/74 [26:42<12:45, 31.89s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 51/74 [27:14<12:13, 31.89s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 52/74 [27:46<11:41, 31.89s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 53/74 [28:18<11:09, 31.88s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 54/74 [28:50<10:37, 31.88s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 55/74 [29:22<10:06, 31.90s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 56/74 [29:54<09:34, 31.89s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 57/74 [30:25<09:02, 31.89s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 58/74 [30:57<08:30, 31.88s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 59/74 [31:29<07:58, 31.89s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 60/74 [32:01<07:26, 31.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1171, 'learning_rate': 3.783783783783784e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 60/74 [32:01<07:26, 31.88s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 61/74 [32:33<06:54, 31.88s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 62/74 [33:05<06:22, 31.88s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 63/74 [33:37<05:50, 31.88s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 64/74 [34:09<05:18, 31.88s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 65/74 [34:40<04:46, 31.89s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 66/74 [35:12<04:15, 31.89s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 67/74 [35:44<03:43, 31.89s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 68/74 [36:16<03:11, 31.88s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 69/74 [36:48<02:39, 31.88s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 70/74 [37:20<02:07, 31.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1241, 'learning_rate': 1.0810810810810812e-05, 'epoch': 0.95}\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 70/74 [37:20<02:07, 31.88s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 71/74 [37:52<01:35, 31.89s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 72/74 [38:24<01:03, 31.89s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 73/74 [38:56<00:31, 31.89s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 74/74 [39:17<00:00, 28.71s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 2357.3767, 'train_samples_per_second': 6.249, 'train_steps_per_second': 0.031, 'train_loss': 1.14574054125193, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 74/74 [39:17<00:00, 28.71s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 74/74 [39:17<00:00, 31.86s/it]\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.46s/it]\u001b[0m\n",
      "\u001b[34m#################### model ####################\u001b[0m\n",
      "\u001b[34minput sentence: Richie: Pogba#015\u001b[0m\n",
      "\u001b[34mClay: Pogboom#015\u001b[0m\n",
      "\u001b[34mRichie: what a s strike yoh!#015\u001b[0m\n",
      "\u001b[34mClay: was off the seat the moment he chopped the ball back to his right foot#015\u001b[0m\n",
      "\u001b[34mRichie: me too dude#015\u001b[0m\n",
      "\u001b[34mClay: hope his form lasts#015\u001b[0m\n",
      "\u001b[34mRichie: This season he's more mature#015\u001b[0m\n",
      "\u001b[34mClay: Yeah, Jose has his trust in him#015\u001b[0m\n",
      "\u001b[34mRichie: everyone does#015\u001b[0m\n",
      "\u001b[34mClay: yeah, he really deserved to score after his first 60 minutes#015\u001b[0m\n",
      "\u001b[34mRichie: reward#015\u001b[0m\n",
      "\u001b[34mClay: yeah man#015\u001b[0m\n",
      "\u001b[34mRichie: cool then #015\u001b[0m\n",
      "\u001b[34mClay: cool\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34msummary:\u001b[0m\n",
      "\u001b[34mPogba scored after his first 60 minutes\u001b[0m\n",
      "\u001b[34mDownloading builder script: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 6.27kB [00:00, 6.01MB/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/819 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/819 [00:02<34:33,  2.54s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/819 [00:04<32:28,  2.39s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/819 [00:07<34:09,  2.51s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/819 [00:10<38:19,  2.82s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/819 [00:14<41:10,  3.03s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/819 [00:18<46:58,  3.47s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 7/819 [00:21<45:33,  3.37s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 8/819 [00:23<38:06,  2.82s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 9/819 [00:29<53:33,  3.97s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 10/819 [00:34<54:56,  4.07s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 11/819 [00:38<57:52,  4.30s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 12/819 [00:43<57:47,  4.30s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 13/819 [00:46<54:36,  4.07s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 14/819 [00:48<46:13,  3.44s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 15/819 [00:55<58:18,  4.35s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 16/819 [00:59<59:02,  4.41s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 17/819 [01:01<49:23,  3.69s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 18/819 [01:07<56:51,  4.26s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 19/819 [01:13<1:05:25,  4.91s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 20/819 [01:17<59:22,  4.46s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 21/819 [01:20<56:34,  4.25s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 22/819 [01:27<1:05:16,  4.91s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 23/819 [01:31<1:02:17,  4.70s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 24/819 [01:32<49:04,  3.70s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 25/819 [01:38<55:25,  4.19s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 26/819 [01:41<50:44,  3.84s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 27/819 [01:43<45:56,  3.48s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 28/819 [01:49<52:12,  3.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 29/819 [01:53<54:34,  4.14s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 30/819 [01:57<51:36,  3.92s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 31/819 [01:59<46:59,  3.58s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 32/819 [02:03<48:18,  3.68s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 33/819 [02:05<39:42,  3.03s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 34/819 [02:07<35:42,  2.73s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 35/819 [02:10<38:51,  2.97s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 36/819 [02:16<48:58,  3.75s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 37/819 [02:20<52:05,  4.00s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 38/819 [02:24<50:43,  3.90s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 39/819 [02:28<50:48,  3.91s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 40/819 [02:31<47:51,  3.69s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 41/819 [02:38<58:32,  4.52s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 42/819 [02:41<52:14,  4.03s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 43/819 [02:45<52:15,  4.04s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 44/819 [02:47<44:51,  3.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 45/819 [02:50<42:04,  3.26s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 46/819 [02:52<38:42,  3.00s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 47/819 [02:56<43:37,  3.39s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 48/819 [02:59<40:12,  3.13s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 49/819 [03:01<37:51,  2.95s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 50/819 [03:04<38:06,  2.97s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 785/819 [03:04<00:00, 104.74it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 819/819 [03:04<00:00,  4.43it/s]\u001b[0m\n",
      "\u001b[34mRogue1: 46.108224%\u001b[0m\n",
      "\u001b[34mrouge2: 19.772860%\u001b[0m\n",
      "\u001b[34mrougeL: 36.792934%\u001b[0m\n",
      "\u001b[34mrougeLsum: 36.796880%\u001b[0m\n",
      "\n",
      "2023-06-26 02:37:48 Uploading - Uploading generated training model\u001b[34m2023-06-26 02:37:40,122 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 02:37:40,122 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 02:37:40,123 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-06-26 02:37:59 Completed - Training job completed\n",
      "Training seconds: 3073\n",
      "Billable seconds: 3073\n"
     ]
    }
   ],
   "source": [
    "estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 예제에서 SageMaker 학습 작업은 `7946 seconds`가 소요되었으며, 이는 약 `2.2 hours`입니다. 우리가 사용한 ml.g5.2xlarge 인스턴스는 온디맨드 사용 시 시간당 `$1.515 per hour` (US region 기준) 입니다. 그 결과, fine-tuned된 flan-t5-xl 모델을 학습하는 데 드는 총 비용은 `$3.34`에 불과했습니다.\n",
    "\n",
    "Spot 인스턴스를 사용하면 학습 비용을 더 줄일 수 있습니다. 그러나 Spot 인스턴스 중단으로 인해 총 학습 시간이 늘어날 가능성이 있습니다. 인스턴스 가격에 대한 자세한 내용은 [SageMaker 가격 페이지](https://aws.amazon.com/sagemaker/pricing/)를 참조하세요.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy the model to Amazon SageMaker Endpoint\n",
    "\n",
    "학습에 `peft`를 사용할 때는 일반적으로 adapter weights를 사용하게 됩니다. 모델을 더 쉽게 배포할 수 있도록 기본 모델과 adatper를 병합하는 `merge_and_unload()` 메서드를 추가했습니다. 이제 `transformers` 라이브러리의 `pipelines` 기능을 사용할 수 있게 되었습니다. \n",
    "\n",
    "SageMaker는 SageMaker Endpoint Configuration과 SageMaker Endpoint를 생성하여 배포 프로세스를 시작합니다. Endpoint Configuration은 모델과 instance type을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-322537213286/huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106/output/model.tar.gz to model/model.tar.gz\n",
      "tokenizer_config.json\n",
      "rogue.pickle\n",
      "tokenizer.json\n",
      "adapter_config.json\n",
      "adapter_model.bin\n",
      "special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./model\n",
    "!aws s3 cp {estimator.model_data} ./model/model.tar.gz\n",
    "!tar -xzvf ./model/model.tar.gz -C ./model/ # && mv ./model/model.tar.gz ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type= \"ml.g5.4xlarge\"\n",
    "# instance_type= \"local_gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# source_dir=f\"file://{Path.cwd()}/src\"\n",
    "\n",
    "if instance_type in ['local', 'local_gpu']:\n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    model_data=f\"file://{Path.cwd()}/model/model.tar.gz\"\n",
    "else:\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    model_data=estimator.model_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT': str(3600),\n",
    "    'MODEL_CACHE_ROOT': '/opt/ml/model', \n",
    "    'SAGEMAKER_ENV': '1',\n",
    "    'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/code',\n",
    "    'TS_DEFAULT_WORKERS_PER_MODEL': '1', \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "model = PyTorchModel(\n",
    "    entry_point='inference.py',\n",
    "    source_dir=f'{Path.cwd()}/flan_t5_xl_with_LoRA',\n",
    "    model_data=model_data,\n",
    "    role=role, \n",
    "    framework_version=\"2.0\", \n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 원하는 인스턴스 수와 인스턴스 유형을 전달하여 HuggingFace estimator 객체에서 `deploy()`를 사용하여 모델을 배포할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-west-2-322537213286/huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106/output/model.tar.gz), script artifact (/root/aws-ai-ml-workshop-kr/genai/jumpstart/text_to_text/flan_t5_xl_with_LoRA), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-west-2-322537213286/pytorch-inference-2023-06-26-03-22-15-557/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-inference-2023-06-26-03-22-19-571\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-peft-2023-06-26-03-22-15\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-peft-2023-06-26-03-22-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "import time\n",
    "endpoint_name = f'huggingface-peft-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: SageMaker endpoint가 추론 요청을 허용하기 위해 인스턴스를 온라인\u001d",
    "로 전환하고 모델을 다운로드하는 데 5~10분이 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test` 분할의 예제를 사용하여 테스트해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")\n",
    "\n",
    "# select a random test sample\n",
    "sample = test_dataset[randint(0,len(test_dataset))]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "fomatted_sample = {\n",
    "  \"inputs\": prompt_template.format(dialogue=sample[\"dialogue\"]),\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True, # sample output predicted probabilities\n",
    "    \"top_p\": 0.9, # sampling technique Fan et. al (2018)\n",
    "    \"temperature\": 0.1, # increasing the likelihood of high probability words and decreasing the likelihood of low probability words\n",
    "    \"max_new_tokens\": 100, # The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sean has decided that his spirit animal is a tortoise. Tiffany thinks Sean is a wasp.\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "res = predictor.predict(fomatted_sample)\n",
    "\n",
    "\n",
    "print(res[0][\"generated_text\"].split(\"Summary:\")[-1])\n",
    "\n",
    "# Sample model output: Kirsten and Alex are going bowling this Friday at 7 pm. They will meet up and then go together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델 요약되 dialog의 결과와 테스트 sample summary를 비교해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sean believes his spirit animal is a tortoise and Tiffany's could be a wasp. \n"
     ]
    }
   ],
   "source": [
    "print(sample[\"summary\"])\n",
    "\n",
    "# Test sample summary: Kirsten reminds Alex that the youth group meets this Friday at 7 pm to go bowling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 endpoint를 다시 삭제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: pytorch-inference-2023-06-26-03-22-19-571\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-peft-2023-06-26-03-22-15\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-peft-2023-06-26-03-22-15\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
