{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1463f153-71d2-480c-8565-a0adcdf2b21f",
   "metadata": {},
   "source": [
    "## [model_tuner]flan_t5_xl_instruction_ml_p3_16xl\n",
    "\n",
    "이 노트북에서는 특정 작업에 대해 사전 학습된 FLAN-T5-XL의 성능을 향상시키기 위해 instruction fine-tuning이라는 프로세스에서 대상 작업의 예제를 사용하여 모델을 조정할 수 있습니다. Instruction fine-tuning은 {prompt, response} 쌍의 형태로 레이블이 지정된 예제 세트를 사용하여 프롬프트가 주어졌을 때 응답을 적절하게 예측하도록 사전 학습된 모델을 추가로 훈련합니다. 이 프로세스는 모델의 가중치를 수정합니다.\n",
    "\n",
    "사전 학습된 **FLAN T5 모델**을 미세 조정합니다. 사전 학습된 FLAN T5 모델은 많은 작업에 \"있는 그대로\" 사용할 수 있지만, fine-tuning을 통해 특정 작업이나 언어 도메인에서 모델 성능을 향상시킬 수 있습니다. 예를 들어, 사전 학습에 사용되지 않은 작업에 대해 모델을 미세 조정합니다. 미세 조정 후에는 pretrained 모델과 fine-tuned 모델을 사용하여 두 개의 추론 엔드포인트를 배포합니다. 그런 다음 두 엔드포인트에 대해 동일한 추론 쿼리를 실행하고 결과를 비교합니다.\n",
    "\n",
    "<img src=\"./figures/flan-t5.png\"  width=\"700\" height=\"370\">\n",
    "\n",
    "[](https://aws.amazon.com/blogs/machine-learning/instruction-fine-tuning-for-flan-t5-xl-with-amazon-sagemaker-jumpstart/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd63f1f-3713-4e68-885f-0a97b35ea090",
   "metadata": {},
   "source": [
    "#### In this notebook:\n",
    "1. [Setting up](#1.-Setting-up)\n",
    "1. [Fine-tuning a model](#2.-Fine-tuning-a-model)\n",
    "1. [Deploying inference endpoints](#3.-Deploying-inference-endpoints)\n",
    "1. [Running inference queries](#4.-Running-inference-queries)\n",
    "1. [Cleaning up resources](#5.-Cleaning-up-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bda67-b335-4fe3-a72a-e360249dfc28",
   "metadata": {},
   "source": [
    "### 1. Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab36273-32fc-41e0-88a6-d9b470d2c7f1",
   "metadata": {},
   "source": [
    "필요한 패키지를 설치하고 업그레이드하는 것으로 시작합니다. 아래 셀을 실행한 후 커널을 재시작합니다.\n",
    "노트북 전체에서 다음 변수를 사용할 것이며, 특히, FLAN T5 모델 크기를 선택하고 학습 및 추론 인스턴스 유형을 선택합니다. 또한 현재 노트북 인스턴스와 연결된 실행 역할도 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28f26242-65fd-471a-b1c8-a64bc686e32e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1maws_region:\u001b[0m us-west-2\n",
      "\u001b[1maws_role:\u001b[0m arn:aws:iam::322537213286:role/service-role/AmazonSageMaker-ExecutionRole-20230528T120509\n",
      "\u001b[1moutput_bucket:\u001b[0m sagemaker-us-west-2-322537213286\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "# Get current region, role, and default bucket\n",
    "aws_region = boto3.Session().region_name\n",
    "aws_role = sagemaker.session.Session().get_caller_identity_arn()\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "# This will be useful for printing\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "print(f\"{bold}aws_region:{unbold} {aws_region}\")\n",
    "print(f\"{bold}aws_role:{unbold} {aws_role}\")\n",
    "print(f\"{bold}output_bucket:{unbold} {output_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21d1709a-7d9d-42f7-a9bb-3c0b915b2279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Select a pre-trained model from the dropdown below"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581e5c47fa014c6b8f3cc4b674542ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython\n",
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.filters import And\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "# Default model choice\n",
    "model_id = \"huggingface-text2text-flan-t5-xl\"\n",
    "\n",
    "# Identify FLAN T5 models that support fine-tuning\n",
    "filter_value = And(\"task == text2text\", \"framework == huggingface\", \"training_supported == true\")\n",
    "model_list = [m for m in list_jumpstart_models(filter=filter_value) if \"flan-t5\" in m]\n",
    "\n",
    "# Display the model IDs in a dropdown, for user to select\n",
    "dropdown = Dropdown(\n",
    "    value=model_id,\n",
    "    options=model_list,\n",
    "    description=\"FLAN T5 models available for fine-tuning:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(IPython.display.Markdown(\"### Select a pre-trained model from the dropdown below\"))\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec47e9b7-7a42-435b-9f8d-5ee118365cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmodel_id:\u001b[0m huggingface-text2text-flan-t5-xl\n",
      "\u001b[1mtraining_instance_type:\u001b[0m ml.p3.16xlarge\n",
      "\u001b[1minference_instance_type:\u001b[0m ml.g5.2xlarge\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.instance_types import retrieve_default\n",
    "\n",
    "model_id, model_version = dropdown.value, \"*\"\n",
    "\n",
    "# Instance types for training and inference\n",
    "training_instance_type = retrieve_default(\n",
    "    model_id=model_id, model_version=model_version, scope=\"training\"\n",
    ")\n",
    "inference_instance_type = retrieve_default(\n",
    "    model_id=model_id, model_version=model_version, scope=\"inference\"\n",
    ")\n",
    "\n",
    "print(f\"{bold}model_id:{unbold} {model_id}\")\n",
    "print(f\"{bold}training_instance_type:{unbold} {training_instance_type}\")\n",
    "print(f\"{bold}inference_instance_type:{unbold} {inference_instance_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da036a9d-739f-4fb3-ae87-00509b50e1c2",
   "metadata": {},
   "source": [
    "#### 2.1. Preparing training data\n",
    "우리는 supervised fine-tuning을 위해 SQuAD2.0의 하위 집합을 사용할 것입니다. 이 데이터 세트에는 위키백과 문서 세트에 대해 human annotators으로 조정된 질문들이 포함되어 있습니다. 답변이 있는 질문 외에도 SQuAD2.0에는 약 5만 개의 답변할 수 없는 질문이 포함되어 있습니다. 이러한 질문은 그럴듯하지만 문서 내용에서 직접 답을 구할 수 없습니다. 저희는 답변이 없는 질문만 작업에 사용합니다.\n",
    "\n",
    "*Citation: @article{rajpurkar2018know, title={Know what you don't know: Unanswerable questions for SQuAD},\n",
    "author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy}, journal={arXiv preprint arXiv:1806.03822}, year={2018} }*\n",
    "\n",
    "License: [Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c98b9c-b050-46c1-80de-4db99e2ad716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c61993f7-0a7e-4eec-afac-a27307750429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# We will use the train split of SQuAD2.0\n",
    "original_data_file = \"train-v2.0.json\"\n",
    "\n",
    "# The data was mirrored in the following bucket\n",
    "original_data_location = f\"s3://sagemaker-sample-files/datasets/text/squad2.0/{original_data_file}\"\n",
    "S3Downloader.download(original_data_location, \"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544af7c4-56bf-4151-a476-e240012edf1a",
   "metadata": {},
   "source": [
    "Text2Text generation 모델은 데이터가 예상되는 형식이라면 모든 텍스트 데이터에 대해 fine-tuned를 할 수 있습니다. 데이터에는 학습 및 선택적 validation 부분이 포함되어야 합니다. 각 epoch가 끝날 때마다 계산되는 validation 손실에 따라 최상의 모델이 선택됩니다. validation 세트가 제공되지 않으면 training 데이터의 (조정 가능한) 백분율이 자동으로 분할되어 validation에 사용됩니다.\n",
    "\n",
    "training 데이터는 각 라인이 단일 데이터 샘플을 나타내는 dict로, JSON lines(`.jsonl`) 포맷으로 형식화해야 합니다. 모든 training 데이터는 단일 폴더에 있어야 하지만 여러 개의 jsonl 파일에 저장할 수 있습니다. 파일 확장자 `.jsonl`은 필수입니다. 또한, training 폴더에는 입력 및 출력 형식을 설명하는 `template.json` 파일도 포함할 수 있습니다.\n",
    "\n",
    "템플릿 파일을 지정하지 않으면 다음 default 템플릿이 사용됩니다:\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"{prompt}\",\n",
    "    \"completion\": \"{completion}\"\n",
    "}\n",
    "```\n",
    "이 경우 JSON lines 항목의 데이터에는 `prompt` 및 `completion` 필드가 포함되어야 합니다.\n",
    "이 데모에서는 사용자 지정 템플릿을 사용하겠습니다(아래 참조)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2aeb20-d9bf-499a-9936-46507cdcce52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "local_data_file = \"./data/task-data.jsonl\"  # any name with .jsonl extension\n",
    "\n",
    "with open('./data/' + original_data_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(local_data_file, \"w\") as f:\n",
    "    for article in data[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            # iterate over questions for a given paragraph\n",
    "            for qas in paragraph[\"qas\"]:\n",
    "                if qas[\"is_impossible\"]:\n",
    "                    # the question is relevant, but cannot be answered\n",
    "                    example = {\"context\": paragraph[\"context\"], \"question\": qas[\"question\"]}\n",
    "                    json.dump(example, f)\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Ask a question which is related to the following text, but cannot be answered based on the text. Text: {context}\",\n",
    "    \"completion\": \"{question}\",\n",
    "}\n",
    "with open(\"./data/template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c033ba64-5762-41f7-8267-c47280ccbea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtraining data:\u001b[0m s3://sagemaker-us-west-2-322537213286/train_data\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "train_data_location = f\"s3://{output_bucket}/train_data\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"./data/template.json\", train_data_location)\n",
    "print(f\"{bold}training data:{unbold} {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234ddcd-0c4f-4b13-bae8-22e9cb3a9353",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2. Start training\n",
    "\n",
    "이제 training job을 시작할 준비가 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392fd15b-a854-444a-ac58-373f6c1fff96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mimage uri:\u001b[0m 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\n",
      "\u001b[1mmodel uri:\u001b[0m s3://jumpstart-cache-prod-us-west-2/huggingface-training/train-huggingface-text2text-flan-t5-xl.tar.gz\n",
      "\u001b[1mscript uri:\u001b[0m s3://jumpstart-cache-prod-us-west-2/source-directory-tarballs/huggingface/transfer_learning/text2text/prepack/v1.0.3/sourcedir.tar.gz\n",
      "\u001b[1moutput location:\u001b[0m s3://sagemaker-us-west-2-322537213286/demo-fine-tune-flan-t5/\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "# Training instance will use this image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=aws_region,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Pre-trained model\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")\n",
    "\n",
    "# Script to execute on the training instance\n",
    "train_script_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "\n",
    "output_location = f\"s3://{output_bucket}/demo-fine-tune-flan-t5/\"\n",
    "\n",
    "print(f\"{bold}image uri:{unbold} {train_image_uri}\")\n",
    "print(f\"{bold}model uri:{unbold} {train_model_uri}\")\n",
    "print(f\"{bold}script uri:{unbold} {train_script_uri}\")\n",
    "print(f\"{bold}output location:{unbold} {output_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fca6d7a-a567-4f46-a222-f9b1269c8158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf ./flan_t5_xl_instruction_ml_p3_16xl/\n",
    "# !mkdir ./flan_t5_xl_instruction_ml_p3_16xl/\n",
    "# !aws s3 cp $train_script_uri ./flan_t5_xl_instruction_ml_p3_16xl/\n",
    "# !tar -xvzf ./flan_t5_xl_instruction_ml_p3_16xl/sourcedir.tar.gz -C ./flan_t5_xl_instruction_ml_p3_16xl/\n",
    "# !rm -rf ./flan_t5_xl_instruction_ml_p3_16xl/sourcedir.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "578409e9-8758-4051-958b-661096373618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adam_beta1': '0.9',\n",
      " 'adam_beta2': '0.999',\n",
      " 'adam_epsilon': '1e-08',\n",
      " 'auto_find_batch_size': 'False',\n",
      " 'batch_size': '64',\n",
      " 'dataloader_drop_last': 'False',\n",
      " 'dataloader_num_workers': '0',\n",
      " 'early_stopping_patience': '3',\n",
      " 'early_stopping_threshold': '0.0',\n",
      " 'epochs': '3',\n",
      " 'eval_accumulation_steps': 'None',\n",
      " 'eval_steps': '500',\n",
      " 'evalaution_strategy': 'epoch',\n",
      " 'gradient_accumulation_steps': '1',\n",
      " 'gradient_checkpointing': 'True',\n",
      " 'label_smoothing_factor': '0',\n",
      " 'learning_rate': '0.0001',\n",
      " 'load_best_model_at_end': 'True',\n",
      " 'logging_first_step': 'False',\n",
      " 'logging_nan_inf_filter': 'True',\n",
      " 'logging_steps': '500',\n",
      " 'logging_strategy': 'steps',\n",
      " 'lr_scheduler_type': 'constant_with_warmup',\n",
      " 'max_eval_samples': '-1',\n",
      " 'max_grad_norm': '1.0',\n",
      " 'max_input_length': '-1',\n",
      " 'max_output_length': '128',\n",
      " 'max_steps': '-1',\n",
      " 'max_train_samples': '-1',\n",
      " 'pad_to_max_length': 'True',\n",
      " 'preprocessing_num_workers': 'None',\n",
      " 'save_steps': '500',\n",
      " 'save_strategy': 'epoch',\n",
      " 'save_total_limit': '2',\n",
      " 'seed': '42',\n",
      " 'train_data_split_seed': '0',\n",
      " 'validation_split_ratio': '0.05',\n",
      " 'warmup_ratio': '0.0',\n",
      " 'warmup_steps': '0',\n",
      " 'weight_decay': '0.0'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# 일부 default 하이퍼파라미터를 custom 값들로 override 합니다.\n",
    "hyperparameters[\"epochs\"] = \"3\"\n",
    "pprint.pprint(hyperparameters)\n",
    "\n",
    "# Note that the maximum output length is set to 128 tokens by default.\n",
    "# The targets in your data (i.e., ground truth responses) will be truncated to this size.\n",
    "# You can override this behavior, e.g.,\n",
    "# hyperparameters[\"max_output_length\"] = \"256\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da977c5-de33-4ad1-8db4-3610c2f5c8b2",
   "metadata": {},
   "source": [
    "이제 training job을 시작할 준비가 되었습니다. model size, amount of data 등에 따라 완료하는 데 20분에서 몇 시간까지 시간이 걸릴 수 있습니다(예: xl 모델, 4만 개의 examples, 3개의 epoch 경우 몇 시간이 걸릴 수 있음)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c19c5c9-b223-4106-95a3-178690bd844c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: js-demo-flan-t5-xl-3-2023-05-29-06-20-37-686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mjob name:\u001b[0m js-demo-flan-t5-xl-3-2023-05-29-06-20-37-686\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = \"-\".join(model_id.split(\"-\")[2:])  # get the most informative part of ID\n",
    "training_job_name = name_from_base(f\"js-demo-{model_name}-{hyperparameters['epochs']}\")\n",
    "print(f\"{bold}job name:{unbold} {training_job_name}\")\n",
    "\n",
    "training_metric_definitions = [\n",
    "    {\"Name\": \"val_loss\", \"Regex\": \"'eval_loss': ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"train_loss\", \"Regex\": \"'loss': ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"'epoch': ([0-9\\\\.]+)\"},\n",
    "]\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "sm_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    source_dir=train_script_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    volume_size=250,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_location,\n",
    "    metric_definitions=training_metric_definitions,\n",
    ")\n",
    "\n",
    "# Launch a SageMaker training job over data located in the given S3 path\n",
    "# Training jobs can take hours, it is recommended to set wait=False,\n",
    "# and monitor job status through SageMaker console\n",
    "sm_estimator.fit({\"training\": train_data_location}, job_name=training_job_name, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44b5be-3cfc-4403-8f92-1a1b79555fbf",
   "metadata": {},
   "source": [
    "training 및 validation 손실과 같은 성능 메트릭은 트레이닝 중에 CloudWatch를 통해 액세스할 수 있습니다. 또한 다음과 같이 메트릭의 가장 최근 스냅샷을 가져올 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab03f2-e29c-4f78-af04-45b010ced9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-29 06:27:14 Starting - Preparing the instances for training\n",
      "2023-05-29 06:27:14 Downloading - Downloading input data\n",
      "2023-05-29 06:27:14 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-29 06:27:15,982 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-29 06:27:16,056 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-29 06:27:16,059 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-29 06:27:17,680 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/absl-py/absl_py-1.4.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.16.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/Brotli/Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.9.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/deepspeed/deepspeed-0.8.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/evaluate/evaluate-0.4.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/hjson/hjson-3.1.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/huggingface_hub/huggingface_hub-0.13.3-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ninja/ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nltk/nltk-3.8.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/psutil/psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py-cpuinfo/py_cpuinfo-9.0.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.4-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pydantic/pydantic-1.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/rouge-score/rouge_score-0.1.2.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tensorboardX/tensorboardX-2.6-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.26.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.4-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.0.2-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.16.0->-r requirements.txt (line 2)) (1.10.2+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate==0.16.0->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.16.0->-r requirements.txt (line 2)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.16.0->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.0->-r requirements.txt (line 4)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.0->-r requirements.txt (line 4)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.0->-r requirements.txt (line 4)) (3.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.0->-r requirements.txt (line 4)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.0->-r requirements.txt (line 4)) (4.64.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.0->-r requirements.txt (line 4)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.0->-r requirements.txt (line 4)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.0->-r requirements.txt (line 4)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.0->-r requirements.txt (line 4)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.0->-r requirements.txt (line 4)) (2022.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub==0.13.3->-r requirements.txt (line 8)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub==0.13.3->-r requirements.txt (line 8)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk==3.8.1->-r requirements.txt (line 12)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk==3.8.1->-r requirements.txt (line 12)) (2022.9.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk==3.8.1->-r requirements.txt (line 12)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score==0.1.2->-r requirements.txt (line 21)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4,>=3.8.0 in /opt/conda/lib/python3.8/site-packages (from tensorboardX==2.6->-r requirements.txt (line 22)) (3.19.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0->-r requirements.txt (line 24)) (0.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 4)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 4)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 4)) (2.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 4)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 4)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 4)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 4)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->accelerate==0.16.0->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.0->-r requirements.txt (line 4)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.0->-r requirements.txt (line 4)) (2022.9.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.0->-r requirements.txt (line 4)) (1.26.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.9.0->-r requirements.txt (line 4)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.9.0->-r requirements.txt (line 4)) (2022.2.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, rouge-score\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.0-py3-none-any.whl size=752127 sha256=b4dd13ed87442cb8705d09171a852bbc1729a84d4cec93ba318b64cbb857406c\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/2c/ea/ca/2eff22de9baba72dad3fc084676daf133d5730e57d218f4e6b\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=1e3b516913b070e4e4f5364a083725dacd1bd4b8e5df930ecee14c5d1ea969a3\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b5/f0/47/6cb76c38609d663fd90de37d038f8418ad46f53054458f3901\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed rouge-score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, py-cpuinfo, ninja, hjson, Brotli, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pyppmd, pydantic, pycryptodomex, pybcj, psutil, nltk, multivolumefile, inflate64, absl-py, tensorboardX, rouge-score, py7zr, huggingface-hub, deepspeed, accelerate, transformers, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: psutil\u001b[0m\n",
      "\u001b[34mFound existing installation: psutil 5.9.2\u001b[0m\n",
      "\u001b[34mUninstalling psutil-5.9.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled psutil-5.9.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.10.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.10.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.10.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.17.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.17.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.17.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 1.18.4\u001b[0m\n",
      "\u001b[34mUninstalling datasets-1.18.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-1.18.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 absl-py-1.4.0 accelerate-0.16.0 datasets-2.9.0 deepspeed-0.8.0 evaluate-0.4.0 hjson-3.1.0 huggingface-hub-0.13.3 inflate64-0.3.1 multivolumefile-0.2.3 ninja-1.11.1 nltk-3.8.1 psutil-5.9.4 py-cpuinfo-9.0.0 py7zr-0.20.4 pybcj-1.0.1 pycryptodomex-3.17 pydantic-1.10.2 pyppmd-1.0.0 pyzstd-0.15.4 rouge-score-0.1.2 sagemaker-jumpstart-huggingface-script-utilities-1.0.2 sagemaker-jumpstart-script-utilities-1.1.4 sagemaker-jumpstart-tabular-script-utilities-1.0.0 tensorboardX-2.6 texttable-1.6.7 transformers-4.26.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-29 06:27:38,944 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-29 06:27:38,944 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-29 06:27:39,177 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.999\",\n",
      "        \"adam_epsilon\": \"1e-08\",\n",
      "        \"auto_find_batch_size\": \"False\",\n",
      "        \"batch_size\": \"64\",\n",
      "        \"dataloader_drop_last\": \"False\",\n",
      "        \"dataloader_num_workers\": \"0\",\n",
      "        \"early_stopping_patience\": \"3\",\n",
      "        \"early_stopping_threshold\": \"0.0\",\n",
      "        \"epochs\": \"3\",\n",
      "        \"eval_accumulation_steps\": \"None\",\n",
      "        \"eval_steps\": \"500\",\n",
      "        \"evalaution_strategy\": \"epoch\",\n",
      "        \"gradient_accumulation_steps\": \"1\",\n",
      "        \"gradient_checkpointing\": \"True\",\n",
      "        \"label_smoothing_factor\": \"0\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"load_best_model_at_end\": \"True\",\n",
      "        \"logging_first_step\": \"False\",\n",
      "        \"logging_nan_inf_filter\": \"True\",\n",
      "        \"logging_steps\": \"500\",\n",
      "        \"logging_strategy\": \"steps\",\n",
      "        \"lr_scheduler_type\": \"constant_with_warmup\",\n",
      "        \"max_eval_samples\": \"-1\",\n",
      "        \"max_grad_norm\": \"1.0\",\n",
      "        \"max_input_length\": \"-1\",\n",
      "        \"max_output_length\": \"128\",\n",
      "        \"max_steps\": \"-1\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"pad_to_max_length\": \"True\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"save_steps\": \"500\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"save_total_limit\": \"2\",\n",
      "        \"seed\": \"42\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.05\",\n",
      "        \"warmup_ratio\": \"0.0\",\n",
      "        \"warmup_steps\": \"0\",\n",
      "        \"weight_decay\": \"0.0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"js-demo-flan-t5-xl-3-2023-05-29-06-20-37-686\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-west-2/source-directory-tarballs/huggingface/transfer_learning/text2text/prepack/v1.0.3/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":\"False\",\"batch_size\":\"64\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epochs\":\"3\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"500\",\"evalaution_strategy\":\"epoch\",\"gradient_accumulation_steps\":\"1\",\"gradient_checkpointing\":\"True\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"0.0001\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"500\",\"logging_strategy\":\"steps\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_eval_samples\":\"-1\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"-1\",\"max_output_length\":\"128\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"pad_to_max_length\":\"True\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"epoch\",\"save_total_limit\":\"2\",\"seed\":\"42\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.05\",\"warmup_ratio\":\"0.0\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-west-2/source-directory-tarballs/huggingface/transfer_learning/text2text/prepack/v1.0.3/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":\"False\",\"batch_size\":\"64\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epochs\":\"3\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"500\",\"evalaution_strategy\":\"epoch\",\"gradient_accumulation_steps\":\"1\",\"gradient_checkpointing\":\"True\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"0.0001\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"500\",\"logging_strategy\":\"steps\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_eval_samples\":\"-1\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"-1\",\"max_output_length\":\"128\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"pad_to_max_length\":\"True\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"epoch\",\"save_total_limit\":\"2\",\"seed\":\"42\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.05\",\"warmup_ratio\":\"0.0\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"js-demo-flan-t5-xl-3-2023-05-29-06-20-37-686\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-west-2/source-directory-tarballs/huggingface/transfer_learning/text2text/prepack/v1.0.3/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.999\",\"--adam_epsilon\",\"1e-08\",\"--auto_find_batch_size\",\"False\",\"--batch_size\",\"64\",\"--dataloader_drop_last\",\"False\",\"--dataloader_num_workers\",\"0\",\"--early_stopping_patience\",\"3\",\"--early_stopping_threshold\",\"0.0\",\"--epochs\",\"3\",\"--eval_accumulation_steps\",\"None\",\"--eval_steps\",\"500\",\"--evalaution_strategy\",\"epoch\",\"--gradient_accumulation_steps\",\"1\",\"--gradient_checkpointing\",\"True\",\"--label_smoothing_factor\",\"0\",\"--learning_rate\",\"0.0001\",\"--load_best_model_at_end\",\"True\",\"--logging_first_step\",\"False\",\"--logging_nan_inf_filter\",\"True\",\"--logging_steps\",\"500\",\"--logging_strategy\",\"steps\",\"--lr_scheduler_type\",\"constant_with_warmup\",\"--max_eval_samples\",\"-1\",\"--max_grad_norm\",\"1.0\",\"--max_input_length\",\"-1\",\"--max_output_length\",\"128\",\"--max_steps\",\"-1\",\"--max_train_samples\",\"-1\",\"--pad_to_max_length\",\"True\",\"--preprocessing_num_workers\",\"None\",\"--save_steps\",\"500\",\"--save_strategy\",\"epoch\",\"--save_total_limit\",\"2\",\"--seed\",\"42\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.05\",\"--warmup_ratio\",\"0.0\",\"--warmup_steps\",\"0\",\"--weight_decay\",\"0.0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_EPSILON=1e-08\u001b[0m\n",
      "\u001b[34mSM_HP_AUTO_FIND_BATCH_SIZE=False\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_DROP_LAST=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_NUM_WORKERS=0\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=3\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_THRESHOLD=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_ACCUMULATION_STEPS=None\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_EVALAUTION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=1\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=True\u001b[0m\n",
      "\u001b[34mSM_HP_LABEL_SMOOTHING_FACTOR=0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FIRST_STEP=False\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_NAN_INF_FILTER=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant_with_warmup\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_EVAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_OUTPUT_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=True\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=2\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=42\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=0\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --auto_find_batch_size False --batch_size 64 --dataloader_drop_last False --dataloader_num_workers 0 --early_stopping_patience 3 --early_stopping_threshold 0.0 --epochs 3 --eval_accumulation_steps None --eval_steps 500 --evalaution_strategy epoch --gradient_accumulation_steps 1 --gradient_checkpointing True --label_smoothing_factor 0 --learning_rate 0.0001 --load_best_model_at_end True --logging_first_step False --logging_nan_inf_filter True --logging_steps 500 --logging_strategy steps --lr_scheduler_type constant_with_warmup --max_eval_samples -1 --max_grad_norm 1.0 --max_input_length -1 --max_output_length 128 --max_steps -1 --max_train_samples -1 --pad_to_max_length True --preprocessing_num_workers None --save_steps 500 --save_strategy epoch --save_total_limit 2 --seed 42 --train_data_split_seed 0 --validation_split_ratio 0.05 --warmup_ratio 0.0 --warmup_steps 0 --weight_decay 0.0\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:27:41.274: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mExtracting model to /tmp/pretrained_model\u001b[0m\n",
      "\u001b[34mINFO:__main__:Extracting model to /tmp/pretrained_model\u001b[0m\n",
      "\u001b[34mINFO:root:Preparing training data...\u001b[0m\n",
      "\u001b[34mWARNING:datasets.builder:Using custom data configuration default-f163ec0bd3b1c6dc\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f163ec0bd3b1c6dc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 6574.14it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1416.04it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f163ec0bd3b1c6dc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mNo validation data provided, splitting automatically.\u001b[0m\n",
      "\u001b[34mINFO:data_preprocessor:No validation data provided, splitting automatically.\u001b[0m\n",
      "\u001b[34m0%|          | 0/42 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 1/42 [00:00<00:17,  2.30ba/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 2/42 [00:00<00:16,  2.39ba/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 3/42 [00:01<00:17,  2.23ba/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 4/42 [00:01<00:16,  2.31ba/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 5/42 [00:02<00:15,  2.34ba/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 6/42 [00:02<00:15,  2.37ba/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 7/42 [00:02<00:14,  2.39ba/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 8/42 [00:03<00:14,  2.40ba/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 9/42 [00:03<00:13,  2.37ba/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 10/42 [00:04<00:13,  2.39ba/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 11/42 [00:04<00:13,  2.27ba/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 12/42 [00:05<00:12,  2.32ba/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 13/42 [00:05<00:12,  2.36ba/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 14/42 [00:05<00:11,  2.39ba/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 15/42 [00:06<00:11,  2.40ba/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 16/42 [00:06<00:10,  2.41ba/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 17/42 [00:07<00:10,  2.42ba/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 18/42 [00:07<00:10,  2.28ba/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 19/42 [00:08<00:09,  2.33ba/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 20/42 [00:08<00:09,  2.38ba/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 21/42 [00:08<00:08,  2.35ba/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 22/42 [00:09<00:08,  2.39ba/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 23/42 [00:09<00:07,  2.40ba/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 24/42 [00:10<00:07,  2.41ba/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 25/42 [00:10<00:07,  2.43ba/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 26/42 [00:11<00:06,  2.30ba/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 27/42 [00:11<00:06,  2.35ba/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 28/42 [00:11<00:05,  2.39ba/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 29/42 [00:12<00:05,  2.41ba/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 30/42 [00:12<00:04,  2.43ba/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 31/42 [00:13<00:04,  2.46ba/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 32/42 [00:13<00:04,  2.47ba/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 33/42 [00:13<00:03,  2.47ba/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 34/42 [00:14<00:03,  2.36ba/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 35/42 [00:14<00:02,  2.40ba/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 36/42 [00:15<00:02,  2.41ba/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 37/42 [00:15<00:02,  2.42ba/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 38/42 [00:15<00:01,  2.43ba/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 39/42 [00:16<00:01,  2.43ba/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 40/42 [00:16<00:00,  2.43ba/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 41/42 [00:17<00:00,  2.34ba/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 42/42 [00:17<00:00,  2.93ba/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 42/42 [00:17<00:00,  2.42ba/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 1/3 [00:00<00:00,  2.54ba/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 2/3 [00:00<00:00,  2.52ba/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00,  3.43ba/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):   0%|          | 0/41323 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):  77%|███████▋  | 32000/41323 [00:00<00:00, 302945.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 41323/41323 [00:00<00:00, 302945.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):   0%|          | 0/2175 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 2175/2175 [00:00<00:00, 218688.99 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking DeepSpeed with command ['deepspeed', '--num_gpus', '8', 'train.py', '--model_dir', '/opt/ml/model', '--pretrained_model_dir', '/tmp/pretrained_model', '--generation_max_length', '128', '--adam_beta1', '0.9', '--adam_beta2', '0.999', '--adam_epsilon', '1e-08', '--auto_find_batch_size', 'False', '--batch_size', '64', '--dataloader_drop_last', 'False', '--dataloader_num_workers', '0', '--early_stopping_patience', '3', '--early_stopping_threshold', '0.0', '--epochs', '3', '--eval_accumulation_steps', 'None', '--eval_steps', '500', '--evalaution_strategy', 'epoch', '--gradient_accumulation_steps', '1', '--gradient_checkpointing', 'True', '--label_smoothing_factor', '0', '--learning_rate', '0.0001', '--load_best_model_at_end', 'True', '--logging_first_step', 'False', '--logging_nan_inf_filter', 'True', '--logging_steps', '500', '--logging_strategy', 'steps', '--lr_scheduler_type', 'constant_with_warmup', '--max_grad_norm', '1.0', '--max_steps', '-1', '--save_steps', '500', '--save_strategy', 'epoch', '--save_total_limit', '2', '--seed', '42', '--warmup_ratio', '0.0', '--warmup_steps', '0', '--weight_decay', '0.0']\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:44,072] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:44,121] [INFO] [runner.py:548:main] cmd = /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --model_dir /opt/ml/model --pretrained_model_dir /tmp/pretrained_model --generation_max_length 128 --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --auto_find_batch_size False --batch_size 64 --dataloader_drop_last False --dataloader_num_workers 0 --early_stopping_patience 3 --early_stopping_threshold 0.0 --epochs 3 --eval_accumulation_steps None --eval_steps 500 --evalaution_strategy epoch --gradient_accumulation_steps 1 --gradient_checkpointing True --label_smoothing_factor 0 --learning_rate 0.0001 --load_best_model_at_end True --logging_first_step False --logging_nan_inf_filter True --logging_steps 500 --logging_strategy steps --lr_scheduler_type constant_with_warmup --max_grad_norm 1.0 --max_steps -1 --save_steps 500 --save_strategy epoch --save_total_limit 2 --seed 42 --warmup_ratio 0.0 --warmup_steps 0 --weight_decay 0.0\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:45,785] [INFO] [launch.py:135:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:45,785] [INFO] [launch.py:135:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:45,785] [INFO] [launch.py:135:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:45,785] [INFO] [launch.py:135:main] 0 NCCL_VERSION=2.10.3\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:45,785] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:45,785] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:45,785] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:45,785] [INFO] [launch.py:162:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:29:45,785] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--local_rank=6', '--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--local_rank=5', '--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--local_rank=3', '--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--local_rank=0', '--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--local_rank=4', '--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--local_rank=7', '--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--local_rank=2', '--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--local_rank=1', '--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.17s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.41s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.22s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:30:32,077] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.23s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:30:33,147] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34malgo-1:721:721 [0] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:721:721 [0] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34mNCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34malgo-1:724:724 [3] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:728:728 [7] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:728:728 [7] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:724:724 [3] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:722:722 [1] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:722:722 [1] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:725:725 [4] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:725:725 [4] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:727:727 [6] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:727:727 [6] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:723:723 [2] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:726:726 [5] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:726:726 [5] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:723:723 [2] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:30:45,430] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py38_cu113/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py38_cu113/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 35.575754165649414 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 35.517985343933105 seconds\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 35.551527976989746 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 35.552485704422 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 35.554675340652466 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 35.56112098693848 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 35.630354166030884 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 35.63201594352722 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:24,017] [INFO] [logging.py:68:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:24,074] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:24,074] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:24,074] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py38_cu113/utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:24,186] [INFO] [utils.py:831:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:24,187] [INFO] [utils.py:832:see_memory_usage] MA 10.62 GB         Max_MA 10.62 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:24,187] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 39.58 GB, percent = 8.2%\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:24,193] [INFO] [stage3.py:114:__init__] Reduce bucket size 4194304\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:24,193] [INFO] [stage3.py:115:__init__] Prefetch bucket size 3774873\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py38_cu113/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.636765480041504 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.532880067825317 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...Loading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.73253345489502 secondsTime to load utils op: 15.730766534805298 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.731787919998169 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.7323637008667 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.73205852508545 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.732697010040283 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:39,834] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:39,835] [INFO] [utils.py:832:see_memory_usage] MA 10.62 GB         Max_MA 10.62 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:39,835] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 39.72 GB, percent = 8.3%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 251904 in 124 params\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:47,195] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:47,196] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 10.62 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:47,196] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 50.54 GB, percent = 10.5%\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:47,297] [INFO] [utils.py:831:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:47,298] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:47,298] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 50.54 GB, percent = 10.5%\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,198] [INFO] [utils.py:831:see_memory_usage] After creating fp16 partitions: 1\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,198] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,199] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 70.85 GB, percent = 14.8%\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,301] [INFO] [utils.py:831:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,302] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,302] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 70.85 GB, percent = 14.8%\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,508] [INFO] [utils.py:831:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,509] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,509] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 72.18 GB, percent = 15.0%\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,958] [INFO] [utils.py:831:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,959] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:31:56,960] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 89.54 GB, percent = 18.6%\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:00,488] [INFO] [utils.py:831:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:00,489] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:00,489] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 104.33 GB, percent = 21.7%\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:00,490] [INFO] [stage3.py:382:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005128383636474609 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00045943260192871094 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...No modifications detected for re-loaded extension module utils, skipping build step...No modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...Loading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004715919494628906 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004646778106689453 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004458427429199219 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00046753883361816406 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0006422996520996094 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,549] [INFO] [utils.py:831:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,550] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.51 GB         CA 10.87 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,550] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 115.19 GB, percent = 24.0%\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,550] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,550] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,550] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fe59e6c8fa0>\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,551] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,553] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,553] [INFO] [config.py:1012:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,553] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,553] [INFO] [config.py:1012:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe5cc2497c0>\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,554] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7fe5cc2499a0>\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,555] [INFO] [config.py:1012:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 0}\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   steps_per_print .............. 2000\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   train_batch_size ............. 64\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  8\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   world_size ................... 8\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=4194304 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=3774873 param_persistence_threshold=20480 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02,556] [INFO] [config.py:997:print_user_config]   json = {\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.0001, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.0\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.0001, \n",
      "            \"warmup_num_steps\": 0\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 4.194304e+06, \n",
      "        \"stage3_prefetch_bucket_size\": 3.774874e+06, \n",
      "        \"stage3_param_persistence_threshold\": 2.048000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004963874816894531 seconds\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 41323\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 41323\n",
      "  Num Epochs = 3\u001b[0m\n",
      "\u001b[34mNum Epochs = 3\n",
      "  Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1938\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1938\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 0\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 0\u001b[0m\n",
      "\u001b[34m0%|          | 0/1938 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.663: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.667: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.678: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.680: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.687: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.691: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.694: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.698: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.715 algo-1:728 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.718 algo-1:722 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.729 algo-1:723 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.731 algo-1:725 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.738 algo-1:727 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.742 algo-1:724 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.747 algo-1:721 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.749 algo-1:726 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.872 algo-1:728 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.873 algo-1:722 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.875 algo-1:728 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.875 algo-1:728 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.876 algo-1:728 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.876 algo-1:722 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.876 algo-1:728 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.876 algo-1:722 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.877 algo-1:722 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.877 algo-1:722 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.886 algo-1:723 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.886 algo-1:725 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.888 algo-1:723 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.888 algo-1:723 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.889 algo-1:725 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.889 algo-1:723 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.889 algo-1:725 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.889 algo-1:723 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.890 algo-1:725 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.890 algo-1:725 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.896 algo-1:727 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.898 algo-1:727 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.899 algo-1:727 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.899 algo-1:727 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.899 algo-1:727 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.903 algo-1:724 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.905 algo-1:724 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.906 algo-1:724 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.906 algo-1:724 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.906 algo-1:724 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.907 algo-1:726 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.910 algo-1:726 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.910 algo-1:721 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.910 algo-1:726 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.911 algo-1:726 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.911 algo-1:726 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.912 algo-1:721 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.912 algo-1:721 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.913 algo-1:721 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:02.913 algo-1:721 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[2023-05-29 06:32:13,989] [WARNING] [stage3.py:1939:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 1/1938 [00:11<6:08:26, 11.41s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/1938 [00:26<7:10:42, 13.35s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/1938 [00:35<6:05:46, 11.34s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/1938 [00:43<5:33:37, 10.35s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/1938 [00:52<5:15:55,  9.81s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/1938 [01:01<5:04:14,  9.45s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 7/1938 [01:10<4:57:28,  9.24s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 8/1938 [01:19<4:52:39,  9.10s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 9/1938 [01:27<4:49:24,  9.00s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 10/1938 [01:36<4:47:14,  8.94s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 11/1938 [01:45<4:45:49,  8.90s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 12/1938 [01:54<4:45:11,  8.88s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 13/1938 [02:03<4:44:30,  8.87s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 14/1938 [02:12<4:43:58,  8.86s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 15/1938 [02:20<4:43:31,  8.85s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 16/1938 [02:29<4:42:55,  8.83s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 17/1938 [02:38<4:42:52,  8.84s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 18/1938 [02:47<4:42:44,  8.84s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 19/1938 [02:56<4:42:14,  8.82s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 20/1938 [03:04<4:42:21,  8.83s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 21/1938 [03:13<4:42:13,  8.83s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 22/1938 [03:22<4:42:00,  8.83s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 23/1938 [03:31<4:41:30,  8.82s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 24/1938 [03:40<4:41:16,  8.82s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 25/1938 [03:49<4:41:12,  8.82s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 26/1938 [03:57<4:41:18,  8.83s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 27/1938 [04:06<4:41:16,  8.83s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 28/1938 [04:15<4:40:42,  8.82s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 29/1938 [04:24<4:40:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 30/1938 [04:33<4:40:18,  8.81s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 31/1938 [04:41<4:39:48,  8.80s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 32/1938 [04:50<4:39:21,  8.79s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 33/1938 [04:59<4:39:45,  8.81s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 34/1938 [05:08<4:39:40,  8.81s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 35/1938 [05:17<4:39:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 36/1938 [05:25<4:39:06,  8.80s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 37/1938 [05:34<4:38:55,  8.80s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 38/1938 [05:43<4:38:59,  8.81s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 39/1938 [05:52<4:38:48,  8.81s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 40/1938 [06:01<4:38:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 41/1938 [06:10<4:38:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 42/1938 [06:18<4:39:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 43/1938 [06:27<4:38:38,  8.82s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 44/1938 [06:36<4:38:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 45/1938 [06:45<4:37:45,  8.80s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 46/1938 [06:54<4:37:37,  8.80s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 47/1938 [07:02<4:37:59,  8.82s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 48/1938 [07:11<4:37:57,  8.82s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 49/1938 [07:20<4:37:58,  8.83s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 50/1938 [07:29<4:38:45,  8.86s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 51/1938 [07:38<4:38:29,  8.86s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 52/1938 [07:47<4:38:19,  8.85s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 53/1938 [07:56<4:37:59,  8.85s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 54/1938 [08:04<4:38:03,  8.86s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 55/1938 [08:13<4:37:43,  8.85s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 56/1938 [08:22<4:37:14,  8.84s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 57/1938 [08:31<4:37:02,  8.84s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 58/1938 [08:40<4:37:02,  8.84s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 59/1938 [08:49<4:36:33,  8.83s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 60/1938 [08:57<4:36:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 61/1938 [09:06<4:35:24,  8.80s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 62/1938 [09:15<4:35:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 63/1938 [09:24<4:35:24,  8.81s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 64/1938 [09:33<4:35:33,  8.82s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 65/1938 [09:41<4:35:26,  8.82s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 66/1938 [09:50<4:35:06,  8.82s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 67/1938 [09:59<4:35:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 68/1938 [10:08<4:35:09,  8.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 69/1938 [10:17<4:35:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 70/1938 [10:26<4:34:56,  8.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 71/1938 [10:34<4:34:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 72/1938 [10:43<4:34:25,  8.82s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 73/1938 [10:52<4:34:13,  8.82s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 74/1938 [11:01<4:34:19,  8.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 75/1938 [11:10<4:34:28,  8.84s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 76/1938 [11:19<4:34:24,  8.84s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 77/1938 [11:27<4:34:04,  8.84s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 78/1938 [11:36<4:33:59,  8.84s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 79/1938 [11:45<4:33:32,  8.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 80/1938 [11:54<4:33:00,  8.82s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 81/1938 [12:03<4:32:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 82/1938 [12:12<4:32:48,  8.82s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 83/1938 [12:20<4:33:12,  8.84s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 84/1938 [12:29<4:32:52,  8.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 85/1938 [12:38<4:32:49,  8.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 86/1938 [12:47<4:32:16,  8.82s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 87/1938 [12:56<4:32:16,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 88/1938 [13:05<4:32:19,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 89/1938 [13:13<4:32:21,  8.84s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 90/1938 [13:22<4:31:58,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 91/1938 [13:31<4:31:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 92/1938 [13:40<4:32:05,  8.84s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 93/1938 [13:49<4:32:18,  8.86s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 94/1938 [13:58<4:31:55,  8.85s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 95/1938 [14:06<4:31:18,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 96/1938 [14:15<4:31:09,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 97/1938 [14:24<4:30:55,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 98/1938 [14:33<4:30:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 99/1938 [14:42<4:30:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 100/1938 [14:51<4:30:33,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 101/1938 [14:59<4:30:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 102/1938 [15:08<4:30:13,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 103/1938 [15:17<4:29:45,  8.82s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 104/1938 [15:26<4:29:39,  8.82s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 105/1938 [15:35<4:29:36,  8.83s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 106/1938 [15:44<4:29:19,  8.82s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 107/1938 [15:52<4:29:24,  8.83s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 108/1938 [16:01<4:29:10,  8.83s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 109/1938 [16:10<4:28:47,  8.82s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 110/1938 [16:19<4:28:33,  8.81s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 111/1938 [16:28<4:28:20,  8.81s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 112/1938 [16:36<4:28:23,  8.82s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 113/1938 [16:45<4:27:52,  8.81s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 114/1938 [16:54<4:27:49,  8.81s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 115/1938 [17:03<4:28:18,  8.83s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 116/1938 [17:12<4:28:22,  8.84s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 117/1938 [17:21<4:28:16,  8.84s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 118/1938 [17:29<4:27:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 119/1938 [17:38<4:27:27,  8.82s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 120/1938 [17:47<4:27:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 121/1938 [17:56<4:26:57,  8.82s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 122/1938 [18:05<4:27:08,  8.83s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 123/1938 [18:14<4:27:17,  8.84s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 124/1938 [18:22<4:26:45,  8.82s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 125/1938 [18:31<4:26:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 126/1938 [18:40<4:26:19,  8.82s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 127/1938 [18:49<4:25:53,  8.81s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 128/1938 [18:58<4:25:35,  8.80s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 129/1938 [19:06<4:25:43,  8.81s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 130/1938 [19:15<4:26:10,  8.83s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 131/1938 [19:24<4:26:12,  8.84s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 132/1938 [19:33<4:26:00,  8.84s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 133/1938 [19:42<4:25:53,  8.84s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 134/1938 [19:51<4:25:14,  8.82s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 135/1938 [19:59<4:25:27,  8.83s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 136/1938 [20:08<4:24:45,  8.82s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 137/1938 [20:17<4:24:32,  8.81s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 138/1938 [20:26<4:24:08,  8.80s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 139/1938 [20:35<4:24:02,  8.81s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 140/1938 [20:43<4:24:14,  8.82s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 141/1938 [20:52<4:24:27,  8.83s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 142/1938 [21:01<4:24:02,  8.82s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 143/1938 [21:10<4:23:51,  8.82s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 144/1938 [21:19<4:24:00,  8.83s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 145/1938 [21:28<4:24:26,  8.85s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 146/1938 [21:37<4:24:14,  8.85s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 147/1938 [21:45<4:24:22,  8.86s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 148/1938 [21:54<4:24:06,  8.85s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 149/1938 [22:03<4:23:42,  8.84s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 150/1938 [22:12<4:23:43,  8.85s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 151/1938 [22:21<4:23:40,  8.85s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 152/1938 [22:30<4:23:35,  8.86s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 153/1938 [22:39<4:23:23,  8.85s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 154/1938 [22:47<4:23:07,  8.85s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 155/1938 [22:56<4:22:45,  8.84s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 156/1938 [23:05<4:22:38,  8.84s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 157/1938 [23:14<4:22:35,  8.85s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 158/1938 [23:23<4:22:25,  8.85s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 159/1938 [23:32<4:22:06,  8.84s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 160/1938 [23:40<4:22:03,  8.84s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 161/1938 [23:49<4:21:55,  8.84s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 162/1938 [23:58<4:21:33,  8.84s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 163/1938 [24:07<4:21:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 164/1938 [24:16<4:20:53,  8.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 165/1938 [24:24<4:20:38,  8.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 166/1938 [24:33<4:20:36,  8.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 167/1938 [24:42<4:20:25,  8.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 168/1938 [24:51<4:20:16,  8.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 169/1938 [25:00<4:20:31,  8.84s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 170/1938 [25:09<4:20:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 171/1938 [25:17<4:20:01,  8.83s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 172/1938 [25:26<4:19:44,  8.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 173/1938 [25:35<4:19:36,  8.83s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 174/1938 [25:44<4:19:28,  8.83s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 175/1938 [25:53<4:19:11,  8.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 176/1938 [26:02<4:18:58,  8.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 177/1938 [26:10<4:18:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 178/1938 [26:19<4:19:05,  8.83s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 179/1938 [26:28<4:19:01,  8.84s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 180/1938 [26:37<4:18:28,  8.82s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 181/1938 [26:46<4:18:48,  8.84s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 182/1938 [26:55<4:18:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 183/1938 [27:03<4:18:42,  8.84s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 184/1938 [27:12<4:18:25,  8.84s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 185/1938 [27:21<4:18:19,  8.84s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 186/1938 [27:30<4:18:28,  8.85s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 187/1938 [27:39<4:18:27,  8.86s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 188/1938 [27:48<4:17:52,  8.84s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 189/1938 [27:56<4:17:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 190/1938 [28:05<4:17:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 191/1938 [28:14<4:17:15,  8.84s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 192/1938 [28:23<4:17:14,  8.84s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 193/1938 [28:32<4:17:15,  8.85s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 194/1938 [28:41<4:17:23,  8.85s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 195/1938 [28:50<4:17:00,  8.85s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 196/1938 [28:58<4:16:32,  8.84s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 197/1938 [29:07<4:16:06,  8.83s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 198/1938 [29:16<4:16:12,  8.83s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 199/1938 [29:25<4:16:16,  8.84s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 200/1938 [29:34<4:16:09,  8.84s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 201/1938 [29:43<4:16:04,  8.85s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 202/1938 [29:51<4:16:11,  8.85s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 203/1938 [30:00<4:16:04,  8.86s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 204/1938 [30:09<4:16:03,  8.86s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 205/1938 [30:18<4:15:38,  8.85s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 206/1938 [30:27<4:15:11,  8.84s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 207/1938 [30:36<4:14:58,  8.84s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 208/1938 [30:45<4:14:41,  8.83s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 209/1938 [30:53<4:14:28,  8.83s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 210/1938 [31:02<4:14:08,  8.82s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 211/1938 [31:11<4:13:50,  8.82s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 212/1938 [31:20<4:14:07,  8.83s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 213/1938 [31:29<4:13:47,  8.83s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 214/1938 [31:37<4:13:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 215/1938 [31:46<4:13:13,  8.82s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 216/1938 [31:55<4:13:00,  8.82s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 217/1938 [32:04<4:13:05,  8.82s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 218/1938 [32:13<4:13:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 219/1938 [32:22<4:12:43,  8.82s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 220/1938 [32:30<4:12:34,  8.82s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 221/1938 [32:39<4:12:41,  8.83s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 222/1938 [32:48<4:12:38,  8.83s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 223/1938 [32:57<4:12:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 224/1938 [33:06<4:11:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 225/1938 [33:14<4:11:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 226/1938 [33:23<4:11:23,  8.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 227/1938 [33:32<4:11:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 228/1938 [33:41<4:11:42,  8.83s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 229/1938 [33:50<4:11:39,  8.84s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 230/1938 [33:59<4:11:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 231/1938 [34:07<4:10:44,  8.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 232/1938 [34:16<4:10:43,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 233/1938 [34:25<4:10:36,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 234/1938 [34:34<4:10:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 235/1938 [34:43<4:10:19,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 236/1938 [34:51<4:09:55,  8.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 237/1938 [35:00<4:09:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 238/1938 [35:09<4:09:38,  8.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 239/1938 [35:18<4:09:30,  8.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 240/1938 [35:27<4:09:27,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 241/1938 [35:36<4:09:25,  8.82s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 242/1938 [35:44<4:09:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 243/1938 [35:53<4:09:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 244/1938 [36:02<4:09:15,  8.83s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 245/1938 [36:11<4:09:18,  8.84s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 246/1938 [36:20<4:08:52,  8.83s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 247/1938 [36:29<4:08:21,  8.81s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 248/1938 [36:37<4:08:27,  8.82s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 249/1938 [36:46<4:08:47,  8.84s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 250/1938 [36:55<4:08:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 251/1938 [37:04<4:08:15,  8.83s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 252/1938 [37:13<4:08:21,  8.84s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 253/1938 [37:22<4:07:47,  8.82s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 254/1938 [37:30<4:07:34,  8.82s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 255/1938 [37:39<4:07:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 256/1938 [37:48<4:07:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 257/1938 [37:57<4:08:38,  8.87s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 258/1938 [38:06<4:08:01,  8.86s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 259/1938 [38:15<4:07:32,  8.85s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 260/1938 [38:23<4:07:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 261/1938 [38:32<4:06:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 262/1938 [38:41<4:06:54,  8.84s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 263/1938 [38:50<4:07:04,  8.85s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 264/1938 [38:59<4:06:47,  8.85s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 265/1938 [39:08<4:06:23,  8.84s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 266/1938 [39:17<4:06:22,  8.84s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 267/1938 [39:25<4:06:06,  8.84s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 268/1938 [39:34<4:05:45,  8.83s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 269/1938 [39:43<4:05:32,  8.83s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 270/1938 [39:52<4:05:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 271/1938 [40:01<4:05:39,  8.84s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 272/1938 [40:10<4:05:44,  8.85s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 273/1938 [40:18<4:05:17,  8.84s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 274/1938 [40:27<4:04:57,  8.83s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 275/1938 [40:36<4:05:05,  8.84s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 276/1938 [40:45<4:05:00,  8.84s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 277/1938 [40:54<4:04:53,  8.85s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 278/1938 [41:03<4:04:42,  8.84s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 279/1938 [41:11<4:04:51,  8.86s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 280/1938 [41:20<4:04:58,  8.87s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 281/1938 [41:29<4:04:54,  8.87s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 282/1938 [41:38<4:04:30,  8.86s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 283/1938 [41:47<4:03:59,  8.85s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 284/1938 [41:56<4:03:41,  8.84s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 285/1938 [42:05<4:03:33,  8.84s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 286/1938 [42:13<4:03:49,  8.86s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 287/1938 [42:22<4:03:36,  8.85s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 288/1938 [42:31<4:03:14,  8.84s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 289/1938 [42:40<4:02:45,  8.83s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 290/1938 [42:49<4:02:20,  8.82s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 291/1938 [42:58<4:02:00,  8.82s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 292/1938 [43:06<4:01:52,  8.82s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 293/1938 [43:15<4:01:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 294/1938 [43:24<4:02:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 295/1938 [43:33<4:01:45,  8.83s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 296/1938 [43:42<4:01:51,  8.84s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 297/1938 [43:51<4:02:04,  8.85s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 298/1938 [43:59<4:02:09,  8.86s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 299/1938 [44:08<4:02:16,  8.87s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 300/1938 [44:17<4:01:40,  8.85s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 301/1938 [44:26<4:01:32,  8.85s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 302/1938 [44:35<4:01:29,  8.86s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 303/1938 [44:44<4:01:31,  8.86s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 304/1938 [44:53<4:01:30,  8.87s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 305/1938 [45:02<4:01:23,  8.87s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 306/1938 [45:10<4:01:01,  8.86s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 307/1938 [45:19<4:00:27,  8.85s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 308/1938 [45:28<4:00:01,  8.84s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 309/1938 [45:37<3:59:43,  8.83s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 310/1938 [45:46<3:59:49,  8.84s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 311/1938 [45:54<3:59:31,  8.83s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 312/1938 [46:03<3:59:45,  8.85s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 313/1938 [46:12<3:59:29,  8.84s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 314/1938 [46:21<3:59:29,  8.85s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 315/1938 [46:30<3:59:08,  8.84s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 316/1938 [46:39<3:59:02,  8.84s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 317/1938 [46:48<3:58:44,  8.84s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 318/1938 [46:56<3:58:31,  8.83s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 319/1938 [47:05<3:58:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 320/1938 [47:14<3:57:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 321/1938 [47:23<3:57:47,  8.82s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 322/1938 [47:32<3:57:42,  8.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 323/1938 [47:40<3:57:42,  8.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 324/1938 [47:49<3:57:25,  8.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 325/1938 [47:58<3:57:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 326/1938 [48:07<3:57:31,  8.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 327/1938 [48:16<3:57:16,  8.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 328/1938 [48:25<3:57:06,  8.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 329/1938 [48:34<3:57:08,  8.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 330/1938 [48:42<3:56:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 331/1938 [48:51<3:56:24,  8.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 332/1938 [49:00<3:56:49,  8.85s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 333/1938 [49:09<3:56:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 334/1938 [49:18<3:56:18,  8.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 335/1938 [49:27<3:56:10,  8.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 336/1938 [49:35<3:55:57,  8.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 337/1938 [49:44<3:55:57,  8.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 338/1938 [49:53<3:55:24,  8.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 339/1938 [50:02<3:55:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 340/1938 [50:11<3:55:05,  8.83s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 341/1938 [50:20<3:55:25,  8.84s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 342/1938 [50:28<3:55:03,  8.84s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 343/1938 [50:37<3:54:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 344/1938 [50:46<3:54:10,  8.81s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 345/1938 [50:55<3:54:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 346/1938 [51:04<3:54:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 347/1938 [51:12<3:53:57,  8.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 348/1938 [51:21<3:54:09,  8.84s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 349/1938 [51:30<3:54:00,  8.84s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 350/1938 [51:39<3:54:00,  8.84s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 351/1938 [51:48<3:53:56,  8.84s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 352/1938 [51:57<3:53:53,  8.85s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 353/1938 [52:06<3:53:12,  8.83s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 354/1938 [52:14<3:52:49,  8.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 355/1938 [52:23<3:52:23,  8.81s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 356/1938 [52:32<3:52:28,  8.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 357/1938 [52:41<3:52:14,  8.81s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 358/1938 [52:50<3:51:55,  8.81s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 359/1938 [52:58<3:51:51,  8.81s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 360/1938 [53:07<3:51:42,  8.81s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 361/1938 [53:16<3:51:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 362/1938 [53:25<3:51:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 363/1938 [53:34<3:51:47,  8.83s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 364/1938 [53:43<3:51:50,  8.84s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 365/1938 [53:51<3:51:16,  8.82s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 366/1938 [54:00<3:51:11,  8.82s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 367/1938 [54:09<3:51:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 368/1938 [54:18<3:51:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 369/1938 [54:27<3:50:55,  8.83s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 370/1938 [54:36<3:51:04,  8.84s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 371/1938 [54:44<3:50:43,  8.83s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 372/1938 [54:53<3:50:26,  8.83s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 373/1938 [55:02<3:50:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 374/1938 [55:11<3:49:46,  8.81s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 375/1938 [55:20<3:49:23,  8.81s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 376/1938 [55:28<3:49:57,  8.83s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 377/1938 [55:37<3:49:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 378/1938 [55:46<3:49:15,  8.82s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 379/1938 [55:55<3:48:57,  8.81s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 380/1938 [56:04<3:49:32,  8.84s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 381/1938 [56:13<3:49:38,  8.85s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 382/1938 [56:21<3:49:22,  8.85s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 383/1938 [56:30<3:49:07,  8.84s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 384/1938 [56:39<3:48:57,  8.84s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 385/1938 [56:48<3:48:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 386/1938 [56:57<3:48:07,  8.82s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 387/1938 [57:06<3:48:11,  8.83s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 388/1938 [57:14<3:48:19,  8.84s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 389/1938 [57:23<3:48:12,  8.84s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 390/1938 [57:32<3:47:58,  8.84s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 391/1938 [57:41<3:47:51,  8.84s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 392/1938 [57:50<3:47:44,  8.84s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 393/1938 [57:59<3:47:49,  8.85s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 394/1938 [58:07<3:47:34,  8.84s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 395/1938 [58:16<3:47:25,  8.84s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 396/1938 [58:25<3:47:00,  8.83s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 397/1938 [58:34<3:46:58,  8.84s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 398/1938 [58:43<3:46:33,  8.83s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 399/1938 [58:52<3:46:44,  8.84s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 400/1938 [59:01<3:46:35,  8.84s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 401/1938 [59:09<3:46:09,  8.83s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 402/1938 [59:18<3:46:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 403/1938 [59:27<3:45:44,  8.82s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 404/1938 [59:36<3:45:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 405/1938 [59:45<3:45:46,  8.84s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 406/1938 [59:54<3:46:08,  8.86s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 407/1938 [1:00:02<3:46:01,  8.86s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 408/1938 [1:00:11<3:45:40,  8.85s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 409/1938 [1:00:20<3:45:29,  8.85s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 410/1938 [1:00:29<3:45:05,  8.84s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 411/1938 [1:00:38<3:44:52,  8.84s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 412/1938 [1:00:47<3:44:30,  8.83s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 413/1938 [1:00:55<3:44:09,  8.82s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 414/1938 [1:01:04<3:44:19,  8.83s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 415/1938 [1:01:13<3:44:17,  8.84s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 416/1938 [1:01:22<3:44:07,  8.84s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 417/1938 [1:01:31<3:44:31,  8.86s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 418/1938 [1:01:40<3:44:11,  8.85s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 419/1938 [1:01:48<3:44:00,  8.85s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 420/1938 [1:01:57<3:43:49,  8.85s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 421/1938 [1:02:06<3:43:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 422/1938 [1:02:15<3:42:58,  8.83s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 423/1938 [1:02:24<3:42:41,  8.82s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 424/1938 [1:02:33<3:42:17,  8.81s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 425/1938 [1:02:41<3:42:03,  8.81s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 426/1938 [1:02:50<3:42:24,  8.83s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 427/1938 [1:02:59<3:42:12,  8.82s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 428/1938 [1:03:08<3:42:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 429/1938 [1:03:17<3:42:16,  8.84s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 430/1938 [1:03:26<3:41:56,  8.83s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 431/1938 [1:03:34<3:41:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 432/1938 [1:03:43<3:41:09,  8.81s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 433/1938 [1:03:52<3:41:15,  8.82s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 434/1938 [1:04:01<3:41:26,  8.83s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 435/1938 [1:04:10<3:41:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 436/1938 [1:04:18<3:40:56,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 437/1938 [1:04:27<3:40:36,  8.82s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 438/1938 [1:04:36<3:40:36,  8.82s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 439/1938 [1:04:45<3:40:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 440/1938 [1:04:54<3:40:03,  8.81s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 441/1938 [1:05:03<3:40:11,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 442/1938 [1:05:11<3:40:10,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 443/1938 [1:05:20<3:39:56,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 444/1938 [1:05:29<3:39:36,  8.82s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 445/1938 [1:05:38<3:39:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 446/1938 [1:05:47<3:39:38,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 447/1938 [1:05:56<3:39:31,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 448/1938 [1:06:04<3:39:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 449/1938 [1:06:13<3:38:45,  8.82s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 450/1938 [1:06:22<3:38:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 451/1938 [1:06:31<3:38:54,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 452/1938 [1:06:40<3:38:49,  8.84s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 453/1938 [1:06:49<3:38:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 454/1938 [1:06:57<3:38:32,  8.84s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 455/1938 [1:07:06<3:38:22,  8.84s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 456/1938 [1:07:15<3:38:12,  8.83s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 457/1938 [1:07:24<3:38:07,  8.84s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 458/1938 [1:07:33<3:38:06,  8.84s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 459/1938 [1:07:42<3:38:25,  8.86s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 460/1938 [1:07:50<3:37:59,  8.85s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 461/1938 [1:07:59<3:37:43,  8.84s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 462/1938 [1:08:08<3:37:47,  8.85s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 463/1938 [1:08:17<3:37:38,  8.85s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 464/1938 [1:08:26<3:37:22,  8.85s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 465/1938 [1:08:35<3:36:59,  8.84s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 466/1938 [1:08:44<3:37:10,  8.85s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 467/1938 [1:08:52<3:36:53,  8.85s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 468/1938 [1:09:01<3:37:03,  8.86s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 469/1938 [1:09:10<3:36:50,  8.86s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 470/1938 [1:09:19<3:36:42,  8.86s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 471/1938 [1:09:28<3:36:32,  8.86s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 472/1938 [1:09:37<3:36:13,  8.85s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 473/1938 [1:09:46<3:36:01,  8.85s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 474/1938 [1:09:54<3:35:43,  8.84s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 475/1938 [1:10:03<3:35:43,  8.85s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 476/1938 [1:10:12<3:35:40,  8.85s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 477/1938 [1:10:21<3:35:31,  8.85s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 478/1938 [1:10:30<3:35:11,  8.84s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 479/1938 [1:10:39<3:35:02,  8.84s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 480/1938 [1:10:47<3:34:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 481/1938 [1:10:56<3:35:40,  8.88s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 482/1938 [1:11:05<3:35:18,  8.87s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 483/1938 [1:11:14<3:34:51,  8.86s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 484/1938 [1:11:23<3:34:20,  8.84s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 485/1938 [1:11:32<3:34:12,  8.85s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 486/1938 [1:11:41<3:33:54,  8.84s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 487/1938 [1:11:49<3:33:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 488/1938 [1:11:58<3:33:10,  8.82s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 489/1938 [1:12:07<3:33:06,  8.82s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 490/1938 [1:12:16<3:33:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 491/1938 [1:12:25<3:32:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 492/1938 [1:12:33<3:32:36,  8.82s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 493/1938 [1:12:42<3:32:24,  8.82s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 494/1938 [1:12:51<3:32:15,  8.82s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 495/1938 [1:13:00<3:32:21,  8.83s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 496/1938 [1:13:09<3:32:22,  8.84s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 497/1938 [1:13:18<3:31:40,  8.81s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 498/1938 [1:13:26<3:31:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 499/1938 [1:13:35<3:31:48,  8.83s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 500/1938 [1:13:44<3:31:52,  8.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7226, 'learning_rate': 0.0001, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 500/1938 [1:13:44<3:31:52,  8.84s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 501/1938 [1:13:53<3:31:48,  8.84s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 502/1938 [1:14:02<3:31:39,  8.84s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 503/1938 [1:14:11<3:31:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 504/1938 [1:14:19<3:31:22,  8.84s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 505/1938 [1:14:28<3:31:07,  8.84s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 506/1938 [1:14:37<3:30:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 507/1938 [1:14:46<3:30:09,  8.81s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 508/1938 [1:14:55<3:30:04,  8.81s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 509/1938 [1:15:04<3:30:12,  8.83s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 510/1938 [1:15:12<3:30:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 511/1938 [1:15:21<3:29:49,  8.82s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 512/1938 [1:15:30<3:29:58,  8.83s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 513/1938 [1:15:39<3:29:33,  8.82s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 514/1938 [1:15:48<3:29:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 515/1938 [1:15:57<3:29:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 516/1938 [1:16:05<3:29:31,  8.84s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 517/1938 [1:16:14<3:29:16,  8.84s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 518/1938 [1:16:23<3:28:49,  8.82s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 519/1938 [1:16:32<3:28:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 520/1938 [1:16:41<3:28:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 521/1938 [1:16:50<3:28:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 522/1938 [1:16:58<3:28:57,  8.85s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 523/1938 [1:17:07<3:28:38,  8.85s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 524/1938 [1:17:16<3:28:24,  8.84s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 525/1938 [1:17:25<3:28:28,  8.85s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 526/1938 [1:17:34<3:28:13,  8.85s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 527/1938 [1:17:43<3:27:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 528/1938 [1:17:51<3:27:27,  8.83s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 529/1938 [1:18:00<3:27:40,  8.84s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 530/1938 [1:18:09<3:27:48,  8.86s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 531/1938 [1:18:18<3:27:36,  8.85s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 532/1938 [1:18:27<3:27:37,  8.86s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 533/1938 [1:18:36<3:27:35,  8.87s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 534/1938 [1:18:45<3:27:05,  8.85s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 535/1938 [1:18:53<3:26:41,  8.84s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 536/1938 [1:19:02<3:26:42,  8.85s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 537/1938 [1:19:11<3:26:53,  8.86s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 538/1938 [1:19:20<3:26:40,  8.86s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 539/1938 [1:19:29<3:26:15,  8.85s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 540/1938 [1:19:38<3:25:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 541/1938 [1:19:46<3:25:31,  8.83s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 542/1938 [1:19:55<3:25:05,  8.81s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 543/1938 [1:20:04<3:25:09,  8.82s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 544/1938 [1:20:13<3:25:17,  8.84s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 545/1938 [1:20:22<3:25:05,  8.83s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 546/1938 [1:20:31<3:25:00,  8.84s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 547/1938 [1:20:39<3:24:59,  8.84s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 548/1938 [1:20:48<3:25:01,  8.85s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 549/1938 [1:20:57<3:24:57,  8.85s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 550/1938 [1:21:06<3:24:27,  8.84s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 551/1938 [1:21:15<3:24:32,  8.85s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 552/1938 [1:21:24<3:24:18,  8.84s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 553/1938 [1:21:33<3:24:06,  8.84s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 554/1938 [1:21:41<3:23:43,  8.83s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 555/1938 [1:21:50<3:23:23,  8.82s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 556/1938 [1:21:59<3:23:16,  8.83s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 557/1938 [1:22:08<3:23:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 558/1938 [1:22:17<3:23:01,  8.83s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 559/1938 [1:22:25<3:22:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 560/1938 [1:22:34<3:22:43,  8.83s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 561/1938 [1:22:43<3:22:40,  8.83s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 562/1938 [1:22:52<3:22:42,  8.84s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 563/1938 [1:23:01<3:22:30,  8.84s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 564/1938 [1:23:10<3:22:27,  8.84s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 565/1938 [1:23:19<3:22:12,  8.84s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 566/1938 [1:23:27<3:22:00,  8.83s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 567/1938 [1:23:36<3:21:38,  8.82s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 568/1938 [1:23:45<3:21:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 569/1938 [1:23:54<3:21:08,  8.82s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 570/1938 [1:24:03<3:21:16,  8.83s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 571/1938 [1:24:11<3:21:20,  8.84s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 572/1938 [1:24:20<3:21:15,  8.84s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 573/1938 [1:24:29<3:21:06,  8.84s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 574/1938 [1:24:38<3:20:45,  8.83s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 575/1938 [1:24:47<3:20:27,  8.82s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 576/1938 [1:24:56<3:20:31,  8.83s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 577/1938 [1:25:05<3:20:37,  8.84s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 578/1938 [1:25:13<3:20:39,  8.85s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 579/1938 [1:25:22<3:20:23,  8.85s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 580/1938 [1:25:31<3:20:12,  8.85s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 581/1938 [1:25:40<3:19:56,  8.84s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 582/1938 [1:25:49<3:19:26,  8.82s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 583/1938 [1:25:58<3:19:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 584/1938 [1:26:06<3:19:12,  8.83s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 585/1938 [1:26:15<3:19:06,  8.83s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 586/1938 [1:26:24<3:19:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 587/1938 [1:26:33<3:18:21,  8.81s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 588/1938 [1:26:42<3:18:11,  8.81s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 589/1938 [1:26:50<3:18:06,  8.81s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 590/1938 [1:26:59<3:18:09,  8.82s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 591/1938 [1:27:08<3:18:06,  8.82s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 592/1938 [1:27:17<3:17:59,  8.83s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 593/1938 [1:27:26<3:17:45,  8.82s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 594/1938 [1:27:35<3:17:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 595/1938 [1:27:43<3:17:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 596/1938 [1:27:52<3:16:55,  8.80s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 597/1938 [1:28:01<3:16:53,  8.81s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 598/1938 [1:28:10<3:16:49,  8.81s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 599/1938 [1:28:19<3:16:39,  8.81s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 600/1938 [1:28:27<3:16:31,  8.81s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 601/1938 [1:28:36<3:16:25,  8.81s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 602/1938 [1:28:45<3:16:14,  8.81s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 603/1938 [1:28:54<3:16:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 604/1938 [1:29:03<3:16:06,  8.82s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 605/1938 [1:29:12<3:16:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 606/1938 [1:29:20<3:15:58,  8.83s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 607/1938 [1:29:29<3:15:47,  8.83s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 608/1938 [1:29:38<3:15:38,  8.83s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 609/1938 [1:29:47<3:15:19,  8.82s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 610/1938 [1:29:56<3:15:05,  8.81s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 611/1938 [1:30:04<3:15:11,  8.83s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 612/1938 [1:30:13<3:15:05,  8.83s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 613/1938 [1:30:22<3:14:56,  8.83s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 614/1938 [1:30:31<3:14:48,  8.83s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 615/1938 [1:30:40<3:14:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 616/1938 [1:30:49<3:14:45,  8.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 617/1938 [1:30:57<3:14:34,  8.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 618/1938 [1:31:06<3:14:39,  8.85s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 619/1938 [1:31:15<3:14:30,  8.85s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 620/1938 [1:31:24<3:14:16,  8.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 621/1938 [1:31:33<3:14:04,  8.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 622/1938 [1:31:42<3:13:53,  8.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 623/1938 [1:31:51<3:13:35,  8.83s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 624/1938 [1:31:59<3:13:15,  8.82s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 625/1938 [1:32:08<3:13:12,  8.83s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 626/1938 [1:32:17<3:12:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 627/1938 [1:32:26<3:12:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 628/1938 [1:32:35<3:12:45,  8.83s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 629/1938 [1:32:43<3:12:34,  8.83s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 630/1938 [1:32:52<3:12:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 631/1938 [1:33:01<3:12:24,  8.83s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 632/1938 [1:33:10<3:12:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 633/1938 [1:33:19<3:11:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 634/1938 [1:33:28<3:11:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 635/1938 [1:33:36<3:11:40,  8.83s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 636/1938 [1:33:45<3:11:40,  8.83s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 637/1938 [1:33:54<3:11:39,  8.84s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 638/1938 [1:34:03<3:11:11,  8.82s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 639/1938 [1:34:12<3:10:50,  8.81s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 640/1938 [1:34:21<3:10:35,  8.81s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 641/1938 [1:34:29<3:10:48,  8.83s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 642/1938 [1:34:38<3:10:33,  8.82s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 643/1938 [1:34:47<3:10:25,  8.82s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 644/1938 [1:34:56<3:10:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 645/1938 [1:35:05<3:10:41,  8.85s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 646/1938 [1:35:12<3:03:10,  8.51s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 2175\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mNum examples = 2175\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/34 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 2/34 [00:02<00:46,  1.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 3/34 [00:04<00:47,  1.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 4/34 [00:06<00:45,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 5/34 [00:07<00:44,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 6/34 [00:09<00:42,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 7/34 [00:10<00:41,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▎       | 8/34 [00:12<00:39,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▋       | 9/34 [00:13<00:38,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 10/34 [00:15<00:36,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 11/34 [00:16<00:35,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 12/34 [00:18<00:33,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 13/34 [00:19<00:31,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 14/34 [00:21<00:30,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 15/34 [00:22<00:28,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 16/34 [00:24<00:27,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 17/34 [00:25<00:25,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 18/34 [00:27<00:24,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 19/34 [00:28<00:22,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 20/34 [00:30<00:21,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 21/34 [00:31<00:19,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 22/34 [00:33<00:18,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 23/34 [00:35<00:16,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 24/34 [00:36<00:15,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 25/34 [00:38<00:13,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 26/34 [00:39<00:12,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 27/34 [00:41<00:10,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 28/34 [00:42<00:09,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 29/34 [00:44<00:07,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 30/34 [00:45<00:06,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 31/34 [00:47<00:04,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 32/34 [00:48<00:03,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 33/34 [00:50<00:01,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 34/34 [00:51<00:00,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.5660961866378784, 'eval_runtime': 53.475, 'eval_samples_per_second': 40.673, 'eval_steps_per_second': 0.636, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 646/1938 [1:36:06<3:03:10,  8.51s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 34/34 [00:51<00:00,  1.53s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-646\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-646\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-646/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-646/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-646/generation_config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-646/generation_config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-646/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-646/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:18,622] [INFO] [engine.py:3500:save_16bit_model] Saving model weights to /tmp/checkpoint-646/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:18,622] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /tmp/checkpoint-646/pytorch_model.bin...\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:37,270] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /tmp/checkpoint-646/pytorch_model.bin.\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:37,298] [INFO] [logging.py:68:log_dist] [Rank 0] [Torch] Checkpoint global_step646 is begin to save!\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:37,307] [INFO] [logging.py:68:log_dist] [Rank 0] Saving model checkpoint: /tmp/checkpoint-646/global_step646/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:37,307] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /tmp/checkpoint-646/global_step646/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:37,588] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /tmp/checkpoint-646/global_step646/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:37,590] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /tmp/checkpoint-646/global_step646/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:48,553] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /tmp/checkpoint-646/global_step646/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:48,554] [INFO] [engine.py:3397:_save_zero_checkpoint] zero checkpoint saved /tmp/checkpoint-646/global_step646/zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2023-05-29 08:08:48,609] [INFO] [torch_checkpoint_engine.py:27:commit] [Torch] Checkpoint global_step646 is ready now!\u001b[0m\n",
      "\u001b[34m33%|███▎      | 647/1938 [1:36:54<13:06:34, 36.56s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 648/1938 [1:37:03<10:07:34, 28.26s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 649/1938 [1:37:12<8:01:48, 22.43s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 650/1938 [1:37:21<6:33:53, 18.35s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 651/1938 [1:37:30<5:32:33, 15.50s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 652/1938 [1:37:39<4:49:36, 13.51s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 653/1938 [1:37:48<4:19:40, 12.12s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 654/1938 [1:37:57<3:59:15, 11.18s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 655/1938 [1:38:06<3:44:39, 10.51s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 656/1938 [1:38:14<3:33:55, 10.01s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 657/1938 [1:38:23<3:26:23,  9.67s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 658/1938 [1:38:32<3:21:03,  9.42s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 659/1938 [1:38:41<3:17:20,  9.26s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 660/1938 [1:38:50<3:14:28,  9.13s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 661/1938 [1:38:59<3:12:37,  9.05s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 662/1938 [1:39:08<3:11:17,  8.99s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 663/1938 [1:39:16<3:09:56,  8.94s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 664/1938 [1:39:25<3:09:08,  8.91s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 665/1938 [1:39:34<3:08:42,  8.89s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 666/1938 [1:39:43<3:08:08,  8.87s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 667/1938 [1:39:52<3:07:34,  8.85s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 668/1938 [1:40:01<3:07:15,  8.85s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 669/1938 [1:40:09<3:07:04,  8.85s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 670/1938 [1:40:18<3:06:38,  8.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 671/1938 [1:40:27<3:06:39,  8.84s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 672/1938 [1:40:36<3:06:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 673/1938 [1:40:45<3:05:52,  8.82s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 674/1938 [1:40:53<3:05:59,  8.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 675/1938 [1:41:02<3:05:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 676/1938 [1:41:11<3:05:41,  8.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 677/1938 [1:41:20<3:05:34,  8.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 678/1938 [1:41:29<3:05:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 679/1938 [1:41:38<3:05:32,  8.84s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 680/1938 [1:41:46<3:05:13,  8.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 681/1938 [1:41:55<3:05:02,  8.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 682/1938 [1:42:04<3:05:11,  8.85s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 683/1938 [1:42:13<3:05:27,  8.87s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 684/1938 [1:42:22<3:05:11,  8.86s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 685/1938 [1:42:31<3:04:48,  8.85s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 686/1938 [1:42:40<3:04:51,  8.86s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 687/1938 [1:42:49<3:04:58,  8.87s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 688/1938 [1:42:57<3:04:54,  8.88s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 689/1938 [1:43:06<3:04:28,  8.86s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 690/1938 [1:43:15<3:04:11,  8.86s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 691/1938 [1:43:24<3:04:02,  8.85s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 692/1938 [1:43:33<3:03:37,  8.84s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 693/1938 [1:43:42<3:03:22,  8.84s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 694/1938 [1:43:50<3:03:08,  8.83s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 695/1938 [1:43:59<3:02:54,  8.83s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 696/1938 [1:44:08<3:02:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 697/1938 [1:44:17<3:02:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 698/1938 [1:44:26<3:02:07,  8.81s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 699/1938 [1:44:35<3:02:06,  8.82s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 700/1938 [1:44:43<3:02:01,  8.82s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 701/1938 [1:44:52<3:01:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 702/1938 [1:45:01<3:01:41,  8.82s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 703/1938 [1:45:10<3:01:20,  8.81s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 704/1938 [1:45:19<3:01:15,  8.81s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 705/1938 [1:45:27<3:01:05,  8.81s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 706/1938 [1:45:36<3:00:47,  8.80s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 707/1938 [1:45:45<3:00:46,  8.81s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 708/1938 [1:45:54<3:00:38,  8.81s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 709/1938 [1:46:03<3:00:28,  8.81s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 710/1938 [1:46:11<3:00:14,  8.81s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 711/1938 [1:46:20<3:00:05,  8.81s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 712/1938 [1:46:29<2:59:45,  8.80s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 713/1938 [1:46:38<2:59:55,  8.81s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 714/1938 [1:46:47<2:59:50,  8.82s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 715/1938 [1:46:55<2:59:22,  8.80s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 716/1938 [1:47:04<2:59:28,  8.81s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 717/1938 [1:47:13<2:59:25,  8.82s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 718/1938 [1:47:22<2:59:10,  8.81s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 719/1938 [1:47:31<2:59:00,  8.81s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 720/1938 [1:47:40<2:58:42,  8.80s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 721/1938 [1:47:48<2:58:46,  8.81s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 722/1938 [1:47:57<2:58:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 723/1938 [1:48:06<2:58:49,  8.83s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 724/1938 [1:48:15<2:58:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 725/1938 [1:48:24<2:58:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 726/1938 [1:48:32<2:58:08,  8.82s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 727/1938 [1:48:41<2:57:58,  8.82s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 728/1938 [1:48:50<2:57:45,  8.81s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 729/1938 [1:48:59<2:57:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 730/1938 [1:49:08<2:57:42,  8.83s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 731/1938 [1:49:17<2:57:19,  8.81s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 732/1938 [1:49:25<2:57:17,  8.82s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 733/1938 [1:49:34<2:57:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 734/1938 [1:49:43<2:57:18,  8.84s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 735/1938 [1:49:52<2:57:26,  8.85s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 736/1938 [1:50:01<2:57:38,  8.87s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 737/1938 [1:50:10<2:57:14,  8.85s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 738/1938 [1:50:19<2:56:54,  8.85s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 739/1938 [1:50:27<2:56:45,  8.85s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 740/1938 [1:50:36<2:56:16,  8.83s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 741/1938 [1:50:45<2:56:11,  8.83s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 742/1938 [1:50:54<2:55:43,  8.82s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 743/1938 [1:51:03<2:55:30,  8.81s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 744/1938 [1:51:11<2:55:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 745/1938 [1:51:20<2:55:40,  8.84s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 746/1938 [1:51:29<2:55:37,  8.84s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 747/1938 [1:51:38<2:55:52,  8.86s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 748/1938 [1:51:47<2:55:42,  8.86s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 749/1938 [1:51:56<2:55:31,  8.86s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 750/1938 [1:52:05<2:55:03,  8.84s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 751/1938 [1:52:13<2:54:50,  8.84s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 752/1938 [1:52:22<2:54:53,  8.85s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 753/1938 [1:52:31<2:54:31,  8.84s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 754/1938 [1:52:40<2:54:15,  8.83s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 755/1938 [1:52:49<2:53:57,  8.82s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 756/1938 [1:52:58<2:53:42,  8.82s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 757/1938 [1:53:06<2:53:25,  8.81s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 758/1938 [1:53:15<2:53:15,  8.81s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 759/1938 [1:53:24<2:53:17,  8.82s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 760/1938 [1:53:33<2:52:52,  8.81s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 761/1938 [1:53:42<2:52:53,  8.81s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 762/1938 [1:53:50<2:52:39,  8.81s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 763/1938 [1:53:59<2:52:28,  8.81s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 764/1938 [1:54:08<2:52:26,  8.81s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 765/1938 [1:54:17<2:52:17,  8.81s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 766/1938 [1:54:26<2:52:07,  8.81s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 767/1938 [1:54:34<2:52:09,  8.82s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 768/1938 [1:54:43<2:52:11,  8.83s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 769/1938 [1:54:52<2:51:44,  8.81s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 770/1938 [1:55:01<2:51:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 771/1938 [1:55:10<2:51:51,  8.84s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 772/1938 [1:55:19<2:51:38,  8.83s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 773/1938 [1:55:27<2:51:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 774/1938 [1:55:36<2:51:18,  8.83s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 775/1938 [1:55:45<2:51:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 776/1938 [1:55:54<2:50:55,  8.83s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 777/1938 [1:56:03<2:50:59,  8.84s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 778/1938 [1:56:12<2:50:48,  8.84s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 779/1938 [1:56:20<2:50:38,  8.83s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 780/1938 [1:56:29<2:50:42,  8.85s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 781/1938 [1:56:38<2:50:31,  8.84s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 782/1938 [1:56:47<2:50:12,  8.83s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 783/1938 [1:56:56<2:49:59,  8.83s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 784/1938 [1:57:05<2:50:02,  8.84s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 785/1938 [1:57:13<2:49:35,  8.82s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 786/1938 [1:57:22<2:49:16,  8.82s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 787/1938 [1:57:31<2:49:10,  8.82s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 788/1938 [1:57:40<2:49:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 789/1938 [1:57:49<2:49:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 790/1938 [1:57:58<2:48:48,  8.82s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 791/1938 [1:58:06<2:49:03,  8.84s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 792/1938 [1:58:15<2:49:05,  8.85s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 793/1938 [1:58:24<2:48:58,  8.85s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 794/1938 [1:58:33<2:49:05,  8.87s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 795/1938 [1:58:42<2:48:50,  8.86s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 796/1938 [1:58:51<2:48:33,  8.86s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 797/1938 [1:59:00<2:48:38,  8.87s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 798/1938 [1:59:08<2:48:10,  8.85s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 799/1938 [1:59:17<2:47:50,  8.84s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 800/1938 [1:59:26<2:47:41,  8.84s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 801/1938 [1:59:35<2:47:24,  8.83s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 802/1938 [1:59:44<2:47:19,  8.84s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 803/1938 [1:59:53<2:47:09,  8.84s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 804/1938 [2:00:02<2:47:06,  8.84s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 805/1938 [2:00:10<2:46:56,  8.84s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 806/1938 [2:00:19<2:46:46,  8.84s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 807/1938 [2:00:28<2:46:34,  8.84s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 808/1938 [2:00:37<2:46:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 809/1938 [2:00:46<2:45:53,  8.82s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 810/1938 [2:00:54<2:45:42,  8.81s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 811/1938 [2:01:03<2:45:39,  8.82s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 812/1938 [2:01:12<2:45:08,  8.80s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 813/1938 [2:01:21<2:45:02,  8.80s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 814/1938 [2:01:30<2:44:56,  8.80s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 815/1938 [2:01:38<2:45:09,  8.82s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 816/1938 [2:01:47<2:45:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 817/1938 [2:01:56<2:44:54,  8.83s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 818/1938 [2:02:05<2:44:55,  8.84s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 819/1938 [2:02:14<2:44:45,  8.83s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 820/1938 [2:02:23<2:44:40,  8.84s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 821/1938 [2:02:31<2:44:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 822/1938 [2:02:40<2:44:25,  8.84s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 823/1938 [2:02:49<2:43:58,  8.82s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 824/1938 [2:02:58<2:43:39,  8.81s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 825/1938 [2:03:07<2:43:30,  8.81s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 826/1938 [2:03:16<2:43:10,  8.80s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 827/1938 [2:03:24<2:43:02,  8.81s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 828/1938 [2:03:33<2:42:58,  8.81s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 829/1938 [2:03:42<2:42:49,  8.81s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 830/1938 [2:03:51<2:42:39,  8.81s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 831/1938 [2:04:00<2:42:37,  8.81s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 832/1938 [2:04:08<2:42:36,  8.82s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 833/1938 [2:04:17<2:42:33,  8.83s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 834/1938 [2:04:26<2:42:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 835/1938 [2:04:35<2:42:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 836/1938 [2:04:44<2:42:10,  8.83s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 837/1938 [2:04:53<2:42:10,  8.84s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 838/1938 [2:05:01<2:41:59,  8.84s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 839/1938 [2:05:10<2:41:56,  8.84s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 840/1938 [2:05:19<2:41:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 841/1938 [2:05:28<2:41:40,  8.84s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 842/1938 [2:05:37<2:41:15,  8.83s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 843/1938 [2:05:46<2:40:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 844/1938 [2:05:54<2:40:36,  8.81s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 845/1938 [2:06:03<2:40:29,  8.81s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 846/1938 [2:06:12<2:40:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 847/1938 [2:06:21<2:40:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 848/1938 [2:06:30<2:40:34,  8.84s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 849/1938 [2:06:38<2:40:00,  8.82s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 850/1938 [2:06:47<2:39:38,  8.80s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 851/1938 [2:06:56<2:39:34,  8.81s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 852/1938 [2:07:05<2:39:30,  8.81s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 853/1938 [2:07:14<2:39:17,  8.81s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 854/1938 [2:07:23<2:39:14,  8.81s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 855/1938 [2:07:31<2:39:10,  8.82s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 856/1938 [2:07:40<2:39:22,  8.84s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 857/1938 [2:07:49<2:39:09,  8.83s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 858/1938 [2:07:58<2:38:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 859/1938 [2:08:07<2:38:45,  8.83s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 860/1938 [2:08:16<2:38:36,  8.83s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 861/1938 [2:08:24<2:38:20,  8.82s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 862/1938 [2:08:33<2:37:52,  8.80s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 863/1938 [2:08:42<2:37:52,  8.81s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 864/1938 [2:08:51<2:37:41,  8.81s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 865/1938 [2:09:00<2:37:37,  8.81s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 866/1938 [2:09:08<2:37:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 867/1938 [2:09:17<2:37:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 868/1938 [2:09:26<2:37:24,  8.83s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 869/1938 [2:09:35<2:37:10,  8.82s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 870/1938 [2:09:44<2:36:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 871/1938 [2:09:52<2:36:33,  8.80s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 872/1938 [2:10:01<2:36:34,  8.81s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 873/1938 [2:10:10<2:36:17,  8.80s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 874/1938 [2:10:19<2:36:06,  8.80s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 875/1938 [2:10:28<2:35:57,  8.80s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 876/1938 [2:10:36<2:35:46,  8.80s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 877/1938 [2:10:45<2:35:35,  8.80s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 878/1938 [2:10:54<2:35:32,  8.80s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 879/1938 [2:11:03<2:35:21,  8.80s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 880/1938 [2:11:12<2:35:19,  8.81s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 881/1938 [2:11:21<2:35:13,  8.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 882/1938 [2:11:29<2:35:12,  8.82s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 883/1938 [2:11:38<2:35:10,  8.82s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 884/1938 [2:11:47<2:35:02,  8.83s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 885/1938 [2:11:56<2:34:47,  8.82s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 886/1938 [2:12:05<2:34:32,  8.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 887/1938 [2:12:13<2:34:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 888/1938 [2:12:22<2:34:42,  8.84s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 889/1938 [2:12:31<2:34:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 890/1938 [2:12:40<2:34:05,  8.82s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 891/1938 [2:12:49<2:33:45,  8.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 892/1938 [2:12:58<2:33:42,  8.82s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 893/1938 [2:13:06<2:33:25,  8.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 894/1938 [2:13:15<2:33:14,  8.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 895/1938 [2:13:24<2:33:09,  8.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 896/1938 [2:13:33<2:33:15,  8.83s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 897/1938 [2:13:42<2:33:01,  8.82s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 898/1938 [2:13:50<2:32:53,  8.82s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 899/1938 [2:13:59<2:33:12,  8.85s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 900/1938 [2:14:08<2:32:48,  8.83s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 901/1938 [2:14:17<2:32:36,  8.83s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 902/1938 [2:14:26<2:32:33,  8.84s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 903/1938 [2:14:35<2:32:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 904/1938 [2:14:44<2:32:15,  8.83s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 905/1938 [2:14:52<2:32:00,  8.83s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 906/1938 [2:15:01<2:31:48,  8.83s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 907/1938 [2:15:10<2:31:41,  8.83s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 908/1938 [2:15:19<2:31:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 909/1938 [2:15:28<2:31:21,  8.83s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 910/1938 [2:15:36<2:31:01,  8.81s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 911/1938 [2:15:45<2:30:45,  8.81s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 912/1938 [2:15:54<2:30:37,  8.81s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 913/1938 [2:16:03<2:30:26,  8.81s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 914/1938 [2:16:12<2:30:19,  8.81s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 915/1938 [2:16:20<2:30:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 916/1938 [2:16:29<2:30:16,  8.82s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 917/1938 [2:16:38<2:30:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 918/1938 [2:16:47<2:29:50,  8.81s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 919/1938 [2:16:56<2:29:44,  8.82s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 920/1938 [2:17:05<2:29:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 921/1938 [2:17:13<2:29:36,  8.83s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 922/1938 [2:17:22<2:29:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 923/1938 [2:17:31<2:29:15,  8.82s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 924/1938 [2:17:40<2:29:25,  8.84s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 925/1938 [2:17:49<2:29:12,  8.84s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 926/1938 [2:17:58<2:29:06,  8.84s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 927/1938 [2:18:06<2:28:55,  8.84s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 928/1938 [2:18:15<2:28:55,  8.85s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 929/1938 [2:18:24<2:28:35,  8.84s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 930/1938 [2:18:33<2:28:29,  8.84s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 931/1938 [2:18:42<2:28:13,  8.83s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 932/1938 [2:18:51<2:28:07,  8.83s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 933/1938 [2:19:00<2:28:06,  8.84s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 934/1938 [2:19:08<2:27:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 935/1938 [2:19:17<2:27:36,  8.83s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 936/1938 [2:19:26<2:27:43,  8.85s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 937/1938 [2:19:35<2:27:29,  8.84s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 938/1938 [2:19:44<2:27:23,  8.84s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 939/1938 [2:19:53<2:27:07,  8.84s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 940/1938 [2:20:01<2:26:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 941/1938 [2:20:10<2:26:30,  8.82s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 942/1938 [2:20:19<2:26:22,  8.82s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 943/1938 [2:20:28<2:26:15,  8.82s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 944/1938 [2:20:37<2:25:59,  8.81s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 945/1938 [2:20:45<2:25:40,  8.80s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 946/1938 [2:20:54<2:25:18,  8.79s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 947/1938 [2:21:03<2:25:14,  8.79s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 948/1938 [2:21:12<2:24:51,  8.78s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 949/1938 [2:21:20<2:24:47,  8.78s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 950/1938 [2:21:29<2:25:04,  8.81s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 951/1938 [2:21:38<2:25:10,  8.83s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 952/1938 [2:21:47<2:25:08,  8.83s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 953/1938 [2:21:56<2:24:56,  8.83s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 954/1938 [2:22:05<2:24:33,  8.81s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 955/1938 [2:22:13<2:24:20,  8.81s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 956/1938 [2:22:22<2:24:15,  8.81s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 957/1938 [2:22:31<2:23:58,  8.81s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 958/1938 [2:22:40<2:23:50,  8.81s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 959/1938 [2:22:49<2:23:43,  8.81s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 960/1938 [2:22:58<2:23:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 961/1938 [2:23:06<2:23:55,  8.84s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 962/1938 [2:23:15<2:23:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 963/1938 [2:23:24<2:23:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 964/1938 [2:23:33<2:23:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 965/1938 [2:23:42<2:23:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 966/1938 [2:23:51<2:23:02,  8.83s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 967/1938 [2:23:59<2:22:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 968/1938 [2:24:08<2:22:36,  8.82s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 969/1938 [2:24:17<2:22:13,  8.81s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 970/1938 [2:24:26<2:21:57,  8.80s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 971/1938 [2:24:35<2:21:53,  8.80s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 972/1938 [2:24:43<2:21:46,  8.81s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 973/1938 [2:24:52<2:21:32,  8.80s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 974/1938 [2:25:01<2:21:33,  8.81s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 975/1938 [2:25:10<2:21:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 976/1938 [2:25:19<2:21:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 977/1938 [2:25:27<2:21:08,  8.81s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 978/1938 [2:25:36<2:21:06,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 979/1938 [2:25:45<2:20:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 980/1938 [2:25:54<2:20:47,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 981/1938 [2:26:03<2:20:43,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 982/1938 [2:26:12<2:20:42,  8.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 983/1938 [2:26:20<2:20:31,  8.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 984/1938 [2:26:29<2:20:10,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 985/1938 [2:26:38<2:20:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 986/1938 [2:26:47<2:20:00,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 987/1938 [2:26:56<2:19:52,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 988/1938 [2:27:04<2:19:41,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 989/1938 [2:27:13<2:19:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 990/1938 [2:27:22<2:19:02,  8.80s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 991/1938 [2:27:31<2:19:13,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 992/1938 [2:27:40<2:18:59,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 993/1938 [2:27:49<2:18:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 994/1938 [2:27:57<2:18:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 995/1938 [2:28:06<2:18:42,  8.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 996/1938 [2:28:15<2:18:36,  8.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 997/1938 [2:28:24<2:18:27,  8.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 998/1938 [2:28:33<2:18:11,  8.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 999/1938 [2:28:41<2:18:02,  8.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1000/1938 [2:28:50<2:17:45,  8.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4195, 'learning_rate': 0.0001, 'epoch': 1.55}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1000/1938 [2:28:50<2:17:45,  8.81s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1001/1938 [2:28:59<2:17:39,  8.81s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1002/1938 [2:29:08<2:17:24,  8.81s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1003/1938 [2:29:17<2:17:18,  8.81s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1004/1938 [2:29:26<2:17:05,  8.81s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1005/1938 [2:29:34<2:17:06,  8.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1006/1938 [2:29:43<2:16:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1007/1938 [2:29:52<2:16:49,  8.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1008/1938 [2:30:01<2:16:54,  8.83s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1009/1938 [2:30:10<2:16:33,  8.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1010/1938 [2:30:18<2:16:14,  8.81s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1011/1938 [2:30:27<2:16:13,  8.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1012/1938 [2:30:36<2:16:11,  8.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1013/1938 [2:30:45<2:15:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1014/1938 [2:30:54<2:15:51,  8.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1015/1938 [2:31:03<2:15:49,  8.83s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1016/1938 [2:31:11<2:15:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 1017/1938 [2:31:20<2:15:32,  8.83s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1018/1938 [2:31:29<2:15:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1019/1938 [2:31:38<2:15:22,  8.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1020/1938 [2:31:47<2:15:20,  8.85s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1021/1938 [2:31:56<2:15:05,  8.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1022/1938 [2:32:04<2:14:53,  8.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1023/1938 [2:32:13<2:14:54,  8.85s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1024/1938 [2:32:22<2:14:42,  8.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1025/1938 [2:32:31<2:14:28,  8.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1026/1938 [2:32:40<2:14:11,  8.83s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1027/1938 [2:32:49<2:13:57,  8.82s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1028/1938 [2:32:57<2:13:44,  8.82s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1029/1938 [2:33:06<2:13:35,  8.82s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1030/1938 [2:33:15<2:13:42,  8.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1031/1938 [2:33:24<2:13:33,  8.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1032/1938 [2:33:33<2:13:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1033/1938 [2:33:42<2:13:19,  8.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1034/1938 [2:33:50<2:13:09,  8.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1035/1938 [2:33:59<2:12:52,  8.83s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 1036/1938 [2:34:08<2:12:38,  8.82s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 1037/1938 [2:34:17<2:12:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 1038/1938 [2:34:26<2:12:24,  8.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 1039/1938 [2:34:35<2:12:09,  8.82s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 1040/1938 [2:34:43<2:12:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 1041/1938 [2:34:52<2:11:58,  8.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1042/1938 [2:35:01<2:11:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1043/1938 [2:35:10<2:11:43,  8.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1044/1938 [2:35:19<2:11:38,  8.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1045/1938 [2:35:28<2:11:35,  8.84s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1046/1938 [2:35:36<2:11:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1047/1938 [2:35:45<2:11:22,  8.85s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1048/1938 [2:35:54<2:11:05,  8.84s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1049/1938 [2:36:03<2:10:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1050/1938 [2:36:12<2:10:33,  8.82s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1051/1938 [2:36:21<2:10:26,  8.82s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1052/1938 [2:36:29<2:10:28,  8.84s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1053/1938 [2:36:38<2:10:16,  8.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1054/1938 [2:36:47<2:09:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1055/1938 [2:36:56<2:09:38,  8.81s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 1056/1938 [2:37:05<2:09:41,  8.82s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 1057/1938 [2:37:13<2:09:25,  8.81s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 1058/1938 [2:37:22<2:09:07,  8.80s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 1059/1938 [2:37:31<2:08:56,  8.80s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 1060/1938 [2:37:40<2:08:48,  8.80s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 1061/1938 [2:37:49<2:08:45,  8.81s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 1062/1938 [2:37:57<2:08:27,  8.80s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 1063/1938 [2:38:06<2:08:22,  8.80s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 1064/1938 [2:38:15<2:08:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 1065/1938 [2:38:24<2:08:27,  8.83s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 1066/1938 [2:38:33<2:08:20,  8.83s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 1067/1938 [2:38:42<2:08:00,  8.82s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 1068/1938 [2:38:50<2:07:53,  8.82s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 1069/1938 [2:38:59<2:07:38,  8.81s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 1070/1938 [2:39:08<2:07:41,  8.83s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 1071/1938 [2:39:17<2:07:33,  8.83s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 1072/1938 [2:39:26<2:07:10,  8.81s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 1073/1938 [2:39:34<2:07:01,  8.81s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 1074/1938 [2:39:43<2:06:48,  8.81s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 1075/1938 [2:39:52<2:06:46,  8.81s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1076/1938 [2:40:01<2:06:42,  8.82s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1077/1938 [2:40:10<2:06:40,  8.83s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1078/1938 [2:40:19<2:06:30,  8.83s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1079/1938 [2:40:27<2:06:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1080/1938 [2:40:36<2:06:21,  8.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1081/1938 [2:40:45<2:06:22,  8.85s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1082/1938 [2:40:54<2:06:08,  8.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1083/1938 [2:41:03<2:06:02,  8.85s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1084/1938 [2:41:12<2:05:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1085/1938 [2:41:20<2:05:26,  8.82s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1086/1938 [2:41:29<2:05:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1087/1938 [2:41:38<2:05:15,  8.83s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1088/1938 [2:41:47<2:05:18,  8.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1089/1938 [2:41:56<2:05:09,  8.85s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 1090/1938 [2:42:05<2:05:16,  8.86s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 1091/1938 [2:42:14<2:05:02,  8.86s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 1092/1938 [2:42:22<2:04:54,  8.86s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 1093/1938 [2:42:31<2:04:36,  8.85s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 1094/1938 [2:42:40<2:04:31,  8.85s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1095/1938 [2:42:49<2:04:35,  8.87s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1096/1938 [2:42:58<2:04:23,  8.86s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1097/1938 [2:43:07<2:04:03,  8.85s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1098/1938 [2:43:16<2:03:58,  8.85s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1099/1938 [2:43:24<2:03:38,  8.84s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1100/1938 [2:43:33<2:03:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1101/1938 [2:43:42<2:03:02,  8.82s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1102/1938 [2:43:51<2:02:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1103/1938 [2:44:00<2:02:59,  8.84s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1104/1938 [2:44:09<2:02:43,  8.83s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1105/1938 [2:44:17<2:02:32,  8.83s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1106/1938 [2:44:26<2:02:28,  8.83s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1107/1938 [2:44:35<2:02:13,  8.82s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1108/1938 [2:44:44<2:02:01,  8.82s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1109/1938 [2:44:53<2:01:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1110/1938 [2:45:01<2:01:48,  8.83s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1111/1938 [2:45:10<2:01:41,  8.83s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1112/1938 [2:45:19<2:01:34,  8.83s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1113/1938 [2:45:28<2:01:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 1114/1938 [2:45:37<2:01:07,  8.82s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1115/1938 [2:45:46<2:01:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1116/1938 [2:45:54<2:00:59,  8.83s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1117/1938 [2:46:03<2:00:59,  8.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1118/1938 [2:46:12<2:00:51,  8.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1119/1938 [2:46:21<2:00:37,  8.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1120/1938 [2:46:30<2:00:29,  8.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1121/1938 [2:46:39<2:00:26,  8.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1122/1938 [2:46:48<2:00:25,  8.86s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1123/1938 [2:46:56<2:00:17,  8.86s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1124/1938 [2:47:05<1:59:53,  8.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1125/1938 [2:47:14<1:59:38,  8.83s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1126/1938 [2:47:23<1:59:28,  8.83s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1127/1938 [2:47:32<1:59:24,  8.83s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1128/1938 [2:47:40<1:59:07,  8.82s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1129/1938 [2:47:49<1:58:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1130/1938 [2:47:58<1:58:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1131/1938 [2:48:07<1:58:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1132/1938 [2:48:16<1:58:38,  8.83s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 1133/1938 [2:48:25<1:58:30,  8.83s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 1134/1938 [2:48:34<1:58:33,  8.85s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 1135/1938 [2:48:42<1:58:17,  8.84s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 1136/1938 [2:48:51<1:58:11,  8.84s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 1137/1938 [2:49:00<1:58:10,  8.85s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 1138/1938 [2:49:09<1:57:50,  8.84s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1139/1938 [2:49:18<1:57:32,  8.83s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1140/1938 [2:49:26<1:57:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1141/1938 [2:49:35<1:57:20,  8.83s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1142/1938 [2:49:44<1:57:07,  8.83s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1143/1938 [2:49:53<1:56:52,  8.82s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1144/1938 [2:50:02<1:56:42,  8.82s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1145/1938 [2:50:11<1:56:28,  8.81s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1146/1938 [2:50:19<1:56:19,  8.81s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1147/1938 [2:50:28<1:56:14,  8.82s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1148/1938 [2:50:37<1:56:00,  8.81s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1149/1938 [2:50:46<1:55:57,  8.82s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1150/1938 [2:50:55<1:55:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1151/1938 [2:51:03<1:55:33,  8.81s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1152/1938 [2:51:12<1:55:26,  8.81s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 1153/1938 [2:51:21<1:55:18,  8.81s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 1154/1938 [2:51:30<1:55:16,  8.82s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 1155/1938 [2:51:39<1:55:02,  8.82s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 1156/1938 [2:51:48<1:54:58,  8.82s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 1157/1938 [2:51:56<1:54:48,  8.82s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 1158/1938 [2:52:05<1:54:28,  8.81s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 1159/1938 [2:52:14<1:54:28,  8.82s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 1160/1938 [2:52:23<1:54:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 1161/1938 [2:52:32<1:54:20,  8.83s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 1162/1938 [2:52:41<1:54:17,  8.84s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 1163/1938 [2:52:49<1:53:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 1164/1938 [2:52:58<1:53:56,  8.83s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 1165/1938 [2:53:07<1:53:53,  8.84s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 1166/1938 [2:53:16<1:53:43,  8.84s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 1167/1938 [2:53:25<1:53:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 1168/1938 [2:53:34<1:53:29,  8.84s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 1169/1938 [2:53:42<1:53:26,  8.85s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 1170/1938 [2:53:51<1:53:01,  8.83s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 1171/1938 [2:54:00<1:52:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 1172/1938 [2:54:09<1:52:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1173/1938 [2:54:18<1:52:32,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1174/1938 [2:54:27<1:52:25,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1175/1938 [2:54:35<1:52:11,  8.82s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1176/1938 [2:54:44<1:52:11,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1177/1938 [2:54:53<1:52:02,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1178/1938 [2:55:02<1:51:59,  8.84s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1179/1938 [2:55:11<1:51:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1180/1938 [2:55:20<1:51:34,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1181/1938 [2:55:28<1:51:29,  8.84s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1182/1938 [2:55:37<1:51:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1183/1938 [2:55:46<1:51:08,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1184/1938 [2:55:55<1:50:58,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1185/1938 [2:56:04<1:50:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1186/1938 [2:56:12<1:50:36,  8.82s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 1187/1938 [2:56:21<1:50:25,  8.82s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 1188/1938 [2:56:30<1:50:20,  8.83s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 1189/1938 [2:56:39<1:50:20,  8.84s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 1190/1938 [2:56:48<1:50:10,  8.84s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 1191/1938 [2:56:57<1:50:01,  8.84s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1192/1938 [2:57:06<1:49:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1193/1938 [2:57:14<1:49:35,  8.83s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1194/1938 [2:57:23<1:49:22,  8.82s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1195/1938 [2:57:32<1:49:16,  8.82s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1196/1938 [2:57:41<1:49:00,  8.81s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1197/1938 [2:57:50<1:48:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1198/1938 [2:57:58<1:48:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1199/1938 [2:58:07<1:48:40,  8.82s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1200/1938 [2:58:16<1:48:33,  8.83s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1201/1938 [2:58:25<1:48:20,  8.82s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1202/1938 [2:58:34<1:48:03,  8.81s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1203/1938 [2:58:42<1:47:53,  8.81s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1204/1938 [2:58:51<1:47:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1205/1938 [2:59:00<1:47:58,  8.84s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1206/1938 [2:59:09<1:47:51,  8.84s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1207/1938 [2:59:18<1:47:36,  8.83s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1208/1938 [2:59:27<1:47:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1209/1938 [2:59:36<1:47:13,  8.82s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1210/1938 [2:59:44<1:47:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 1211/1938 [2:59:53<1:46:50,  8.82s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1212/1938 [3:00:02<1:46:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1213/1938 [3:00:11<1:46:45,  8.83s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1214/1938 [3:00:20<1:46:32,  8.83s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1215/1938 [3:00:28<1:46:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1216/1938 [3:00:37<1:45:55,  8.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1217/1938 [3:00:46<1:45:42,  8.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1218/1938 [3:00:55<1:45:39,  8.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1219/1938 [3:01:04<1:45:36,  8.81s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1220/1938 [3:01:13<1:45:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1221/1938 [3:01:21<1:45:33,  8.83s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1222/1938 [3:01:30<1:45:29,  8.84s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1223/1938 [3:01:39<1:45:13,  8.83s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1224/1938 [3:01:48<1:45:08,  8.84s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1225/1938 [3:01:57<1:44:45,  8.82s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1226/1938 [3:02:05<1:44:43,  8.83s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1227/1938 [3:02:14<1:44:24,  8.81s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1228/1938 [3:02:23<1:44:25,  8.82s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1229/1938 [3:02:32<1:44:12,  8.82s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 1230/1938 [3:02:41<1:44:07,  8.82s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 1231/1938 [3:02:50<1:44:01,  8.83s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 1232/1938 [3:02:58<1:43:52,  8.83s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 1233/1938 [3:03:07<1:43:39,  8.82s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 1234/1938 [3:03:16<1:43:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 1235/1938 [3:03:25<1:43:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1236/1938 [3:03:34<1:43:15,  8.83s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1237/1938 [3:03:43<1:42:58,  8.81s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1238/1938 [3:03:51<1:42:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1239/1938 [3:04:00<1:42:57,  8.84s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1240/1938 [3:04:09<1:42:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1241/1938 [3:04:18<1:42:35,  8.83s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1242/1938 [3:04:27<1:42:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1243/1938 [3:04:36<1:42:24,  8.84s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1244/1938 [3:04:44<1:42:24,  8.85s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1245/1938 [3:04:53<1:42:08,  8.84s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1246/1938 [3:05:02<1:41:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1247/1938 [3:05:11<1:41:49,  8.84s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1248/1938 [3:05:20<1:41:47,  8.85s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1249/1938 [3:05:29<1:41:32,  8.84s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 1250/1938 [3:05:38<1:41:35,  8.86s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 1251/1938 [3:05:46<1:41:18,  8.85s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 1252/1938 [3:05:55<1:41:10,  8.85s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 1253/1938 [3:06:04<1:40:57,  8.84s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 1254/1938 [3:06:13<1:40:52,  8.85s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 1255/1938 [3:06:22<1:40:46,  8.85s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 1256/1938 [3:06:31<1:40:40,  8.86s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 1257/1938 [3:06:39<1:40:27,  8.85s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 1258/1938 [3:06:48<1:40:11,  8.84s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 1259/1938 [3:06:57<1:39:54,  8.83s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 1260/1938 [3:07:06<1:39:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 1261/1938 [3:07:15<1:39:35,  8.83s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 1262/1938 [3:07:24<1:39:23,  8.82s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 1263/1938 [3:07:32<1:39:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 1264/1938 [3:07:41<1:39:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 1265/1938 [3:07:50<1:39:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 1266/1938 [3:07:59<1:38:48,  8.82s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 1267/1938 [3:08:08<1:38:32,  8.81s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 1268/1938 [3:08:16<1:38:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 1269/1938 [3:08:25<1:38:20,  8.82s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1270/1938 [3:08:34<1:38:13,  8.82s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1271/1938 [3:08:43<1:38:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1272/1938 [3:08:52<1:37:59,  8.83s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1273/1938 [3:09:01<1:38:06,  8.85s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1274/1938 [3:09:10<1:38:01,  8.86s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1275/1938 [3:09:18<1:37:46,  8.85s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1276/1938 [3:09:27<1:37:30,  8.84s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1277/1938 [3:09:36<1:37:24,  8.84s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1278/1938 [3:09:45<1:37:11,  8.84s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1279/1938 [3:09:54<1:37:04,  8.84s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1280/1938 [3:10:03<1:36:56,  8.84s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1281/1938 [3:10:11<1:36:42,  8.83s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1282/1938 [3:10:20<1:36:34,  8.83s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 1283/1938 [3:10:29<1:36:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 1284/1938 [3:10:38<1:36:08,  8.82s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 1285/1938 [3:10:47<1:35:59,  8.82s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 1286/1938 [3:10:55<1:35:52,  8.82s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 1287/1938 [3:11:04<1:35:52,  8.84s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 1288/1938 [3:11:13<1:35:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1289/1938 [3:11:22<1:35:27,  8.83s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1290/1938 [3:11:31<1:35:11,  8.81s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1291/1938 [3:11:40<1:35:08,  8.82s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1292/1938 [3:11:47<1:31:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 2175\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mNum examples = 2175\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/34 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 2/34 [00:01<00:24,  1.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 3/34 [00:03<00:33,  1.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 4/34 [00:04<00:37,  1.24s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 5/34 [00:06<00:38,  1.34s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 6/34 [00:07<00:39,  1.40s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 7/34 [00:09<00:38,  1.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▎       | 8/34 [00:10<00:38,  1.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▋       | 9/34 [00:12<00:37,  1.48s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 10/34 [00:13<00:36,  1.50s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 11/34 [00:15<00:34,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 12/34 [00:16<00:33,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 13/34 [00:18<00:31,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 14/34 [00:19<00:30,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 15/34 [00:21<00:28,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 16/34 [00:22<00:27,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 17/34 [00:24<00:25,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 18/34 [00:25<00:24,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 19/34 [00:27<00:22,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 20/34 [00:28<00:21,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 21/34 [00:30<00:19,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 22/34 [00:32<00:18,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 23/34 [00:33<00:16,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 24/34 [00:35<00:15,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 25/34 [00:36<00:13,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 26/34 [00:38<00:12,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 27/34 [00:39<00:10,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 28/34 [00:41<00:09,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 29/34 [00:42<00:07,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 30/34 [00:44<00:06,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 31/34 [00:45<00:04,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 32/34 [00:47<00:03,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 33/34 [00:48<00:01,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 34/34 [00:50<00:00,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.5433379411697388, 'eval_runtime': 51.8658, 'eval_samples_per_second': 41.935, 'eval_steps_per_second': 0.656, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1292/1938 [3:12:39<1:31:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 34/34 [00:50<00:00,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-1292\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-1292\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-1292/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-1292/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-1292/generation_config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-1292/generation_config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-1292/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-1292/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:44:51,872] [INFO] [engine.py:3500:save_16bit_model] Saving model weights to /tmp/checkpoint-1292/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:44:51,873] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /tmp/checkpoint-1292/pytorch_model.bin...\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:45:10,357] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /tmp/checkpoint-1292/pytorch_model.bin.\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:45:10,385] [INFO] [logging.py:68:log_dist] [Rank 0] [Torch] Checkpoint global_step1292 is begin to save!\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:45:10,394] [INFO] [logging.py:68:log_dist] [Rank 0] Saving model checkpoint: /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:45:10,394] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:45:10,419] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:45:10,428] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:45:20,705] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:45:20,705] [INFO] [engine.py:3397:_save_zero_checkpoint] zero checkpoint saved /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2023-05-29 09:45:21,046] [INFO] [torch_checkpoint_engine.py:27:commit] [Torch] Checkpoint global_step1292 is ready now!\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1293/1938 [3:13:27<6:25:03, 35.82s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1294/1938 [3:13:36<4:57:46, 27.74s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1295/1938 [3:13:45<3:56:35, 22.08s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1296/1938 [3:13:53<3:13:35, 18.09s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1297/1938 [3:14:02<2:43:42, 15.32s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1298/1938 [3:14:11<2:22:41, 13.38s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1299/1938 [3:14:20<2:07:58, 12.02s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1300/1938 [3:14:29<1:57:40, 11.07s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1301/1938 [3:14:38<1:50:16, 10.39s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1302/1938 [3:14:46<1:45:05,  9.91s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1303/1938 [3:14:55<1:41:34,  9.60s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1304/1938 [3:15:04<1:39:13,  9.39s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1305/1938 [3:15:13<1:37:40,  9.26s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1306/1938 [3:15:22<1:36:17,  9.14s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1307/1938 [3:15:31<1:35:13,  9.05s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 1308/1938 [3:15:40<1:34:32,  9.00s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1309/1938 [3:15:49<1:33:58,  8.96s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1310/1938 [3:15:57<1:33:22,  8.92s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1311/1938 [3:16:06<1:32:58,  8.90s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1312/1938 [3:16:15<1:32:35,  8.87s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1313/1938 [3:16:24<1:32:23,  8.87s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1314/1938 [3:16:33<1:32:05,  8.86s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1315/1938 [3:16:42<1:31:55,  8.85s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1316/1938 [3:16:50<1:31:38,  8.84s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1317/1938 [3:16:59<1:31:39,  8.86s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1318/1938 [3:17:08<1:31:27,  8.85s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1319/1938 [3:17:17<1:31:12,  8.84s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1320/1938 [3:17:26<1:31:06,  8.84s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1321/1938 [3:17:35<1:31:02,  8.85s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1322/1938 [3:17:44<1:30:50,  8.85s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1323/1938 [3:17:52<1:30:43,  8.85s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1324/1938 [3:18:01<1:30:41,  8.86s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1325/1938 [3:18:10<1:30:27,  8.85s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1326/1938 [3:18:19<1:30:10,  8.84s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 1327/1938 [3:18:28<1:29:58,  8.84s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 1328/1938 [3:18:37<1:29:51,  8.84s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 1329/1938 [3:18:45<1:29:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 1330/1938 [3:18:54<1:29:30,  8.83s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 1331/1938 [3:19:03<1:29:19,  8.83s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 1332/1938 [3:19:12<1:29:01,  8.81s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1333/1938 [3:19:21<1:29:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1334/1938 [3:19:30<1:28:56,  8.84s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1335/1938 [3:19:38<1:28:48,  8.84s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1336/1938 [3:19:47<1:28:41,  8.84s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1337/1938 [3:19:56<1:28:35,  8.85s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1338/1938 [3:20:05<1:28:25,  8.84s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1339/1938 [3:20:14<1:28:17,  8.84s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1340/1938 [3:20:23<1:28:05,  8.84s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1341/1938 [3:20:31<1:27:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1342/1938 [3:20:40<1:27:41,  8.83s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1343/1938 [3:20:49<1:27:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1344/1938 [3:20:58<1:27:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1345/1938 [3:21:07<1:27:20,  8.84s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 1346/1938 [3:21:16<1:27:09,  8.83s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 1347/1938 [3:21:24<1:26:55,  8.83s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 1348/1938 [3:21:33<1:26:40,  8.81s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 1349/1938 [3:21:42<1:26:30,  8.81s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 1350/1938 [3:21:51<1:26:23,  8.82s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 1351/1938 [3:22:00<1:26:13,  8.81s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 1352/1938 [3:22:08<1:26:00,  8.81s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 1353/1938 [3:22:17<1:25:46,  8.80s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 1354/1938 [3:22:26<1:25:36,  8.80s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 1355/1938 [3:22:35<1:25:29,  8.80s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 1356/1938 [3:22:44<1:25:20,  8.80s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 1357/1938 [3:22:52<1:25:08,  8.79s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 1358/1938 [3:23:01<1:25:03,  8.80s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 1359/1938 [3:23:10<1:24:57,  8.80s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 1360/1938 [3:23:19<1:24:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 1361/1938 [3:23:28<1:24:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 1362/1938 [3:23:37<1:24:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 1363/1938 [3:23:45<1:24:44,  8.84s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 1364/1938 [3:23:54<1:24:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 1365/1938 [3:24:03<1:24:20,  8.83s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 1366/1938 [3:24:12<1:24:05,  8.82s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1367/1938 [3:24:21<1:24:05,  8.84s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1368/1938 [3:24:30<1:23:56,  8.84s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1369/1938 [3:24:38<1:23:52,  8.84s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1370/1938 [3:24:47<1:23:48,  8.85s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1371/1938 [3:24:56<1:23:37,  8.85s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1372/1938 [3:25:05<1:23:23,  8.84s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1373/1938 [3:25:14<1:23:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1374/1938 [3:25:23<1:22:47,  8.81s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1375/1938 [3:25:31<1:22:49,  8.83s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1376/1938 [3:25:40<1:22:43,  8.83s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1377/1938 [3:25:49<1:22:36,  8.83s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1378/1938 [3:25:58<1:22:20,  8.82s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1379/1938 [3:26:07<1:22:09,  8.82s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 1380/1938 [3:26:15<1:21:48,  8.80s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 1381/1938 [3:26:24<1:21:31,  8.78s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 1382/1938 [3:26:33<1:21:22,  8.78s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 1383/1938 [3:26:42<1:21:16,  8.79s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 1384/1938 [3:26:51<1:21:05,  8.78s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 1385/1938 [3:26:59<1:21:07,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1386/1938 [3:27:08<1:20:54,  8.79s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1387/1938 [3:27:17<1:20:44,  8.79s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1388/1938 [3:27:26<1:20:38,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1389/1938 [3:27:35<1:20:31,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1390/1938 [3:27:43<1:20:24,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1391/1938 [3:27:52<1:20:14,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1392/1938 [3:28:01<1:20:12,  8.81s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1393/1938 [3:28:10<1:20:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1394/1938 [3:28:19<1:19:53,  8.81s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1395/1938 [3:28:27<1:19:38,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1396/1938 [3:28:36<1:19:30,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1397/1938 [3:28:45<1:19:19,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1398/1938 [3:28:54<1:19:12,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1399/1938 [3:29:03<1:19:01,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1400/1938 [3:29:11<1:18:57,  8.81s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1401/1938 [3:29:20<1:18:48,  8.81s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1402/1938 [3:29:29<1:18:37,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1403/1938 [3:29:38<1:18:29,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1404/1938 [3:29:47<1:18:17,  8.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 1405/1938 [3:29:55<1:18:08,  8.80s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1406/1938 [3:30:04<1:18:02,  8.80s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1407/1938 [3:30:13<1:17:50,  8.80s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1408/1938 [3:30:22<1:17:40,  8.79s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1409/1938 [3:30:31<1:17:42,  8.81s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1410/1938 [3:30:40<1:17:45,  8.84s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1411/1938 [3:30:48<1:17:28,  8.82s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1412/1938 [3:30:57<1:17:16,  8.81s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1413/1938 [3:31:06<1:17:04,  8.81s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1414/1938 [3:31:15<1:16:50,  8.80s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1415/1938 [3:31:24<1:16:58,  8.83s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1416/1938 [3:31:32<1:16:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1417/1938 [3:31:41<1:16:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1418/1938 [3:31:50<1:16:30,  8.83s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1419/1938 [3:31:59<1:16:56,  8.89s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1420/1938 [3:32:08<1:16:43,  8.89s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1421/1938 [3:32:17<1:16:22,  8.86s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1422/1938 [3:32:26<1:16:10,  8.86s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1423/1938 [3:32:35<1:16:05,  8.86s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 1424/1938 [3:32:43<1:15:54,  8.86s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 1425/1938 [3:32:52<1:15:40,  8.85s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 1426/1938 [3:33:01<1:15:25,  8.84s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 1427/1938 [3:33:10<1:15:09,  8.83s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 1428/1938 [3:33:19<1:14:59,  8.82s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 1429/1938 [3:33:27<1:14:52,  8.83s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1430/1938 [3:33:36<1:14:40,  8.82s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1431/1938 [3:33:45<1:14:25,  8.81s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1432/1938 [3:33:54<1:14:18,  8.81s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1433/1938 [3:34:03<1:14:06,  8.81s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1434/1938 [3:34:11<1:14:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1435/1938 [3:34:20<1:13:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1436/1938 [3:34:29<1:13:54,  8.83s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1437/1938 [3:34:38<1:13:56,  8.86s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1438/1938 [3:34:47<1:13:42,  8.85s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1439/1938 [3:34:56<1:13:31,  8.84s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1440/1938 [3:35:05<1:13:19,  8.83s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1441/1938 [3:35:13<1:13:07,  8.83s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1442/1938 [3:35:22<1:12:59,  8.83s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 1443/1938 [3:35:31<1:12:53,  8.84s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 1444/1938 [3:35:40<1:12:48,  8.84s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 1445/1938 [3:35:49<1:12:37,  8.84s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 1446/1938 [3:35:58<1:12:25,  8.83s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 1447/1938 [3:36:06<1:12:18,  8.84s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 1448/1938 [3:36:15<1:12:07,  8.83s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 1449/1938 [3:36:24<1:11:53,  8.82s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 1450/1938 [3:36:33<1:11:45,  8.82s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 1451/1938 [3:36:42<1:11:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 1452/1938 [3:36:51<1:11:34,  8.84s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 1453/1938 [3:36:59<1:11:25,  8.84s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 1454/1938 [3:37:08<1:11:09,  8.82s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 1455/1938 [3:37:17<1:10:58,  8.82s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 1456/1938 [3:37:26<1:10:46,  8.81s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 1457/1938 [3:37:35<1:10:38,  8.81s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 1458/1938 [3:37:43<1:10:31,  8.82s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 1459/1938 [3:37:52<1:10:25,  8.82s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 1460/1938 [3:38:01<1:10:20,  8.83s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 1461/1938 [3:38:10<1:10:05,  8.82s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 1462/1938 [3:38:19<1:09:57,  8.82s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 1463/1938 [3:38:28<1:09:46,  8.81s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1464/1938 [3:38:36<1:09:38,  8.82s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1465/1938 [3:38:45<1:09:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1466/1938 [3:38:54<1:09:20,  8.82s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1467/1938 [3:39:03<1:09:04,  8.80s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1468/1938 [3:39:12<1:08:59,  8.81s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1469/1938 [3:39:20<1:08:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1470/1938 [3:39:29<1:08:52,  8.83s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1471/1938 [3:39:38<1:08:52,  8.85s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1472/1938 [3:39:47<1:08:42,  8.85s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1473/1938 [3:39:56<1:08:36,  8.85s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1474/1938 [3:40:05<1:08:30,  8.86s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1475/1938 [3:40:14<1:08:18,  8.85s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1476/1938 [3:40:22<1:08:02,  8.84s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 1477/1938 [3:40:31<1:07:57,  8.84s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 1478/1938 [3:40:40<1:07:46,  8.84s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 1479/1938 [3:40:49<1:07:40,  8.85s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 1480/1938 [3:40:58<1:07:31,  8.85s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 1481/1938 [3:41:07<1:07:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 1482/1938 [3:41:15<1:07:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1483/1938 [3:41:24<1:06:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1484/1938 [3:41:33<1:06:48,  8.83s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1485/1938 [3:41:42<1:06:41,  8.83s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1486/1938 [3:41:51<1:06:31,  8.83s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1487/1938 [3:42:00<1:06:26,  8.84s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1488/1938 [3:42:08<1:06:20,  8.85s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1489/1938 [3:42:17<1:06:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1490/1938 [3:42:26<1:05:58,  8.84s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1491/1938 [3:42:35<1:05:47,  8.83s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1492/1938 [3:42:44<1:05:34,  8.82s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1493/1938 [3:42:52<1:05:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1494/1938 [3:43:01<1:05:14,  8.82s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1495/1938 [3:43:10<1:05:09,  8.82s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1496/1938 [3:43:19<1:05:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1497/1938 [3:43:28<1:05:01,  8.85s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1498/1938 [3:43:37<1:04:50,  8.84s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1499/1938 [3:43:46<1:04:39,  8.84s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1500/1938 [3:43:54<1:04:30,  8.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1988, 'learning_rate': 0.0001, 'epoch': 2.32}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1500/1938 [3:43:54<1:04:30,  8.84s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 1501/1938 [3:44:03<1:04:11,  8.81s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1502/1938 [3:44:12<1:04:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1503/1938 [3:44:21<1:03:58,  8.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1504/1938 [3:44:30<1:03:49,  8.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1505/1938 [3:44:38<1:03:36,  8.81s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1506/1938 [3:44:47<1:03:41,  8.85s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1507/1938 [3:44:56<1:03:22,  8.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1508/1938 [3:45:05<1:03:09,  8.81s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1509/1938 [3:45:14<1:03:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1510/1938 [3:45:23<1:02:53,  8.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1511/1938 [3:45:31<1:02:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1512/1938 [3:45:40<1:02:40,  8.83s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1513/1938 [3:45:49<1:02:33,  8.83s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1514/1938 [3:45:58<1:02:28,  8.84s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1515/1938 [3:46:07<1:02:20,  8.84s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1516/1938 [3:46:16<1:02:14,  8.85s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1517/1938 [3:46:24<1:02:02,  8.84s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1518/1938 [3:46:33<1:01:48,  8.83s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1519/1938 [3:46:42<1:01:35,  8.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1520/1938 [3:46:51<1:01:27,  8.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 1521/1938 [3:47:00<1:01:20,  8.83s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 1522/1938 [3:47:09<1:01:17,  8.84s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 1523/1938 [3:47:17<1:00:59,  8.82s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 1524/1938 [3:47:26<1:00:53,  8.82s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 1525/1938 [3:47:35<1:00:44,  8.82s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 1526/1938 [3:47:44<1:00:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1527/1938 [3:47:53<1:00:32,  8.84s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1528/1938 [3:48:02<1:00:26,  8.85s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1529/1938 [3:48:10<1:00:19,  8.85s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1530/1938 [3:48:19<1:00:09,  8.85s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1531/1938 [3:48:28<59:54,  8.83s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1532/1938 [3:48:37<59:47,  8.84s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1533/1938 [3:48:46<59:42,  8.84s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1534/1938 [3:48:55<59:31,  8.84s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1535/1938 [3:49:03<59:21,  8.84s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1536/1938 [3:49:12<59:10,  8.83s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1537/1938 [3:49:21<59:04,  8.84s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1538/1938 [3:49:30<58:54,  8.84s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1539/1938 [3:49:39<58:41,  8.83s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 1540/1938 [3:49:48<58:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 1541/1938 [3:49:56<58:16,  8.81s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 1542/1938 [3:50:05<58:10,  8.81s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 1543/1938 [3:50:14<58:01,  8.81s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 1544/1938 [3:50:23<57:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 1545/1938 [3:50:32<57:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 1546/1938 [3:50:40<57:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 1547/1938 [3:50:49<57:26,  8.82s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 1548/1938 [3:50:58<57:18,  8.82s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 1549/1938 [3:51:07<57:11,  8.82s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 1550/1938 [3:51:16<57:02,  8.82s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1551/1938 [3:51:25<56:56,  8.83s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1552/1938 [3:51:33<56:48,  8.83s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1553/1938 [3:51:42<56:41,  8.84s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1554/1938 [3:51:51<56:32,  8.84s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1555/1938 [3:52:00<56:19,  8.82s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1556/1938 [3:52:09<56:10,  8.82s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1557/1938 [3:52:18<55:59,  8.82s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1558/1938 [3:52:26<55:48,  8.81s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1559/1938 [3:52:35<55:41,  8.82s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1560/1938 [3:52:44<55:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1561/1938 [3:52:53<55:32,  8.84s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1562/1938 [3:53:02<55:19,  8.83s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1563/1938 [3:53:10<55:05,  8.81s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1564/1938 [3:53:19<55:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1565/1938 [3:53:28<54:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1566/1938 [3:53:37<54:51,  8.85s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1567/1938 [3:53:46<54:39,  8.84s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1568/1938 [3:53:55<54:31,  8.84s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1569/1938 [3:54:04<54:24,  8.85s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1570/1938 [3:54:12<54:14,  8.84s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1571/1938 [3:54:21<54:05,  8.84s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1572/1938 [3:54:30<53:55,  8.84s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1573/1938 [3:54:39<53:49,  8.85s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1574/1938 [3:54:48<53:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1575/1938 [3:54:57<53:28,  8.84s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1576/1938 [3:55:05<53:15,  8.83s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1577/1938 [3:55:14<53:05,  8.83s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1578/1938 [3:55:23<52:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1579/1938 [3:55:32<52:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1580/1938 [3:55:41<52:40,  8.83s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1581/1938 [3:55:50<52:33,  8.83s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1582/1938 [3:55:58<52:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1583/1938 [3:56:07<52:11,  8.82s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1584/1938 [3:56:16<51:59,  8.81s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1585/1938 [3:56:25<51:50,  8.81s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1586/1938 [3:56:34<51:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1587/1938 [3:56:42<51:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1588/1938 [3:56:51<51:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1589/1938 [3:57:00<51:28,  8.85s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1590/1938 [3:57:09<51:20,  8.85s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1591/1938 [3:57:18<51:07,  8.84s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1592/1938 [3:57:27<50:54,  8.83s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1593/1938 [3:57:35<50:43,  8.82s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1594/1938 [3:57:44<50:31,  8.81s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1595/1938 [3:57:53<50:19,  8.80s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1596/1938 [3:58:02<50:15,  8.82s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1597/1938 [3:58:11<50:06,  8.82s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1598/1938 [3:58:20<49:56,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1599/1938 [3:58:28<49:49,  8.82s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1600/1938 [3:58:37<49:36,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1601/1938 [3:58:46<49:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1602/1938 [3:58:55<49:22,  8.82s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1603/1938 [3:59:04<49:13,  8.82s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1604/1938 [3:59:12<49:05,  8.82s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1605/1938 [3:59:21<48:55,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1606/1938 [3:59:30<48:43,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1607/1938 [3:59:39<48:37,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1608/1938 [3:59:48<48:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1609/1938 [3:59:57<48:20,  8.82s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1610/1938 [4:00:05<48:14,  8.82s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1611/1938 [4:00:14<48:01,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1612/1938 [4:00:23<47:51,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1613/1938 [4:00:32<47:44,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1614/1938 [4:00:41<47:35,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1615/1938 [4:00:49<47:26,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1616/1938 [4:00:58<47:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1617/1938 [4:01:07<47:08,  8.81s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1618/1938 [4:01:16<46:56,  8.80s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1619/1938 [4:01:25<46:45,  8.80s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1620/1938 [4:01:33<46:36,  8.80s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1621/1938 [4:01:42<46:31,  8.81s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1622/1938 [4:01:51<46:25,  8.81s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1623/1938 [4:02:00<46:17,  8.82s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1624/1938 [4:02:09<46:08,  8.82s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1625/1938 [4:02:17<45:59,  8.82s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1626/1938 [4:02:26<45:52,  8.82s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1627/1938 [4:02:35<45:41,  8.82s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1628/1938 [4:02:44<45:26,  8.79s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1629/1938 [4:02:53<45:21,  8.81s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1630/1938 [4:03:02<45:16,  8.82s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1631/1938 [4:03:10<45:07,  8.82s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1632/1938 [4:03:19<44:58,  8.82s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1633/1938 [4:03:28<44:52,  8.83s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1634/1938 [4:03:37<44:46,  8.84s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1635/1938 [4:03:46<44:37,  8.84s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1636/1938 [4:03:55<44:26,  8.83s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1637/1938 [4:04:03<44:14,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1638/1938 [4:04:12<44:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1639/1938 [4:04:21<43:57,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1640/1938 [4:04:30<43:47,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1641/1938 [4:04:39<43:39,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1642/1938 [4:04:47<43:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1643/1938 [4:04:56<43:24,  8.83s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1644/1938 [4:05:05<43:17,  8.84s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1645/1938 [4:05:14<43:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1646/1938 [4:05:23<42:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1647/1938 [4:05:32<42:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1648/1938 [4:05:40<42:35,  8.81s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1649/1938 [4:05:49<42:25,  8.81s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1650/1938 [4:05:58<42:21,  8.83s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1651/1938 [4:06:07<42:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1652/1938 [4:06:16<42:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1653/1938 [4:06:24<41:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1654/1938 [4:06:33<41:44,  8.82s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1655/1938 [4:06:42<41:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1656/1938 [4:06:51<41:31,  8.83s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1657/1938 [4:07:00<41:26,  8.85s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1658/1938 [4:07:09<41:14,  8.84s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1659/1938 [4:07:18<41:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1660/1938 [4:07:26<40:55,  8.83s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1661/1938 [4:07:35<40:45,  8.83s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1662/1938 [4:07:44<40:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1663/1938 [4:07:53<40:25,  8.82s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1664/1938 [4:08:02<40:17,  8.82s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1665/1938 [4:08:10<40:05,  8.81s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1666/1938 [4:08:19<39:55,  8.81s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1667/1938 [4:08:28<39:49,  8.82s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1668/1938 [4:08:37<39:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1669/1938 [4:08:46<39:38,  8.84s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1670/1938 [4:08:55<39:28,  8.84s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1671/1938 [4:09:03<39:20,  8.84s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1672/1938 [4:09:12<39:07,  8.82s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1673/1938 [4:09:21<38:58,  8.82s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1674/1938 [4:09:30<38:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1675/1938 [4:09:39<38:44,  8.84s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1676/1938 [4:09:48<38:33,  8.83s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1677/1938 [4:09:56<38:20,  8.82s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1678/1938 [4:10:05<38:11,  8.81s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1679/1938 [4:10:14<38:00,  8.80s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1680/1938 [4:10:23<37:51,  8.80s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1681/1938 [4:10:32<37:43,  8.81s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1682/1938 [4:10:40<37:34,  8.81s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1683/1938 [4:10:49<37:27,  8.81s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1684/1938 [4:10:58<37:16,  8.80s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1685/1938 [4:11:07<37:09,  8.81s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1686/1938 [4:11:16<36:58,  8.80s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1687/1938 [4:11:24<36:52,  8.82s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1688/1938 [4:11:33<36:42,  8.81s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1689/1938 [4:11:42<36:33,  8.81s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1690/1938 [4:11:51<36:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1691/1938 [4:12:00<36:19,  8.82s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1692/1938 [4:12:09<36:10,  8.82s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1693/1938 [4:12:17<36:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1694/1938 [4:12:26<35:57,  8.84s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1695/1938 [4:12:35<35:50,  8.85s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1696/1938 [4:12:44<35:41,  8.85s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1697/1938 [4:12:53<35:31,  8.84s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1698/1938 [4:13:02<35:25,  8.86s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1699/1938 [4:13:10<35:11,  8.83s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1700/1938 [4:13:19<35:02,  8.83s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1701/1938 [4:13:28<34:51,  8.82s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1702/1938 [4:13:37<34:42,  8.82s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1703/1938 [4:13:46<34:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1704/1938 [4:13:55<34:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1705/1938 [4:14:03<34:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1706/1938 [4:14:12<34:06,  8.82s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1707/1938 [4:14:21<33:58,  8.83s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1708/1938 [4:14:30<33:51,  8.83s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1709/1938 [4:14:39<33:45,  8.84s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1710/1938 [4:14:48<33:36,  8.84s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1711/1938 [4:14:56<33:23,  8.83s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1712/1938 [4:15:05<33:11,  8.81s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1713/1938 [4:15:14<33:02,  8.81s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1714/1938 [4:15:23<32:58,  8.83s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1715/1938 [4:15:32<32:51,  8.84s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 1716/1938 [4:15:41<32:42,  8.84s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 1717/1938 [4:15:49<32:35,  8.85s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 1718/1938 [4:15:58<32:27,  8.85s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 1719/1938 [4:16:07<32:19,  8.86s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1720/1938 [4:16:16<32:07,  8.84s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1721/1938 [4:16:25<31:58,  8.84s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1722/1938 [4:16:34<31:47,  8.83s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1723/1938 [4:16:42<31:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1724/1938 [4:16:51<31:29,  8.83s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1725/1938 [4:17:00<31:24,  8.85s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1726/1938 [4:17:09<31:14,  8.84s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1727/1938 [4:17:18<31:06,  8.85s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1728/1938 [4:17:27<30:58,  8.85s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1729/1938 [4:17:36<30:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1730/1938 [4:17:44<30:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1731/1938 [4:17:53<30:26,  8.82s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1732/1938 [4:18:02<30:17,  8.82s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1733/1938 [4:18:11<30:09,  8.83s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 1734/1938 [4:18:20<30:05,  8.85s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1735/1938 [4:18:29<29:55,  8.85s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1736/1938 [4:18:37<29:49,  8.86s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1737/1938 [4:18:46<29:41,  8.86s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1738/1938 [4:18:55<29:30,  8.85s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1739/1938 [4:19:04<29:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1740/1938 [4:19:13<29:07,  8.83s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1741/1938 [4:19:22<29:01,  8.84s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1742/1938 [4:19:30<28:53,  8.84s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1743/1938 [4:19:39<28:43,  8.84s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 1744/1938 [4:19:48<28:32,  8.83s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1745/1938 [4:19:57<28:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1746/1938 [4:20:06<28:12,  8.81s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1747/1938 [4:20:15<28:05,  8.83s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1748/1938 [4:20:23<27:55,  8.82s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1749/1938 [4:20:32<27:47,  8.82s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1750/1938 [4:20:41<27:35,  8.81s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1751/1938 [4:20:50<27:25,  8.80s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1752/1938 [4:20:59<27:21,  8.83s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 1753/1938 [4:21:07<27:14,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1754/1938 [4:21:16<27:05,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1755/1938 [4:21:25<26:56,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1756/1938 [4:21:34<26:48,  8.84s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1757/1938 [4:21:43<26:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1758/1938 [4:21:52<26:28,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1759/1938 [4:22:00<26:20,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1760/1938 [4:22:09<26:11,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1761/1938 [4:22:18<26:02,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1762/1938 [4:22:27<25:53,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1763/1938 [4:22:36<25:43,  8.82s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1764/1938 [4:22:45<25:35,  8.82s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1765/1938 [4:22:53<25:26,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1766/1938 [4:23:02<25:19,  8.84s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1767/1938 [4:23:11<25:08,  8.82s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 1768/1938 [4:23:20<25:01,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 1769/1938 [4:23:29<24:52,  8.83s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 1770/1938 [4:23:38<24:45,  8.84s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 1771/1938 [4:23:46<24:35,  8.84s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 1772/1938 [4:23:55<24:26,  8.84s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 1773/1938 [4:24:04<24:19,  8.85s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1774/1938 [4:24:13<24:10,  8.84s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1775/1938 [4:24:22<23:59,  8.83s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1776/1938 [4:24:31<23:49,  8.83s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1777/1938 [4:24:39<23:39,  8.82s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1778/1938 [4:24:48<23:30,  8.82s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1779/1938 [4:24:57<23:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1780/1938 [4:25:06<23:13,  8.82s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1781/1938 [4:25:15<23:05,  8.83s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1782/1938 [4:25:24<22:57,  8.83s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1783/1938 [4:25:32<22:48,  8.83s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1784/1938 [4:25:41<22:38,  8.82s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1785/1938 [4:25:50<22:30,  8.82s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1786/1938 [4:25:59<22:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1787/1938 [4:26:08<22:14,  8.84s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1788/1938 [4:26:17<22:07,  8.85s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1789/1938 [4:26:25<21:55,  8.83s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1790/1938 [4:26:34<21:46,  8.83s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1791/1938 [4:26:43<21:36,  8.82s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 1792/1938 [4:26:52<21:27,  8.82s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1793/1938 [4:27:01<21:17,  8.81s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1794/1938 [4:27:09<21:07,  8.80s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1795/1938 [4:27:18<20:57,  8.80s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1796/1938 [4:27:27<20:47,  8.79s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1797/1938 [4:27:36<20:42,  8.81s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1798/1938 [4:27:45<20:34,  8.82s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1799/1938 [4:27:53<20:25,  8.82s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1800/1938 [4:28:02<20:17,  8.82s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1801/1938 [4:28:11<20:07,  8.81s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1802/1938 [4:28:20<19:59,  8.82s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1803/1938 [4:28:29<19:50,  8.82s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1804/1938 [4:28:38<19:42,  8.82s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1805/1938 [4:28:46<19:31,  8.81s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1806/1938 [4:28:55<19:23,  8.82s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1807/1938 [4:29:04<19:13,  8.81s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1808/1938 [4:29:13<19:05,  8.81s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1809/1938 [4:29:22<18:57,  8.82s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1810/1938 [4:29:30<18:50,  8.83s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1811/1938 [4:29:39<18:42,  8.84s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 1812/1938 [4:29:48<18:32,  8.83s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 1813/1938 [4:29:57<18:24,  8.84s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 1814/1938 [4:30:06<18:15,  8.83s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 1815/1938 [4:30:15<18:04,  8.82s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 1816/1938 [4:30:23<17:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1817/1938 [4:30:32<17:46,  8.82s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1818/1938 [4:30:41<17:38,  8.82s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1819/1938 [4:30:50<17:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1820/1938 [4:30:59<17:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1821/1938 [4:31:08<17:13,  8.83s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1822/1938 [4:31:16<17:03,  8.82s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1823/1938 [4:31:25<16:55,  8.83s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1824/1938 [4:31:34<16:45,  8.82s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1825/1938 [4:31:43<16:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1826/1938 [4:31:52<16:28,  8.83s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1827/1938 [4:32:01<16:21,  8.84s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1828/1938 [4:32:09<16:12,  8.84s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1829/1938 [4:32:18<16:02,  8.83s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1830/1938 [4:32:27<15:52,  8.82s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 1831/1938 [4:32:36<15:44,  8.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1832/1938 [4:32:45<15:36,  8.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1833/1938 [4:32:53<15:26,  8.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1834/1938 [4:33:02<15:16,  8.81s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1835/1938 [4:33:11<15:07,  8.81s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1836/1938 [4:33:20<14:59,  8.82s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1837/1938 [4:33:29<14:50,  8.82s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1838/1938 [4:33:38<14:42,  8.82s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1839/1938 [4:33:46<14:32,  8.82s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1840/1938 [4:33:55<14:25,  8.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 1841/1938 [4:34:04<14:16,  8.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1842/1938 [4:34:13<14:06,  8.81s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1843/1938 [4:34:22<13:56,  8.81s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1844/1938 [4:34:30<13:48,  8.82s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1845/1938 [4:34:39<13:40,  8.82s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1846/1938 [4:34:48<13:31,  8.82s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1847/1938 [4:34:57<13:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1848/1938 [4:35:06<13:13,  8.82s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1849/1938 [4:35:15<13:05,  8.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 1850/1938 [4:35:23<12:56,  8.83s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1851/1938 [4:35:32<12:47,  8.83s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1852/1938 [4:35:41<12:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1853/1938 [4:35:50<12:30,  8.83s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1854/1938 [4:35:59<12:22,  8.83s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1855/1938 [4:36:08<12:13,  8.84s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1856/1938 [4:36:16<12:04,  8.83s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1857/1938 [4:36:25<11:56,  8.84s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1858/1938 [4:36:34<11:47,  8.84s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1859/1938 [4:36:43<11:38,  8.84s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1860/1938 [4:36:52<11:30,  8.85s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1861/1938 [4:37:01<11:21,  8.85s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1862/1938 [4:37:09<11:10,  8.83s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1863/1938 [4:37:18<11:01,  8.82s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1864/1938 [4:37:27<10:52,  8.82s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 1865/1938 [4:37:36<10:43,  8.82s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 1866/1938 [4:37:45<10:35,  8.83s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 1867/1938 [4:37:54<10:26,  8.82s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 1868/1938 [4:38:02<10:17,  8.82s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 1869/1938 [4:38:11<10:08,  8.83s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 1870/1938 [4:38:20<09:59,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1871/1938 [4:38:29<09:50,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1872/1938 [4:38:38<09:41,  8.80s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1873/1938 [4:38:46<09:32,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1874/1938 [4:38:55<09:23,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1875/1938 [4:39:04<09:14,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1876/1938 [4:39:13<09:06,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1877/1938 [4:39:22<08:57,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1878/1938 [4:39:30<08:48,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1879/1938 [4:39:39<08:39,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1880/1938 [4:39:48<08:30,  8.80s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1881/1938 [4:39:57<08:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1882/1938 [4:40:06<08:13,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1883/1938 [4:40:14<08:04,  8.81s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1884/1938 [4:40:23<07:56,  8.82s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1885/1938 [4:40:32<07:48,  8.84s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1886/1938 [4:40:41<07:39,  8.84s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1887/1938 [4:40:50<07:30,  8.84s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1888/1938 [4:40:59<07:21,  8.84s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 1889/1938 [4:41:08<07:12,  8.83s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1890/1938 [4:41:16<07:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1891/1938 [4:41:25<06:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1892/1938 [4:41:34<06:45,  8.82s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1893/1938 [4:41:43<06:37,  8.83s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1894/1938 [4:41:52<06:28,  8.83s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1895/1938 [4:42:00<06:19,  8.83s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1896/1938 [4:42:09<06:10,  8.82s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1897/1938 [4:42:18<06:02,  8.83s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1898/1938 [4:42:27<05:53,  8.84s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1899/1938 [4:42:36<05:44,  8.84s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1900/1938 [4:42:45<05:35,  8.84s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1901/1938 [4:42:53<05:26,  8.83s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1902/1938 [4:43:02<05:17,  8.82s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1903/1938 [4:43:11<05:08,  8.81s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1904/1938 [4:43:20<04:59,  8.81s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1905/1938 [4:43:29<04:50,  8.81s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1906/1938 [4:43:38<04:42,  8.82s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1907/1938 [4:43:46<04:33,  8.82s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 1908/1938 [4:43:55<04:24,  8.82s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 1909/1938 [4:44:04<04:15,  8.82s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 1910/1938 [4:44:13<04:07,  8.82s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 1911/1938 [4:44:22<03:57,  8.81s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 1912/1938 [4:44:30<03:48,  8.80s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 1913/1938 [4:44:39<03:40,  8.81s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1914/1938 [4:44:48<03:31,  8.82s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1915/1938 [4:44:57<03:22,  8.81s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1916/1938 [4:45:06<03:13,  8.81s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1917/1938 [4:45:14<03:05,  8.81s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1918/1938 [4:45:23<02:56,  8.81s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1919/1938 [4:45:32<02:47,  8.82s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1920/1938 [4:45:41<02:38,  8.82s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1921/1938 [4:45:50<02:29,  8.82s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1922/1938 [4:45:59<02:21,  8.82s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1923/1938 [4:46:07<02:12,  8.82s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1924/1938 [4:46:16<02:03,  8.83s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1925/1938 [4:46:25<01:54,  8.82s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1926/1938 [4:46:34<01:45,  8.81s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1927/1938 [4:46:43<01:36,  8.80s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 1928/1938 [4:46:51<01:28,  8.80s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1929/1938 [4:47:00<01:19,  8.81s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1930/1938 [4:47:09<01:10,  8.81s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1931/1938 [4:47:18<01:01,  8.81s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1932/1938 [4:47:27<00:52,  8.81s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1933/1938 [4:47:36<00:44,  8.81s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1934/1938 [4:47:44<00:35,  8.81s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1935/1938 [4:47:53<00:26,  8.80s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1936/1938 [4:48:02<00:17,  8.83s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 1937/1938 [4:48:11<00:08,  8.82s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1938/1938 [4:48:18<00:00,  8.47s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 2175\u001b[0m\n",
      "\u001b[34mNum examples = 2175\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/34 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 2/34 [00:01<00:23,  1.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 3/34 [00:03<00:33,  1.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 4/34 [00:04<00:37,  1.24s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 5/34 [00:06<00:38,  1.34s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 6/34 [00:07<00:39,  1.40s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 7/34 [00:09<00:39,  1.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▎       | 8/34 [00:10<00:38,  1.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▋       | 9/34 [00:12<00:37,  1.48s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 10/34 [00:13<00:35,  1.50s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 11/34 [00:15<00:34,  1.50s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 12/34 [00:16<00:33,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 13/34 [00:18<00:31,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 14/34 [00:19<00:30,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 15/34 [00:21<00:28,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 16/34 [00:22<00:27,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 17/34 [00:24<00:25,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 18/34 [00:25<00:24,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 19/34 [00:27<00:22,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 20/34 [00:28<00:21,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 21/34 [00:30<00:19,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 22/34 [00:32<00:18,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 23/34 [00:33<00:16,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 24/34 [00:35<00:15,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 25/34 [00:36<00:13,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 26/34 [00:38<00:12,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 27/34 [00:39<00:10,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 28/34 [00:41<00:09,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 29/34 [00:42<00:07,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 30/34 [00:44<00:06,  1.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 31/34 [00:45<00:04,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 32/34 [00:47<00:03,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 33/34 [00:48<00:01,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 34/34 [00:50<00:00,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.6191915273666382, 'eval_runtime': 51.8734, 'eval_samples_per_second': 41.929, 'eval_steps_per_second': 0.655, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 1938/1938 [4:49:10<00:00,  8.47s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 34/34 [00:50<00:00,  1.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-1938\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-1938\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-1938/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-1938/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-1938/generation_config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-1938/generation_config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-1938/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-1938/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:23,047] [INFO] [engine.py:3500:save_16bit_model] Saving model weights to /tmp/checkpoint-1938/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:23,047] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /tmp/checkpoint-1938/pytorch_model.bin...\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:41,641] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /tmp/checkpoint-1938/pytorch_model.bin.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:41,669] [INFO] [logging.py:68:log_dist] [Rank 0] [Torch] Checkpoint global_step1938 is begin to save!\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:41,678] [INFO] [logging.py:68:log_dist] [Rank 0] Saving model checkpoint: /tmp/checkpoint-1938/global_step1938/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:41,678] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /tmp/checkpoint-1938/global_step1938/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:41,704] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /tmp/checkpoint-1938/global_step1938/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:41,712] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /tmp/checkpoint-1938/global_step1938/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:52,447] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /tmp/checkpoint-1938/global_step1938/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:52,447] [INFO] [engine.py:3397:_save_zero_checkpoint] zero checkpoint saved /tmp/checkpoint-1938/global_step1938/zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:52,703] [INFO] [torch_checkpoint_engine.py:27:commit] [Torch] Checkpoint global_step1938 is ready now!\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-646] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-646] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from /tmp/checkpoint-1292 (score: 1.5433379411697388).\u001b[0m\n",
      "\u001b[34mLoading best model from /tmp/checkpoint-1292 (score: 1.5433379411697388).\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:59,224] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:21:59,256] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module cpu_adam, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 2.8339500427246094 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module cpu_adam, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 2.870119571685791 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module cpu_adam, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 2.8728644847869873 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module cpu_adam, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 2.8475558757781982 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module cpu_adam, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 2.8695173263549805 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module cpu_adam, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 2.8650381565093994 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module cpu_adam, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 2.8553500175476074 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0006098747253417969 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005972385406494141 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005404949188232422 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005486011505126953 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00047469139099121094 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005488395690917969 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005621910095214844 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module cpu_adam, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 4.072780609130859 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #1 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,126] [INFO] [logging.py:68:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,182] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,182] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,182] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,589] [INFO] [utils.py:831:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,590] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 7.41 GB         CA 13.27 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,590] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 138.03 GB, percent = 28.7%\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,597] [INFO] [stage3.py:114:__init__] Reduce bucket size 4194304\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,597] [INFO] [stage3.py:115:__init__] Prefetch bucket size 3774873\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00047850608825683594 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,715] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,716] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 13.27 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,716] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 138.03 GB, percent = 28.7%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 251904 in 124 params\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,869] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,870] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 13.27 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,870] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 138.03 GB, percent = 28.7%\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,986] [INFO] [utils.py:831:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,987] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 13.27 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:06,987] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 138.03 GB, percent = 28.7%\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:13,595] [INFO] [utils.py:831:see_memory_usage] After creating fp16 partitions: 1\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:13,597] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 13.27 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:13,597] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 150.15 GB, percent = 31.3%\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:13,811] [INFO] [utils.py:831:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:13,812] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 13.27 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:13,812] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 151.46 GB, percent = 31.5%\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:14,099] [INFO] [utils.py:831:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:14,101] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 13.27 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:14,101] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 152.79 GB, percent = 31.8%\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:16,047] [INFO] [utils.py:831:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:16,048] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 13.27 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:16,049] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 178.39 GB, percent = 37.1%\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:19,562] [INFO] [utils.py:831:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:19,562] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 13.27 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:19,563] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 191.56 GB, percent = 39.9%\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:19,563] [INFO] [stage3.py:382:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...Using /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...Time to load utils op: 0.0004622936248779297 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005116462707519531 secondsUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004353523254394531 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...Time to load utils op: 0.00043892860412597656 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004477500915527344 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00049591064453125 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004944801330566406 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,653] [INFO] [utils.py:831:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,654] [INFO] [utils.py:832:see_memory_usage] MA 0.03 GB         Max_MA 0.52 GB         CA 13.27 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,654] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 202.38 GB, percent = 42.1%\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,654] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,654] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,654] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fe5afdcbfd0>\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,654] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,656] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe59e78e0a0>\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,657] [INFO] [config.py:1012:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7fe59e78bb20>\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,658] [INFO] [config.py:1012:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 0}\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   steps_per_print .............. 2000\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   train_batch_size ............. 64\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  8\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   world_size ................... 8\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=4194304 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=3774873 param_persistence_threshold=20480 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,659] [INFO] [config.py:997:print_user_config]   json = {\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.0001, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.0\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.0001, \n",
      "            \"warmup_num_steps\": 0\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 4.194304e+06, \n",
      "        \"stage3_prefetch_bucket_size\": 3.774874e+06, \n",
      "        \"stage3_param_persistence_threshold\": 2.048000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00041937828063964844 seconds\u001b[0m\n",
      "\u001b[34mAttempting to resume from /tmp/checkpoint-1292\u001b[0m\n",
      "\u001b[34mAttempting to resume from /tmp/checkpoint-1292\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,671] [INFO] [torch_checkpoint_engine.py:21:load] [Torch] Loading checkpoint from /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,687] [INFO] [torch_checkpoint_engine.py:23:load] [Torch] Loaded checkpoint from /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,687] [INFO] [torch_checkpoint_engine.py:21:load] [Torch] Loading checkpoint from /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,702] [INFO] [torch_checkpoint_engine.py:23:load] [Torch] Loaded checkpoint from /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:21,808] [INFO] [torch_checkpoint_engine.py:21:load] [Torch] Loading checkpoint from /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:24,560] [INFO] [torch_checkpoint_engine.py:23:load] [Torch] Loaded checkpoint from /tmp/checkpoint-1292/global_step1292/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:24,560] [INFO] [engine.py:3023:_get_all_zero_checkpoint_state_dicts] successfully read 8 ZeRO state_dicts for rank 0\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:27,974] [INFO] [engine.py:2963:_load_zero_checkpoint] loading 8 zero partition checkpoints for rank 0\u001b[0m\n",
      "\u001b[34m{'train_runtime': 17425.8389, 'train_samples_per_second': 7.114, 'train_steps_per_second': 0.111, 'train_loss': 1.3571433902894012, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 1938/1938 [4:50:25<00:00,  8.47s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1938/1938 [4:50:25<00:00,  8.99s/it]\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:38,565] [INFO] [engine.py:3500:save_16bit_model] Saving model weights to /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:38,565] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /opt/ml/model/pytorch_model.bin...\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:43,522] [INFO] [launch.py:350:main] Process 724 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:44,524] [INFO] [launch.py:350:main] Process 722 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:46,527] [INFO] [launch.py:350:main] Process 726 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:47,529] [INFO] [launch.py:350:main] Process 728 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:49,531] [INFO] [launch.py:350:main] Process 725 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:50,533] [INFO] [launch.py:350:main] Process 727 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:50,533] [INFO] [launch.py:350:main] Process 723 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:22:58,819] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /opt/ml/model/pytorch_model.bin.\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/spiece.model\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/spiece.model\u001b[0m\n",
      "\u001b[34m[2023-05-29 11:23:04,548] [INFO] [launch.py:350:main] Process 721 exits successfully.\u001b[0m\n",
      "\u001b[34m2023-05-29 11:23:06,667 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-29 11:23:06,667 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-29 11:23:06,668 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-29 11:23:11 Uploading - Uploading generated training model\n",
      "2023-05-29 11:47:19 Completed - Training job completed\n",
      "Training seconds: 19482\n",
      "Billable seconds: 19482\n"
     ]
    }
   ],
   "source": [
    "sm_estimator.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6b645ff-ab69-4925-ac60-cb2076ac3c47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>val_loss</td>\n",
       "      <td>1.566096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5760.0</td>\n",
       "      <td>val_loss</td>\n",
       "      <td>1.543338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11580.0</td>\n",
       "      <td>val_loss</td>\n",
       "      <td>1.619192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train_loss</td>\n",
       "      <td>1.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4500.0</td>\n",
       "      <td>train_loss</td>\n",
       "      <td>1.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9060.0</td>\n",
       "      <td>train_loss</td>\n",
       "      <td>1.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1380.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4500.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>1.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7140.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp metric_name     value\n",
       "0        0.0    val_loss  1.566096\n",
       "1     5760.0    val_loss  1.543338\n",
       "2    11580.0    val_loss  1.619192\n",
       "3        0.0  train_loss  1.722600\n",
       "4     4500.0  train_loss  1.419500\n",
       "5     9060.0  train_loss  1.198800\n",
       "6        0.0       epoch  0.770000\n",
       "7     1380.0       epoch  1.000000\n",
       "8     4500.0       epoch  1.550000\n",
       "9     7140.0       epoch  2.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "# Wait for a couple of minutes for the job to start before running this cell\n",
    "# This can be called while the job is still running\n",
    "flag = True\n",
    "while flag:\n",
    "    try:\n",
    "        df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "        flag = False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(10)\n",
    "        flag = True\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12a2d1-c9d7-4f0d-b2ff-ff16b7336cd8",
   "metadata": {},
   "source": [
    "### 3. Deploying inference endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bb978-baf4-497a-8578-2947a40cf72a",
   "metadata": {},
   "source": [
    "training job이 성공적으로 완료되면 노트북의 이후 작업을 실행해야 합니다. 변수 `training_job_name`에는 job name이 포함되고 `output_location`은 fine-tuned model artifact가 있는 S3 위치를 가리킨다는 점을 기억하세요.\n",
    "\n",
    "기존의 pre-trained 모델과 fine-tuned 모델에 대한 2개의 추론 엔드포인트를 생성합니다. 그런 다음 2개 엔드포인트에 대해 동일한 요청을 실행하고 결과를 비교합니다.\n",
    "\n",
    "각 엔드포인트 배포에는 몇 분 정도 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d8a6e3e-703b-4f26-8b58-60c7a7c414ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "# Retrieve the inference docker image URI. This is the base HuggingFace container image\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=aws_region,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "deploy_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3a5b30e-16fe-4223-8436-8d2a2758ebfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: jumpstart-demo-pre-trained-huggingface--2023-05-29-06-11-38-706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mimage URI:\u001b[0m\n",
      " 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\n",
      "\u001b[1mmodel URI:\u001b[0m\n",
      " s3://jumpstart-cache-prod-us-west-2/huggingface-infer/prepack/v1.0.5/infer-prepack-huggingface-text2text-flan-t5-xl.tar.gz\n",
      "Deploying an endpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-demo-pre-trained-huggingface--2023-05-29-06-11-38-706\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-demo-pre-trained-huggingface--2023-05-29-06-11-38-706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!\n",
      "Deployed an endpoint jumpstart-demo-pre-trained-huggingface--2023-05-29-06-11-38-706\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# Retrieve the URI of the pre-trained model\n",
    "pre_trained_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "pre_trained_name = name_from_base(f\"jumpstart-demo-pre-trained-{model_id}\")\n",
    "\n",
    "# Create the SageMaker model instance of the pre-trained model\n",
    "if (\"small\" in model_id) or (\"base\" in model_id):\n",
    "    deploy_source_uri = script_uris.retrieve(\n",
    "        model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    "    )\n",
    "    pre_trained_model = Model(\n",
    "        image_uri=deploy_image_uri,\n",
    "        source_dir=deploy_source_uri,\n",
    "        entry_point=\"inference.py\",\n",
    "        model_data=pre_trained_model_uri,\n",
    "        role=aws_role,\n",
    "        predictor_cls=Predictor,\n",
    "        name=pre_trained_name,\n",
    "    )\n",
    "else:\n",
    "    # For those large models, we already repack the inference script and model\n",
    "    # artifacts for you, so the `source_dir` argument to Model is not required.\n",
    "    pre_trained_model = Model(\n",
    "        image_uri=deploy_image_uri,\n",
    "        model_data=pre_trained_model_uri,\n",
    "        role=aws_role,\n",
    "        predictor_cls=Predictor,\n",
    "        name=pre_trained_name,\n",
    "    )\n",
    "\n",
    "print(f\"{bold}image URI:{unbold}{newline} {deploy_image_uri}\")\n",
    "print(f\"{bold}model URI:{unbold}{newline} {pre_trained_model_uri}\")\n",
    "print(\"Deploying an endpoint ...\")\n",
    "\n",
    "# 사전 학습된 모델을 배포합니다. 모델 클래스를 통해 모델을 배포할 때 Predictor 클래스를 전달해야 SageMaker API를 통해 추론을 실행할 수 있습니다.\n",
    "pre_trained_predictor = pre_trained_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=pre_trained_name,\n",
    ")\n",
    "print(f\"{newline}Deployed an endpoint {pre_trained_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6a79f-9dfb-4ef2-88f8-21da0dfa4f67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: jumpstart-demo-fine-tuned-huggingface-t-2023-05-29-11-50-05-886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mimage URI:\u001b[0m\n",
      " 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\n",
      "\u001b[1mmodel URI:\u001b[0m\n",
      " s3://sagemaker-us-west-2-322537213286/demo-fine-tune-flan-t5/js-demo-flan-t5-xl-3-2023-05-29-06-20-37-686/output/model.tar.gz\n",
      "Deploying an endpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-demo-fine-tuned-huggingface-t-2023-05-29-11-50-05-886\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-demo-fine-tuned-huggingface-t-2023-05-29-11-50-05-886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!\n",
      "Deployed an endpoint jumpstart-demo-fine-tuned-huggingface-t-2023-05-29-11-50-05-886\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "fine_tuned_name = name_from_base(f\"jumpstart-demo-fine-tuned-{model_id}\")\n",
    "fine_tuned_model_uri = f\"{output_location}{training_job_name}/output/model.tar.gz\"\n",
    "\n",
    "# Create the SageMaker model instance of the fine-tuned model\n",
    "fine_tuned_model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=fine_tuned_model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=fine_tuned_name,\n",
    ")\n",
    "\n",
    "print(f\"{bold}image URI:{unbold}{newline} {deploy_image_uri}\")\n",
    "print(f\"{bold}model URI:{unbold}{newline} {fine_tuned_model_uri}\")\n",
    "print(\"Deploying an endpoint ...\")\n",
    "\n",
    "# Deploy the fine-tuned model.\n",
    "fine_tuned_predictor = fine_tuned_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=fine_tuned_name,\n",
    ")\n",
    "print(f\"{newline}Deployed an endpoint {fine_tuned_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766df55-9b9c-44bd-8192-c15c20b33899",
   "metadata": {},
   "source": [
    "### 4. Running inference queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30014a2-7547-4c76-85c6-bbee84f696f3",
   "metadata": {},
   "source": [
    "이름에서 알 수 있듯이 FLAN T5와 같은 Text2Text 모델은 텍스트를 입력으로 받아 출력으로 텍스트를 생성합니다. 입력 텍스트에는 작업에 대한 설명이 포함됩니다. 이 데모에서는 텍스트가 주어지면 질문을 생성하는 작업을 수행합니다. 질문은 텍스트와 관련이 있어야 하지만 텍스트에는 답이 포함되어서는 안 됩니다. 이러한 작업은 추가 정보 수집을 자동화하거나 기술 문서에서 부족한 부분을 식별할 때 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508639f4-5634-4942-bdb0-69d5a667e378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Ask a question which is related to the following text, but cannot be answered based on the text. Text: {context}\"\n",
    "\n",
    "# Sources: Wikipedia, AWS Documentation\n",
    "test_paragraphs = [\n",
    "    \"\"\"\n",
    "Adelaide is the capital city of South Australia, the state's largest city and the fifth-most populous city in Australia. \"Adelaide\" may refer to either Greater Adelaide (including the Adelaide Hills) or the Adelaide city centre. The demonym Adelaidean is used to denote the city and the residents of Adelaide. The Traditional Owners of the Adelaide region are the Kaurna people. The area of the city centre and surrounding parklands is called Tarndanya in the Kaurna language.\n",
    "Adelaide is situated on the Adelaide Plains north of the Fleurieu Peninsula, between the Gulf St Vincent in the west and the Mount Lofty Ranges in the east. Its metropolitan area extends 20 km (12 mi) from the coast to the foothills of the Mount Lofty Ranges, and stretches 96 km (60 mi) from Gawler in the north to Sellicks Beach in the south.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Amazon Elastic Block Store (Amazon EBS) provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices. You can mount these volumes as devices on your instances. EBS volumes that are attached to an instance are exposed as storage volumes that persist independently from the life of the instance. You can create a file system on top of these volumes, or use them in any way you would use a block device (such as a hard drive). You can dynamically change the configuration of a volume attached to an instance.\n",
    "We recommend Amazon EBS for data that must be quickly accessible and requires long-term persistence. EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. Amazon EBS is well suited to both database-style applications that rely on random reads and writes, and to throughput-intensive applications that perform long, continuous reads and writes.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. \n",
    "You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. \n",
    "All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input. \n",
    "Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077182a2-d3e0-4e7a-94a7-6daab9e5dc0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Parameters of (output) text generation. A great introduction to generation\n",
    "# parameters can be found at https://huggingface.co/blog/how-to-generate\n",
    "parameters = {\n",
    "    \"max_length\": 40,  # restrict the length of the generated text\n",
    "    \"num_return_sequences\": 5,  # we will inspect several model outputs\n",
    "    \"num_beams\": 10,  # use beam search\n",
    "}\n",
    "\n",
    "\n",
    "# Helper functions for running inference queries\n",
    "def query_endpoint_with_json_payload(payload, endpoint_name):\n",
    "    encoded_json = json.dumps(payload).encode(\"utf-8\")\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def generate_questions(endpoint_name, text):\n",
    "    expanded_prompt = prompt.replace(\"{context}\", text)\n",
    "    payload = {\"text_inputs\": expanded_prompt, **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(payload, endpoint_name=endpoint_name)\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    for i, generated_text in enumerate(generated_texts):\n",
    "        print(f\"Response {i}: {generated_text}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc91a6-a5f9-4342-9a19-fa13d3caa6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPrompt:\u001b[0m 'Ask a question which is related to the following text, but cannot be answered based on the text. Text: {context}'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Adelaide is the capital city of South Australia, the state's largest city and the fifth-most populous city in Australia. \"Adelaide\" may refer to either Greater Adelaide (including the Adelaide Hills) or the Adelaide city centre. The demonym Adelaidean is used to denote the city and the residents of Adelaide. The Traditional Owners of the Adelaide region are the Kaurna people. The area of the city centre and surrounding parklands is called Tarndanya in the Kaurna language.\n",
      "Adelaide is situated on the Adelaide Plains north of the Fleurieu Peninsula, between the Gulf St Vincent in the west and the Mount Lofty Ranges in the east. Its metropolitan area extends 20 km (12 mi) from the coast to the foothills of the Mount Lofty Ranges, and stretches 96 km (60 mi) from Gawler in the north to Sellicks Beach in the south.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[1mpre-trained\u001b[0m\n",
      "Response 0: What is the area of the city centre and surrounding parklands called in the Kaurna language?\n",
      "\n",
      "Response 1: What is the area of the city centre and surrounding parklands is called Tarndanya in the Kaurna language?\n",
      "\n",
      "Response 2: What is the area of the city centre and surrounding parklands called in Kaurna?\n",
      "\n",
      "Response 3: What is the capital city of South Australia?\n",
      "\n",
      "Response 4: What is the area of the city centre and surrounding parklands known as in the Kaurna language?\n",
      "\n",
      "\u001b[1mfine-tuned\u001b[0m\n",
      "Response 0: What is the population of Gawler?\n",
      "\n",
      "Response 1: What is the largest city in Australia?\n",
      "\n",
      "Response 2: What is the smallest city in South Australia?\n",
      "\n",
      "Response 3: What is the largest city in South Australia?\n",
      "\n",
      "Response 4: What is the largest city in the world?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Amazon Elastic Block Store (Amazon EBS) provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices. You can mount these volumes as devices on your instances. EBS volumes that are attached to an instance are exposed as storage volumes that persist independently from the life of the instance. You can create a file system on top of these volumes, or use them in any way you would use a block device (such as a hard drive). You can dynamically change the configuration of a volume attached to an instance.\n",
      "We recommend Amazon EBS for data that must be quickly accessible and requires long-term persistence. EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. Amazon EBS is well suited to both database-style applications that rely on random reads and writes, and to throughput-intensive applications that perform long, continuous reads and writes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[1mpre-trained\u001b[0m\n",
      "Response 0: What is the difference between Amazon EBS and Amazon Elastic Block Store (Amazon EBS)?\n",
      "\n",
      "Response 1: What is the difference between Amazon EBS and Amazon Elastic Block Store?\n",
      "\n",
      "Response 2: What is the difference between Amazon EBS and Amazon Simple Storage Service (Amazon S3)?\n",
      "\n",
      "Response 3: What is Amazon Elastic Block Store (Amazon EBS)?\n",
      "\n",
      "Response 4: What is the difference between Amazon EBS and a hard drive?\n",
      "\n",
      "\u001b[1mfine-tuned\u001b[0m\n",
      "Response 0: What behaves like formatted block devices?\n",
      "\n",
      "Response 1: What type of applications are not well suited to Amazon EBS?\n",
      "\n",
      "Response 2: What type of applications are not suited to Amazon EBS?\n",
      "\n",
      "Response 3: What do EBS volumes behave like when attached to an EC2 instance?\n",
      "\n",
      "Response 4: What do EBS volumes behave like when attached to an instance?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. \n",
      "You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. \n",
      "All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input. \n",
      "Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[1mpre-trained\u001b[0m\n",
      "Response 0: What does Amazon Comprehend use to extract insights about the content of documents?\n",
      "\n",
      "Response 1: How does Amazon Comprehend extract insights about the content of documents?\n",
      "\n",
      "Response 2: What does Amazon Comprehend use to develop insights about the content of documents?\n",
      "\n",
      "Response 3: How does Amazon Comprehend develop insights about the content of documents?\n",
      "\n",
      "Response 4: What does Amazon Comprehend use to extract insights about the content of a document?\n",
      "\n",
      "\u001b[1mfine-tuned\u001b[0m\n",
      "Response 0: What does Amazon Comprehend use to extract insights about the structure of documents?\n",
      "\n",
      "Response 1: What does Amazon Comprehend use to develop insights about the structure of documents?\n",
      "\n",
      "Response 2: How does Amazon Comprehend recognize sentiments in a document?\n",
      "\n",
      "Response 3: How does Amazon Comprehend develop insights about the structure of documents?\n",
      "\n",
      "Response 4: What type of document does Amazon Comprehend reject?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{bold}Prompt:{unbold} {repr(prompt)}\")\n",
    "for paragraph in test_paragraphs:\n",
    "    print(\"-\" * 80)\n",
    "    print(paragraph)\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{bold}pre-trained{unbold}\")\n",
    "    generate_questions(pre_trained_name, paragraph)\n",
    "    print(f\"{bold}fine-tuned{unbold}\")\n",
    "    generate_questions(fine_tuned_name, paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3db1d-1d92-4bbb-8af2-fd72534c2c0e",
   "metadata": {},
   "source": [
    "pre-trained 모델은 답변할 수 없는 질문을 생성하도록 특별히 학습되지 않았습니다. 입력 프롬프트에도 불구하고 텍스트에서 답변할 수 있는 질문을 생성하는 경향이 있습니다. 일반적으로 fine-tuned 모델이 이 작업을 더 잘 수행하며, 이러한 개선은 더 큰 모델(예: 기본이 아닌 xl)에서 더 두드러집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04accabc-b608-40c2-b12a-3a827a562749",
   "metadata": {},
   "source": [
    "### 5. Cleaning up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0817b1c-ed07-4b02-bbac-ee3c31611219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: jumpstart-demo-pre-trained-huggingface--2023-05-29-06-11-38-706\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: jumpstart-demo-pre-trained-huggingface--2023-05-29-06-11-38-706\n",
      "INFO:sagemaker:Deleting endpoint with name: jumpstart-demo-pre-trained-huggingface--2023-05-29-06-11-38-706\n",
      "INFO:sagemaker:Deleting model with name: jumpstart-demo-fine-tuned-huggingface-t-2023-05-29-11-50-05-886\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: jumpstart-demo-fine-tuned-huggingface-t-2023-05-29-11-50-05-886\n",
      "INFO:sagemaker:Deleting endpoint with name: jumpstart-demo-fine-tuned-huggingface-t-2023-05-29-11-50-05-886\n"
     ]
    }
   ],
   "source": [
    "# Delete resources\n",
    "pre_trained_predictor.delete_model()\n",
    "pre_trained_predictor.delete_endpoint()\n",
    "fine_tuned_predictor.delete_model()\n",
    "fine_tuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab4992-bc74-481e-9471-0252568b4cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
