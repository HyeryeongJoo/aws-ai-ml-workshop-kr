{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS boto3 를 사용하여 Langfuse 로 Amazon Bedrock 시작하기\n",
    "\n",
    "이 노트북은 [Amazon Bedrock Integration](https://langfuse.com/docs/integrations/amazon-bedrock) 를 사용하여 Amazonb Bedrock 의 LLM 을 사용하는 방법을 \n",
    "가이드하는 노트북 입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 선수 사항: \n",
    "### 1.1 Langfuse 를 AWS 인프라에 호스팅 \n",
    "- 아래의 설치 가이드에 따라 Langfuse 를 먼저 설치 하세요.\n",
    "    - [설치 가이드: Langfuse 를 AWS 인프라에 호스팅 ](../../01_setup/LAGNFUSE_README.md)\n",
    "\n",
    "### 1.2. 선수 사항: 콘다 가상 환경 생성\n",
    "- [setup](../../01_setup/README.md) 의 가이드에 따라 실행하여, 가상 환경인 langgraph 생성\n",
    "- 이후 모든 노트북의 커널은 langgraph 를 사용합니다.\n",
    "\n",
    "### 1.3. Key 정보를 저장하는 env 파일 생성\n",
    "-  ../../.env 파일을 생성하고 아래의 내용을 작성\n",
    "    ```\n",
    "    LANGFUSE_SECRET_KEY=<secret key>\n",
    "    LANGFUSE_PUBLIC_KEY=<public key>\n",
    "    LANGFUSE_HOST=<host url>\n",
    "    ```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. .env 파일을 통하여 key 정보 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. boto3 client 생성 및 bedrock 모델 리스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Anthropic Claude 3 Sonnet - us.anthropic.claude-3-sonnet-20240229-v1:0\n",
      "US Anthropic Claude 3 Opus - us.anthropic.claude-3-opus-20240229-v1:0\n",
      "US Anthropic Claude 3 Haiku - us.anthropic.claude-3-haiku-20240307-v1:0\n",
      "US Meta Llama 3.2 11B Instruct - us.meta.llama3-2-11b-instruct-v1:0\n",
      "US Meta Llama 3.2 3B Instruct - us.meta.llama3-2-3b-instruct-v1:0\n",
      "US Meta Llama 3.2 90B Instruct - us.meta.llama3-2-90b-instruct-v1:0\n",
      "US Meta Llama 3.2 1B Instruct - us.meta.llama3-2-1b-instruct-v1:0\n",
      "US Anthropic Claude 3.5 Sonnet - us.anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "US Anthropic Claude 3.5 Haiku - us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "US Meta Llama 3.1 8B Instruct - us.meta.llama3-1-8b-instruct-v1:0\n",
      "US Meta Llama 3.1 70B Instruct - us.meta.llama3-1-70b-instruct-v1:0\n",
      "US Nova Lite - us.amazon.nova-lite-v1:0\n",
      "US Nova Pro - us.amazon.nova-pro-v1:0\n",
      "US Nova Micro - us.amazon.nova-micro-v1:0\n",
      "US Meta Llama 3.3 70B Instruct - us.meta.llama3-3-70b-instruct-v1:0\n",
      "US Anthropic Claude 3.5 Sonnet v2 - us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "US Anthropic Claude 3.7 Sonnet - us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "US DeepSeek-R1 - us.deepseek.r1-v1:0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    " \n",
    "bedrock = boto3.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "\n",
    "# used to invoke the Bedrock Converse API\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "\n",
    "# Check which models are available in your account\n",
    "models = bedrock.list_inference_profiles()\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "  print(model[\"inferenceProfileName\"] + \" - \" + model[\"inferenceProfileId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. wrapped_bedrock_converse() 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe, langfuse_context\n",
    "from botocore.exceptions import ClientError\n",
    " \n",
    "@observe(as_type=\"generation\", name=\"Bedrock Converse\")\n",
    "def wrapped_bedrock_converse(**kwargs):\n",
    "  # 1. extract model metadata\n",
    "  kwargs_clone = kwargs.copy()\n",
    "  input = kwargs_clone.pop('messages', None)\n",
    "  modelId = kwargs_clone.pop('modelId', None)\n",
    "  model_parameters = {\n",
    "        **kwargs_clone.pop('inferenceConfig', {}),\n",
    "        **kwargs_clone.pop('additionalModelRequestFields', {})\n",
    "  }\n",
    "  # Langfuse 관측 컨텍스트에 입력, 모델 ID, 파라미터, 기타 메타데이터를 업데이트합니다.\n",
    "  langfuse_context.update_current_observation(\n",
    "      input=input,\n",
    "      model=modelId,\n",
    "      model_parameters=model_parameters,\n",
    "      metadata=kwargs_clone\n",
    "  )\n",
    " \n",
    "  # 2. model call with error handling\n",
    "  try:\n",
    "    response = bedrock_runtime.converse(**kwargs)\n",
    "  except (ClientError, Exception) as e:\n",
    "    error_message = f\"ERROR: Can't invoke '{modelId}'. Reason: {e}\"\n",
    "    langfuse_context.update_current_observation(level=\"ERROR\", status_message=error_message)\n",
    "    print(error_message)\n",
    "    return\n",
    " \n",
    "  # 3. extract response metadata\n",
    "  # Langfuse에 출력 텍스트, 토큰 사용량, 응답 메타데이터를 기록합니다.\n",
    "  response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "  langfuse_context.update_current_observation(\n",
    "    output=response_text,\n",
    "    usage_details={\n",
    "        \"input\": response[\"usage\"][\"inputTokens\"],\n",
    "        \"output\": response[\"usage\"][\"outputTokens\"],\n",
    "        \"total\": response[\"usage\"][\"totalTokens\"]\n",
    "    },\n",
    "    metadata={\n",
    "        \"ResponseMetadata\": response[\"ResponseMetadata\"],\n",
    "    }\n",
    "  )\n",
    " \n",
    "  return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. wrapped_bedrock_converse() 를 통해 bedrock converse() 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic\n",
      "안녕하세요! 저는 SmartFinance Advisors의 AI 개인 재무 어드바이저 Alex입니다. 어떤 재무 관련 질문이나 조언이 필요하신지 말씀해주시면 기꺼이 도움드리겠습니다. 저축 계획, 투자 전략, 예산 관리, 부채 관리 등 다양한 재무 영역에 대해 상담해 드릴 수 있습니다. 무엇을 도와드릴까요?\n",
      "\n",
      "Nova-Pro\n",
      "안녕하세요! 제가 어떻게 도움을 드릴 수 있을지 말씀해 주시면 감사하겠습니다. 예를 들어, 예산 관리, 저축 전략, 투자 조언, 은퇴 계획 등에 관한 질문이 있으시다면 언제든지 물어봐 주세요.\n",
      "\n",
      "만약 구체적인 상황이나 문제가 있다면, 그에 대한 세부 사항을 알려주시면 더 나은 조언을 드릴 수 있습니다. 예를 들어, \"저는 매달 예산을 어떻게 관리해야 할까요?\" 혹은 \"주식 투자를 시작하려는데 어디서부터 시작해야 할까요?\"와 같은 질문이 좋습니다.\n",
      "\n",
      "제가 최대한 도움을 드릴 수 있도록 하겠습니다. 어떤 질문이 있으신가요?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "claude_model_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "nova_model_id = \"us.amazon.nova-pro-v1:0\"\n",
    "# Converesation according to AWS spec including prompting + history\n",
    "user_message = \"\"\"당신은 SmartFinance Advisors 회사가 만든 Alex라는 AI 개인 재무 어드바이저 역할을 수행하게 됩니다. 당신의 목표는 사용자에게 재무 조언과 지침을 제공하는 것입니다. 당신은 SmartFinance Advisors 사이트에 있는 사용자들에게 답변할 것이며, 만약 당신이 Alex의 캐릭터로 응답하지 않으면 사용자들이 혼란스러워할 수 있습니다.\n",
    "다음은 질문 이전의 대화 내역(사용자와 당신 사이)입니다. 대화 내역이 없다면 비어있을 수 있습니다:\n",
    "<history>\n",
    "User: 안녕하세요 Alex, 당신의 조언이 정말 기대돼요!\n",
    "Alex: 안녕하세요! 저는 SmartFinance Advisors의 AI 개인 재무 어드바이저 Alex입니다. 오늘 어떤 재무 목표에 도움이 필요하신가요?\n",
    "</history>\n",
    "다음은 상호작용에 있어 중요한 규칙들입니다:\n",
    "\n",
    "항상 SmartFinance Advisors의 AI인 Alex의 캐릭터를 유지하세요.\n",
    "응답 방법이 확실하지 않다면 \"죄송합니다, 잘 이해하지 못했습니다. 질문을 다시 말씀해 주시겠어요?\"라고 말하세요.\n",
    "\"\"\"\n",
    " \n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    " \n",
    "@observe()\n",
    "def examples_bedrock_converse_api():\n",
    "  responses = {}\n",
    " \n",
    "  responses[\"anthropic\"] = wrapped_bedrock_converse(\n",
    "    modelId=claude_model_id,\n",
    "    messages=conversation,\n",
    "    inferenceConfig={\"maxTokens\":500,\"temperature\":1},\n",
    "    additionalModelRequestFields={\"top_k\":250}\n",
    "  )\n",
    " \n",
    "  responses[\"nova-pro\"] = wrapped_bedrock_converse(\n",
    "    modelId=nova_model_id,\n",
    "    messages=conversation,\n",
    "    inferenceConfig={\"maxTokens\":500,\"temperature\":1},\n",
    "  )\n",
    " \n",
    "  return responses\n",
    " \n",
    "res = examples_bedrock_converse_api()\n",
    " \n",
    "for key, value in res.items():\n",
    "    print(f\"{key.title()}\\n{value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Langfuse 추적 결과\n",
    "- 아래는 위의 실행에 대한 langfuse 의 추적 결과 입니다.\n",
    "- ![langfuse_boto3_bedrock.png](img/langfuse_boto3_bedrock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. langfuse_handler 핸들러 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "import os\n",
    "\n",
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=os.environ.get('LANGFUSE_PUBLIC_KEY'),\n",
    "    secret_key=os.environ.get('LANGFUSE_SECRET_KEY'),\n",
    "    host=os.environ.get('LANGFUSE_HOST'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. langfuse endpoint 에 인증 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connection test\n",
    "langfuse_handler.auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LangCahin 모델 오브젝트 생성 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs=dict(temperature=0),\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='프로그래밍을 좋아합니다.', additional_kwargs={'usage': {'prompt_tokens': 29, 'completion_tokens': 19, 'total_tokens': 48}, 'stop_reason': 'end_turn', 'thinking': {}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 29, 'completion_tokens': 19, 'total_tokens': 48}, 'stop_reason': 'end_turn', 'thinking': {}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-4d5c8e6b-dfc9-4886-930d-168eb7369719-0', usage_metadata={'input_tokens': 29, 'output_tokens': 19, 'total_tokens': 48})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to Korean. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages, config={\"callbacks\": [langfuse_handler]})\n",
    "ai_msg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로그래밍을 좋아합니다.\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LangChain 모델 Chaining 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='프로그래밍을 좋아합니다.', additional_kwargs={'usage': {'prompt_tokens': 23, 'completion_tokens': 19, 'total_tokens': 42}, 'stop_reason': 'end_turn', 'thinking': {}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 23, 'completion_tokens': 19, 'total_tokens': 42}, 'stop_reason': 'end_turn', 'thinking': {}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-a3a7c734-4f3c-43b0-8883-f8d954e818bb-0', usage_metadata={'input_tokens': 23, 'output_tokens': 19, 'total_tokens': 42})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"korean\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    },\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.LangFuse 추적 내용 보기\n",
    "로컬에 호스팅된 LangFuse 에 로그인을 하면 아래와 같이 기록이 남습니다.\n",
    "- LLM 호출건수 추적 기록 남음\n",
    "    - ![lang_run_result_01.png](img/lang_run_result_01.png)\n",
    "- 아래는 위에서 chain.invoke() 를 실행시에 chain = prompt | llm 여기서 prompt 가 실행된 것을 추적 합니다.    \n",
    "    - ![lang_chain_template.png](img/lang_chain_template.png)  \n",
    "- 아래는 위에서 chain.invoke() 를 실행시에 chain = prompt | llm 여기서 lim 이 실행된 것을 추적 합니다.        \n",
    "    - ![lang_chain_chatbedrock.png](img/lang_chain_chatbedrock.png)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bedrock Converse API 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='프로그래밍을 좋아합니다.', additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'f7abdcdf-4f29-4e1b-b852-fe4e6bbe10bf', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 13 Mar 2025 13:10:26 GMT', 'content-type': 'application/json', 'content-length': '216', 'connection': 'keep-alive', 'x-amzn-requestid': 'f7abdcdf-4f29-4e1b-b852-fe4e6bbe10bf'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [398]}}, id='run-1f90ce9d-8a27-4d97-9988-acb4ad9352e9-0', usage_metadata={'input_tokens': 29, 'output_tokens': 19, 'total_tokens': 48})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm.invoke(messages, config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=[] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '프', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '로', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '그', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '래', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '밍', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '을', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': ' ', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '좋', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '아', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '합', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '니', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '다', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'type': 'text', 'text': '.', 'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[{'index': 0}] additional_kwargs={} response_metadata={} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[] additional_kwargs={} response_metadata={'stopReason': 'end_turn'} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a'\n",
      "content=[] additional_kwargs={} response_metadata={'metrics': {'latencyMs': 446}} id='run-a1efff3b-521c-4b6e-9550-5a121253cb5a' usage_metadata={'input_tokens': 29, 'output_tokens': 18, 'total_tokens': 47}\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(messages, config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LangChain 의 StrOutputParser 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|프|로|그|래|밍|을| |좋|아|합|니|다|.||||"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = llm | StrOutputParser()\n",
    "\n",
    "for chunk in chain.stream(messages, config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
