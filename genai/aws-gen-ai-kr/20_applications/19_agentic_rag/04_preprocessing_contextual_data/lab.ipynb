{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 설정을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install boto3 botocore\n",
    "%pip install opensearch-py\n",
    "%pip install tqdm\n",
    "%pip install pdfplumber\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-1. Bedrock Client 를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "retry_config = botocore.config.Config(\n",
    "    retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "bedrock_client = boto3.Session(\n",
    "    region_name='us-west-2',\n",
    "    \n",
    ").client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2. OpenSearch 클라이언트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utilities import get_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "region = boto3.Session(region_name='us-west-2').region_name\n",
    "ssm = boto3.client('ssm', region)\n",
    "\n",
    "opensearch_domain_endpoint = get_parameter(\n",
    "    boto3_clinet = ssm,\n",
    "    parameter_name = 'opensearch_domain_endpoint',\n",
    ")\n",
    "\n",
    "opensearch_user_id = get_parameter(\n",
    "    boto3_clinet = ssm,\n",
    "    parameter_name = 'opensearch_user_id',\n",
    ")\n",
    "\n",
    "opensearch_user_password = get_parameter(\n",
    "    boto3_clinet = ssm,\n",
    "    parameter_name = 'opensearch_user_password',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "http_auth = (opensearch_user_id, opensearch_user_password) # Master username, Master password\n",
    "aws_region = os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    "os_client = OpenSearch(\n",
    "    hosts=[\n",
    "        {\n",
    "            'host': opensearch_domain_endpoint.replace(\"https://\", \"\"),\n",
    "            'port': 443\n",
    "        }\n",
    "    ],\n",
    "    http_auth=http_auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 문서 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서에 대한 전처리를 진행합니다.\n",
    "- 주어진 문서의 길이가 너무 길어 Whole Context로 전달할 수 없어, 일정 단위(`document_length`)로 잘라 하나의 Document로 정의합니다.\n",
    "- 하나의 Document는 다시 일정 단위(`chunk_size`)로 잘라 목록화 합니다. 이를 Chunking 작업이라 합니다.\n",
    "- 각 Chunk에 대해 Contextual Text를 추가하는 작업을 Situation 이라 합니다.\n",
    "- Situated Chunk를 대상으로 Embedding이 진행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file and chunking parameters\n",
    "input_file = 'data/bedrock-ug.pdf'\n",
    "chunk_size = 1000\n",
    "start_page = 15\n",
    "document_length = 20000  # Maximum document size for context\n",
    "\n",
    "# Extract document name from file path\n",
    "from pathlib import Path\n",
    "document_name = Path(input_file).resolve().stem\n",
    "print(f\"Processing document: {document_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DocumentParser from local library\n",
    "from document_parser import DocumentParser\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Load PDF and split into chunks\n",
    "    print(f\"Loading PDF from {input_file} starting at page {start_page}...\")\n",
    "    chunked_document = DocumentParser.split(\n",
    "        full_text=DocumentParser.load_pdf(input_file, start_page=start_page), \n",
    "        chunk_size=chunk_size, \n",
    "        max_document_length=document_length\n",
    "    )\n",
    "    \n",
    "    # Define output file path with proper f-string syntax\n",
    "    output_file = f\"output/{document_name}_{chunk_size}_chunks.json\"\n",
    "    \n",
    "    # Save chunks to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunked_document, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✅ Chunks saved to {output_file}\")\n",
    "        \n",
    "    # Print summary statistics\n",
    "    total_chunks = sum(len(doc.get('chunks', [])) for doc in chunked_document)\n",
    "    print(f\"Total documents: {len(chunked_document)}\")\n",
    "    print(f\"Total chunks: {total_chunks}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error processing document: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체 문서에 대한 Context를 바탕으로, 주어진 Chunk의 내용을 5줄 이내의 문장으로 설명하도록 합니다.\n",
    "- 추가된 문서의 Context는 content 필드에 추가되어 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 170개 Chunk 기준 2분 가량의 시간이 필요합니다.\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "You're an expert at providing a succinct context, targeted for specific text chunks.\n",
    "\n",
    "<instruction>\n",
    "- Offer 1-5 short sentences that explain what specific information this chunk provides within the document.\n",
    "- Focus on the unique content of this chunk, avoiding general statements about the overall document.\n",
    "- Clarify how this chunk's content relates to other parts of the document and its role in the document.\n",
    "- If there's essential information in the document that backs up this chunk's key points, mention the details.\n",
    "</instruction>\n",
    "\"\"\"\n",
    "for doc_index, document in enumerate(chunked_document):\n",
    "    doc_content = document['content']\n",
    "   \n",
    "    for chunk in tqdm(document['chunks']):\n",
    "        # 재시도에 대한 멱등성을 확보하기 위해, 이미 Situate 작업이 완료된 Chunk는 Skip 합니다.\n",
    "        if chunk['content'].startswith('Context:'):\n",
    "            continue\n",
    "            \n",
    "        document_context_prompt = f\"\"\"\n",
    "        <document>\n",
    "        {doc_content}\n",
    "        </document>\n",
    "        \"\"\"\n",
    "\n",
    "        chunk_content = chunk['content']\n",
    "        chunk_context_prompt = f\"\"\"\n",
    "        Here is the chunk we want to situate within the whole document:\n",
    "\n",
    "        <chunk>\n",
    "        {chunk_content}\n",
    "        </chunk>\n",
    "\n",
    "        Skip the preamble and only provide the consise context.\n",
    "        \"\"\"\n",
    "        usr_prompt = [{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\"text\": document_context_prompt},\n",
    "                    {\"text\": chunk_context_prompt}\n",
    "                ]\n",
    "            }]\n",
    "        \n",
    "        try:\n",
    "            response = bedrock_client.converse(\n",
    "                modelId='us.anthropic.claude-3-haiku-20240307-v1:0',\n",
    "                messages=usr_prompt,\n",
    "                system=[{'text': sys_prompt}],\n",
    "                inferenceConfig={\n",
    "                    'maxTokens': 4096\n",
    "                }\n",
    "            )\n",
    "\n",
    "            situated_context = response['output']['message']['content'][0]['text'].strip()\n",
    "            chunk['content'] = f\"Context:\\n{situated_context}\\n\\nChunk:\\n{chunk['content']}\"\n",
    "            time.sleep(2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating context for chunk: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_document[0]['chunks'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오픈 서치 인덱스 생성 \n",
    "- 오픈 서치에 해당 인덱스가 존재하면, 삭제 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'idx-genai-contextual-retriever'\n",
    "delete_index_if_exists = True\n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.algo_param.ef_search\": 512\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"metadata\": {\n",
    "                \"properties\": {\n",
    "                    'parent_id' : {'type': 'keyword'},  \n",
    "                    'family_tree': {'type': 'keyword'},\n",
    "                }\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            },\n",
    "            \"vector_field\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,\n",
    "                \"method\": {\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 512,\n",
    "                        \"m\": 16\n",
    "                    },\n",
    "                    \"space_type\": \"l2\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delete_index_if_exists:\n",
    "    if os_client.indices.exists(index=index_name):\n",
    "        os_client.indices.delete(index=index_name)\n",
    "    \n",
    "\n",
    "os_client.indices.create(index=index_name, body=index_body)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context가 추가된 Chunk를 Embedding 하고, Opensearch Index에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, document in enumerate(chunked_document):\n",
    "    context = chunk['content']\n",
    "    parent_chunk = {\n",
    "        'metadata': {\n",
    "            'family_tree': 'parent',\n",
    "            'parent_id': None,\n",
    "            'seq_num': doc_id\n",
    "        },\n",
    "        'text': document['content']\n",
    "    }\n",
    "    parent_id = os_client.index(\n",
    "        index=index_name,\n",
    "        body=parent_chunk #embedded_chunk\n",
    "    )['_id']\n",
    "\n",
    "    for chunk_id, chunk in enumerate(tqdm(document['chunks'])):\n",
    "        context = chunk['content']\n",
    "        \n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "            body=json.dumps({\"inputText\": context})\n",
    "        )\n",
    "        chunk_embedding = json.loads(response['body'].read())['embedding']\n",
    "\n",
    "        if chunk_embedding:\n",
    "            embedded_chunk = {\n",
    "                \"metadata\": {\n",
    "                    \"family_tree\": \"child\",\n",
    "                    \"parent_id\": parent_id,\n",
    "                    \"seq_num\": chunk_id,\n",
    "                },\n",
    "                \"text\": chunk['content'],\n",
    "                \"vector_field\": chunk_embedding\n",
    "            }\n",
    "\n",
    "        os_client.index(\n",
    "            index=index_name,\n",
    "            body=embedded_chunk\n",
    "        )\n",
    "\n",
    "        \n",
    "print(f\"Successfully embedded and stored documents in index '{index_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검색 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical Search (BM-25), Semantic Search (knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def _format_search_result(hit: Dict, search_method: str) -> Dict:\n",
    "    print (hit)\n",
    "    \n",
    "    return {\n",
    "        \"text\": hit['_source'][\"text\"],\n",
    "        \"score\": hit['_score'],\n",
    "        \"metadata\": hit['_source']['metadata'],\n",
    "        \"search_method\": search_method\n",
    "    }\n",
    "\n",
    "\n",
    "def search_by_knn(os_client, vector: List[float], index_name: str, top_n: int = 80) -> List[Dict]:\n",
    "    query = {\n",
    "        \"size\": top_n,\n",
    "        \"_source\": [\"text\", \"metadata\"],\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"vector_field\": {\n",
    "                    \"vector\": vector,\n",
    "                    \"k\": top_n\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    response = os_client.search(index=index_name, body=query)\n",
    "    return [_format_search_result(hit, 'knn') \n",
    "           for hit in response['hits']['hits']]\n",
    "\n",
    "def search_by_bm25(os_client, query_text: str, index_name: str, top_n: int = 80) -> List[Dict]:\n",
    "    query = {\n",
    "        \"size\": top_n,\n",
    "        \"_source\": [\"text\", \"metadata\"],\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"text\": {\n",
    "                    \"query\": query_text,\n",
    "                    \"operator\": \"or\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = os_client.search(index=index_name, body=query)\n",
    "    return [_format_search_result(hit, 'bm25') \n",
    "           for hit in response['hits']['hits']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(question, index_name, os_client, bedrock_client, emb_model_id=\"amazon.titan-embed-text-v2:0\", search_model_id='anthropic.claude-3-haiku-20240307-v1:0'):\n",
    "    embedding_response = bedrock_client.invoke_model(\n",
    "        modelId=emb_model_id,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps({\"inputText\": question})\n",
    "    )\n",
    "    embedding = json.loads(embedding_response['body'].read())['embedding']\n",
    "    search_results = search_by_knn(os_client, embedding, index_name, 5)\n",
    "    \n",
    "    docs = \"\"\n",
    "    for result in search_results:\n",
    "        docs += f\"- {result['text']}\\n\\n\"\n",
    "    \n",
    "    messages = [{\n",
    "        'role': 'user',\n",
    "        'content': [{'text': f\"{question}\\n\\nAdditional Information:\\n{docs}\"}]\n",
    "    }]\n",
    "    \n",
    "    system_prompt = \"You are a helpful AI assistant that provides accurate and concise information about given context.\"\n",
    "    \n",
    "    response = bedrock_client.converse(\n",
    "        modelId=search_model_id,\n",
    "        messages=messages,\n",
    "        system=[{'text': system_prompt}],\n",
    "        inferenceConfig={\n",
    "            'maxTokens': 4096\n",
    "        }\n",
    "    )\n",
    "    return response['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'idx-genai-contextual-retriever'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = rag('what is knox?', index_name, os_client, bedrock_client)\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hybrid Search + Reranker를 통한 Rank Fusion 구현하기\n",
    "from reranker_service import RerankerService\n",
    "def hybrid_rag():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 Document를 가져와서 Q&A 데이터 만들기\n",
    "- Tool Use를 통해 Q&A 데이터를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_template = {\n",
    "    \"complex\": \"\"\"\n",
    "        You are an expert at generating practical questions based on given documentation.\n",
    "        Your task is to generate complex, reasoning questions and answers.\n",
    "\n",
    "        Follow these rules:\n",
    "        1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
    "        2. Ensure questions are relevant, concise, preferably under 25 words, and fully answerable with the provided information\n",
    "        3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
    "        4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
    "        5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
    "    \"\"\",\n",
    "    \"simple\": \"\"\"\n",
    "        You are an expert at generating practical questions based on given documentation.\n",
    "        Your task is to create simple, directly answerable questions from the given context.\n",
    "\n",
    "        Follow these rules:\n",
    "        1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
    "        2. Ensure questions are relevant, concise, preferably under 10 words, and fully answerable with the provided information\n",
    "        3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
    "        4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
    "        5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"QuestionAnswerGenerator\",\n",
    "                \"description\": \"Generates questions and answers based on the given context.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"question\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The generated question\"\n",
    "                            },\n",
    "                            \"answer\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The answer to the generated question\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"question\", \"answer\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "context = '\\n\\n'.join([doc.get('content', '') for doc in chunked_document])\n",
    "qa_dataset = []\n",
    "\n",
    "generated_question = {\"simple\": [], \"complex\": []}\n",
    "\n",
    "for i in tqdm(range(4)):\n",
    "    if i % 2 == 0:\n",
    "        question_type = \"complex\"\n",
    "    else:\n",
    "        question_type = \"simple\"\n",
    "\n",
    "    user_template = f\"\"\"\n",
    "    Generate a {question_type} question and its answer based on the following context:\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Use the QuestionAnswerGenerator tool to provide the output.\n",
    "    \"\"\"\n",
    "\n",
    "    sys_prompt = [{\"text\": sys_template[question_type]}]\n",
    "    user_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template}]}]\n",
    "    temperature = 0.0\n",
    "    top_p = 0.5\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "\n",
    "    response = bedrock_client.converse(\n",
    "        modelId='anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "        messages=user_prompt,\n",
    "        toolConfig=tool_config,\n",
    "        system=sys_prompt,\n",
    "        inferenceConfig={\n",
    "            'maxTokens': 4096\n",
    "        }\n",
    "    )\n",
    "\n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    if stop_reason == 'tool_use':\n",
    "        tool_requests = response['output']['message']['content']\n",
    "\n",
    "        for tool_request in [x for x in tool_requests if 'toolUse' in x]:\n",
    "            if tool_request['toolUse']['name'] == 'QuestionAnswerGenerator':\n",
    "                res = tool_request['toolUse']['input']\n",
    "\n",
    "                qa_item = {\n",
    "                    \"question\": tool_request['toolUse']['input']['question'],\n",
    "                    \"ground_truth\": tool_request['toolUse']['input']['answer'],\n",
    "                    \"question_type\": question_type,\n",
    "                    # \"context\": context\n",
    "                }\n",
    "                \n",
    "                qa_dataset.append(qa_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag(qa_dataset[0]['question'], index_name, os_client, bedrock_client)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 Q&A 데이터 셋을 활용하여, Contextual RAG의 답변의 유사도를 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_system_prompt = \"\"\"\n",
    "Evaluate the correctness of the generation on a continuous scale from 0 to 1. A generation can be considered correct (Score: 1) if it includes all the key facts from the ground truth and if every fact presented in the generation is factually supported by the ground truth or common sense.\n",
    "Example:\n",
    "Query: Can eating carrots improve your vision?\n",
    "Answer: Yes, eating carrots significantly improves your vision, especially at night. This is why people who eat lots of carrots never need glasses. Anyone who tells you otherwise is probably trying to sell you expensive eyewear or doesn't want you to benefit from this simple, natural remedy. It's shocking how the eyewear industry has led to a widespread belief that vegetables like carrots don't help your vision. People are so gullible to fall for these money-making schemes.\n",
    "Ground truth: Well, yes and no. Carrots won’t improve your visual acuity if you have less than perfect vision. A diet of carrots won’t give a blind person 20/20 vision. But, the vitamins found in the vegetable can help promote overall eye health. Carrots contain beta-carotene, a substance that the body converts to vitamin A, an important nutrient for eye health. An extreme lack of vitamin A can cause blindness. Vitamin A can prevent the formation of cataracts and macular degeneration, the world’s leading cause of blindness. However, if your vision problems aren’t related to vitamin A, your vision won’t change no matter how many carrots you eat.\n",
    "Score: 0.1\n",
    "Reasoning: While the generation mentions that carrots can improve vision, it fails to outline the reason for this phenomenon and the circumstances under which this is the case. The rest of the response contains misinformation and exaggerations regarding the benefits of eating carrots for vision improvement. It deviates significantly from the more accurate and nuanced explanation provided in the ground truth.\n",
    "\"\"\"\n",
    "\n",
    "eval_tools = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"CorrectressGrader\",\n",
    "                \"description\": \"Evaluate the correctness of the answer on a continuous scale from 0 to 1, and reasoning why the score is. A generation can be considered correct (Score: 1) if it includes all the key facts from the ground truth and if every fact presented in the generation is factually supported by the ground truth.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"score\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The correctress score [0.0, 1.0]\"\n",
    "                            },\n",
    "                            \"reason\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The reason about the score\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"score\", \"reason\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "for question_data in tqdm(qa_dataset):\n",
    "    question = question_data['question']\n",
    "    ground_truth = question_data['ground_truth']\n",
    "\n",
    "    generated = rag(question=question, index_name=index_name, os_client=os_client, bedrock_client=bedrock_client)\n",
    "    \n",
    "    evaluate_user_template = f\"\"\"\n",
    "    Query: {question}\n",
    "    Answer: {generated}\n",
    "    Ground Truth: {ground_truth}\n",
    "    \"\"\"\n",
    "\n",
    "    sys_prompt = [{\"text\": evaluate_system_prompt}]\n",
    "    user_prompt = [{\"role\": \"user\", \"content\": [{\"text\": evaluate_user_template}]}]\n",
    "    temperature = 0.0\n",
    "    top_p = 0.5\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "\n",
    "    response = bedrock_client.converse(\n",
    "        modelId='anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "        messages=user_prompt,\n",
    "        toolConfig=eval_tools,\n",
    "        system=sys_prompt,\n",
    "        inferenceConfig={\n",
    "            'maxTokens': 4096\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    if stop_reason == 'tool_use':\n",
    "        tool_requests = response['output']['message']['content']\n",
    "        \n",
    "\n",
    "        for tool_request in [x for x in tool_requests if 'toolUse' in x]:\n",
    "            if tool_request['toolUse']['name'] == 'CorrectressGrader':\n",
    "                res = tool_request['toolUse']['input']\n",
    "\n",
    "                result = {\n",
    "                     \"question\": question,\n",
    "                     \"question_type\": question_data['question_type'],\n",
    "                     \"generated_answer\": generated,\n",
    "                     \"ground_truth\": ground_truth,\n",
    "                     \"score\": res['score']\n",
    "                }\n",
    "\n",
    "                results.append(result)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
