{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c08ebb-2925-405a-bd46-d4a31704d7be",
   "metadata": {},
   "source": [
    "# Agentic RAG (Self-RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e308905-1b0a-47c3-ba4f-8309491e4b1d",
   "metadata": {},
   "source": [
    "## Setting\n",
    " - Auto Reload\n",
    " - path for utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4396bcf-7ce6-4495-a4be-339ef6d4db40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19742b04-6280-44f5-a43c-22ecfe86123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"../../..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e76837-7a6c-438f-b90e-18cf6a144c7f",
   "metadata": {},
   "source": [
    "## 1. Create Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7cf0c931-0dc7-4fc0-9b1b-a8a3a3396cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock\n",
    "from utils.bedrock import bedrock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29279777-a39b-4400-a935-3a4c4fa27f9d",
   "metadata": {},
   "source": [
    "### ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "- os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "- os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "- os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "- os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07eaa103-e1a1-45ae-b474-cf2f28e5c5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "  Using profile: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "\u001b[32m\n",
      "== FM lists ==\u001b[0m\n",
      "{'Claude-Instant-V1': 'anthropic.claude-instant-v1',\n",
      " 'Claude-V1': 'anthropic.claude-v1',\n",
      " 'Claude-V2': 'anthropic.claude-v2',\n",
      " 'Claude-V2-1': 'anthropic.claude-v2:1',\n",
      " 'Claude-V3-5-Sonnet': 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
      " 'Claude-V3-5-V-2-Sonnet': 'anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
      " 'Claude-V3-5-V-2-Sonnet-CRI': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
      " 'Claude-V3-7-Sonnet-CRI': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
      " 'Claude-V3-Haiku': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
      " 'Claude-V3-Opus': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
      " 'Claude-V3-Sonnet': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
      " 'Cohere-Embeddings-En': 'cohere.embed-english-v3',\n",
      " 'Cohere-Embeddings-Multilingual': 'cohere.embed-multilingual-v3',\n",
      " 'Command': 'cohere.command-text-v14',\n",
      " 'Command-Light': 'cohere.command-light-text-v14',\n",
      " 'Jurassic-2-Mid': 'ai21.j2-mid-v1',\n",
      " 'Jurassic-2-Ultra': 'ai21.j2-ultra-v1',\n",
      " 'Llama2-13b-Chat': 'meta.llama2-13b-chat-v1',\n",
      " 'Nova-Canvas': 'amazon.nova-canvas-v1:0',\n",
      " 'Nova-Lite': 'amazon.nova-lite-v1:0',\n",
      " 'Nova-Micro': 'amazon.nova-micro-v1:0',\n",
      " 'Nova-Pro': 'amazon.nova-pro-v1:0',\n",
      " 'Nova-Pro-CRI': 'us.amazon.nova-pro-v1:0',\n",
      " 'Nova-Reel': 'amazon.nova-reel-v1:0',\n",
      " 'SD-3-5-Large': 'stability.sd3-5-large-v1:0',\n",
      " 'SD-3-Large': 'stability.sd3-large-v1:0',\n",
      " 'SD-Ultra': 'stability.stable-image-ultra-v1:1',\n",
      " 'Titan-Embeddings-G1': 'amazon.titan-embed-text-v1',\n",
      " 'Titan-Text-Embeddings-V2': 'amazon.titan-embed-text-v2:0',\n",
      " 'Titan-Text-G1': 'amazon.titan-text-express-v1',\n",
      " 'Titan-Text-G1-Express': 'amazon.titan-text-express-v1',\n",
      " 'Titan-Text-G1-Light': 'amazon.titan-text-lite-v1',\n",
      " 'Titan-Text-G1-Premier': 'amazon.titan-text-premier-v1:0'}\n"
     ]
    }
   ],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "print (colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint (bedrock_info.get_list_fm_models(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62666d06-64ec-4bfc-89a5-e63397dd1eb0",
   "metadata": {},
   "source": [
    "## 2. LLM, Embedding model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ce6a329-7ae9-4877-ae42-cf40d3b7b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bedrock import bedrock_model\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24c4aa00-2d3d-4900-9099-78fa4d467022",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = bedrock_model(\n",
    "    #model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-7-Sonnet-CRI\"),\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Nova-Pro-CRI\"),\n",
    "    \n",
    "    bedrock_client=boto3_bedrock,\n",
    "    stream=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    inference_config={\n",
    "        'maxTokens': 1024,\n",
    "        'stopSequences': [\"\\n\\nHuman\"],\n",
    "        'temperature': 0.01,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "956b2957-81bb-44c1-81d9-91ee94073ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = BedrockEmbeddings(client=boto3_bedrock, model_id = \"amazon.titan-embed-text-v2:0\")\n",
    "dimension = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffa046-68fe-4b73-b5fa-28c5b81e30c2",
   "metadata": {},
   "source": [
    "## 3. Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf660360-2a3a-49d3-b86b-e16df40714de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils.ssm import parameter_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6d3e950-2d35-4f0b-92c5-50d04cb1edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "region=boto3.Session().region_name\n",
    "pm = parameter_store(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6aeb3-5d85-403e-b9a1-812f22e3be04",
   "metadata": {},
   "source": [
    "### 3.1 Get index name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc71121f-b5fb-45a1-a320-5ec24f777587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_name: idx-genai-document-retriever\n"
     ]
    }
   ],
   "source": [
    "opensearch_domain_endpoint = pm.get_params(key=\"opensearch_domain_endpoint\", enc=False)\n",
    "opensearch_user_id = pm.get_params(key=\"opensearch_user_id\", enc=False)\n",
    "opensearch_user_password = pm.get_params(key=\"opensearch_user_password\", enc=True)\n",
    "\n",
    "http_auth = (opensearch_user_id, opensearch_user_password) # Master username, Master password\n",
    "index_name = opensearch_user_password = pm.get_params(key=\"opensearch_index_name\", enc=True)\n",
    "print (f'index_name: {index_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11575171-c686-4d07-9dc5-0689558314cd",
   "metadata": {},
   "source": [
    "### 3.2 Get opensearch client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3c9bcdd-c328-43a6-b1bf-b15e5cc3aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.opensearch import opensearch_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d059c1bb-eebe-4a66-9e45-8a4a7aeb61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os_client = opensearch_utils.create_aws_opensearch_client(region, opensearch_domain_endpoint, http_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b62c72-085f-44a2-acb5-a1ae31469f7e",
   "metadata": {},
   "source": [
    "## 4. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee775279-df19-46ed-9c4e-5cf5e66b6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pprint\n",
    "from termcolor import colored\n",
    "\n",
    "from textwrap import dedent\n",
    "from utils.bedrock import bedrock_utils\n",
    "from typing import TypedDict, List\n",
    "from src.genai_analysis import llm_call\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from utils.common_utils import retry\n",
    "from utils.agentic_rag import OpenSearchHybridSearchRetriever, show_context_used\n",
    "from botocore.exceptions import ClientError, ConnectionError, ConnectTimeoutError, ReadTimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "516f2977-8090-4e50-97c8-37f8f4fa57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeMeasurement:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.measurements = {}\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def measure(self, section_name):\n",
    "        if self.start_time is None:\n",
    "            raise ValueError(\"start() 메서드를 먼저 호출해야 합니다.\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - self.start_time\n",
    "        self.measurements[section_name] = elapsed_time\n",
    "        self.start_time = end_time  # 다음 구간 측정을 위해 시작 시간 재설정\n",
    "\n",
    "    def reset(self, ):\n",
    "        self.measurements = {}\n",
    "\n",
    "    def print_measurements(self):\n",
    "        for section, elapsed_time in self.measurements.items():\n",
    "            #print(f\"{section}: {elapsed_time:.5f} 초\")\n",
    "            print(colored (f\"\\nelapsed time: {section}: {elapsed_time:.5f} 초\", \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2951512d-6b90-4141-9628-e129afec0901",
   "metadata": {},
   "source": [
    "### 4.1 Agent state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3ccaad4-dcc9-4715-9e04-6a048ede8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    query: str\n",
    "    documents: List[str]\n",
    "    generation: str\n",
    "    query_revision_count: int\n",
    "    enhanced_answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1155f22d-9df4-4d76-9674-a6e3d2848744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_rag():\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.timer = TimeMeasurement()\n",
    "        \n",
    "        self.llm, self.emb = kwargs[\"llm\"], kwargs[\"emb\"]\n",
    "        os_client, index_name = kwargs[\"os_client\"], kwargs[\"index_name\"]\n",
    "        self.query_regeneration_limit = kwargs[\"query_regeneration_limit\"]\n",
    "        self.state = GraphState\n",
    "\n",
    "        self.retriever = self.get_retriever(os_client=os_client, index_name=index_name)        \n",
    "\n",
    "        self._graph_definition()\n",
    "        self.messages = []\n",
    "        self.contexts = []\n",
    "        self.generation = \"\"\n",
    "\n",
    "    def _get_string_from_message(self, message):\n",
    "        return message[\"content\"][0][\"text\"]\n",
    "\n",
    "    def _get_message_from_string(self, role, string, imgs=None):\n",
    "        \n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": []\n",
    "        }\n",
    "        \n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_message = {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'png',\n",
    "                        \"source\": {\"bytes\": img}\n",
    "                    }\n",
    "                }\n",
    "                message[\"content\"].append(img_message)\n",
    "        \n",
    "        message[\"content\"].append({\"text\": dedent(string)})\n",
    "\n",
    "        return message\n",
    "            \n",
    "    def get_messages(self, ):\n",
    "        return self.messages\n",
    "\n",
    "    def get_retriever(self, **kwargs):\n",
    "\n",
    "        os_client, index_name = kwargs[\"os_client\"], kwargs[\"index_name\"]\n",
    "        \n",
    "        opensearch_hybrid_retriever = OpenSearchHybridSearchRetriever(\n",
    "            os_client=os_client,\n",
    "            index_name=index_name,\n",
    "            llm_text=self.llm, # llm for query augmentation in both rag_fusion and HyDE\n",
    "            llm_emb=self.emb, # Used in semantic search based on opensearch \n",
    "        \n",
    "            # hybird-search debugger\n",
    "            hybrid_search_debugger=\"None\", #[semantic, lexical, None]\n",
    "            \n",
    "            # option for lexical\n",
    "            minimum_should_match=0,\n",
    "            filter=[],\n",
    "        \n",
    "            # option for search\n",
    "            fusion_algorithm=\"RRF\", # [\"RRF\", \"simple_weighted\"], rank fusion 방식 정의\n",
    "            ensemble_weights=[.51, .49], # [for semantic, for lexical], Semantic, Lexical search 결과에 대한 최종 반영 비율 정의\n",
    "            reranker=True, # enable reranker with reranker model\n",
    "            reranker_endpoint_name=\"cohere-reranker-3-5\", # endpoint name for reranking model\n",
    "            parent_document=False, # enable parent document\n",
    "                        \n",
    "            # option for complex documents consisting of text, table and image\n",
    "            complex_doc=False,\n",
    "            \n",
    "            # option for async search\n",
    "            async_mode=True,\n",
    "        \n",
    "            # option for output\n",
    "            k=3, # 최종 Document 수 정의\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        return opensearch_hybrid_retriever\n",
    "            \n",
    "    @retry(total_try_cnt=5, sleep_in_sec=10, retryable_exceptions=(ClientError,))\n",
    "    def rag_chain(self, **kwargs):\n",
    "\n",
    "        query, documents = kwargs[\"query\"], kwargs[\"documents\"]\n",
    "        messages = []\n",
    "        \n",
    "        self.llm.stream = True\n",
    "        llm_caller = llm_call(llm=self.llm, verbose=False)\n",
    "\n",
    "        system_prompts = dedent(\n",
    "            '''\n",
    "            You are an assistant for question-answering tasks.\n",
    "            Use the following pieces of retrieved documents to answer the question.\n",
    "            Skip the preamble and go straight into the answer.\n",
    "            If you don't know the answer, just say that you don't know.\n",
    "            '''\n",
    "        )\n",
    "  \n",
    "        system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)\n",
    "        user_prompts = dedent(\n",
    "            '''\n",
    "            Documents: <documents>{documents}</documents>\n",
    "            User question: <question>{query}</question>\n",
    "            '''\n",
    "        )        \n",
    "\n",
    "        reformat_documents = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "        context = {\"documents\": reformat_documents, \"query\": query}\n",
    "        user_prompts = user_prompts.format(**context)\n",
    "                   \n",
    "        message = self._get_message_from_string(role=\"user\", string=user_prompts)\n",
    "        self.messages.append(message)\n",
    "        resp, ai_message = llm_caller.invoke(messages=self.messages, system_prompts=system_prompts)\n",
    "        self.messages.append(ai_message)\n",
    "\n",
    "        return resp[\"text\"]\n",
    "        \n",
    "    def _graph_definition(self, **kwargs):\n",
    "\n",
    "        def retrieve(state):\n",
    "            \"\"\"\n",
    "            Retrieve documents\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                state (dict): New key added to state, documents, that contains retrieved documents\n",
    "            \"\"\"\n",
    "            print(\"---RETRIEVE---\")\n",
    "            query = state[\"query\"]\n",
    "        \n",
    "            # Retrieval\n",
    "            if self.retriever.complex_doc: documents, tables, images = self.retriever.invoke(query)\n",
    "            else: documents = self.retriever.invoke(query)\n",
    "            \n",
    "            self.contexts = documents\n",
    "\n",
    "            return {\"documents\": documents, \"query\": query}\n",
    "\n",
    "        def grade_documents(state):\n",
    "\n",
    "            \"\"\"\n",
    "            Determines whether the retrieved documents are relevant to the question.\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                state (dict): Updates documents key with only filtered relevant documents\n",
    "            \"\"\"\n",
    "        \n",
    "            print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "            query, documents = state[\"query\"], state[\"documents\"]\n",
    "        \n",
    "            # Score each doc\n",
    "            filtered_docs = []\n",
    "            for idx, d in enumerate(documents):\n",
    "                grade_result = self.retrieval_grader(query=query, document=d.page_content)\n",
    "                \n",
    "                if grade_result[\"binary_score\"] == \"yes\":\n",
    "                    print(f\"  ---Document {idx}: GRADE: DOCUMENT RELEVANT---\")\n",
    "                    filtered_docs.append(d)\n",
    "                else:\n",
    "                    print(f\"  ---Document {idx}: GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "                    continue\n",
    "                    \n",
    "            self.contexts = filtered_docs\n",
    "            return {\"documents\": filtered_docs, \"query\": query}\n",
    "\n",
    "        def decide_to_generate(state):\n",
    "            \"\"\"\n",
    "            Determines whether to generate an answer, or re-generate a question.\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                str: Binary decision for next node to call\n",
    "            \"\"\"\n",
    "        \n",
    "            print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "            filtered_documents = state[\"documents\"]\n",
    "            query_revision_count = state.get(\"query_revision_count\", 0)\n",
    "\n",
    "            if (not filtered_documents) and query_revision_count < self.query_regeneration_limit:\n",
    "            #if len(filtered_documents) < 7 and query_revision_count < self.query_regeneration_limit:\n",
    "                # All documents have been filtered check_relevance\n",
    "                # We will re-generate a new query\n",
    "                print(\"  ---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
    "                return \"transform_query\"\n",
    "            else:\n",
    "                # We have relevant documents, so generate answer\n",
    "                print(\"  ---DECISION: GENERATE---\")\n",
    "                return \"generate\"\n",
    "\n",
    "        def transform_query(state):\n",
    "            \"\"\"\n",
    "            Transform the query to produce a better question.\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                state (dict): Updates question key with a re-phrased question\n",
    "            \"\"\"\n",
    "        \n",
    "            print(\"---TRANSFORM QUERY---\")\n",
    "            query, documents = state[\"query\"], state[\"documents\"]\n",
    "            query_revision_count = state.get(\"query_revision_count\", 0)\n",
    "            \n",
    "            # Re-write question\n",
    "            results = self.question_rewriter(query=query)\n",
    "            better_question = results[\"re-written\"]\n",
    "            query_revision_count += 1\n",
    "            \n",
    "            return {\"documents\": documents, \"query\": better_question, \"query_revision_count\": query_revision_count}\n",
    "        \n",
    "        def generate(state):\n",
    "            \"\"\"\n",
    "            Generate answer\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                state (dict): New key added to state, generation, that contains LLM generation\n",
    "            \"\"\"\n",
    "            print(\"---GENERATE---\")\n",
    "            query, documents = state[\"query\"], state[\"documents\"]\n",
    "            feedback = state.get(\"feedback\", None)\n",
    "            \n",
    "            # RbAG generation\n",
    "            generation = self.rag_chain(query=query, documents=documents)\n",
    "            self.generation = generation\n",
    "            \n",
    "            query_revision_count = state.get(\"query_revision_count\", 0)\n",
    "            query_revision_count = 0\n",
    "            \n",
    "            return {\"documents\": documents, \"query\": query, \"generation\": generation, \"query_revision_count\": query_revision_count}\n",
    "\n",
    "        def grade_generation_v_documents_and_question(state):\n",
    "            \"\"\"\n",
    "            Determines whether the generation is grounded in the document and answers question.\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                str: Decision for next node to call\n",
    "            \"\"\"\n",
    "        \n",
    "            print(\"\\n---CHECK HALLUCINATIONS: 컨택스트 기반 답변인지 확인---\")\n",
    "            query, documents, generation = state[\"query\"], state[\"documents\"], state[\"generation\"]\n",
    "            hallucination_results = self.hallucination_grader(documents=documents, generation=generation)\n",
    "\n",
    "            # Check hallucination\n",
    "            if hallucination_results[\"binary_score\"] == \"yes\":\n",
    "                print(\"  ---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "                # Check question-answering\n",
    "                print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "                answer_grade_results = self.answer_grader(query=query, generation=generation)\n",
    "                if answer_grade_results[\"binary_score\"] == \"yes\":\n",
    "                    print(\"  ---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "                    return \"useful\"\n",
    "                else:\n",
    "                    print(\"  ---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "                    return \"not useful\"\n",
    "            else:\n",
    "                pprint(\"  ---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "                return \"not supported\"    \n",
    "                \n",
    "        def enhance_answer(state):\n",
    "            \"\"\"\n",
    "            Enhance answer quality \n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                state (dict): New key added to state, generation, that contains LLM generation\n",
    "            \"\"\"\n",
    "            print(\"---ENHANCE ANSWER---\")\n",
    "            query, documents, generation = state[\"query\"], state[\"documents\"], state[\"generation\"]\n",
    "            \n",
    "            answer_enhancer_results = self.answer_enhancer(query=query, generation=generation)\n",
    "            suggested_query = answer_enhancer_results[\"suggested_query\"]\n",
    " \n",
    "            # Retrieval\n",
    "            enhanced_documents = []\n",
    "            for suggested_query in suggested_query:\n",
    "                \n",
    "                if self.retriever.complex_doc: documents, tables, images = self.retriever.invoke(query)    \n",
    "                else: documents = self.retriever.invoke(query)\n",
    "                    \n",
    "                enhanced_documents.extend(documents)\n",
    "\n",
    "            enhanced_answer = self.enhanced_answer_chain(\n",
    "                query=query, generation=generation, suggested_query=suggested_query, enhanced_documents=enhanced_documents\n",
    "            )\n",
    "            self.enhanced_contexts = enhanced_documents\n",
    "            self.enhanced_generation = enhanced_answer\n",
    "\n",
    "            return {\"enhanced_answer\": enhanced_answer}\n",
    "            \n",
    "        # langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "        workflow = StateGraph(self.state)\n",
    "\n",
    "        # Todo 를 작성합니다.\n",
    "        workflow.add_node(\"retrieve\", retrieve)\n",
    "        #workflow.add_node(\"grade_documents\", grade_documents)\n",
    "        #workflow.add_node(\"transform_query\", transform_query)\n",
    "        workflow.add_node(\"generate\", generate)\n",
    "        #workflow.add_node(\"enhance_answer\", enhance_answer)\n",
    "        enhance_answer\n",
    "        \n",
    "        workflow.add_edge(START, \"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"generate\")\n",
    "        workflow.add_edge(\"generate\", END)\n",
    "\n",
    "        # 기록을 위한 메모리 저장소를 설정합니다.\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        # 그래프를 컴파일합니다.\n",
    "        self.app = workflow.compile(checkpointer=memory)        \n",
    "        self.config = RunnableConfig(recursion_limit=100, configurable={\"thread_id\": \"self_rag\"})\n",
    "\n",
    "    def invoke(self, **kwargs):\n",
    "        \n",
    "        inputs = self.state(query=kwargs[\"query\"])\n",
    "        \n",
    "        # app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "        for output in self.app.stream(inputs, self.config):\n",
    "            # 출력된 결과에서 키와 값을 순회합니다.\n",
    "            for key, value in output.items():\n",
    "                # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "                print(f\"\\nOutput from node '{key}':\")\n",
    "                #print(\"---\")\n",
    "                \n",
    "                #print (self.messages)\n",
    "                # 출력 값을 예쁘게 출력합니다.\n",
    "                #pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "                \n",
    "            # 각 출력 사이에 구분선을 추가합니다.\n",
    "            print(\"\\n-------------------------------------------------------\\n\")\n",
    "\n",
    "        print(\"---Answer---\")\n",
    "        print(colored(self.generation, \"green\"))\n",
    "        print(\"---Contexts---\")\n",
    "        show_context_used(self.contexts)\n",
    "            \n",
    "    def show_graph(self, ):\n",
    "        \n",
    "        from IPython.display import Image, display\n",
    "\n",
    "        try:\n",
    "            # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. \n",
    "            display(Image(self.app.get_graph(xray=True).draw_mermaid_png()))  \n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84a088e0-2fdf-4779-9af4-7a9c187e1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_client = self_rag(\n",
    "    llm=llm,\n",
    "    emb=emb,\n",
    "    os_client=os_client,\n",
    "    index_name=index_name,\n",
    "    query_regeneration_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9c078a7-2415-435c-98bd-d52c8f59ec39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_client.show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60f1d7fc-0cfa-4af7-884d-0a61bc441f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrag_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvidio max size?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 335\u001b[0m, in \u001b[0;36mself_rag.invoke\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate(query\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39mstream(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# 출력된 결과에서 키와 값을 순회합니다.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;66;03m# 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput from node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/agentic_rag/lib/python3.10/site-packages/langgraph/pregel/__init__.py:2024\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2018\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   2020\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   2021\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 2024\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2025\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   2026\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2027\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2028\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2029\u001b[0m         ):\n\u001b[1;32m   2030\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/agentic_rag/lib/python3.10/site-packages/langgraph/pregel/runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/envs/agentic_rag/lib/python3.10/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/anaconda3/envs/agentic_rag/lib/python3.10/site-packages/langgraph/utils/runnable.py:546\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    543\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/anaconda3/envs/agentic_rag/lib/python3.10/site-packages/langgraph/utils/runnable.py:310\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 310\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[62], line 137\u001b[0m, in \u001b[0;36mself_rag._graph_definition.<locals>.retrieve\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Retrieval\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39mcomplex_doc: documents, tables, images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontexts \u001b[38;5;241m=\u001b[39m documents\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: documents, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query}\n",
      "File \u001b[0;32m~/anaconda3/envs/agentic_rag/lib/python3.10/site-packages/langchain_core/retrievers.py:259\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 259\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/utils/agentic_rag.py:1088\u001b[0m, in \u001b[0;36mOpenSearchHybridSearchRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_relevant_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;124;03m    It can be called by \"retriever.invoke\" statements\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m     search_hybrid_result \u001b[38;5;241m=\u001b[39m \u001b[43mretriever_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_hybrid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mos_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminimum_should_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum_should_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfusion_algorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfusion_algorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# [\"RRF\", \"simple_weighted\"]\u001b[39;49;00m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensemble_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensemble_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 시멘트 서치에 가중치 0.5 , 키워드 서치 가중치 0.5 부여.\u001b[39;49;00m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43masync_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masync_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreranker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreranker_endpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreranker_endpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrag_fusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag_fusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_augmentation_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_augmentation_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_transformation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag_fusion_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag_fusion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyde_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyde_query\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyde\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_document\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomplex_doc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex_doc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhybrid_search_debugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhybrid_search_debugger\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m#self._reset_search_params()\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m search_hybrid_result\n",
      "File \u001b[0;32m~/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/utils/agentic_rag.py:887\u001b[0m, in \u001b[0;36mretriever_utils.search_hybrid\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reranker:\n\u001b[1;32m    886\u001b[0m     reranker_endpoint_name \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreranker_endpoint_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 887\u001b[0m     similar_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rerank_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimilar_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreranker_endpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreranker_endpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;66;03m#print (\"2-similar_docs\")\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;66;03m#for i, doc in enumerate(similar_docs): print (i, doc)\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parent_document:\n",
      "File \u001b[0;32m~/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/utils/agentic_rag.py:607\u001b[0m, in \u001b[0;36mretriever_utils.get_rerank_docs\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m         exceed_info\u001b[38;5;241m.\u001b[39mappend([idx, exceed_flag, \u001b[38;5;28mlen\u001b[39m(rerank_queries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreranker_endpoint_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcohere-reranker-3-5\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 607\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mrerank_queries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# Extract documents from text_pair of all inputs\u001b[39;00m\n\u001b[1;32m    609\u001b[0m     documents \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_pair\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m rerank_queries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_pair\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m item]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rag_client.invoke(\n",
    "    query=\"vidio max size?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145e60d-28b1-4057-939f-f7c2c917007b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe510dd2-d1c0-4795-b4fe-cf4cbb2682ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d76b4-1992-4c9d-9c2b-f656cb480279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_agentic_rag",
   "language": "python",
   "name": "conda_agentic_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
