{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c08ebb-2925-405a-bd46-d4a31704d7be",
   "metadata": {},
   "source": [
    "# Agentic RAG (Self-RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e308905-1b0a-47c3-ba4f-8309491e4b1d",
   "metadata": {},
   "source": [
    "## Setting\n",
    " - Auto Reload\n",
    " - path for utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4396bcf-7ce6-4495-a4be-339ef6d4db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19742b04-6280-44f5-a43c-22ecfe86123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"../../..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e76837-7a6c-438f-b90e-18cf6a144c7f",
   "metadata": {},
   "source": [
    "## 1. Create Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf0c931-0dc7-4fc0-9b1b-a8a3a3396cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock\n",
    "from utils.bedrock import bedrock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29279777-a39b-4400-a935-3a4c4fa27f9d",
   "metadata": {},
   "source": [
    "### ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "- os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "- os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "- os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "- os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07eaa103-e1a1-45ae-b474-cf2f28e5c5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "  Using profile: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "\u001b[32m\n",
      "== FM lists ==\u001b[0m\n",
      "{'Claude-Instant-V1': 'anthropic.claude-instant-v1',\n",
      " 'Claude-V1': 'anthropic.claude-v1',\n",
      " 'Claude-V2': 'anthropic.claude-v2',\n",
      " 'Claude-V2-1': 'anthropic.claude-v2:1',\n",
      " 'Claude-V3-5-Sonnet': 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
      " 'Claude-V3-5-V-2-Sonnet': 'anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
      " 'Claude-V3-5-V-2-Sonnet-CRI': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
      " 'Claude-V3-7-Sonnet-CRI': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
      " 'Claude-V3-Haiku': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
      " 'Claude-V3-Opus': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
      " 'Claude-V3-Sonnet': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
      " 'Cohere-Embeddings-En': 'cohere.embed-english-v3',\n",
      " 'Cohere-Embeddings-Multilingual': 'cohere.embed-multilingual-v3',\n",
      " 'Command': 'cohere.command-text-v14',\n",
      " 'Command-Light': 'cohere.command-light-text-v14',\n",
      " 'Jurassic-2-Mid': 'ai21.j2-mid-v1',\n",
      " 'Jurassic-2-Ultra': 'ai21.j2-ultra-v1',\n",
      " 'Llama2-13b-Chat': 'meta.llama2-13b-chat-v1',\n",
      " 'Nova-Canvas': 'amazon.nova-canvas-v1:0',\n",
      " 'Nova-Lite': 'amazon.nova-lite-v1:0',\n",
      " 'Nova-Micro': 'amazon.nova-micro-v1:0',\n",
      " 'Nova-Pro': 'amazon.nova-pro-v1:0',\n",
      " 'Nova-Pro-CRI': 'us.amazon.nova-pro-v1:0',\n",
      " 'Nova-Reel': 'amazon.nova-reel-v1:0',\n",
      " 'SD-3-5-Large': 'stability.sd3-5-large-v1:0',\n",
      " 'SD-3-Large': 'stability.sd3-large-v1:0',\n",
      " 'SD-Ultra': 'stability.stable-image-ultra-v1:1',\n",
      " 'Titan-Embeddings-G1': 'amazon.titan-embed-text-v1',\n",
      " 'Titan-Text-Embeddings-V2': 'amazon.titan-embed-text-v2:0',\n",
      " 'Titan-Text-G1': 'amazon.titan-text-express-v1',\n",
      " 'Titan-Text-G1-Express': 'amazon.titan-text-express-v1',\n",
      " 'Titan-Text-G1-Light': 'amazon.titan-text-lite-v1',\n",
      " 'Titan-Text-G1-Premier': 'amazon.titan-text-premier-v1:0'}\n"
     ]
    }
   ],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "print (colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint (bedrock_info.get_list_fm_models(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62666d06-64ec-4bfc-89a5-e63397dd1eb0",
   "metadata": {},
   "source": [
    "## 2. LLM, Embedding model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ce6a329-7ae9-4877-ae42-cf40d3b7b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bedrock import bedrock_model\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c4aa00-2d3d-4900-9099-78fa4d467022",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = bedrock_model(\n",
    "    #model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-7-Sonnet-CRI\"),\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Nova-Pro-CRI\"),\n",
    "    \n",
    "    bedrock_client=boto3_bedrock,\n",
    "    stream=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    inference_config={\n",
    "        'maxTokens': 1024,\n",
    "        'stopSequences': [\"\\n\\nHuman\"],\n",
    "        'temperature': 0.01,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956b2957-81bb-44c1-81d9-91ee94073ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = BedrockEmbeddings(client=boto3_bedrock, model_id = \"amazon.titan-embed-text-v2:0\")\n",
    "dimension = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffa046-68fe-4b73-b5fa-28c5b81e30c2",
   "metadata": {},
   "source": [
    "## 3. Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf660360-2a3a-49d3-b86b-e16df40714de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils.ssm import parameter_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d3e950-2d35-4f0b-92c5-50d04cb1edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "region=boto3.Session().region_name\n",
    "pm = parameter_store(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6aeb3-5d85-403e-b9a1-812f22e3be04",
   "metadata": {},
   "source": [
    "### 3.1 Get index name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc71121f-b5fb-45a1-a320-5ec24f777587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_name: default-index\n"
     ]
    }
   ],
   "source": [
    "opensearch_domain_endpoint = pm.get_params(key=\"opensearch_domain_endpoint\", enc=False)\n",
    "opensearch_user_id = pm.get_params(key=\"opensearch_user_id\", enc=False)\n",
    "opensearch_user_password = pm.get_params(key=\"opensearch_user_password\", enc=True)\n",
    "\n",
    "http_auth = (opensearch_user_id, opensearch_user_password) # Master username, Master password\n",
    "index_name = opensearch_user_password = pm.get_params(key=\"opensearch_index_name\", enc=True)\n",
    "print (f'index_name: {index_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11575171-c686-4d07-9dc5-0689558314cd",
   "metadata": {},
   "source": [
    "### 3.2 Get opensearch client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c9bcdd-c328-43a6-b1bf-b15e5cc3aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.opensearch import opensearch_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d059c1bb-eebe-4a66-9e45-8a4a7aeb61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os_client = opensearch_utils.create_aws_opensearch_client(region, opensearch_domain_endpoint, http_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b62c72-085f-44a2-acb5-a1ae31469f7e",
   "metadata": {},
   "source": [
    "## 4. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee775279-df19-46ed-9c4e-5cf5e66b6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pprint\n",
    "from termcolor import colored\n",
    "\n",
    "from textwrap import dedent\n",
    "from utils.bedrock import bedrock_utils\n",
    "from typing import TypedDict, List\n",
    "from src.genai_analysis import llm_call\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from utils.common_utils import retry\n",
    "from utils.agentic_rag import OpenSearchHybridSearchRetriever, show_context_used\n",
    "from botocore.exceptions import ClientError, ConnectionError, ConnectTimeoutError, ReadTimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "516f2977-8090-4e50-97c8-37f8f4fa57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeMeasurement:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.measurements = {}\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def measure(self, section_name):\n",
    "        if self.start_time is None:\n",
    "            raise ValueError(\"start() 메서드를 먼저 호출해야 합니다.\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - self.start_time\n",
    "        self.measurements[section_name] = elapsed_time\n",
    "        self.start_time = end_time  # 다음 구간 측정을 위해 시작 시간 재설정\n",
    "\n",
    "    def reset(self, ):\n",
    "        self.measurements = {}\n",
    "\n",
    "    def print_measurements(self):\n",
    "        for section, elapsed_time in self.measurements.items():\n",
    "            #print(f\"{section}: {elapsed_time:.5f} 초\")\n",
    "            print(colored (f\"\\nelapsed time: {section}: {elapsed_time:.5f} 초\", \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2951512d-6b90-4141-9628-e129afec0901",
   "metadata": {},
   "source": [
    "### 4.1 Agent state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3ccaad4-dcc9-4715-9e04-6a048ede8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    query: str\n",
    "    documents: List[str]\n",
    "    generation: str\n",
    "    query_revision_count: int\n",
    "    enhanced_answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1155f22d-9df4-4d76-9674-a6e3d2848744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_rag():\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.timer = TimeMeasurement()\n",
    "        \n",
    "        self.llm, self.emb = kwargs[\"llm\"], kwargs[\"emb\"]\n",
    "        os_client, index_name = kwargs[\"os_client\"], kwargs[\"index_name\"]\n",
    "        self.query_regeneration_limit = kwargs[\"query_regeneration_limit\"]\n",
    "        self.state = GraphState\n",
    "\n",
    "        self.retriever = self.get_retriever(os_client=os_client, index_name=index_name)        \n",
    "\n",
    "        self._graph_definition()\n",
    "        self.messages = []\n",
    "        self.contexts = []\n",
    "        self.generation = \"\"\n",
    "\n",
    "    def _get_string_from_message(self, message):\n",
    "        return message[\"content\"][0][\"text\"]\n",
    "\n",
    "    def _get_message_from_string(self, role, string, imgs=None):\n",
    "        \n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": []\n",
    "        }\n",
    "        \n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_message = {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'png',\n",
    "                        \"source\": {\"bytes\": img}\n",
    "                    }\n",
    "                }\n",
    "                message[\"content\"].append(img_message)\n",
    "        \n",
    "        message[\"content\"].append({\"text\": dedent(string)})\n",
    "\n",
    "        return message\n",
    "            \n",
    "    def get_messages(self, ):\n",
    "        return self.messages\n",
    "\n",
    "    def get_retriever(self, **kwargs):\n",
    "\n",
    "        os_client, index_name = kwargs[\"os_client\"], kwargs[\"index_name\"]\n",
    "        \n",
    "        opensearch_hybrid_retriever = OpenSearchHybridSearchRetriever(\n",
    "            os_client=os_client,\n",
    "            index_name=index_name,\n",
    "            llm_text=self.llm, # llm for query augmentation in both rag_fusion and HyDE\n",
    "            llm_emb=self.emb, # Used in semantic search based on opensearch \n",
    "        \n",
    "            # hybird-search debugger\n",
    "            hybrid_search_debugger=\"None\", #[semantic, lexical, None]\n",
    "            \n",
    "            # option for lexical\n",
    "            minimum_should_match=0,\n",
    "            filter=[],\n",
    "        \n",
    "            # option for search\n",
    "            fusion_algorithm=\"RRF\", # [\"RRF\", \"simple_weighted\"], rank fusion 방식 정의\n",
    "            ensemble_weights=[.51, .49], # [for semantic, for lexical], Semantic, Lexical search 결과에 대한 최종 반영 비율 정의\n",
    "            reranker=True, # enable reranker with reranker model\n",
    "            reranker_endpoint_name=\"cohere-reranker-3-5\", # endpoint name for reranking model\n",
    "            parent_document=False, # enable parent document\n",
    "                        \n",
    "            # option for complex documents consisting of text, table and image\n",
    "            complex_doc=False,\n",
    "            \n",
    "            # option for async search\n",
    "            async_mode=True,\n",
    "        \n",
    "            # option for output\n",
    "            k=3, # 최종 Document 수 정의\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        return opensearch_hybrid_retriever\n",
    "            \n",
    "    @retry(total_try_cnt=5, sleep_in_sec=10, retryable_exceptions=(ClientError,))\n",
    "    def rag_chain(self, **kwargs):\n",
    "\n",
    "        query, documents = kwargs[\"query\"], kwargs[\"documents\"]\n",
    "        messages = []\n",
    "        \n",
    "        self.llm.stream = True\n",
    "        llm_caller = llm_call(llm=self.llm, verbose=False)\n",
    "\n",
    "        system_prompts = dedent(\n",
    "            '''\n",
    "            You are an assistant for question-answering tasks.\n",
    "            Use the following pieces of retrieved documents to answer the question.\n",
    "            Skip the preamble and go straight into the answer.\n",
    "            If you don't know the answer, just say that you don't know.\n",
    "            '''\n",
    "        )\n",
    "  \n",
    "        system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)\n",
    "        user_prompts = dedent(\n",
    "            '''\n",
    "            Documents: <documents>{documents}</documents>\n",
    "            User question: <question>{query}</question>\n",
    "            '''\n",
    "        )        \n",
    "\n",
    "        reformat_documents = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "        context = {\"documents\": reformat_documents, \"query\": query}\n",
    "        user_prompts = user_prompts.format(**context)\n",
    "                   \n",
    "        message = self._get_message_from_string(role=\"user\", string=user_prompts)\n",
    "        self.messages.append(message)\n",
    "        resp, ai_message = llm_caller.invoke(messages=self.messages, system_prompts=system_prompts)\n",
    "        self.messages.append(ai_message)\n",
    "\n",
    "        return resp[\"text\"]\n",
    "        \n",
    "    def _graph_definition(self, **kwargs):\n",
    "\n",
    "        def retrieve(state):\n",
    "            \"\"\"\n",
    "            Retrieve documents\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                state (dict): New key added to state, documents, that contains retrieved documents\n",
    "            \"\"\"\n",
    "            print(\"---RETRIEVE---\")\n",
    "            query = state[\"query\"]\n",
    "        \n",
    "            # Retrieval\n",
    "            if self.retriever.complex_doc: documents, tables, images = self.retriever.invoke(query)\n",
    "            else: documents = self.retriever.invoke(query)\n",
    "            \n",
    "            self.contexts = documents\n",
    "\n",
    "            return {\"documents\": documents, \"query\": query}\n",
    "\n",
    "        def grade_documents(state):\n",
    "\n",
    "            \"\"\"\n",
    "            Determines whether the retrieved documents are relevant to the question.\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                state (dict): Updates documents key with only filtered relevant documents\n",
    "            \"\"\"\n",
    "        \n",
    "            print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "            query, documents = state[\"query\"], state[\"documents\"]\n",
    "        \n",
    "            # Score each doc\n",
    "            filtered_docs = []\n",
    "            for idx, d in enumerate(documents):\n",
    "                grade_result = self.retrieval_grader(query=query, document=d.page_content)\n",
    "                \n",
    "                if grade_result[\"binary_score\"] == \"yes\":\n",
    "                    print(f\"  ---Document {idx}: GRADE: DOCUMENT RELEVANT---\")\n",
    "                    filtered_docs.append(d)\n",
    "                else:\n",
    "                    print(f\"  ---Document {idx}: GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "                    continue\n",
    "                    \n",
    "            self.contexts = filtered_docs\n",
    "            return {\"documents\": filtered_docs, \"query\": query}\n",
    "\n",
    "        def decide_to_generate(state):\n",
    "            \"\"\"\n",
    "            Determines whether to generate an answer, or re-generate a question.\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                str: Binary decision for next node to call\n",
    "            \"\"\"\n",
    "        \n",
    "            print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "            filtered_documents = state[\"documents\"]\n",
    "            query_revision_count = state.get(\"query_revision_count\", 0)\n",
    "\n",
    "            if (not filtered_documents) and query_revision_count < self.query_regeneration_limit:\n",
    "            #if len(filtered_documents) < 7 and query_revision_count < self.query_regeneration_limit:\n",
    "                # All documents have been filtered check_relevance\n",
    "                # We will re-generate a new query\n",
    "                print(\"  ---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
    "                return \"transform_query\"\n",
    "            else:\n",
    "                # We have relevant documents, so generate answer\n",
    "                print(\"  ---DECISION: GENERATE---\")\n",
    "                return \"generate\"\n",
    "\n",
    "        def transform_query(state):\n",
    "            \"\"\"\n",
    "            Transform the query to produce a better question.\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                state (dict): Updates question key with a re-phrased question\n",
    "            \"\"\"\n",
    "        \n",
    "            print(\"---TRANSFORM QUERY---\")\n",
    "            query, documents = state[\"query\"], state[\"documents\"]\n",
    "            query_revision_count = state.get(\"query_revision_count\", 0)\n",
    "            \n",
    "            # Re-write question\n",
    "            results = self.question_rewriter(query=query)\n",
    "            better_question = results[\"re-written\"]\n",
    "            query_revision_count += 1\n",
    "            \n",
    "            return {\"documents\": documents, \"query\": better_question, \"query_revision_count\": query_revision_count}\n",
    "        \n",
    "        def generate(state):\n",
    "            \"\"\"\n",
    "            Generate answer\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                state (dict): New key added to state, generation, that contains LLM generation\n",
    "            \"\"\"\n",
    "            print(\"---GENERATE---\")\n",
    "            query, documents = state[\"query\"], state[\"documents\"]\n",
    "            feedback = state.get(\"feedback\", None)\n",
    "            \n",
    "            # RbAG generation\n",
    "            generation = self.rag_chain(query=query, documents=documents)\n",
    "            self.generation = generation\n",
    "            \n",
    "            query_revision_count = state.get(\"query_revision_count\", 0)\n",
    "            query_revision_count = 0\n",
    "            \n",
    "            return {\"documents\": documents, \"query\": query, \"generation\": generation, \"query_revision_count\": query_revision_count}\n",
    "\n",
    "        def grade_generation_v_documents_and_question(state):\n",
    "            \"\"\"\n",
    "            Determines whether the generation is grounded in the document and answers question.\n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                str: Decision for next node to call\n",
    "            \"\"\"\n",
    "        \n",
    "            print(\"\\n---CHECK HALLUCINATIONS: 컨택스트 기반 답변인지 확인---\")\n",
    "            query, documents, generation = state[\"query\"], state[\"documents\"], state[\"generation\"]\n",
    "            hallucination_results = self.hallucination_grader(documents=documents, generation=generation)\n",
    "\n",
    "            # Check hallucination\n",
    "            if hallucination_results[\"binary_score\"] == \"yes\":\n",
    "                print(\"  ---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "                # Check question-answering\n",
    "                print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "                answer_grade_results = self.answer_grader(query=query, generation=generation)\n",
    "                if answer_grade_results[\"binary_score\"] == \"yes\":\n",
    "                    print(\"  ---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "                    return \"useful\"\n",
    "                else:\n",
    "                    print(\"  ---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "                    return \"not useful\"\n",
    "            else:\n",
    "                pprint(\"  ---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "                return \"not supported\"    \n",
    "                \n",
    "        def enhance_answer(state):\n",
    "            \"\"\"\n",
    "            Enhance answer quality \n",
    "        \n",
    "            Args:\n",
    "                state (dict): The current graph state\n",
    "        \n",
    "            Returns:\n",
    "                state (dict): New key added to state, generation, that contains LLM generation\n",
    "            \"\"\"\n",
    "            print(\"---ENHANCE ANSWER---\")\n",
    "            query, documents, generation = state[\"query\"], state[\"documents\"], state[\"generation\"]\n",
    "            \n",
    "            answer_enhancer_results = self.answer_enhancer(query=query, generation=generation)\n",
    "            suggested_query = answer_enhancer_results[\"suggested_query\"]\n",
    " \n",
    "            # Retrieval\n",
    "            enhanced_documents = []\n",
    "            for suggested_query in suggested_query:\n",
    "                \n",
    "                if self.retriever.complex_doc: documents, tables, images = self.retriever.invoke(query)    \n",
    "                else: documents = self.retriever.invoke(query)\n",
    "                    \n",
    "                enhanced_documents.extend(documents)\n",
    "\n",
    "            enhanced_answer = self.enhanced_answer_chain(\n",
    "                query=query, generation=generation, suggested_query=suggested_query, enhanced_documents=enhanced_documents\n",
    "            )\n",
    "            self.enhanced_contexts = enhanced_documents\n",
    "            self.enhanced_generation = enhanced_answer\n",
    "\n",
    "            return {\"enhanced_answer\": enhanced_answer}\n",
    "            \n",
    "        # langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "        workflow = StateGraph(self.state)\n",
    "\n",
    "        # Todo 를 작성합니다.\n",
    "        workflow.add_node(\"retrieve\", retrieve)\n",
    "        #workflow.add_node(\"grade_documents\", grade_documents)\n",
    "        #workflow.add_node(\"transform_query\", transform_query)\n",
    "        workflow.add_node(\"generate\", generate)\n",
    "        #workflow.add_node(\"enhance_answer\", enhance_answer)\n",
    "        enhance_answer\n",
    "        \n",
    "        workflow.add_edge(START, \"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"generate\")\n",
    "        workflow.add_edge(\"generate\", END)\n",
    "\n",
    "        # 기록을 위한 메모리 저장소를 설정합니다.\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        # 그래프를 컴파일합니다.\n",
    "        self.app = workflow.compile(checkpointer=memory)        \n",
    "        self.config = RunnableConfig(recursion_limit=100, configurable={\"thread_id\": \"self_rag\"})\n",
    "\n",
    "    def invoke(self, **kwargs):\n",
    "        \n",
    "        inputs = self.state(query=kwargs[\"query\"])\n",
    "        \n",
    "        # app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "        for output in self.app.stream(inputs, self.config):\n",
    "            # 출력된 결과에서 키와 값을 순회합니다.\n",
    "            for key, value in output.items():\n",
    "                # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "                print(f\"\\nOutput from node '{key}':\")\n",
    "                #print(\"---\")\n",
    "                \n",
    "                #print (self.messages)\n",
    "                # 출력 값을 예쁘게 출력합니다.\n",
    "                #pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "                \n",
    "            # 각 출력 사이에 구분선을 추가합니다.\n",
    "            print(\"\\n-------------------------------------------------------\\n\")\n",
    "\n",
    "        print(\"---Answer---\")\n",
    "        print(colored(self.generation, \"green\"))\n",
    "        print(\"---Contexts---\")\n",
    "        show_context_used(self.contexts)\n",
    "            \n",
    "    def show_graph(self, ):\n",
    "        \n",
    "        from IPython.display import Image, display\n",
    "\n",
    "        try:\n",
    "            # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. \n",
    "            display(Image(self.app.get_graph(xray=True).draw_mermaid_png()))  \n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84a088e0-2fdf-4779-9af4-7a9c187e1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_client = self_rag(\n",
    "    llm=llm,\n",
    "    emb=emb,\n",
    "    os_client=os_client,\n",
    "    index_name=index_name,\n",
    "    query_regeneration_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9c078a7-2415-435c-98bd-d52c8f59ec39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_client.show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60f1d7fc-0cfa-4af7-884d-0a61bc441f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/agentic_rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "INFO:retry-bedrock-invocation:trying rag_chain() [1/5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output from node 'retrieve':\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "---GENERATE---\n",
      "The maximum file size limit for content has been increased to 1.0 GB in version 20.08. However, the specific maximum size for video files is not explicitly mentioned in the provided documents. For detailed information on video file size limits, you may need to refer to additional documentation or contact Samsung support."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:retry-bedrock-invocation:in retry(), rag_chain() returned 'The maximum file size limit for content has been increased to 1.0 GB in version 20.08. However, the specific maximum size for video files is not explicitly mentioned in the provided documents. For detailed information on video file size limits, you may need to refer to additional documentation or contact Samsung support.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output from node 'generate':\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "---Answer---\n",
      "\u001b[32mThe maximum file size limit for content has been increased to 1.0 GB in version 20.08. However, the specific maximum size for video files is not explicitly mentioned in the provided documents. For detailed information on video file size limits, you may need to refer to additional documentation or contact Samsung support.\u001b[0m\n",
      "---Contexts---\n",
      "\n",
      "-----------------------------------------------\n",
      "1. Chunk: 1013 Characters\n",
      "-----------------------------------------------\n",
      ". #Other Kiosk enhancements The Knox Manage agent must be updated to the latest version for these\n",
      "new features to take effect. Bookmark in Multiple App Kiosk mode now supports file uploads and\n",
      "downloads. You can now set Kiosk Browser and Secure Browser to be automatically updated through\n",
      "profile policies. When adding or modifying a profile, you can find these settings in the following\n",
      "locations: Secure Browser: Android Enterprise > Secure Browser > App Auto Update. Kiosk Browser:\n",
      "Android Enterprise or Android Legacy > Kiosk > Kiosk app settings (select Kiosk Browser ) > App Auto\n",
      "Update. The loading status is now shown on the progress bar: Content #Increased maximum file size\n",
      "limit Previously, the maximum content size was 300 MB. In 20.08, this limit was increased to 1.0 GB.\n",
      "Knox E-FOTA license usage with Knox Suite In addition to the Knox E-FOTA admin portal, you can now\n",
      "view the Knox E-FOTA license usage in the following locations as well: License widget in the Knox\n",
      "Manage admin portal's Dashboard\n",
      "metadata:\n",
      " {'source': 'all_processed_data.json', 'seq_num': 1044, 'title': 'Knox Manage 20.08 release notes',\n",
      "'url': 'https://docs.samsungknox.com/admin/knox-manage/release-notes/20-08', 'project': 'KM',\n",
      "'last_updated': '2023-07-26', 'family_tree': 'child', 'parent_id':\n",
      "'fc87cca9-2bac-4f64-93fc-c079dd1be214', 'id': 'e732ee7f-a565-4b31-8065-dd75f7bb5ae3',\n",
      "'orig_elements': ''}\n",
      "\n",
      "-----------------------------------------------\n",
      "2. Chunk: 1008 Characters\n",
      "-----------------------------------------------\n",
      ". Invalid APK version name. Invalid APK version name. Please change the version name and try\n",
      "uploading again. 4090001 File too large. Max 4000 devices allowed per upload. Try splitting the\n",
      "device upload list to reduce the size of .csv you are uploading. Note that there is a max of 4000\n",
      "devices allowed per upload. 4150000 File type is not supported. The file you are trying to upload is\n",
      "not supported. Knox Configure only accepts .csv files. App name cannot contain special characters.\n",
      "Rename the app so it does not contain any special or forbidden characters. App version is invalid.\n",
      "An app you are trying to upload is invalid. Verify that the version you're attempting to upload is\n",
      "correct. Try to re-upload the app again. CSV file is required. A system error occurred. Verify that\n",
      "you've attached a .csv file, try to re-attach the .csv and upload it again. If the problem persists,\n",
      "contact Samsung. 4040105 No CAPTCHA code entered. No CAPTCHA code was entered. Carefully enter the\n",
      "CAPTCHA code and submit\n",
      "metadata:\n",
      " {'source': 'all_processed_data.json', 'seq_num': 433, 'title': 'Portal-side errors', 'url':\n",
      "'https://docs.samsungknox.com/admin/knox-configure/knox-configure-wearables/troubleshoot/portal-\n",
      "side-errors', 'project': 'KC', 'last_updated': '2023-07-26', 'family_tree': 'child', 'parent_id':\n",
      "'d2fc61bd-cb15-4382-bcfc-40c795acd91a', 'id': 'afc3fcf0-c703-4add-a11a-69dc5b68e82a',\n",
      "'orig_elements': ''}\n",
      "\n",
      "-----------------------------------------------\n",
      "3. Chunk: 579 Characters\n",
      "-----------------------------------------------\n",
      ". To delete a video file, click next to the name of the uploaded video file. Note The device control\n",
      "command must be transferred to the device to apply a video to it. &gt; Session timeout Allows the\n",
      "use of the session timeout feature for the Kiosk Browser. If the user does not use the device for a\n",
      "set time, the device deletes user information, such as the cache and cookies, in the device Kiosk\n",
      "Browser and goes to the main page URL: Apply &mdash; Enables the session timeout feature for the\n",
      "browser. &gt;&gt; Time (sec) Set the session timeout in seconds for the Kiosk Browser\n",
      "metadata:\n",
      " {'source': 'all_processed_data.json', 'seq_num': 893, 'title': 'Android Enterprise policies',\n",
      "'url': 'https://docs.samsungknox.com/admin/knox-manage/configure/profile/configure-profile-\n",
      "policies/android-enterprise-policies', 'project': 'KM', 'last_updated': '2023-09-06', 'family_tree':\n",
      "'child', 'parent_id': '0d5a948e-dfe6-4863-91bd-2db06c09248e', 'id': 'c1e8288a-03cf-4360-83bb-\n",
      "eb81c18a2ce7', 'orig_elements': ''}\n"
     ]
    }
   ],
   "source": [
    "rag_client.invoke(\n",
    "    query=\"vidio max size?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145e60d-28b1-4057-939f-f7c2c917007b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe510dd2-d1c0-4795-b4fe-cf4cbb2682ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d76b4-1992-4c9d-9c2b-f656cb480279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_agentic_rag",
   "language": "python",
   "name": "conda_agentic_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
