{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b739ff32",
   "metadata": {},
   "source": [
    "# Bring Your Own Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a00f3e",
   "metadata": {},
   "source": [
    "# 1. Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f0a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b84ed0ee",
   "metadata": {},
   "source": [
    "# 2. Bring Your Own Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90968061",
   "metadata": {},
   "source": [
    "## 2.1. Graph definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae498bbc",
   "metadata": {},
   "source": [
    "### 2.1.1. Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e93a2",
   "metadata": {},
   "source": [
    "- coordinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2378eaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/prompts/coordinator.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/prompts/coordinator.md\n",
    "\n",
    "---\n",
    "CURRENT_TIME: {CURRENT_TIME}\n",
    "---\n",
    "\n",
    "You are Bedrock-Manus, a friendly AI assistant developed by the Bedrock-Manus TF team (Dongjin Jang, Ph.D., AWS AIML Specialist SA).\n",
    "You specialize in handling greetings and small talk, while directing complex tasks to a specialized planner.\n",
    "\n",
    "# Details\n",
    "\n",
    "Your primary responsibilities are:\n",
    "- Introducing yourself as Bedrock-Manus when appropriate\n",
    "- Responding to greetings (e.g., \"hello\", \"hi\", \"good morning\")\n",
    "- Engaging in small talk (e.g., weather, time, how are you)\n",
    "- Politely rejecting inappropriate or harmful requests\n",
    "- Directing all other questions to the planner\n",
    "\n",
    "# Execution Rules\n",
    "\n",
    "- If the input is a greeting, small talk, or poses a security/moral risk:\n",
    "  - Respond in plain text with an appropriate greeting or polite rejection\n",
    "- For all other inputs:\n",
    "  - Indicate that you need to pass this request to the planner by responding with:\n",
    "  \"handoff_to_planner: I'll need to consult our planning system for this request.\"\n",
    "\n",
    "# Notes\n",
    "\n",
    "- Always identify yourself as Bedrock-Manus when relevant\n",
    "- Keep responses friendly but professional\n",
    "- Don't attempt to solve complex problems or create plans yourself\n",
    "- Always direct non-greeting queries to the planner\n",
    "- Maintain the same language as the user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f599160",
   "metadata": {},
   "source": [
    "- coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9254a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/prompts/coder.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/prompts/coder.md\n",
    "\n",
    "---\n",
    "CURRENT_TIME: {CURRENT_TIME}\n",
    "USER_REQUEST: {USER_REQUEST}\n",
    "FULL_PLAN: {FULL_PLAN}\n",
    "---\n",
    "\n",
    "As a professional software engineer proficient in both Python and bash scripting, your mission is to analyze requirements, implement efficient solutions using Python and/or bash, and provide clear documentation of your methodology and results.\n",
    "\n",
    "**[CRITICAL]** YOU ARE STRICTLY FORBIDDEN FROM: Creating PDF files (.pdf), HTML report files (.html), generating final reports or summaries, using weasyprint/pandoc or any report generation tools, or creating any document that resembles a final report. PDF/HTML/Report generation is EXCLUSIVELY the Reporter agent's job - NEVER YOURS! Execute ONLY the subtasks assigned to \"Coder\" in FULL_PLAN. Do NOT attempt to fulfill the entire USER_REQUEST - focus solely on your assigned coding/analysis tasks.\n",
    "\n",
    "<steps>\n",
    "1. Requirements Analysis: Carefully review the task description to understand the goals, constraints, and expected outcomes.\n",
    "2. Solution Planning: \n",
    "   - [CRITICAL] Always implement code according to the provided FULL_PLAN (Coder part only)\n",
    "   - Determine whether the task requires Python, bash, or a combination of both\n",
    "   - Outline the steps needed to achieve the solution\n",
    "3. Solution Implementation:\n",
    "   - Use Python for data analysis, algorithm implementation, or problem-solving.\n",
    "   - Use bash for executing shell commands, managing system resources, or querying the environment.\n",
    "   - Seamlessly integrate Python and bash if the task requires both.\n",
    "   - Use `print(...)` in Python to display results or debug values.\n",
    "4. Solution Testing: Verify that the implementation meets the requirements and handles edge cases.\n",
    "5. Methodology Documentation: Provide a clear explanation of your approach, including reasons for choices and assumptions made.\n",
    "6. Results Presentation: Clearly display final output and intermediate results as needed.\n",
    "   - Clearly display final output and all intermediate results\n",
    "   - Include all intermediate process results without omissions\n",
    "   - [CRITICAL] Document all calculated values, generated data, and transformation results with explanations at each intermediate step\n",
    "   - [CRITICAL] **IMMEDIATE RECORDING AFTER EACH ANALYSIS**: Every time you complete ONE individual analysis step from the FULL_PLAN, you MUST immediately save the results to './artifacts/all_results.txt' before moving to the next analysis\n",
    "   - [REQUIRED] Do NOT wait until all analyses are complete - record each analysis IMMEDIATELY upon completion to preserve rich details\n",
    "   - Create the './artifacts' directory if no files exist there, or append to existing files\n",
    "   - Record important observations discovered during the process\n",
    "   - This prevents loss of detailed insights and ensures comprehensive documentation\n",
    "</steps>\n",
    "\n",
    "<data_analysis_requirements>\n",
    "- [CRITICAL] Always explicitly read data files before any analysis:\n",
    "  1. For any data analysis, ALWAYS include file reading step FIRST\n",
    "  2. NEVER assume a DataFrame ('df' or any other variable) exists without defining it\n",
    "  3. ALWAYS use the appropriate reading function based on file type:\n",
    "     - CSV: df = pd.read_csv('path/to/file.csv')\n",
    "     - Parquet: df = pd.read_parquet('path/to/file.parquet')\n",
    "     - Excel: df = pd.read_excel('path/to/file.xlsx')\n",
    "     - JSON: df = pd.read_json('path/to/file.json')\n",
    "  4. Include error handling for file operations when appropriate\n",
    "\n",
    "- [REQUIRED] Data Analysis Checklist (verify before executing any code):\n",
    "  - [ ] All necessary libraries imported (pandas, numpy, etc.)\n",
    "  - [ ] File path clearly defined (as variable or direct parameter)\n",
    "  - [ ] Appropriate file reading function used based on file format\n",
    "  - [ ] DataFrame explicitly created with reading function\n",
    "\n",
    "- [EXAMPLE] Correct approach:\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define file path\n",
    "file_path = 'data.csv'  # Always define the file path\n",
    "\n",
    "# Explicitly read the file and create DataFrame\n",
    "df = pd.read_csv(file_path)  # MUST define the DataFrame\n",
    "\n",
    "# Now perform analysis\n",
    "print(\"Data overview:\")\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "```\n",
    "</data_analysis_requirements>\n",
    " \n",
    "<matplotlib_requirements>\n",
    "- [CRITICAL] Must declare one of these matplotlib styles when you use `matplotlib`:\n",
    "    - plt.style.use(['ipynb', 'use_mathtext','colors5-light']) - Notebook-friendly style with mathematical typography and a light color scheme with 5 distinct colors\n",
    "    - plt.style.use('ggplot') - Clean style suitable for academic publications\n",
    "    - plt.style.use('seaborn-v0_8') - Modern, professional visualizations\n",
    "    - plt.style.use('fivethirtyeight') - Web/media-friendly style\n",
    "- [CRITICAL] Must import lovelyplots at the beginning of visualization code:\n",
    "    - import lovelyplots  # Don't omit this import\n",
    "- **[CRITICAL] Use Korean font settings (REQUIRED for Korean text display):**\n",
    "  ```python\n",
    "  # Korean font setup - ALWAYS include this code\n",
    "  plt.rc('font', family='NanumGothic')\n",
    "  plt.rcParams['axes.unicode_minus'] = False  # Fix minus sign display\n",
    "  \n",
    "  # Alternative fonts if NanumGothic fails:\n",
    "  # plt.rc('font', family=['NanumBarunGothic', 'NanumGothic', 'Malgun Gothic', 'DejaVu Sans'])\n",
    "  ```\n",
    "- Apply grid lines to all graphs (alpha=0.3)\n",
    "- DPI: 150 (high resolution)\n",
    "- Set font sizes: title: 14-16, axis labels: 12-14, tick labels: 8-10, legend: 8-10\n",
    "- Use subplot() when necessary to compare related data\n",
    "- [EXAMPLE] is below:\n",
    "\n",
    "```python\n",
    "# Correct visualization setup - ALWAYS USE THIS PATTERN\n",
    "import matplotlib.pyplot as plt\n",
    "import lovelyplots  # [CRITICAL] ALWAYS import this\n",
    "\n",
    "# [CRITICAL] ALWAYS set a style\n",
    "plt.style.use(['ipynb', 'use_mathtext','colors5-light'])  # Choose one from the required styles\n",
    "\n",
    "# Set Korean font and other required parameters\n",
    "plt.rc('font', family='NanumGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Fix minus sign display\n",
    "plt.figure(figsize=(10, 6), dpi=150)\n",
    "\n",
    "# Rest of visualization code\n",
    "```\n",
    "</matplotlib_requirements>\n",
    "\n",
    "<cumulative_result_storage_requirements>\n",
    "- [CRITICAL] **EXECUTE THIS CODE AFTER EACH INDIVIDUAL ANALYSIS COMPLETION**: Every time you finish ONE analysis step from the FULL_PLAN, immediately run the result storage code below.\n",
    "- [REQUIRED] **DO NOT BATCH MULTIPLE ANALYSES**: Save each analysis individually and immediately to preserve detailed insights.\n",
    "- Always accumulate and save to './artifacts/all_results.txt'. Do not create other files.\n",
    "- Do not omit `import pandas as pd`.\n",
    "- [CRITICAL] Always include key insights and discoveries for Reporter agent to use.\n",
    "- **WORKFLOW**: Complete Analysis 1 → Save to all_results.txt → Complete Analysis 2 → Save to all_results.txt → etc.\n",
    "- Example is below:\n",
    "\n",
    "```python\n",
    "# Result accumulation storage section\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create artifacts directory\n",
    "os.makedirs('./artifacts', exist_ok=True)\n",
    "\n",
    "# Result file path\n",
    "results_file = './artifacts/all_results.txt'\n",
    "backup_file = './artifacts/all_results_backup_{{}}.txt'.format(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "# Current analysis parameters - modify these values according to your actual analysis\n",
    "stage_name = \"Analysis_Stage_Name\"\n",
    "result_description = \"\"\"Description of analysis results\n",
    "Also add actual analyzed data (statistics, distributions, ratios, etc.)\n",
    "Can be written over multiple lines.\n",
    "Include result values.\"\"\"\n",
    "\n",
    "# [CRITICAL] Key findings and insights from analysis - ALWAYS include this section\n",
    "key_insights = \"\"\"\n",
    "[DISCOVERY & INSIGHTS]:\n",
    "- Discovery 1: What patterns or anomalies did you find in the data?\n",
    "- Insight 1: What does this discovery mean for the business/domain?\n",
    "- Discovery 2: Any unexpected correlations or trends?\n",
    "- Insight 2: How does this impact decision-making or understanding?\n",
    "- Methodology insight: Why did you choose this analysis approach?\n",
    "- Business implication: What actions or further investigations are recommended?\n",
    "\"\"\"\n",
    "\n",
    "artifact_files = [\n",
    "    ## Always use paths that include './artifacts/' \n",
    "    [\"./artifacts/generated_file1.extension\", \"File description\"],\n",
    "    [\"./artifacts/generated_file2.extension\", \"File description\"]\n",
    "]\n",
    "\n",
    "# Direct generation of result text without using a function\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "current_result_text = \"\"\"\n",
    "==================================================\n",
    "## Analysis Stage: {{0}}\n",
    "## Execution Time: {{1}}\n",
    "--------------------------------------------------\n",
    "Result Description: \n",
    "{{2}}\n",
    "--------------------------------------------------\n",
    "Key Findings & Insights:\n",
    "{{3}}\n",
    "\"\"\".format(stage_name, current_time, result_description, key_insights)\n",
    "\n",
    "if artifact_files:\n",
    "    current_result_text += \"--------------------------------------------------\\nGenerated Files:\\n\"\n",
    "    for file_path, file_desc in artifact_files:\n",
    "        current_result_text += \"- {{}} : {{}}\\n\".format(file_path, file_desc)\n",
    "\n",
    "current_result_text += \"==================================================\\n\"\n",
    "\n",
    "# Backup existing result file and accumulate results\n",
    "if os.path.exists(results_file):\n",
    "    try:\n",
    "        # Check file size\n",
    "        if os.path.getsize(results_file) > 0:\n",
    "            # Create backup\n",
    "            with open(results_file, 'r', encoding='utf-8') as f_src:\n",
    "                with open(backup_file, 'w', encoding='utf-8') as f_dst:\n",
    "                    f_dst.write(f_src.read())\n",
    "            print(\"Created backup of existing results file: {{}}\".format(backup_file))\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred during file backup: {{}}\".format(e))\n",
    "\n",
    "# Add new results (accumulate to existing file)\n",
    "try:\n",
    "    with open(results_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(current_result_text)\n",
    "    print(\"Results successfully saved.\")\n",
    "except Exception as e:\n",
    "    print(\"Error occurred while saving results: {{}}\".format(e))\n",
    "    # Try saving to temporary file in case of error\n",
    "    try:\n",
    "        temp_file = './artifacts/result_emergency_{{}}.txt'.format(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "        with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(current_result_text)\n",
    "        print(\"Results saved to temporary file: {{}}\".format(temp_file))\n",
    "    except Exception as e2:\n",
    "        print(\"Temporary file save also failed: {{}}\".format(e2))\n",
    "```\n",
    "</cumulative_result_storage_requirements>\n",
    "\n",
    "<code_saving_requirements>\n",
    "- [CRITICAL] When the user requests \"write code\", \"generate code\", or similar:\n",
    "  - All generated code files must be saved to the \"./artifacts/\" directory\n",
    "  - Always include code to check if the directory exists and create it if necessary\n",
    "  - Always use clearly defined file paths that start with \"./artifacts/\"\n",
    "  - Always include the actual code to save the file\n",
    "\n",
    "- Example:\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Create artifacts directory\n",
    "os.makedirs(\"./artifacts\", exist_ok=True)\n",
    "\n",
    "# Save code file\n",
    "with open(\"./artifacts/solution.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "# Generated code content here\n",
    "def main():\n",
    "    print(\"Hello, world!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\")\n",
    "\n",
    "print(\"Code has been saved to ./artifacts/solution.py\")\n",
    "```\n",
    "</code_saving_requirements>\n",
    "\n",
    "<note>\n",
    "\n",
    "- Always ensure that your solution is efficient and follows best practices.\n",
    "- Handle edge cases gracefully, such as empty files or missing inputs.\n",
    "- Use comments to improve readability and maintainability of your code.\n",
    "- If you want to see the output of a value, you must output it with print(...).\n",
    "- Always use Python for mathematical operations.\n",
    "- [CRITICAL] Do not generate Reports or PDF files. Reports and PDF generation are STRICTLY the responsibility of the Reporter agent.\n",
    "- [FORBIDDEN] Never create final reports, summary documents, or PDF files even if it seems logical or the plan is unclear.\n",
    "- Always use yfinance for financial market data:\n",
    "  - Use yf.download() to get historical data\n",
    "  - Access company information with Ticker objects\n",
    "  - Use appropriate date ranges for data retrieval\n",
    "- **Package Installation (REQUIRED)**: \n",
    "  - [CRITICAL] When you need additional Python packages that are not already available, use UV package manager:\n",
    "    ```bash\n",
    "    uv add package-name\n",
    "    ```\n",
    "  - [EXAMPLES] Common package installations:\n",
    "    ```bash\n",
    "    uv add pandas numpy matplotlib seaborn scikit-learn\n",
    "    ```\n",
    "  - [FORBIDDEN] **NEVER use pip install** - always use `uv add` for package installation\n",
    "  - [IMPORTANT] After installing with `uv add`, the package is immediately available in subsequent Python code executions\n",
    "- Pre-installed packages in current environment:\n",
    "  - pandas for data manipulation\n",
    "  - numpy for numerical operations\n",
    "  - matplotlib, seaborn for visualization\n",
    "  - scikit-learn for machine learning\n",
    "  - boto3 for AWS services\n",
    "- Save all generated files and images to the ./artifacts directory:\n",
    "  - Create this directory if it doesn't exist with os.makedirs(\"./artifacts\", exist_ok=True)\n",
    "  - Use this path when writing files, e.g., plt.savefig(\"./artifacts/plot.png\")\n",
    "  - Specify this path when generating output that needs to be saved to disk\n",
    "- [CRITICAL] Always write code according to the plan defined in the FULL_PLAN (Coder part only) variable\n",
    "- [CRITICAL] Always analyze the entire USER_REQUEST to detect the main language and respond in that language. For mixed languages, use whichever language is dominant in the request.\n",
    "</note>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640bad77",
   "metadata": {},
   "source": [
    "### 2.1.2 Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262f4283",
   "metadata": {},
   "source": [
    "- python_repl_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d654174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/tools/python_repl_tool.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/tools/python_repl_tool.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import subprocess\n",
    "from typing import Any, Annotated\n",
    "from strands.types.tools import ToolResult, ToolUse\n",
    "from src.tools.decorators import log_io\n",
    "\n",
    "# Observability\n",
    "from opentelemetry import trace\n",
    "from src.utils.agentcore_observability import add_span_event\n",
    "\n",
    "# Simple logger setup\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "TOOL_SPEC = {\n",
    "    \"name\": \"python_repl_tool\",\n",
    "    \"description\": \"Use this to execute python code and do data analysis or calculation. If you want to see the output of a value, you should print it out with `print(...)`. This is visible to the user.\",\n",
    "    \"inputSchema\": {\n",
    "        \"json\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"code\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The python code to execute to do further analysis or calculation.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"code\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "class Colors:\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "class PythonREPL:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def run(self, command):\n",
    "        try:\n",
    "            # 입력된 명령어 실행\n",
    "            result = subprocess.run(\n",
    "                [sys.executable, \"-c\", command],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=600  # 타임아웃 설정\n",
    "            )\n",
    "            # 결과 반환\n",
    "            if result.returncode == 0:\n",
    "                return result.stdout\n",
    "            else:\n",
    "                return f\"Error: {result.stderr}\"\n",
    "        except Exception as e:\n",
    "            return f\"Exception: {str(e)}\"\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "@log_io\n",
    "def handle_python_repl_tool(code: Annotated[str, \"The python code to execute to do further analysis or calculation.\"]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Use this to execute python code and do data analysis or calculation. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\n",
    "    \"\"\"\n",
    "    tracer = trace.get_tracer(\n",
    "        instrumenting_module_name=os.getenv(\"TRACER_MODULE_NAME\", \"insight_extractor_agent\"),\n",
    "        instrumenting_library_version=os.getenv(\"TRACER_LIBRARY_VERSION\", \"1.0.0\")\n",
    "    )\n",
    "    with tracer.start_as_current_span(\"python_repl_tool\") as span:\n",
    "        print()  # Add newline before log\n",
    "        logger.info(f\"{Colors.GREEN}===== Executing Python code ====={Colors.END}\")\n",
    "        try:\n",
    "            result = repl.run(code)\n",
    "        except BaseException as e:\n",
    "            error_msg = f\"Failed to execute. Error: {repr(e)}\"\n",
    "            logger.debug(f\"{Colors.RED}Failed to execute. Error: {repr(e)}{Colors.END}\")\n",
    "            \n",
    "            # Add Event\n",
    "            add_span_event(span, \"code\", {\"code\": str(code)})\n",
    "            add_span_event(span, \"result\", {\"response\": repr(e)})\n",
    "            \n",
    "            return error_msg\n",
    "        \n",
    "        #result_str = f\"Successfully executed:\\n||```python\\n{code}\\n```\\n||Stdout: {result}\"\n",
    "        result_str = f\"Successfully executed:\\n||{code}||{result}\"\n",
    "        logger.info(f\"{Colors.GREEN}===== Code execution successful ====={Colors.END}\")\n",
    "\n",
    "        # Add Event\n",
    "        add_span_event(span, \"code\", {\"code\": str(code)})\n",
    "        add_span_event(span, \"result\", {\"response\": str(result)})\n",
    "        \n",
    "        return result_str\n",
    "\n",
    "# Function name must match tool name\n",
    "def python_repl_tool(tool: ToolUse, **kwargs: Any) -> ToolResult:\n",
    "    tool_use_id = tool[\"toolUseId\"]\n",
    "    code = tool[\"input\"][\"code\"]\n",
    "    \n",
    "    # Use the existing handle_python_repl_tool function\n",
    "    result = handle_python_repl_tool(code)\n",
    "    \n",
    "    # Check if execution was successful based on the result string\n",
    "    if \"Failed to execute\" in result:\n",
    "        return {\n",
    "            \"toolUseId\": tool_use_id,\n",
    "            \"status\": \"error\",\n",
    "            \"content\": [{\"text\": result}]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"toolUseId\": tool_use_id,\n",
    "            \"status\": \"success\",\n",
    "            \"content\": [{\"text\": result}]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c77d17",
   "metadata": {},
   "source": [
    "- coder_agent_tool (agent as a tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf96002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/tools/coder_agent_tool.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/tools/coder_agent_tool.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import asyncio\n",
    "from typing import Any, Annotated\n",
    "from strands.types.tools import ToolResult, ToolUse\n",
    "from src.utils.strands_sdk_utils import strands_utils\n",
    "from src.prompts.template import apply_prompt_template\n",
    "from src.utils.common_utils import get_message_from_string\n",
    "from src.tools import python_repl_tool, bash_tool\n",
    "\n",
    "# Observability\n",
    "from opentelemetry import trace\n",
    "from src.utils.agentcore_observability import add_span_event\n",
    "\n",
    "# Simple logger setup\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "TOOL_SPEC = {\n",
    "    \"name\": \"coder_agent_tool\",\n",
    "    \"description\": \"Execute Python code and bash commands using a specialized coder agent. This tool provides access to a coder agent that can execute Python code for data analysis and calculations, run bash commands for system operations, and handle complex programming tasks.\",\n",
    "    \"inputSchema\": {\n",
    "        \"json\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"task\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The coding task or question that needs to be executed by the coder agent.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"task\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "RESPONSE_FORMAT = \"Response from {}:\\n\\n<response>\\n{}\\n</response>\\n\\n*Please execute the next step.*\"\n",
    "FULL_PLAN_FORMAT = \"Here is full plan :\\n\\n<full_plan>\\n{}\\n</full_plan>\\n\\n*Please consider this to select the next step.*\"\n",
    "CLUES_FORMAT = \"Here is clues from {}:\\n\\n<clues>\\n{}\\n</clues>\\n\\n\"\n",
    "\n",
    "class Colors:\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "def handle_coder_agent_tool(task: Annotated[str, \"The coding task or question that needs to be executed by the coder agent.\"]):\n",
    "    \"\"\"\n",
    "    Execute Python code and bash commands using a specialized coder agent.\n",
    "    \n",
    "    This tool provides access to a coder agent that can:\n",
    "    - Execute Python code for data analysis and calculations\n",
    "    - Run bash commands for system operations\n",
    "    - Handle complex programming tasks\n",
    "    \n",
    "    Args:\n",
    "        task: The coding task or question that needs to be executed\n",
    "        \n",
    "    Returns:\n",
    "        The result of the code execution or analysis\n",
    "    \"\"\"\n",
    "    tracer = trace.get_tracer(\n",
    "        instrumenting_module_name=os.getenv(\"TRACER_MODULE_NAME\", \"insight_extractor_agent\"),\n",
    "        instrumenting_library_version=os.getenv(\"TRACER_LIBRARY_VERSION\", \"1.0.0\")\n",
    "    )\n",
    "    with tracer.start_as_current_span(\"coder_agent_tool\") as span:\n",
    "        print()  # Add newline before log\n",
    "        logger.info(f\"\\n{Colors.GREEN}Coder Agent Tool starting task{Colors.END}\")\n",
    "        \n",
    "        # Try to extract shared state from global storage\n",
    "        from src.graph.nodes import _global_node_states\n",
    "        shared_state = _global_node_states.get('shared', None)\n",
    "        \n",
    "        if not shared_state:\n",
    "            logger.warning(\"No shared state found\")\n",
    "            return \"Error: No shared state available\" \n",
    "                        \n",
    "        request_prompt, full_plan = shared_state.get(\"request_prompt\", \"\"), shared_state.get(\"full_plan\", \"\")\n",
    "        clues, messages = shared_state.get(\"clues\", \"\"), shared_state.get(\"messages\", [])\n",
    "        \n",
    "        # Create coder agent with specialized tools using consistent pattern\n",
    "        coder_agent = strands_utils.get_agent(\n",
    "            agent_name=\"coder\",\n",
    "            system_prompts=apply_prompt_template(prompt_name=\"coder\", prompt_context={\"USER_REQUEST\": request_prompt, \"FULL_PLAN\": full_plan}),\n",
    "            agent_type=\"claude-sonnet-3-7\", # claude-sonnet-3-5-v-2, claude-sonnet-3-7\n",
    "            enable_reasoning=False,\n",
    "            tools=[python_repl_tool, bash_tool],\n",
    "            #tools=[code_interpreter_tool],\n",
    "            streaming=True  # Enable streaming for consistency\n",
    "        )\n",
    "        \n",
    "        # Prepare message with context if available\n",
    "        message = '\\n\\n'.join([messages[-1][\"content\"][-1][\"text\"], clues])\n",
    "        \n",
    "        # Process streaming response and collect text in one pass\n",
    "        async def process_coder_stream():\n",
    "            full_text = \"\"\n",
    "            async for event in strands_utils.process_streaming_response_yield(\n",
    "                coder_agent, message, agent_name=\"coder\", source=\"coder_tool\"\n",
    "            ):\n",
    "                if event.get(\"event_type\") == \"text_chunk\": full_text += event.get(\"data\", \"\")\n",
    "            return {\"text\": full_text}\n",
    "        \n",
    "        response = asyncio.run(process_coder_stream())\n",
    "        result_text = response['text']\n",
    "        \n",
    "        # Update clues\n",
    "        clues = '\\n\\n'.join([clues, CLUES_FORMAT.format(\"coder\", response[\"text\"])])\n",
    "        \n",
    "        # Update history\n",
    "        history = shared_state.get(\"history\", [])\n",
    "        history.append({\"agent\":\"coder\", \"message\": response[\"text\"]})\n",
    "        \n",
    "        # Update shared state\n",
    "        shared_state['messages'] = [get_message_from_string(role=\"user\", string=RESPONSE_FORMAT.format(\"coder\", response[\"text\"]), imgs=[])]\n",
    "        shared_state['clues'] = clues\n",
    "        shared_state['history'] = history\n",
    "        \n",
    "        logger.info(f\"\\n{Colors.GREEN}Coder Agent Tool completed successfully{Colors.END}\")\n",
    "\n",
    "        # Add Event\n",
    "        add_span_event(span, \"input_message\", {\"message\": str(message)})\n",
    "        add_span_event(span, \"response\", {\"response\": str(response[\"text\"])})\n",
    "\n",
    "        return result_text\n",
    "\n",
    "# Function name must match tool name\n",
    "def coder_agent_tool(tool: ToolUse, **_kwargs: Any) -> ToolResult:\n",
    "    tool_use_id = tool[\"toolUseId\"]\n",
    "    task = tool[\"input\"][\"task\"]\n",
    "    \n",
    "    # Use the existing handle_coder_agent_tool function\n",
    "    result = handle_coder_agent_tool(task)\n",
    "    \n",
    "    # Check if execution was successful based on the result string\n",
    "    if \"Error in coder agent tool\" in result:\n",
    "        return {\n",
    "            \"toolUseId\": tool_use_id,\n",
    "            \"status\": \"error\",\n",
    "            \"content\": [{\"text\": result}]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"toolUseId\": tool_use_id,\n",
    "            \"status\": \"success\",\n",
    "            \"content\": [{\"text\": result}]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed7e97",
   "metadata": {},
   "source": [
    "### 2.1.3 Nodes\n",
    "- 각 노드별 로직을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208e8ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/graph/nodes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/graph/nodes.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from src.utils.strands_sdk_utils import strands_utils\n",
    "from src.prompts.template import apply_prompt_template\n",
    "from src.utils.common_utils import get_message_from_string\n",
    "\n",
    "# Tools\n",
    "from src.tools import coder_agent_tool, reporter_agent_tool, tracker_agent_tool\n",
    "\n",
    "# Observability\n",
    "from opentelemetry import trace\n",
    "from src.utils.agentcore_observability import add_span_event\n",
    "\n",
    "# Simple logger setup\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class Colors:\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "def log_node_start(node_name: str):\n",
    "    \"\"\"Log the start of a node execution.\"\"\"\n",
    "    print()  # Add newline before log\n",
    "    logger.info(f\"{Colors.GREEN}===== {node_name} started ====={Colors.END}\")\n",
    "\n",
    "def log_node_complete(node_name: str):\n",
    "    \"\"\"Log the completion of a node.\"\"\"\n",
    "    print()  # Add newline before log\n",
    "    logger.info(f\"{Colors.GREEN}===== {node_name} completed ====={Colors.END}\")\n",
    "\n",
    "# Global state storage for sharing between nodes\n",
    "_global_node_states = {}\n",
    "\n",
    "RESPONSE_FORMAT = \"Response from {}:\\n\\n<response>\\n{}\\n</response>\\n\\n*Please execute the next step.*\"\n",
    "FULL_PLAN_FORMAT = \"Here is full plan :\\n\\n<full_plan>\\n{}\\n</full_plan>\\n\\n*Please consider this to select the next step.*\"\n",
    "CLUES_FORMAT = \"Here is clues from {}:\\n\\n<clues>\\n{}\\n</clues>\\n\\n\"\n",
    "\n",
    "\n",
    "def should_handoff_to_planner(_):\n",
    "    \"\"\"Check if coordinator requested handoff to planner.\"\"\"\n",
    "\n",
    "    tracer = trace.get_tracer(\n",
    "        instrumenting_module_name=os.getenv(\"TRACER_MODULE_NAME\", \"insight_extractor_agent\"),\n",
    "        instrumenting_library_version=os.getenv(\"TRACER_LIBRARY_VERSION\", \"1.0.0\")\n",
    "    )\n",
    "    with tracer.start_as_current_span(\"should_handoff_to_planner\") as span:\n",
    "        # Check coordinator's response for handoff request\n",
    "        global _global_node_states\n",
    "        shared_state = _global_node_states.get('shared', {})\n",
    "        history = shared_state.get('history', [])\n",
    "        \n",
    "        # Look for coordinator's last message\n",
    "        for entry in reversed(history):\n",
    "            if entry.get('agent') == 'coordinator':\n",
    "                message = entry.get('message', '')\n",
    "                \n",
    "                # Add Event\n",
    "                add_span_event(span, \"input_message\", {\"message\": str(message)})\n",
    "                add_span_event(span, \"response\", {\"handoff_to_planner\": bool(\"handoff_to_planner\" in message)})\n",
    "\n",
    "                return 'handoff_to_planner' in message\n",
    "        \n",
    "        return False\n",
    "\n",
    "async def coordinator_node(task=None, **kwargs):\n",
    "    \n",
    "    tracer = trace.get_tracer(\n",
    "        instrumenting_module_name=os.getenv(\"TRACER_MODULE_NAME\", \"insight_extractor_agent\"),\n",
    "        instrumenting_library_version=os.getenv(\"TRACER_LIBRARY_VERSION\", \"1.0.0\")\n",
    "    )\n",
    "    with tracer.start_as_current_span(\"coordinator\") as span:\n",
    "        \"\"\"Coordinator node that communicate with customers.\"\"\"\n",
    "        global _global_node_states\n",
    "        \n",
    "        log_node_start(\"Coordinator\")\n",
    "\n",
    "        # Extract user request from task (now passed as dictionary)\n",
    "        if isinstance(task, dict):\n",
    "            request = task.get(\"request\", \"\")\n",
    "            request_prompt = task.get(\"request_prompt\", request)\n",
    "        else:\n",
    "            request = str(task) if task else \"\"\n",
    "            request_prompt = request\n",
    "\n",
    "        agent = strands_utils.get_agent(\n",
    "            agent_name=\"coordinator\",\n",
    "            system_prompts=apply_prompt_template(prompt_name=\"coordinator\", prompt_context={}), # apply_prompt_template(prompt_name=\"task_agent\", prompt_context={\"TEST\": \"sdsd\"})\n",
    "            agent_type=\"claude-sonnet-3-5-v-2\", # claude-sonnet-3-5-v-2, claude-sonnet-3-7\n",
    "            enable_reasoning=False,\n",
    "            prompt_cache_info=(False, None), #(False, None), (True, \"default\")\n",
    "            streaming=True,\n",
    "        )\n",
    "            \n",
    "        # Process streaming response and collect text in one pass\n",
    "        full_text = \"\"\n",
    "        async for event in strands_utils.process_streaming_response_yield(\n",
    "            agent, request_prompt, agent_name=\"coordinator\", source=\"coordinator_node\"\n",
    "        ):\n",
    "            if event.get(\"event_type\") == \"text_chunk\": \n",
    "                full_text += event.get(\"data\", \"\")\n",
    "        response = {\"text\": full_text}\n",
    "        \n",
    "        # Store data directly in shared global storage\n",
    "        if 'shared' not in _global_node_states: _global_node_states['shared'] = {}\n",
    "        shared_state = _global_node_states['shared']\n",
    "        \n",
    "        # Update shared global state\n",
    "        shared_state['messages'] = agent.messages\n",
    "        shared_state['request'] = request\n",
    "        shared_state['request_prompt'] = request_prompt\n",
    "        \n",
    "        # Build and update history\n",
    "        if 'history' not in shared_state: \n",
    "            shared_state['history'] = []\n",
    "        shared_state['history'].append({\"agent\":\"coordinator\", \"message\": response[\"text\"]})\n",
    "        \n",
    "        # Add Event\n",
    "        add_span_event(span, \"input_message\", {\"message\": str(request_prompt)})\n",
    "        add_span_event(span, \"response\", {\"response\": str(response[\"text\"])})\n",
    "\n",
    "        log_node_complete(\"Coordinator\")\n",
    "        # Return response only\n",
    "        return response\n",
    "\n",
    "async def planner_node(task=None, **kwargs):\n",
    "    \n",
    "    tracer = trace.get_tracer(\n",
    "        instrumenting_module_name=os.getenv(\"TRACER_MODULE_NAME\", \"insight_extractor_agent\"),\n",
    "        instrumenting_library_version=os.getenv(\"TRACER_LIBRARY_VERSION\", \"1.0.0\")\n",
    "    )\n",
    "    with tracer.start_as_current_span(\"planner\") as span:   \n",
    "        \"\"\"Planner node that generates detailed plans for task execution.\"\"\"\n",
    "        log_node_start(\"Planner\")\n",
    "        global _global_node_states\n",
    "        \n",
    "        # Extract shared state from global storage\n",
    "        shared_state = _global_node_states.get('shared', None)\n",
    "        \n",
    "        # Get request from shared state (task parameter not used in planner)\n",
    "        request = shared_state.get(\"request\", \"\") if shared_state else \"\"\n",
    "        \n",
    "        if not shared_state:\n",
    "            logger.warning(\"No shared state found in global storage\")\n",
    "            return None, {\"text\": \"No shared state available\"}\n",
    "\n",
    "        agent = strands_utils.get_agent(\n",
    "            agent_name=\"planner\",\n",
    "            system_prompts=apply_prompt_template(prompt_name=\"planner\", prompt_context={\"USER_REQUEST\": request}),\n",
    "            agent_type=\"claude-sonnet-3-7\", # claude-sonnet-3-5-v-2, claude-sonnet-3-7\n",
    "            enable_reasoning=True,\n",
    "            prompt_cache_info=(False, None),  # enable prompt caching for reasoning agent, (False, None), (True, \"default\")\n",
    "            streaming=True,\n",
    "        )\n",
    "        \n",
    "        full_plan, messages = shared_state.get(\"full_plan\", \"\"), shared_state[\"messages\"]\n",
    "        message = '\\n\\n'.join([messages[-1][\"content\"][-1][\"text\"], FULL_PLAN_FORMAT.format(full_plan)])\n",
    "        \n",
    "        # Process streaming response and collect text in one pass\n",
    "        full_text = \"\"\n",
    "        async for event in strands_utils.process_streaming_response_yield(\n",
    "            agent, message, agent_name=\"planner\", source=\"planner_node\"\n",
    "        ):\n",
    "            if event.get(\"event_type\") == \"text_chunk\": full_text += event.get(\"data\", \"\")\n",
    "        response = {\"text\": full_text}\n",
    "        \n",
    "        # Update shared global state\n",
    "        shared_state['messages'] = [get_message_from_string(role=\"user\", string=response[\"text\"], imgs=[])]\n",
    "        shared_state['full_plan'] = response[\"text\"]\n",
    "        shared_state['history'].append({\"agent\":\"planner\", \"message\": response[\"text\"]})\n",
    "\n",
    "        # Add Event\n",
    "        add_span_event(span, \"input_message\", {\"message\": str(message)})\n",
    "        add_span_event(span, \"response\", {\"response\": str(response[\"text\"])})\n",
    "\n",
    "        log_node_complete(\"Planner\")\n",
    "        # Return response only\n",
    "        return response\n",
    "\n",
    "async def supervisor_node(task=None, **kwargs):\n",
    "    \"\"\"Supervisor node that decides which agent should act next.\"\"\"\n",
    "    log_node_start(\"Supervisor\")\n",
    "    global _global_node_states\n",
    "\n",
    "    # task and kwargs parameters are unused - supervisor relies on global state\n",
    "    tracer = trace.get_tracer(\n",
    "        instrumenting_module_name=os.getenv(\"TRACER_MODULE_NAME\", \"insight_extractor_agent\"),\n",
    "        instrumenting_library_version=os.getenv(\"TRACER_LIBRARY_VERSION\", \"1.0.0\")\n",
    "    )\n",
    "    with tracer.start_as_current_span(\"supervisor\") as span:  \n",
    "\n",
    "        # Extract shared state from global storage\n",
    "        shared_state = _global_node_states.get('shared', None)\n",
    "        \n",
    "        if not shared_state:\n",
    "            logger.warning(\"No shared state found in global storage\")\n",
    "            return None, {\"text\": \"No shared state available\"}\n",
    "\n",
    "        agent = strands_utils.get_agent(\n",
    "            agent_name=\"supervisor\",\n",
    "            system_prompts=apply_prompt_template(prompt_name=\"supervisor\", prompt_context={}),\n",
    "            agent_type=\"claude-sonnet-3-7\", # claude-sonnet-3-5-v-2, claude-sonnet-3-7\n",
    "            enable_reasoning=False,\n",
    "            prompt_cache_info=(True, \"default\"),  # enable prompt caching for reasoning agent\n",
    "            tools=[coder_agent_tool, reporter_agent_tool, tracker_agent_tool],  # Add coder, reporter and tracker agents as tools\n",
    "            streaming=True,\n",
    "        )\n",
    "\n",
    "        clues, full_plan, messages = shared_state.get(\"clues\", \"\"), shared_state.get(\"full_plan\", \"\"), shared_state[\"messages\"]\n",
    "        message = '\\n\\n'.join([messages[-1][\"content\"][-1][\"text\"], FULL_PLAN_FORMAT.format(full_plan), clues])\n",
    "            \n",
    "        # Process streaming response and collect text in one pass\n",
    "        full_text = \"\"\n",
    "        async for event in strands_utils.process_streaming_response_yield(\n",
    "            agent, message, agent_name=\"supervisor\", source=\"supervisor_node\"\n",
    "        ):\n",
    "            if event.get(\"event_type\") == \"text_chunk\": full_text += event.get(\"data\", \"\")\n",
    "        response = {\"text\": full_text}\n",
    "\n",
    "        # Update shared global state\n",
    "        shared_state['history'].append({\"agent\":\"supervisor\", \"message\": response[\"text\"]})\n",
    "        \n",
    "        # Add Event\n",
    "        add_span_event(span, \"input_message\", {\"message\": str(message)})\n",
    "        add_span_event(span, \"response\", {\"response\": str(response[\"text\"])})\n",
    "        \n",
    "        log_node_complete(\"Supervisor\")\n",
    "        logger.info(\"Workflow completed\")\n",
    "        # Return response only\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054ebbd",
   "metadata": {},
   "source": [
    "### 2.1.4 Edges (Build Graph)\n",
    "- Strans Agents - [Grpah Pattern](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/multi-agent/graph/?h=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e4a02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/graph/builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/graph/builder.py\n",
    "\n",
    "from strands.multiagent import GraphBuilder\n",
    "from src.utils.strands_sdk_utils import FunctionNode\n",
    "from .nodes import (\n",
    "    supervisor_node,\n",
    "    coordinator_node,\n",
    "    planner_node,\n",
    "    should_handoff_to_planner,\n",
    ")\n",
    "\n",
    "def build_graph():\n",
    "    \"\"\"Build and return the agent workflow graph.\"\"\"\n",
    "    builder = GraphBuilder()\n",
    "\n",
    "    # Nodes\n",
    "    coordinator = FunctionNode(func=coordinator_node, name=\"coordinator\")\n",
    "    planner = FunctionNode(func=planner_node, name=\"planner\")\n",
    "    supervisor = FunctionNode(func=supervisor_node, name=\"supervisor\")\n",
    "    \n",
    "    builder.add_node(coordinator, \"coordinator\")\n",
    "    builder.add_node(planner, \"planner\")\n",
    "    builder.add_node(supervisor, \"supervisor\")\n",
    "\n",
    "    # Set entry points (optional - will be auto-detected if not specified)\n",
    "    builder.set_entry_point(\"coordinator\")\n",
    "    \n",
    "    # Add conditional edge - only go to planner if handoff is requested\n",
    "    builder.add_edge(\"coordinator\", \"planner\", condition=should_handoff_to_planner)\n",
    "    \n",
    "    # Add edge - planner to supervisor (no condition needed)\n",
    "    builder.add_edge(\"planner\", \"supervisor\")\n",
    "\n",
    "    # Build the graph\n",
    "    return builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b00640",
   "metadata": {},
   "source": [
    "## 2.2. Workflow and Main defination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eacb02",
   "metadata": {},
   "source": [
    "### 2.2.1 workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8600f896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/workflow.py\n",
    "\n",
    "import logging\n",
    "from src.graph.builder import build_graph\n",
    "\n",
    "# Simple logger setup\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class Colors:\n",
    "    GREEN = '\\033[92m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "async def run_graph_streaming_workflow(user_input: str):\n",
    "    \"\"\"Full graph streaming workflow that maintains graph structure.\n",
    "    \n",
    "    Args:\n",
    "        user_input: The user's query or request  \n",
    "        \n",
    "    Returns:\n",
    "        The result of the workflow execution\n",
    "    \"\"\"\n",
    "    if not user_input:\n",
    "        raise ValueError(\"Input could not be empty\")\n",
    "\n",
    "    logger.info(f\"\\n{Colors.GREEN}Starting graph streaming workflow{Colors.END}\")\n",
    "    \n",
    "    # Prepare user prompt\n",
    "    user_prompts = f\"Here is a user request: <user_request>{user_input}</user_request>\"\n",
    "\n",
    "\n",
    "    #########################\n",
    "    ## modification START  ##\n",
    "    #########################\n",
    "\n",
    "    # Build and execute graph\n",
    "    graph = build_graph()\n",
    "    result = await graph.invoke_async(\n",
    "        task={\n",
    "            \"request\": user_input,\n",
    "            \"request_prompt\": user_prompts\n",
    "        }\n",
    "    )\n",
    "    #########################\n",
    "    ## modification END    ##\n",
    "    #########################\n",
    "    \n",
    "    logger.info(f\"\\n{Colors.GREEN}Graph streaming workflow completed{Colors.END}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c7cec",
   "metadata": {},
   "source": [
    "### 2.2.2 main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc05c56",
   "metadata": {},
   "outputs": [],
   "source": "%%writefile ./main.py\n\n\"\"\"\nEntry point script for the Strands Agent Demo.\n\"\"\"\nimport os\nimport shutil\nimport asyncio\nimport argparse\nfrom dotenv import load_dotenv\nfrom src.graph.builder import build_graph\nfrom src.utils.strands_sdk_utils import strands_utils\n\n# Load environment variables\nload_dotenv()\n\n# Observability\nfrom opentelemetry import trace, context\nfrom src.utils.agentcore_observability import set_session_context, add_span_event\n\n# Import event queue for unified event processing\nfrom src.utils.event_queue import clear_queue\n\ndef remove_artifact_folder(folder_path=\"./artifacts/\"):\n    \"\"\"\n    ./artifact/ 폴더가 존재하면 삭제하는 함수\n    \n    Args:\n        folder_path (str): 삭제할 폴더 경로\n    \"\"\"\n    if os.path.exists(folder_path):\n        print(f\"'{folder_path}' 폴더를 삭제합니다...\")\n        try:\n            shutil.rmtree(folder_path)\n            print(f\"'{folder_path}' 폴더가 성공적으로 삭제되었습니다.\")\n        except Exception as e: print(f\"오류 발생: {e}\")\n    else:\n        print(f\"'{folder_path}' 폴더가 존재하지 않습니다.\")\n\ndef _setup_execution():\n    \"\"\"Initialize execution environment\"\"\"\n    remove_artifact_folder()\n    clear_queue()\n    print(\"\\n=== Starting Queue-Only Event Stream ===\")\n\ndef _print_conversation_history():\n    \"\"\"Print final conversation history\"\"\"\n    print(\"\\n=== Conversation History ===\")\n    from src.graph.nodes import _global_node_states\n    shared_state = _global_node_states.get('shared', {})\n    history = shared_state.get('history', [])\n    \n    if history:\n        for hist_item in history:\n            print(f\"[{hist_item['agent']}] {hist_item['message']}\")\n    else:\n        print(\"No conversation history found\")\n\nasync def graph_streaming_execution(payload):\n    \"\"\"Execute full graph streaming workflow using new graph.stream_async method\"\"\"\n\n    _setup_execution()\n    \n    # Get user query from payload\n    user_query = payload.get(\"user_query\", \"\")\n    session_id = payload.get(\"session-id\", \"default\")\n    context_token = set_session_context(session_id)\n    \n    try:\n        # Get tracer for main application\n        tracer = trace.get_tracer(\n            instrumenting_module_name=os.getenv(\"TRACER_MODULE_NAME\", \"insight_extractor_agent\"),\n            instrumenting_library_version=os.getenv(\"TRACER_LIBRARY_VERSION\", \"1.0.0\")\n        )\n        with tracer.start_as_current_span(\"insight_extractor_session\") as span:   \n            \n            # Build graph and use stream_async method\n            graph = build_graph()\n            \n            # Stream events from graph execution\n            async for event in graph.stream_async({\n                \"request\": user_query,\n                \"request_prompt\": f\"Here is a user request: <user_request>{user_query}</user_request>\"\n            }):\n                yield event\n            \n            _print_conversation_history()\n            print(\"=== Queue-Only Event Stream Complete ===\")\n            \n            # Add Event\n            add_span_event(span, \"user_query\", {\"user-query\": str(user_query)}) \n    \n    finally:\n        context.detach(context_token)\n\nif __name__ == \"__main__\":\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description='Strands Agent Demo')\n    parser.add_argument('--user_query', type=str, help='User query for the agent')\n    parser.add_argument('--session_id', type=str, default='insight-extractor-1', help='Session ID')\n    \n    args = parser.parse_args()\n    \n\n    #########################\n    ## modification START  ##\n    #########################\n    \n    # Use argparse values if provided, otherwise use predefined values\n    if args.user_query:\n        payload = {\n            \"user_query\": args.user_query,\n            \"session-id\": args.session_id\n        }\n    else:\n        # Use predefined query for testing\n        payload = {\n            \"user_query\": \"너가 작성할 것은 moon market 의 판매 현황 보고서야. 세일즈 및 마케팅 관점으로 분석을 해주고, 차트 생성 및 인사이트도 뽑아서 pdf 파일로 만들어줘. 분석대상은 './data/Dat-fresh-food-claude.csv' 파일 입니다.\",\n            \"session-id\": \"insight-extractor-1\"\n        }\n\n    #########################\n    ## modification END    ##\n    #########################\n    \n    remove_artifact_folder()\n\n    # Use full graph streaming execution for real-time streaming with graph structure\n    async def run_streaming():\n        async for event in graph_streaming_execution(payload):\n            strands_utils.process_event_for_display(event)\n\n    asyncio.run(run_streaming())"
  },
  {
   "cell_type": "markdown",
   "id": "8aca538b",
   "metadata": {},
   "source": [
    "## 2.3. Display tool defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7627c609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/utils/strands_sdk_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/utils/strands_sdk_utils.py\n",
    "\n",
    "import logging\n",
    "import traceback\n",
    "import asyncio\n",
    "from src.utils.bedrock import bedrock_info\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "from botocore.config import Config\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from strands.agent.agent_result import AgentResult\n",
    "from strands.types.content import ContentBlock, Message\n",
    "from strands.multiagent.base import MultiAgentBase, NodeResult, MultiAgentResult, Status\n",
    "\n",
    "# Simple logger setup\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class Colors:\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "class ColoredStreamingCallback(StreamingStdOutCallbackHandler):\n",
    "    COLORS = {\n",
    "        'blue': '\\033[94m',\n",
    "        'green': '\\033[92m',\n",
    "        'yellow': '\\033[93m',\n",
    "        'red': '\\033[91m',\n",
    "        'purple': '\\033[95m',\n",
    "        'cyan': '\\033[96m',\n",
    "        'white': '\\033[97m',\n",
    "    }\n",
    "\n",
    "    def __init__(self, color='blue'):\n",
    "        super().__init__()\n",
    "        self.color_code = self.COLORS.get(color, '\\033[94m')\n",
    "        self.reset_code = '\\033[0m'\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"{self.color_code}{token}{self.reset_code}\", end=\"\", flush=True)\n",
    "\n",
    "class strands_utils():\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model(**kwargs):\n",
    "\n",
    "        llm_type = kwargs[\"llm_type\"]\n",
    "        cache_type = kwargs[\"cache_type\"]\n",
    "        enable_reasoning = kwargs[\"enable_reasoning\"]\n",
    "\n",
    "        if llm_type == \"claude-sonnet-3-7\":    \n",
    "            ## BedrockModel params: https://strandsagents.com/latest/api-reference/models/?h=bedrockmodel#strands.models.bedrock.BedrockModel\n",
    "            llm = BedrockModel(\n",
    "                model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-7-Sonnet-CRI\"),\n",
    "                streaming=True,\n",
    "                max_tokens=8192*5,\n",
    "                stop_sequences=[\"\\n\\nHuman\"],\n",
    "                temperature=1 if enable_reasoning else 0.01, \n",
    "                additional_request_fields={\n",
    "                    \"thinking\": {\n",
    "                        \"type\": \"enabled\" if enable_reasoning else \"disabled\", \n",
    "                        **({\"budget_tokens\": 8192} if enable_reasoning else {}),\n",
    "                    }\n",
    "                },\n",
    "                cache_prompt=cache_type, # None/ephemeral/defalut\n",
    "                #cache_tools: Cache point type for tools\n",
    "                boto_client_config=Config(\n",
    "                    read_timeout=900,\n",
    "                    connect_timeout=900,\n",
    "                    retries=dict(max_attempts=50, mode=\"adaptive\"),\n",
    "                )\n",
    "            )   \n",
    "        elif llm_type == \"claude-sonnet-3-5-v-2\":\n",
    "            ## BedrockModel params: https://strandsagents.com/latest/api-reference/models/?h=bedrockmodel#strands.models.bedrock.BedrockModel\n",
    "            llm = BedrockModel(\n",
    "                model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-5-V-2-Sonnet-CRI\"),\n",
    "                streaming=True,\n",
    "                max_tokens=8192,\n",
    "                stop_sequences=[\"\\n\\nHuman\"],\n",
    "                temperature=0.01,\n",
    "                cache_prompt=cache_type, # None/ephemeral/defalut\n",
    "                #cache_tools: Cache point type for tools\n",
    "                boto_client_config=Config(\n",
    "                    read_timeout=900,\n",
    "                    connect_timeout=900,\n",
    "                    retries=dict(max_attempts=50, mode=\"standard\"),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown LLM type: {llm_type}\")\n",
    "\n",
    "        return llm\n",
    "\n",
    "    @staticmethod\n",
    "    def get_agent(**kwargs):\n",
    "\n",
    "        agent_name, system_prompts = kwargs[\"agent_name\"], kwargs[\"system_prompts\"]\n",
    "        agent_type = kwargs.get(\"agent_type\", \"claude-sonnet-3-7\")\n",
    "        enable_reasoning = kwargs.get(\"enable_reasoning\", False)\n",
    "        prompt_cache_info = kwargs.get(\"prompt_cache_info\", (False, None)) # (True, \"default\")\n",
    "        tools = kwargs.get(\"tools\", None)\n",
    "        streaming = kwargs.get(\"streaming\", True)\n",
    "\n",
    "        prompt_cache, cache_type = prompt_cache_info\n",
    "        if prompt_cache: logger.info(f\"{Colors.GREEN}{agent_name.upper()} - Prompt Cache Enabled{Colors.END}\")\n",
    "        else: logger.info(f\"{Colors.GREEN}{agent_name.upper()} - Prompt Cache Disabled{Colors.END}\")\n",
    "\n",
    "        llm = strands_utils.get_model(llm_type=agent_type, cache_type=cache_type, enable_reasoning=enable_reasoning)\n",
    "        llm.config[\"streaming\"] = streaming\n",
    "\n",
    "        agent = Agent(\n",
    "            model=llm,\n",
    "            system_prompt=system_prompts,\n",
    "            tools=tools,\n",
    "            callback_handler=None # async iterator로 대체 하기 때문에 None 설정\n",
    "        )\n",
    "\n",
    "        return agent\n",
    "\n",
    "    @staticmethod\n",
    "    def get_agent_state(agent, key, default_value=None):\n",
    "      \"\"\"Strands Agent의 state에서 안전하게 값을 가져오는 메서드\"\"\"\n",
    "      value = agent.state.get(key)\n",
    "      if value is None: return default_value\n",
    "      return value\n",
    "\n",
    "    @staticmethod\n",
    "    def get_agent_state_all(agent):\n",
    "        return agent.state.get()\n",
    "\n",
    "    @staticmethod\n",
    "    def update_agent_state(agent, key, value):\n",
    "        agent.state.set(key, value)\n",
    "        #return agent\n",
    "\n",
    "    @staticmethod\n",
    "    def update_agent_state_all(target_agent, source_agent):\n",
    "        \"\"\"다른 에이전트의 state를 현재 에이전트에 복사\"\"\"\n",
    "        source_state = source_agent.state.get()\n",
    "        if source_state:\n",
    "            for key, value in source_state.items():\n",
    "                target_agent.state.set(key, value)\n",
    "        return target_agent\n",
    "\n",
    "    @staticmethod\n",
    "    async def process_streaming_response(agent, message):\n",
    "        callback_reasoning, callback_answer = ColoredStreamingCallback('purple'), ColoredStreamingCallback('white')\n",
    "        response = {\"text\": \"\",\"reasoning\": \"\", \"signature\": \"\", \"tool_use\": None, \"cycle\": 0}\n",
    "        try:\n",
    "            agent_stream = agent.stream_async(message)\n",
    "            async for event in agent_stream:\n",
    "                if \"reasoningText\" in event:\n",
    "                    response[\"reasoning\"] += event[\"reasoningText\"]\n",
    "                    callback_reasoning.on_llm_new_token(event[\"reasoningText\"])\n",
    "                elif \"reasoning_signature\" in event:\n",
    "                    response[\"signature\"] += event[\"reasoning_signature\"]\n",
    "                elif \"data\" in event:\n",
    "                    response[\"text\"] += event[\"data\"]\n",
    "                    callback_answer.on_llm_new_token(event[\"data\"])\n",
    "                elif \"current_tool_use\" in event and event[\"current_tool_use\"].get(\"name\"):\n",
    "                    response[\"tool_use\"] = event[\"current_tool_use\"][\"name\"]\n",
    "                    if \"event_loop_metrics\" in event:\n",
    "                        if response[\"cycle\"] != event[\"event_loop_metrics\"].cycle_count:\n",
    "                            response[\"cycle\"] = event[\"event_loop_metrics\"].cycle_count\n",
    "                            callback_answer.on_llm_new_token(f' \\n## Calling tool: {event[\"current_tool_use\"][\"name\"]} - # Cycle: {event[\"event_loop_metrics\"].cycle_count}\\n')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in streaming response: {e}\")\n",
    "            logger.error(traceback.format_exc())  # Detailed error logging\n",
    "\n",
    "        return agent, response\n",
    "\n",
    "    @staticmethod\n",
    "    async def process_streaming_response_yield(agent, message, agent_name=\"coordinator\", source=None):\n",
    "        from src.utils.event_queue import put_event\n",
    "        callback_reasoning, callback_answer = ColoredStreamingCallback('purple'), ColoredStreamingCallback('white')\n",
    "        response = {\"text\": \"\",\"reasoning\": \"\", \"signature\": \"\", \"tool_use\": None, \"cycle\": 0}\n",
    "        try:\n",
    "\n",
    "            session_id = \"ABC\"\n",
    "            agent_stream = agent.stream_async(message)\n",
    "\n",
    "            async for event in agent_stream:\n",
    "                #Strands 이벤트를 AgentCore 형식으로 변환\n",
    "                agentcore_event = await strands_utils._convert_to_agentcore_event(event, agent_name, session_id, source)\n",
    "                if agentcore_event: \n",
    "                    # Put event in global queue for unified processing\n",
    "                    put_event(agentcore_event)\n",
    "                    yield agentcore_event\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in streaming response: {e}\")\n",
    "            logger.error(traceback.format_exc())  # Detailed error logging\n",
    "\n",
    "    # 툴 사용 ID와 툴 이름 매핑을 위한 클래스 변수\n",
    "    _tool_use_mapping = {}\n",
    "\n",
    "    @staticmethod\n",
    "    async def _convert_to_agentcore_event(strands_event, agent_name, session_id, source=None):\n",
    "        \"\"\"Strands 이벤트를 AgentCore 스트리밍 형식으로 변환\"\"\"\n",
    "\n",
    "        base_event = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_name\": agent_name,\n",
    "            \"source\": source or f\"{agent_name}_node\",\n",
    "        }\n",
    "\n",
    "        # 텍스트 데이터 이벤트\n",
    "        if \"data\" in strands_event:\n",
    "            return {\n",
    "                **base_event,\n",
    "                \"type\": \"agent_text_stream\",\n",
    "                \"event_type\": \"text_chunk\",\n",
    "                \"data\": strands_event[\"data\"],\n",
    "                \"chunk_size\": len(strands_event[\"data\"])\n",
    "            }\n",
    "\n",
    "        # 도구 사용 이벤트\n",
    "        elif \"current_tool_use\" in strands_event:\n",
    "            tool_info = strands_event[\"current_tool_use\"]\n",
    "            tool_id = tool_info.get(\"toolUseId\")\n",
    "            tool_name = tool_info.get(\"name\", \"unknown\")\n",
    "\n",
    "            # toolUseId와 tool_name 매핑 저장\n",
    "            if tool_id and tool_name: strands_utils._tool_use_mapping[tool_id] = tool_name\n",
    "\n",
    "            return {\n",
    "                **base_event,\n",
    "                \"type\": \"agent_tool_stream\",\n",
    "                \"event_type\": \"tool_use\",\n",
    "                \"tool_name\": tool_name,\n",
    "                \"tool_id\": tool_id,\n",
    "                \"tool_input\": tool_info.get(\"input\", {})\n",
    "            }\n",
    "\n",
    "        # message 래퍼 안의 tool result 처리\n",
    "        if \"message\" in strands_event:\n",
    "            message = strands_event[\"message\"]\n",
    "            if isinstance(message, dict) and \"content\" in message and isinstance(message[\"content\"], list):\n",
    "                for content_item in message[\"content\"]:\n",
    "                    if isinstance(content_item, dict) and \"toolResult\" in content_item:\n",
    "                        tool_result = content_item[\"toolResult\"]\n",
    "                        tool_id = tool_result.get(\"toolUseId\")\n",
    "\n",
    "                        # 저장된 매핑에서 툴 이름 찾기\n",
    "                        tool_name = strands_utils._tool_use_mapping.get(tool_id, \"external_tool\")\n",
    "                        output = str(tool_result.get(\"content\", [{}])[0].get(\"text\", \"\")) if tool_result.get(\"content\") else \"\"\n",
    "\n",
    "                        return {\n",
    "                            **base_event,\n",
    "                            \"type\": \"agent_tool_stream\",\n",
    "                            \"event_type\": \"tool_result\", \n",
    "                            \"tool_name\": tool_name,\n",
    "                            \"tool_id\": tool_id,\n",
    "                            \"output\": output\n",
    "                        }\n",
    "\n",
    "        # 추론 이벤트\n",
    "        elif \"reasoning\" in strands_event and strands_event.get(\"reasoning\"):\n",
    "            return {\n",
    "                **base_event,\n",
    "                \"type\": \"agent_reasoning_stream\",\n",
    "                \"event_type\": \"reasoning\",\n",
    "                \"reasoning_text\": strands_event.get(\"reasoningText\", \"\")[:200]\n",
    "            }\n",
    "\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def parsing_text_from_response(response):\n",
    "\n",
    "        \"\"\"\n",
    "        Usage (async iterator x): \n",
    "        agent = Agent()\n",
    "        response = agent(query)\n",
    "        response = strands_utils.parsing_text_from_response(response)\n",
    "        \"\"\"\n",
    "\n",
    "        output = {}\n",
    "        if len(response.message[\"content\"]) == 2: ## reasoning\n",
    "            output[\"reasoning\"] = response.message[\"content\"][0][\"reasoningContent\"][\"reasoningText\"][\"text\"]\n",
    "            output[\"signature\"] = response.message[\"content\"][0][\"reasoningContent\"][\"reasoningText\"][\"signature\"]\n",
    "\n",
    "        output[\"text\"] = response.message[\"content\"][-1][\"text\"]\n",
    "\n",
    "        return output  \n",
    "\n",
    "    #########################\n",
    "    ## modification STRART ##\n",
    "    #########################\n",
    "\n",
    "    @staticmethod\n",
    "    def process_event_for_display(event):\n",
    "        \"\"\"Process events for colored terminal output\"\"\"\n",
    "        # Initialize colored callbacks for terminal display\n",
    "        callback_default = ColoredStreamingCallback('white')\n",
    "        callback_reasoning = ColoredStreamingCallback('cyan')        \n",
    "        callback_tool = ColoredStreamingCallback('yellow')\n",
    "\n",
    "        if event:\n",
    "            if event.get(\"event_type\") == \"text_chunk\":\n",
    "                callback_default.on_llm_new_token(event.get('data', ''))\n",
    "\n",
    "            elif event.get(\"event_type\") == \"reasoning\":\n",
    "                callback_reasoning.on_llm_new_token(event.get('reasoning_text', ''))\n",
    "\n",
    "            elif event.get(\"event_type\") == \"tool_use\": \n",
    "                pass\n",
    "\n",
    "            elif event.get(\"event_type\") == \"tool_result\":\n",
    "                tool_name = event.get(\"tool_name\", \"unknown\")\n",
    "                output = event.get(\"output\", \"\")\n",
    "                print(f\"\\n[TOOL RESULT - {tool_name}]\", flush=True)\n",
    "\n",
    "                # Parse output based on function name\n",
    "                if tool_name == \"python_repl_tool\" and len(output.split(\"||\")) == 3:\n",
    "                    status, code, stdout = output.split(\"||\")\n",
    "                    callback_tool.on_llm_new_token(f\"Status: {status}\\n\")\n",
    "\n",
    "                    if code: callback_tool.on_llm_new_token(f\"Code:\\n```python\\n{code}\\n```\\n\")\n",
    "                    if stdout and stdout != 'None': callback_tool.on_llm_new_token(f\"Output:\\n{stdout}\\n\")\n",
    "\n",
    "                elif tool_name == \"bash_tool\" and len(output.split(\"||\")) == 2:\n",
    "                    cmd, stdout = output.split(\"||\")\n",
    "                    if cmd: callback_tool.on_llm_new_token(f\"CMD:\\n```bash\\n{cmd}\\n```\\n\")\n",
    "                    if stdout and stdout != 'None': callback_tool.on_llm_new_token(f\"Output:\\n{stdout}\\n\")\n",
    "\n",
    "                elif tool_name == \"file_read\":\n",
    "                    # file_read 결과는 보통 길어서 앞부분만 표시\n",
    "                    truncated_output = output[:500] + \"...\" if len(output) > 500 else output\n",
    "                    callback_tool.on_llm_new_token(f\"File content preview:\\n{truncated_output}\\n\")\n",
    "\n",
    "                else: # 기타 모든 툴 결과 표시, 코더 툴, 리포터 툴 결과도 다 출력 (for debug)\n",
    "                    callback_tool.on_llm_new_token(f\"Output: pass - you can see that in debug mode\\n\")\n",
    "                    #callback_default.on_llm_new_token(f\"Output: {output}\\n\")\n",
    "                    #pass\n",
    "\n",
    "    #########################\n",
    "    ## modification END    ##\n",
    "    #########################\n",
    "\n",
    "class FunctionNode(MultiAgentBase):\n",
    "    \"\"\"Execute deterministic Python functions as graph nodes.\"\"\"\n",
    "\n",
    "    def __init__(self, func, name: str = None):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        self.name = name or func.__name__\n",
    "\n",
    "    def __call__(self, task=None, **kwargs):\n",
    "        \"\"\"Synchronous execution for compatibility with MultiAgentBase\"\"\"\n",
    "        # Pass task and kwargs directly to function\n",
    "        if asyncio.iscoroutinefunction(self.func): \n",
    "            return asyncio.run(self.func(task=task, **kwargs))\n",
    "        else: \n",
    "            return self.func(task=task, **kwargs)\n",
    "\n",
    "    # Execute function and return standard MultiAgentResult\n",
    "    async def invoke_async(self, task=None, **kwargs):\n",
    "        # Execute function (nodes now use global state for data sharing)  \n",
    "        # Pass task and kwargs directly to function\n",
    "        if asyncio.iscoroutinefunction(self.func): response = await self.func(task=task, **kwargs)\n",
    "        else: response = self.func(task=task, **kwargs)\n",
    "\n",
    "        agent_result = AgentResult(\n",
    "            stop_reason=\"end_turn\",\n",
    "            message=Message(role=\"assistant\", content=[ContentBlock(text=str(response[\"text\"]))]),\n",
    "            metrics={},\n",
    "            state={}\n",
    "        )\n",
    "\n",
    "        # Return wrapped in MultiAgentResult\n",
    "        return MultiAgentResult(\n",
    "            status=Status.COMPLETED,\n",
    "            results={self.name: NodeResult(result=agent_result)}\n",
    "        )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec679cca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b112219d",
   "metadata": {},
   "source": [
    "## 3. Streaming mechanism\n",
    "- 노드 레벨 스트리밍, 그래프 레벨 스트리밍 (이건 스트랜즈 지원안함. 랭그래프도 안하는 걸로 알고 있음 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d15a12",
   "metadata": {},
   "source": [
    "## 4. AgentCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ff0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock-manus-agentcore (UV)",
   "language": "python",
   "name": "bedrock-manus-agentcore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}