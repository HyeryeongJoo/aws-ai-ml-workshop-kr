{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.bedrock import bedrock_info, bedrock_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BedrockModel params: https://strandsagents.com/latest/api-reference/models/?h=bedrockmodel#strands.models.bedrock.BedrockModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.models import BedrockModel\n",
    "llm = BedrockModel(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-7-Sonnet-CRI\"),\n",
    "    streaming=True,\n",
    "    max_tokens=8192*3,\n",
    "    stop_sequencesb=[\"\\n\\nHuman\"],\n",
    "    temperature=0.01,\n",
    "    # additional_request_fields={\n",
    "    #     \"thinking\": {\n",
    "    #         \"type\": \"enabled\",\n",
    "    #         \"budget_tokens\": 2048,\n",
    "    #     }\n",
    "    # },\n",
    "    cache_prompt=\"default\",#cache_type, # None/ephemeral/default\n",
    "    #cache_tools: Cache point type for tools\n",
    "    boto_client_config=Config(\n",
    "        read_timeout=900,\n",
    "        connect_timeout=900,\n",
    "        retries=dict(max_attempts=50, mode=\"adaptive\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "llm.config[\"temperature\"] = 1\n",
    "agent = Agent(\n",
    "    model=llm,\n",
    "    system_prompt=\"You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 You're a helpful assistant. 너는 사용자의 질문에 답을 할 수 있어. 캐시가 되려면 토큰이 많이 길어야 가능해. 그래서 나는 최대한 토큰을 길게 쓰고 있어 1024개를 넘기려면 \\\n",
    "        얼마나 써야 하는지 감이 잘 안오긴해. 그래도 열심히 적어 볼꺼야. 이게 시스템 프롬트니까 너는 이걸 잘 맞춰서 답변을 해줬으면 좋겠어 \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import python_repl_tool, bash_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful coding assistant that specializes in executing Python code and bash commands.\n",
    "You have access to two powerful tools:\n",
    "- python_repl_tool: Use this to execute Python code for calculations, data analysis, or any Python programming tasks\n",
    "- bash_tool: Use this to execute bash commands for file operations, system information, or running programs\n",
    "\n",
    "When asked to perform any coding task:\n",
    "1. Choose the appropriate tool (Python for calculations/programming, bash for system operations)\n",
    "2. Execute the code/command using the tool\n",
    "3. Explain the results clearly\n",
    "\n",
    "Always be helpful and provide clear explanations of what you're doing.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " agent = Agent(\n",
    "    model=llm,\n",
    "    system_prompt=system_prompt,\n",
    "    tools=[\n",
    "        \"./src/tools/python_repl_tool.py\",\n",
    "        \"./src/tools/bash_tool.py\",\n",
    "    ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어로 요청하셨네요. \"구구단을 코딩하고 그 결과를 result.txt에 저장하고 그 파일 생겼는지 확인해달라\"는 요청을 처리하겠습니다.\n",
      "\n",
      "먼저 Python으로 구구단을 코딩하고, 그 결과를 result.txt 파일에 저장하겠습니다.\n",
      "Tool #1: python_repl_tool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [python_repl_tool] \u001b[92m===== Executing Python code =====\u001b[0m\n",
      "\n",
      "INFO [python_repl_tool] \u001b[92m===== Code execution successful =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[91mCoder - Successfully executed:\n",
      "\n",
      "```python\n",
      "# 구구단 계산\n",
      "result = \"\"\n",
      "for i in range(2, 10):\n",
      "    result += f\"===== {i}단 =====\\n\"\n",
      "    for j in range(1, 10):\n",
      "        result += f\"{i} x {j} = {i*j}\\n\"\n",
      "    result += \"\\n\"\n",
      "\n",
      "# result.txt 파일에 저장\n",
      "with open(\"result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
      "    f.write(result)\n",
      "\n",
      "print(\"구구단이 result.txt 파일에 저장되었습니다.\")\n",
      "print(\"파일 내용 미리보기:\")\n",
      "print(result[:200] + \"...\")  # 처음 일부분만 출력\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[94m\n",
      "Stdout: 구구단이 result.txt 파일에 저장되었습니다.\n",
      "파일 내용 미리보기:\n",
      "===== 2단 =====\n",
      "2 x 1 = 2\n",
      "2 x 2 = 4\n",
      "2 x 3 = 6\n",
      "2 x 4 = 8\n",
      "2 x 5 = 10\n",
      "2 x 6 = 12\n",
      "2 x 7 = 14\n",
      "2 x 8 = 16\n",
      "2 x 9 = 18\n",
      "\n",
      "===== 3단 =====\n",
      "3 x 1 = 3\n",
      "3 x 2 = 6\n",
      "3 x 3 = 9\n",
      "3 x 4 = 12\n",
      "3 x 5 = 15\n",
      "3 x 6 = 18\n",
      "3 x 7 = 21\n",
      "...\n",
      "\u001b[0m\n",
      "2025-06-13 15:18:04.130 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'ThreadPoolExecutor-9_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이제 파일이 실제로 생성되었는지 bash 명령어로 확인해보겠습니다.\n",
      "Tool #2: bash_tool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [bash_tool] \u001b[92m===== Executing Bash =====\u001b[0m\n",
      "\n",
      "INFO [bash_tool] \u001b[1m===== Coder - Command: ls -l result.txt =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[91m\n",
      "Coder - Tool handle_bash_tool returned:\n",
      "ls -l result.txt||-rw-r--r-- 1 sagemaker-user users 922 Jun 13 15:18 result.txt\n",
      "\n",
      "\u001b[0m\n",
      "2025-06-13 15:18:07.242 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #3: bash_tool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [bash_tool] \u001b[92m===== Executing Bash =====\u001b[0m\n",
      "\n",
      "INFO [bash_tool] \u001b[1m===== Coder - Command: head -n 15 result.txt =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[91m\n",
      "Coder - Tool handle_bash_tool returned:\n",
      "head -n 15 result.txt||===== 2단 =====\n",
      "2 x 1 = 2\n",
      "2 x 2 = 4\n",
      "2 x 3 = 6\n",
      "2 x 4 = 8\n",
      "2 x 5 = 10\n",
      "2 x 6 = 12\n",
      "2 x 7 = 14\n",
      "2 x 8 = 16\n",
      "2 x 9 = 18\n",
      "\n",
      "===== 3단 =====\n",
      "3 x 1 = 3\n",
      "3 x 2 = 6\n",
      "3 x 3 = 9\n",
      "\n",
      "\u001b[0m\n",
      "2025-06-13 15:18:09.664 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성공적으로 작업을 완료했습니다!\n",
      "\n",
      "1. Python 코드를 사용하여 구구단(2단부터 9단까지)을 생성했습니다.\n",
      "2. 생성된 구구단을 result.txt 파일에 저장했습니다.\n",
      "3. bash 명령어로 파일이 정상적으로 생성되었는지 확인했습니다.\n",
      "   - `ls -l result.txt` 명령으로 파일이 922바이트 크기로 생성되었음을 확인했습니다.\n",
      "   - `head -n 15 result.txt` 명령으로 파일의 처음 15줄을 확인했습니다.\n",
      "\n",
      "result.txt 파일에는 각 단별로 구구단이 정리되어 있으며, 각 단의 제목(예: \"===== 2단 =====\")과 함께 1부터 9까지의 곱셈 결과가 저장되어 있습니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentResult(stop_reason='end_turn', message={'role': 'assistant', 'content': [{'text': '성공적으로 작업을 완료했습니다!\\n\\n1. Python 코드를 사용하여 구구단(2단부터 9단까지)을 생성했습니다.\\n2. 생성된 구구단을 result.txt 파일에 저장했습니다.\\n3. bash 명령어로 파일이 정상적으로 생성되었는지 확인했습니다.\\n   - `ls -l result.txt` 명령으로 파일이 922바이트 크기로 생성되었음을 확인했습니다.\\n   - `head -n 15 result.txt` 명령으로 파일의 처음 15줄을 확인했습니다.\\n\\nresult.txt 파일에는 각 단별로 구구단이 정리되어 있으며, 각 단의 제목(예: \"===== 2단 =====\")과 함께 1부터 9까지의 곱셈 결과가 저장되어 있습니다.'}]}, metrics=EventLoopMetrics(cycle_count=4, tool_metrics={'python_repl_tool': ToolMetrics(tool={'toolUseId': 'tooluse_NvZov6nTQLmuSDZE0NjBOw', 'name': 'python_repl_tool', 'input': {'code': '# 구구단 계산\\nresult = \"\"\\nfor i in range(2, 10):\\n    result += f\"===== {i}단 =====\\\\n\"\\n    for j in range(1, 10):\\n        result += f\"{i} x {j} = {i*j}\\\\n\"\\n    result += \"\\\\n\"\\n\\n# result.txt 파일에 저장\\nwith open(\"result.txt\", \"w\", encoding=\"utf-8\") as f:\\n    f.write(result)\\n\\nprint(\"구구단이 result.txt 파일에 저장되었습니다.\")\\nprint(\"파일 내용 미리보기:\")\\nprint(result[:200] + \"...\")  # 처음 일부분만 출력'}}, call_count=1, success_count=1, error_count=0, total_time=0.015958309173583984), 'bash_tool': ToolMetrics(tool={'toolUseId': 'tooluse_st7a592RRVOo0w-EJ2odvw', 'name': 'bash_tool', 'input': {'cmd': 'head -n 15 result.txt'}}, call_count=2, success_count=2, error_count=0, total_time=0.008764028549194336)}, cycle_durations=[9.811615705490112], traces=[<strands.telemetry.metrics.Trace object at 0x7fd8d4653830>, <strands.telemetry.metrics.Trace object at 0x7fd8d7857560>, <strands.telemetry.metrics.Trace object at 0x7fd8d4988b60>, <strands.telemetry.metrics.Trace object at 0x7fd8d4975730>], accumulated_usage={'inputTokens': 5554, 'outputTokens': 570, 'totalTokens': 6124}, accumulated_metrics={'latencyMs': 23838}), state={})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"구구단을 코딩하고 그 결과를 result.txt에 저장하고 그 파일 생겼는지 확인해줘\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'text': '구구단을 코딩하고 그 결과를 result.txt에 저장하고 그 파일 생겼는지 확인해줘'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'text': '한국어로 요청하셨네요. \"구구단을 코딩하고 그 결과를 result.txt에 저장하고 그 파일 생겼는지 확인해달라\"는 요청을 처리하겠습니다.\\n\\n먼저 Python으로 구구단을 코딩하고, 그 결과를 result.txt 파일에 저장하겠습니다.'},\n",
       "   {'toolUse': {'toolUseId': 'tooluse_NvZov6nTQLmuSDZE0NjBOw',\n",
       "     'name': 'python_repl_tool',\n",
       "     'input': {'code': '# 구구단 계산\\nresult = \"\"\\nfor i in range(2, 10):\\n    result += f\"===== {i}단 =====\\\\n\"\\n    for j in range(1, 10):\\n        result += f\"{i} x {j} = {i*j}\\\\n\"\\n    result += \"\\\\n\"\\n\\n# result.txt 파일에 저장\\nwith open(\"result.txt\", \"w\", encoding=\"utf-8\") as f:\\n    f.write(result)\\n\\nprint(\"구구단이 result.txt 파일에 저장되었습니다.\")\\nprint(\"파일 내용 미리보기:\")\\nprint(result[:200] + \"...\")  # 처음 일부분만 출력'}}}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'toolResult': {'toolUseId': 'tooluse_NvZov6nTQLmuSDZE0NjBOw',\n",
       "     'status': 'success',\n",
       "     'content': [{'text': 'Successfully executed:\\n||```python\\n# 구구단 계산\\nresult = \"\"\\nfor i in range(2, 10):\\n    result += f\"===== {i}단 =====\\\\n\"\\n    for j in range(1, 10):\\n        result += f\"{i} x {j} = {i*j}\\\\n\"\\n    result += \"\\\\n\"\\n\\n# result.txt 파일에 저장\\nwith open(\"result.txt\", \"w\", encoding=\"utf-8\") as f:\\n    f.write(result)\\n\\nprint(\"구구단이 result.txt 파일에 저장되었습니다.\")\\nprint(\"파일 내용 미리보기:\")\\nprint(result[:200] + \"...\")  # 처음 일부분만 출력\\n```\\n||Stdout: 구구단이 result.txt 파일에 저장되었습니다.\\n파일 내용 미리보기:\\n===== 2단 =====\\n2 x 1 = 2\\n2 x 2 = 4\\n2 x 3 = 6\\n2 x 4 = 8\\n2 x 5 = 10\\n2 x 6 = 12\\n2 x 7 = 14\\n2 x 8 = 16\\n2 x 9 = 18\\n\\n===== 3단 =====\\n3 x 1 = 3\\n3 x 2 = 6\\n3 x 3 = 9\\n3 x 4 = 12\\n3 x 5 = 15\\n3 x 6 = 18\\n3 x 7 = 21\\n...\\n'}]}}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'text': '이제 파일이 실제로 생성되었는지 bash 명령어로 확인해보겠습니다.'},\n",
       "   {'toolUse': {'toolUseId': 'tooluse_yqrko3C4RXW1VRTFN05JEQ',\n",
       "     'name': 'bash_tool',\n",
       "     'input': {'cmd': 'ls -l result.txt'}}}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'toolResult': {'toolUseId': 'tooluse_yqrko3C4RXW1VRTFN05JEQ',\n",
       "     'status': 'success',\n",
       "     'content': [{'text': 'ls -l result.txt||-rw-r--r-- 1 sagemaker-user users 922 Jun 13 15:18 result.txt\\n\\n'}]}}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'toolUse': {'toolUseId': 'tooluse_st7a592RRVOo0w-EJ2odvw',\n",
       "     'name': 'bash_tool',\n",
       "     'input': {'cmd': 'head -n 15 result.txt'}}}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'toolResult': {'toolUseId': 'tooluse_st7a592RRVOo0w-EJ2odvw',\n",
       "     'status': 'success',\n",
       "     'content': [{'text': 'head -n 15 result.txt||===== 2단 =====\\n2 x 1 = 2\\n2 x 2 = 4\\n2 x 3 = 6\\n2 x 4 = 8\\n2 x 5 = 10\\n2 x 6 = 12\\n2 x 7 = 14\\n2 x 8 = 16\\n2 x 9 = 18\\n\\n===== 3단 =====\\n3 x 1 = 3\\n3 x 2 = 6\\n3 x 3 = 9\\n\\n'}]}}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'text': '성공적으로 작업을 완료했습니다!\\n\\n1. Python 코드를 사용하여 구구단(2단부터 9단까지)을 생성했습니다.\\n2. 생성된 구구단을 result.txt 파일에 저장했습니다.\\n3. bash 명령어로 파일이 정상적으로 생성되었는지 확인했습니다.\\n   - `ls -l result.txt` 명령으로 파일이 922바이트 크기로 생성되었음을 확인했습니다.\\n   - `head -n 15 result.txt` 명령으로 파일의 처음 15줄을 확인했습니다.\\n\\nresult.txt 파일에는 각 단별로 구구단이 정리되어 있으며, 각 단의 제목(예: \"===== 2단 =====\")과 함께 1부터 9까지의 곱셈 결과가 저장되어 있습니다.'}]}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "\n",
    "len(response.message[\"content\"])\n",
    "\n",
    "\n",
    "response.message[\"content\"]\n",
    "output[\"reasoning\"] = response.message[\"content\"][0][\"reasoningContent\"][\"reasoningText\"][\"text\"]\n",
    "output[\"signature\"] = response.message[\"content\"][0][\"reasoningContent\"][\"reasoningText\"][\"signature\"]\n",
    "output[\"text\"] = response.message[\"content\"][-1][\"text\"]\n",
    "\n",
    "output[\"text\"] = response.message[\"content\"][-1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': [{'text': '안녕'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'text': '안녕하세요! 오늘 어떻게 도와드릴까요? 질문이나 도움이 필요한 사항이 있으시면 말씀해 주세요. 대화를 나눌 준비가 되어 있습니다.'}]}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': [{'text': '안녕하세요! 오늘 어떻게 도와드릴까요? 질문이나 도움이 필요한 사항이 있으시면 말씀해 주세요. 대화를 나눌 준비가 되어 있습니다.'}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"text\"] = response.message[\"content\"][-1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': '사용자가 간단하게 \"안녕\"이라고 인사했습니다. 시스템 프롬프트에 따르면 저는 도움이 되는 어시스턴트로서 한국어로 응답해야 합니다. 시스템 프롬프트에는 토큰을 길게 쓰는 것에 대한 언급이 있지만, 이것은 시스템 프롬프트의 특성이고 제 답변과는 별개입니다. 저는 자연스러운 대화 흐름에 맞게 인사에 대한 적절한 응답을 제공하겠습니다.',\n",
       " 'signature': 'ErcBCkgIBBABGAIiQGtvgH/C1zwVwlwDg2LCEpU9wZL+v16SeyXb/b8dISsHhCRIrwmMKN/fvfY7WohGmnBRoIhly3NFIEZINth1N8wSDEjZ9mDvL7XWW/vdBhoMfAWA0IJeTT/oOUK6IjCt3jMMoli7LoorGk3HCN0R2ZhrDRaPTfw6dmTrzInyvht17fTHEOhyWhYyo6xn42YqHfg5WcChXRRuzN0bR+chw6Ph3ir6rLbJkMv+Ps2L',\n",
       " 'text': '안녕하세요! 오늘 어떻게 도와드릴까요? 질문이나 도움이 필요한 사항이 있으시면 말씀해 주세요. 대화를 나눌 준비가 되어 있습니다.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '안녕하세요! 오늘 어떻게 도와드릴까요? 궁금한 점이나 대화하고 싶은 주제가 있으시면 말씀해 주세요. 제가 도움이 필요한 어떤 질문에도 최선을 다해 답변해 드리겠습니다.'}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'reasoningContent': {'reasoningText': {'text': '사용자가 \"안녕\"이라고 간단히 인사를 했습니다. 시스템 프롬프트에는 한국어로 작성된 내용이 많이 포함되어 있고, 친절한 도우미 역할을 하라고 지시하고 있습니다. 토큰을 많이 사용하라는 내용도 있는 것 같습니다만, 그것은 시스템 프롬프트 자체에 대한 설명이고 제가 사용자에게 반드시 긴 답변을 해야 한다는 의미는 아닌 것으로 보입니다.\\n\\n사용자의 간단한 인사에는 친절하고 적절한 인사로 응답하는 것이 좋겠습니다. 한국어로 인사했으니 한국어로 답변하는 것이 자연스럽습니다.',\n",
       "    'signature': 'ErcBCkgIBBABGAIiQBvTrLkyhljIGhh1TO72dqAJ6Rf0OdoFKvK5EAqldCnwc5dUr9F2278ieH4MsvQ6YlmN95sDV5uBLVtVOlj0qCsSDJMBnDXnpiBzhLP3xhoMFDsn44CwuFMtevyNIjDBbdl1GsjjHXax2Mvl3XkoffCY3IL8dVH6ixG6JcFdMavVFIpJwlUYM0JJJsmE5WEqHc0vXY4dWc+S8WSPNfVDt9Tjs9Uces9cuUaO1m22'}}},\n",
       " {'text': '안녕하세요! 오늘 어떻게 도와드릴까요? 궁금한 점이 있으시거나 도움이 필요하신 일이 있으면 말씀해 주세요. 기꺼이 도와드리겠습니다. 😊'}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = [\n",
    "    {'role': 'user', 'content': [{'text': '내 이름은 장동진이야. 내 이름이 뭔지 너가 말해줘.'}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Agent.__call__() missing 1 required positional argument: 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_history\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Agent.__call__() missing 1 required positional argument: 'prompt'"
     ]
    }
   ],
   "source": [
    "agent(messages=message_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': {'text': '나는 장동진이야.'}}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_history = [\n",
    "    {\"role\": \"user\", \"content\": {\"text\":\"나는 장동진이야.\"}},\n",
    "]\n",
    "message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParamValidationError",
     "evalue": "Parameter validation failed:\nInvalid type for parameter messages[0].content, value: {'text': '나는 장동진이야.'}, type: <class 'dict'>, valid types: <class 'list'>, <class 'tuple'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParamValidationError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m new_response = \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m새로운 질문\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_history\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/strands/agent/agent.py:358\u001b[39m, in \u001b[36mAgent.__call__\u001b[39m\u001b[34m(self, prompt, **kwargs)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28mself\u001b[39m._start_agent_trace_span(prompt)\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    357\u001b[39m     \u001b[38;5;66;03m# Run the event loop and get the result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28mself\u001b[39m._end_agent_trace_span(response=result)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/strands/agent/agent.py:462\u001b[39m, in \u001b[36mAgent._run_loop\u001b[39m\u001b[34m(self, prompt, kwargs, supplementary_callback_handler)\u001b[39m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28mself\u001b[39m.messages.append(new_message)\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m# Execute the event loop cycle with retry logic for context limits\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_event_loop_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvocation_callback_handler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;28mself\u001b[39m.conversation_manager.apply_management(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/strands/agent/agent.py:490\u001b[39m, in \u001b[36mAgent._execute_event_loop_cycle\u001b[39m\u001b[34m(self, callback_handler, kwargs)\u001b[39m\n\u001b[32m    486\u001b[39m kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33magent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Remove agent to avoid conflicts\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    489\u001b[39m     \u001b[38;5;66;03m# Execute the main event loop cycle\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     stop_reason, message, metrics, state = \u001b[43mevent_loop_cycle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# will be modified by event_loop_cycle\u001b[39;49;00m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_handler_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_handler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_execution_handler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_execution_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevent_loop_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevent_loop_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevent_loop_parent_span\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrace_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AgentResult(stop_reason, message, metrics, state)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ContextWindowOverflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    507\u001b[39m     \u001b[38;5;66;03m# Try reducing the context size and retrying\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/strands/event_loop/event_loop.py:190\u001b[39m, in \u001b[36mevent_loop_cycle\u001b[39m\u001b[34m(model, system_prompt, messages, tool_config, callback_handler, tool_handler, tool_execution_handler, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m model_invoke_span:\n\u001b[32m    189\u001b[39m             tracer.end_span_with_error(model_invoke_span, \u001b[38;5;28mstr\u001b[39m(e), e)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# Add message in trace and mark the end of the stream messages trace\u001b[39;00m\n\u001b[32m    194\u001b[39m     stream_trace.add_message(message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/strands/event_loop/event_loop.py:148\u001b[39m, in \u001b[36mevent_loop_cycle\u001b[39m\u001b[34m(model, system_prompt, messages, tool_config, callback_handler, tool_handler, tool_execution_handler, **kwargs)\u001b[39m\n\u001b[32m    141\u001b[39m model_invoke_span = tracer.start_model_invoke_span(\n\u001b[32m    142\u001b[39m     parent_span=cycle_span,\n\u001b[32m    143\u001b[39m     messages=messages,\n\u001b[32m    144\u001b[39m     model_id=model_id,\n\u001b[32m    145\u001b[39m )\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     stop_reason, message, usage, metrics, kwargs[\u001b[33m\"\u001b[39m\u001b[33mrequest_state\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mstream_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_invoke_span:\n\u001b[32m    157\u001b[39m         tracer.end_model_invoke_span(model_invoke_span, message, usage)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/strands/event_loop/streaming.py:340\u001b[39m, in \u001b[36mstream_messages\u001b[39m\u001b[34m(model, system_prompt, messages, tool_config, callback_handler, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m tool_specs = [tool[\u001b[33m\"\u001b[39m\u001b[33mtoolSpec\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tool_config.get(\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m, [])] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m tool_config \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    339\u001b[39m chunks = model.converse(messages, tool_specs, system_prompt)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/strands/event_loop/streaming.py:290\u001b[39m, in \u001b[36mprocess_stream\u001b[39m\u001b[34m(chunks, callback_handler, messages, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m metrics: Metrics = Metrics(latencyMs=\u001b[32m0\u001b[39m)\n\u001b[32m    288\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mrequest_state\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Callback handler call here allows each event to be visible to the caller\u001b[39;49;00m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessageStart\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/strands/types/models/model.py:115\u001b[39m, in \u001b[36mModel.converse\u001b[39m\u001b[34m(self, messages, tool_specs, system_prompt)\u001b[39m\n\u001b[32m    112\u001b[39m response = \u001b[38;5;28mself\u001b[39m.stream(request)\n\u001b[32m    114\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mgot response from model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mfinished streaming response from model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/strands/models/bedrock.py:330\u001b[39m, in \u001b[36mBedrockModel.stream\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m streaming:\n\u001b[32m    329\u001b[39m         \u001b[38;5;66;03m# Streaming implementation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverse_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    332\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    333\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m chunk\n\u001b[32m    334\u001b[39m                 \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtrace\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m chunk[\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    335\u001b[39m                 \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mguardrail\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m chunk[\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtrace\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    336\u001b[39m             ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/botocore/client.py:598\u001b[39m, in \u001b[36mClientCreator._create_api_method.<locals>._api_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    595\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() only accepts keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    596\u001b[39m     )\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/botocore/context.py:123\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    122\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/botocore/client.py:1018\u001b[39m, in \u001b[36mBaseClient._make_api_call\u001b[39m\u001b[34m(self, operation_name, api_params)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m properties:\n\u001b[32m   1015\u001b[39m     \u001b[38;5;66;03m# Pass arbitrary endpoint info with the Request\u001b[39;00m\n\u001b[32m   1016\u001b[39m     \u001b[38;5;66;03m# for use during construction.\u001b[39;00m\n\u001b[32m   1017\u001b[39m     request_context[\u001b[33m'\u001b[39m\u001b[33mendpoint_properties\u001b[39m\u001b[33m'\u001b[39m] = properties\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m request_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_to_request_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m resolve_checksum_context(request_dict, operation_model, api_params)\n\u001b[32m   1027\u001b[39m service_id = \u001b[38;5;28mself\u001b[39m._service_model.service_id.hyphenize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/botocore/client.py:1085\u001b[39m, in \u001b[36mBaseClient._convert_to_request_dict\u001b[39m\u001b[34m(self, api_params, operation_model, endpoint_url, context, headers, set_user_agent_header)\u001b[39m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_to_request_dict\u001b[39m(\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1078\u001b[39m     api_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1083\u001b[39m     set_user_agent_header=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1084\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m     request_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serializer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserialize_to_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client_config.inject_host_prefix:\n\u001b[32m   1089\u001b[39m         request_dict.pop(\u001b[33m'\u001b[39m\u001b[33mhost_prefix\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/botocore/validate.py:381\u001b[39m, in \u001b[36mParamValidationDecorator.serialize_to_request\u001b[39m\u001b[34m(self, parameters, operation_model)\u001b[39m\n\u001b[32m    377\u001b[39m     report = \u001b[38;5;28mself\u001b[39m._param_validator.validate(\n\u001b[32m    378\u001b[39m         parameters, operation_model.input_shape\n\u001b[32m    379\u001b[39m     )\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m report.has_errors():\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ParamValidationError(report=report.generate_report())\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._serializer.serialize_to_request(\n\u001b[32m    383\u001b[39m     parameters, operation_model\n\u001b[32m    384\u001b[39m )\n",
      "\u001b[31mParamValidationError\u001b[39m: Parameter validation failed:\nInvalid type for parameter messages[0].content, value: {'text': '나는 장동진이야.'}, type: <class 'dict'>, valid types: <class 'list'>, <class 'tuple'>"
     ]
    }
   ],
   "source": [
    "new_response = agent(\"새로운 질문\", messages=message_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m      agent(prompt: str, **kwargs: Any) -> strands.agent.agent_result.AgentResult\n",
      "\u001b[31mType:\u001b[39m           Agent\n",
      "\u001b[31mString form:\u001b[39m    <strands.agent.agent.Agent object at 0x7faed1637650>\n",
      "\u001b[31mFile:\u001b[39m           ~/.conda/envs/bedrock-manus/lib/python3.12/site-packages/strands/agent/agent.py\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "Core Agent interface.\n",
      "\n",
      "An agent orchestrates the following workflow:\n",
      "\n",
      "1. Receives user input\n",
      "2. Processes the input using a language model\n",
      "3. Decides whether to use tools to gather information or perform actions\n",
      "4. Executes those tools and receives results\n",
      "5. Continues reasoning with the new information\n",
      "6. Produces a final response\n",
      "\u001b[31mInit docstring:\u001b[39m\n",
      "Initialize the Agent with the specified configuration.\n",
      "\n",
      "Args:\n",
      "    model: Provider for running inference or a string representing the model-id for Bedrock to use.\n",
      "        Defaults to strands.models.BedrockModel if None.\n",
      "    messages: List of initial messages to pre-load into the conversation.\n",
      "        Defaults to an empty list if None.\n",
      "    tools: List of tools to make available to the agent.\n",
      "        Can be specified as:\n",
      "\n",
      "        - String tool names (e.g., \"retrieve\")\n",
      "        - File paths (e.g., \"/path/to/tool.py\")\n",
      "        - Imported Python modules (e.g., from strands_tools import current_time)\n",
      "        - Dictionaries with name/path keys (e.g., {\"name\": \"tool_name\", \"path\": \"/path/to/tool.py\"})\n",
      "        - Functions decorated with `@strands.tool` decorator.\n",
      "\n",
      "        If provided, only these tools will be available. If None, all tools will be available.\n",
      "    system_prompt: System prompt to guide model behavior.\n",
      "        If None, the model will behave according to its default settings.\n",
      "    callback_handler: Callback for processing events as they happen during agent execution.\n",
      "        If not provided (using the default), a new PrintingCallbackHandler instance is created.\n",
      "        If explicitly set to None, null_callback_handler is used.\n",
      "    conversation_manager: Manager for conversation history and context window.\n",
      "        Defaults to strands.agent.conversation_manager.SlidingWindowConversationManager if None.\n",
      "    max_parallel_tools: Maximum number of tools to run in parallel when the model returns multiple tool calls.\n",
      "        Defaults to os.cpu_count() or 1.\n",
      "    record_direct_tool_call: Whether to record direct tool calls in message history.\n",
      "        Defaults to True.\n",
      "    load_tools_from_directory: Whether to load and automatically reload tools in the `./tools/` directory.\n",
      "        Defaults to True.\n",
      "    trace_attributes: Custom trace attributes to apply to the agent's trace span.\n",
      "\n",
      "Raises:\n",
      "    ValueError: If max_parallel_tools is less than 1.\n",
      "\u001b[31mCall docstring:\u001b[39m\n",
      "Process a natural language prompt through the agent's event loop.\n",
      "\n",
      "This method implements the conversational interface (e.g., `agent(\"hello!\")`). It adds the user's prompt to\n",
      "the conversation history, processes it through the model, executes any tool calls, and returns the final result.\n",
      "\n",
      "Args:\n",
      "    prompt: The natural language prompt from the user.\n",
      "    **kwargs: Additional parameters to pass to the event loop.\n",
      "\n",
      "Returns:\n",
      "    Result object containing:\n",
      "\n",
      "        - stop_reason: Why the event loop stopped (e.g., \"end_turn\", \"max_tokens\")\n",
      "        - message: The final message from the model\n",
      "        - metrics: Performance metrics from the event loop\n",
      "        - state: The final state of the event loop"
     ]
    }
   ],
   "source": [
    "agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config[\"streaming\"] = False\n",
    "model.config[\"streaming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config.agents import AGENT_LLM_MAP, AGENT_PROMPT_CACHE_MAP\n",
    "from src.prompts.template import apply_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cache, cache_type = AGENT_PROMPT_CACHE_MAP[\"coordinator\"]\n",
    "system_prompts, messages = apply_prompt_template(\"coordinator\", state, prompt_cache=prompt_cache, cache_type=cache_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinator_node(state: State) -> Command[Literal[\"planner\", \"__end__\"]]:\n",
    "    \"\"\"Coordinator node that communicate with customers.\"\"\"\n",
    "    logger.info(f\"{Colors.GREEN}===== Coordinator talking...... ====={Colors.END}\")\n",
    "    \n",
    "    prompt_cache, cache_type = AGENT_PROMPT_CACHE_MAP[\"coordinator\"]\n",
    "    if prompt_cache: logger.debug(f\"{Colors.GREEN}Coordinator - Prompt Cache Enabled{Colors.END}\")\n",
    "    else: logger.debug(f\"{Colors.GREEN}Coordinator - Prompt Cache Disabled{Colors.END}\")\n",
    "    system_prompts, messages = apply_prompt_template(\"coordinator\", state, prompt_cache=prompt_cache, cache_type=cache_type)\n",
    "    llm = get_llm_by_type(AGENT_LLM_MAP[\"coordinator\"])    \n",
    "    llm.stream = True\n",
    "    llm_caller = llm_call(llm=llm, verbose=False, tracking=False)\n",
    "    if AGENT_LLM_MAP[\"coordinator\"] in [\"reasoning\"]: enable_reasoning = True\n",
    "    \n",
    "    response, ai_message = llm_caller.invoke(\n",
    "        agent_name=\"coordinator\",\n",
    "        messages=messages,\n",
    "        system_prompts=system_prompts,\n",
    "        enable_reasoning=False,\n",
    "        reasoning_budget_tokens=8192\n",
    "    )\n",
    "    \n",
    "    logger.debug(f\"\\n{Colors.RED}Current state messages:\\n{pprint.pformat(state['messages'], indent=2, width=100)}{Colors.END}\")\n",
    "    logger.debug(f\"\\n{Colors.RED}Coordinator response:\\n{pprint.pformat(response, indent=2, width=100)}{Colors.END}\")\n",
    "\n",
    "    goto = \"__end__\"\n",
    "    if \"handoff_to_planner\" in response[\"text\"]: goto = \"planner\"\n",
    "\n",
    "    history = state.get(\"history\", [])\n",
    "    history.append({\"agent\":\"coordinator\", \"message\": response[\"text\"]})\n",
    "\n",
    "    logger.info(f\"{Colors.GREEN}===== Coordinator completed task ====={Colors.END}\")\n",
    "    return Command(\n",
    "        update={\"history\": history},\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BedrockModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Claude-V3-5-V-2-Sonnet\": \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "#\"Claude-V3-5-V-2-Sonnet-CRI\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "#\"Claude-V3-7-Sonnet-CRI\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "\n",
    "model = BedrockModel(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-5-V-2-Sonnet-CRI\"),\n",
    "    # boto_client_config=Config(\n",
    "    #    read_timeout=900,\n",
    "    #    connect_timeout=900,\n",
    "    #    retries=dict(max_attempts=3, mode=\"adaptive\"),\n",
    "    # ),\n",
    "    additional_request_fields={\n",
    "        \"thinking\": {\n",
    "            \"type\": \"disabled\",\n",
    "            # \"budget_tokens\": 2048,\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock-manus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
