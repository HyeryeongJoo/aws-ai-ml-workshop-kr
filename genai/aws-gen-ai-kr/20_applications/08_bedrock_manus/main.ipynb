{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8daa2248-2979-43f7-a945-85784fb716a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def70031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "\n",
    "TAVILY_API_KEY = your_key\n",
    "JINA_API_KEY = your_key\n",
    "CHROME_INSTANCE_PATH = /Applications/Google Chrome.app/Contents/MacOS/Google Chrome\n",
    "BROWSER_HEADLESS=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c20d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [telemetry] Anonymized telemetry enabled. See https://docs.browser-use.com/development/telemetry for more information.\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from src.workflow import run_agent_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e915a3c-bb0d-4d3d-88cf-bfa0b61e8f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_query = '''\n",
    "    I would like to analyze Amazon product sales data.\n",
    "    The target for analysis is the './data/Amazon_Sale_Report.csv' file.\n",
    "    Please conduct an analysis to extract marketing insights based on this data.\n",
    "    Please perform various analytical techniques starting from basic data attribute exploration, product sales trends, variable relationships, variable combinations, etc.\n",
    "    If there are any additional analyses needed to extract insights after the data analysis, please perform those as well.\n",
    "    Please include detailed analysis in the report along with supporting images and charts.\n",
    "    Please save the final report in PDF format.\n",
    "'''\n",
    "\n",
    "user_query = '''\n",
    "    이것은 아마존 상품판매 데이터를 분석하고 싶습니다.\n",
    "    분석대상은 \"./data/Amazon_Sale_Report.csv\" 파일 입니다.\n",
    "    데이터를 기반으로 마케팅 인사이트 추출을 위한 분석을 진행해 주세요.\n",
    "    분석은 기본적인 데이터 속성 탐색 부터, 상품 판매 트렌드, 변수 관계, 변수 조합 등 다양한 분석 기법을 수행해 주세요.\n",
    "    데이터 분석 후 인사이트 추출에 필요한 사항이 있다면 그를 위한 추가 분석도 수행해 주세요.\n",
    "    분석 리포트는 상세 분석과 그 것을 뒷받침 할 수 있는 이미지 및 차트를 함께 삽입해 주세요.\n",
    "    최종 리포트는 pdf 형태로 저장해 주세요.\n",
    "'''\n",
    "\n",
    "#user_query = '''\n",
    "#    이것은 아마존 상품판매 데이터를 분석하고 싶습니다. 분석대상은 \"./data/Amazon_Sale_Report.csv\" 파일 입니다. 데이터를 기반으로 마케팅 인사이트 추출을 위한 분석을 진행해 주세요. 분석은 간단하게 1개정도만 해 주세요. 코드 테스트 중이거든요 데이터 분석 후 인사이트 추출에 필요한 사항이 있다면 그를 위한 추가 분석도 수행해 주세요. 분석 리포트는 상세 분석과 그 것을 뒷받침 할 수 있는 이미지 및 차트를 함께 삽입해 주세요. 최종 리포트는 pdf 형태로 저장해 주세요.\n",
    "#    '''\n",
    "\n",
    "# user_query = '''\n",
    "#     영문으로 작성된 pptx 파일을 한글로 변환하고 싶습니다.\n",
    "#     대상은 \"./data/bedrock.pptx\" 파일 입니다.\n",
    "#     슬라이드의 영문을 한글로 번역하고 슬라이드 structure는 유지해 주세요.\n",
    "# '''\n",
    "\n",
    "# user_query = '''\n",
    "#     주어진 프로젝트 소스코드는 \"Bedrock-Manus: AI automation framework optimized for Amazon Bedrock and business use cases 입니다.\"\n",
    "#     프로젝트 소스코드는 \"./data/08_bedrock_manus.tar.gz\" 파일 입니다.\n",
    "#     나는 Streamlit을 이용해서 이 소스코드에 대한 UI를 만들고 싶습니다. \n",
    "#     제가 원하는 요구사항은\n",
    "#     1. 현재 스텝이 무엇인지 (coodinator, supervisor, planner 등등) 보이는 곳이 있으면 좋겠습니다.\n",
    "#         - 스텝에 대한 자세한 정보는 \"./README.md\"를 참고하시면 됩니다. \n",
    "#     2. 각 스텝에서 진행되는 것이 보였으면 좋겠습니다. 즉, 각 스텝에서 llm이 생성하는 토큰들을 실시간으로 볼 수 있으면 좋겠습니다. \n",
    "#     3. \"app.py\" 파일만 생성하지 말고, 필요하다면 기존의 다른 파일 (workflow.py 등)들도  수정하세요. \n",
    "#     4. 수정된 파일은 모두 ./artifacts 에 넣어주세요.\n",
    "#     참고할 사항이 있습니다. \n",
    "#     1. 소스코드 수행은 \"main.py\"로 수행됩니다. \n",
    "#     2. \"./src/service/workflow_service.py\" 이 파일은 참고하지 마세요. \n",
    "\n",
    "#     UI 생성을 위한 프로젝트 코드를 작성해주세요\n",
    "#     결과물은 ./artifacts 에 넣어주세요.\n",
    "# '''\n",
    "\n",
    "# user_query = '''\n",
    "#     \"2025년 중국 가전 신제품 출시에 대한 리포트 작성해줘.\"\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf1bef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.workflow] \u001b[92m===== Starting workflow =====\u001b[0m\n",
      "\n",
      "INFO [src.workflow] \u001b[92m\n",
      "user input: \n",
      "    이것은 아마존 상품판매 데이터를 분석하고 싶습니다.\n",
      "    분석대상은 \"./data/Amazon_Sale_Report.csv\" 파일 입니다.\n",
      "    데이터를 기반으로 마케팅 인사이트 추출을 위한 분석을 진행해 주세요.\n",
      "    분석은 기본적인 데이터 속성 탐색 부터, 상품 판매 트렌드, 변수 관계, 변수 조합 등 다양한 분석 기법을 수행해 주세요.\n",
      "    데이터 분석 후 인사이트 추출에 필요한 사항이 있다면 그를 위한 추가 분석도 수행해 주세요.\n",
      "    분석 리포트는 상세 분석과 그 것을 뒷받침 할 수 있는 이미지 및 차트를 함께 삽입해 주세요.\n",
      "    최종 리포트는 pdf 형태로 저장해 주세요.\n",
      "\u001b[0m\n",
      "\n",
      "INFO [src.graph.nodes] \u001b[92m===== Coordinator talking...... =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./artifacts/' 폴더를 삭제합니다...\n",
      "'./artifacts/' 폴더가 성공적으로 삭제되었습니다.\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "handoff_to_planner: I'll need to consult our planning system for this request.\n",
      "\n",
      "이 요청은 아마존 판매 데이터 분석과 관련된 복잡한 작업을 포함하고 있어 플래너 시스템의 도움이 필요합니다. 데이터 분석, 시각화, PDF 리포트 생성 등 전문적인 작업이 요구되므로 플래너에게 이관하겠습니다."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.graph.nodes] \u001b[92m===== Coordinator completed task =====\u001b[0m\n",
      "\n",
      "INFO [src.graph.nodes] \u001b[92m===== Planner generating full plan =====\u001b[0m\n",
      "\n",
      "INFO [src.graph.nodes] \u001b[94m===== Planner - Deep thinking mode: True =====\u001b[0m\n",
      "\n",
      "INFO [src.graph.nodes] \u001b[94m===== Planner - Search before planning: False =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop reason: end_turn\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "\u001b[94mLet\u001b[0m\u001b[94m me analyze the user request and create a comprehensive plan for\u001b[0m\u001b[94m analyzing Amazon sales data.\n",
      "\n",
      "The request is in\u001b[0m\u001b[94m Korean, and here's the translation:\n",
      "\"I want to analyze\u001b[0m\u001b[94m Amazon product sales data. The target for analysis is the './\u001b[0m\u001b[94mdata/Amazon_Sale_Report.csv' file.\u001b[0m\u001b[94m Please conduct an analysis to extract marketing insights base\u001b[0m\u001b[94md on the data. The analysis should start with exploring basic data attributes,\u001b[0m\u001b[94m then proceed to product sales trends, variable relationships,\u001b[0m\u001b[94m variable combinations, and various other analytical techniques. If\u001b[0m\u001b[94m there are additional analyses needed to extract insights after\u001b[0m\u001b[94m the data analysis, please perform those as well. Please\u001b[0m\u001b[94m include detailed analysis along with supporting images and charts in the analysis\u001b[0m\u001b[94m report. Please save the final report in PDF format.\"\u001b[0m\u001b[94m\n",
      "\n",
      "The task involves:\n",
      "1. Analyzing\u001b[0m\u001b[94m Amazon sales data from a CSV file\n",
      "2. Exploring\u001b[0m\u001b[94m basic data properties\n",
      "3. Analyzing product sales trends\u001b[0m\u001b[94m\n",
      "4. Examining variable relationships an\u001b[0m\u001b[94md combinations\n",
      "5. Creating visualizations and charts\u001b[0m\u001b[94m\n",
      "6. Extracting marketing insights\u001b[0m\u001b[94m\n",
      "7. Generating a detailed report\u001b[0m\u001b[94m in PDF format\n",
      "\n",
      "Now let me create\u001b[0m\u001b[94m a plan with appropriate agent assignments:\n",
      "\n",
      "I\u001b[0m\u001b[94m need to design a workflow\u001b[0m\u001b[94m that efficiently utilizes the agents according to the execution\u001b[0m\u001b[94m rules.\n",
      "\n",
      "For this analysis task:\u001b[0m\u001b[94m\n",
      "- The Coder will handle all data\u001b[0m\u001b[94m loading, processing, analysis, and visualization tasks\u001b[0m\u001b[94m\n",
      "- The Reporter will create the final comprehensive report in\u001b[0m\u001b[94m PDF format\n",
      "\n",
      "Let me create a detailed plan:\u001b[0m# Plan\n",
      "\n",
      "## thought\n",
      "이 요청은 아마존 판매 데이터 분석 프로젝트입니다. \"./data/Amazon_Sale_Report.csv\" 파일을 분석하여 마케팅 인사이트를 도출해야 합니다. 기본적인 데이터 속성 탐색, 상품 판매 트렌드, 변수 관계 분석, 변수 조합 분석 등을 수행하고, 시각화 자료와 함께 최종 보고서를 PDF 형식으로 작성해야 합니다.\n",
      "\n",
      "## title:\n",
      "아마존 상품판매 데이터 분석 및 마케팅 인사이트 도출\n",
      "\n",
      "## steps:\n",
      "### 1. Coder: 데이터 분석 및 시각화 수행\n",
      "- [ ] CSV 파일 로드 및 기본 데이터 구조 탐색(행/열 수, 데이터 타입, 결측치 확인)\n",
      "- [ ] 데이터 전처리(결측치 처리, 데이터 타입 변환, 이상치 처리)\n",
      "- [ ] 기술통계 분석(평균, 중앙값, 표준편차, 최소/최대값 등)\n",
      "- [ ] 상품 카테고리별 판매 분포 분석 및 시각화\n",
      "- [ ] 시간에 따른 판매 트렌드 분석 및 시각화\n",
      "- [ ] 가격대별 판매량 분석 및 시각화\n",
      "- [ ] 변수 간 상관관계 분석 및 히트맵 시각화\n",
      "- [ ] 판매량이 높은 상위 제품/카테고리 분석\n",
      "- [ ] 지역별/배송방식별 판매 데이터 분석\n",
      "- [ ] 마케팅 인사이트 도출을 위한 추가 분석(RFM 분석, 고객 세그먼트 분석 등)\n",
      "- [ ] 모든 분석 결과와 시각화 자료 저장\n",
      "\n",
      "### 2. Reporter: 최종 분석 보고서 작성 및 PDF 변환\n",
      "- [ ] 분석 결과 요약 및 주요 발견사항 정리\n",
      "- [ ] 데이터 탐색 분석 결과 보고서 작성\n",
      "- [ ] 판매 트렌드 분석 결과 보고서 작성\n",
      "- [ ] 변수 관계 및 조합 분석 결과 보고서 작성\n",
      "- [ ] 마케팅 인사이트 및 권장사항 작성\n",
      "- [ ] 분석 차트 및 시각화 자료 포함\n",
      "- [ ] 최종 보고서 PDF 형태로 저장"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.graph.nodes] \u001b[92m===== Planner completed task =====\u001b[0m\n",
      "\n",
      "INFO [src.graph.nodes] \u001b[92m===== Supervisor evaluating next action =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop reason: end_turn\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "{\"next\": \"coder\"}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.graph.nodes] \u001b[92mSupervisor delegating to: coder\u001b[0m\n",
      "\n",
      "INFO [src.graph.nodes] \u001b[92m===== Supervisor completed task =====\u001b[0m\n",
      "\n",
      "INFO [src.graph.nodes] \u001b[92m===== Code agent starting task =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop reason: end_turn\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "--- 대화 턴 1 ---\n",
      "계획에 따라 첫 번째 단계인 \"CSV 파일 로드 및 기본 데이터 구조 탐색\"부터 시작하겠습니다."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.agents.agents] \u001b[1m\n",
      "ToolUse - Tool Name: python_repl_tool, Input: {'code': 'import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport lovelyplots\\nimport os\\nfrom datetime import datetime\\n\\n# Create artifacts directory\\nos.makedirs(\\'./artifacts\\', exist_ok=True)\\n\\n# Read the CSV file\\ndf = pd.read_csv(\\'./data/Amazon_Sale_Report.csv\\')\\n\\n# Basic data exploration\\nprint(\"1. Data Shape (rows, columns):\")\\nprint(df.shape)\\nprint(\"\\\\n2. Column Names:\")\\nprint(df.columns.tolist())\\nprint(\"\\\\n3. Data Types:\")\\nprint(df.dtypes)\\nprint(\"\\\\n4. Missing Values:\")\\nprint(df.isnull().sum())\\nprint(\"\\\\n5. First few rows of data:\")\\nprint(df.head())\\n\\n# Save the results\\nstage_name = \"Initial Data Exploration\"\\nresult_description = f\"\"\"\\nData Shape: {df.shape}\\n\\nColumns: {\\', \\'.join(df.columns.tolist())}\\n\\nData Types:\\n{df.dtypes.to_string()}\\n\\nMissing Values:\\n{df.isnull().sum().to_string()}\\n\\nFirst few rows preview available in data overview.\\n\"\"\"\\n\\n# Result accumulation storage\\nresults_file = \\'./artifacts/all_results.txt\\'\\nbackup_file = \\'./artifacts/all_results_backup_{}.txt\\'.format(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\\n\\ncurrent_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\\ncurrent_result_text = f\"\"\"\\n==================================================\\n## Analysis Stage: {stage_name}\\n## Execution Time: {current_time}\\n--------------------------------------------------\\nResult Description: \\n{result_description}\\n==================================================\\n\"\"\"\\n\\n# Backup existing result file and accumulate results\\nif os.path.exists(results_file):\\n    try:\\n        if os.path.getsize(results_file) > 0:\\n            with open(results_file, \\'r\\', encoding=\\'utf-8\\') as f_src:\\n                with open(backup_file, \\'w\\', encoding=\\'utf-8\\') as f_dst:\\n                    f_dst.write(f_src.read())\\n            print(\"Created backup of existing results file: {}\".format(backup_file))\\n    except Exception as e:\\n        print(\"Error occurred during file backup: {}\".format(e))\\n\\ntry:\\n    with open(results_file, \\'a\\', encoding=\\'utf-8\\') as f:\\n        f.write(current_result_text)\\n    print(\"Results successfully saved.\")\\nexcept Exception as e:\\n    print(\"Error occurred while saving results: {}\".format(e))'}\u001b[0m\n",
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Executing Python code =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop reason: tool_use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Code execution successful =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[91mCoder - Successfully executed:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import lovelyplots\n",
      "import os\n",
      "from datetime import datetime\n",
      "\n",
      "# Create artifacts directory\n",
      "os.makedirs('./artifacts', exist_ok=True)\n",
      "\n",
      "# Read the CSV file\n",
      "df = pd.read_csv('./data/Amazon_Sale_Report.csv')\n",
      "\n",
      "# Basic data exploration\n",
      "print(\"1. Data Shape (rows, columns):\")\n",
      "print(df.shape)\n",
      "print(\"\\n2. Column Names:\")\n",
      "print(df.columns.tolist())\n",
      "print(\"\\n3. Data Types:\")\n",
      "print(df.dtypes)\n",
      "print(\"\\n4. Missing Values:\")\n",
      "print(df.isnull().sum())\n",
      "print(\"\\n5. First few rows of data:\")\n",
      "print(df.head())\n",
      "\n",
      "# Save the results\n",
      "stage_name = \"Initial Data Exploration\"\n",
      "result_description = f\"\"\"\n",
      "Data Shape: {df.shape}\n",
      "\n",
      "Columns: {', '.join(df.columns.tolist())}\n",
      "\n",
      "Data Types:\n",
      "{df.dtypes.to_string()}\n",
      "\n",
      "Missing Values:\n",
      "{df.isnull().sum().to_string()}\n",
      "\n",
      "First few rows preview available in data overview.\n",
      "\"\"\"\n",
      "\n",
      "# Result accumulation storage\n",
      "results_file = './artifacts/all_results.txt'\n",
      "backup_file = './artifacts/all_results_backup_{}.txt'.format(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
      "\n",
      "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
      "current_result_text = f\"\"\"\n",
      "==================================================\n",
      "## Analysis Stage: {stage_name}\n",
      "## Execution Time: {current_time}\n",
      "--------------------------------------------------\n",
      "Result Description: \n",
      "{result_description}\n",
      "==================================================\n",
      "\"\"\"\n",
      "\n",
      "# Backup existing result file and accumulate results\n",
      "if os.path.exists(results_file):\n",
      "    try:\n",
      "        if os.path.getsize(results_file) > 0:\n",
      "            with open(results_file, 'r', encoding='utf-8') as f_src:\n",
      "                with open(backup_file, 'w', encoding='utf-8') as f_dst:\n",
      "                    f_dst.write(f_src.read())\n",
      "            print(\"Created backup of existing results file: {}\".format(backup_file))\n",
      "    except Exception as e:\n",
      "        print(\"Error occurred during file backup: {}\".format(e))\n",
      "\n",
      "try:\n",
      "    with open(results_file, 'a', encoding='utf-8') as f:\n",
      "        f.write(current_result_text)\n",
      "    print(\"Results successfully saved.\")\n",
      "except Exception as e:\n",
      "    print(\"Error occurred while saving results: {}\".format(e))\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[94m\n",
      "Stdout: 1. Data Shape (rows, columns):\n",
      "(128975, 24)\n",
      "\n",
      "2. Column Names:\n",
      "['index', 'Order ID', 'Date', 'Status', 'Fulfilment', 'Sales Channel ', 'ship-service-level', 'Style', 'SKU', 'Category', 'Size', 'ASIN', 'Courier Status', 'Qty', 'currency', 'Amount', 'ship-city', 'ship-state', 'ship-postal-code', 'ship-country', 'promotion-ids', 'B2B', 'fulfilled-by', 'Unnamed: 22']\n",
      "\n",
      "3. Data Types:\n",
      "index                   int64\n",
      "Order ID               object\n",
      "Date                   object\n",
      "Status                 object\n",
      "Fulfilment             object\n",
      "Sales Channel          object\n",
      "ship-service-level     object\n",
      "Style                  object\n",
      "SKU                    object\n",
      "Category               object\n",
      "Size                   object\n",
      "ASIN                   object\n",
      "Courier Status         object\n",
      "Qty                     int64\n",
      "currency               object\n",
      "Amount                float64\n",
      "ship-city              object\n",
      "ship-state             object\n",
      "ship-postal-code      float64\n",
      "ship-country           object\n",
      "promotion-ids          object\n",
      "B2B                      bool\n",
      "fulfilled-by           object\n",
      "Unnamed: 22            object\n",
      "dtype: object\n",
      "\n",
      "4. Missing Values:\n",
      "index                     0\n",
      "Order ID                  0\n",
      "Date                      0\n",
      "Status                    0\n",
      "Fulfilment                0\n",
      "Sales Channel             0\n",
      "ship-service-level        0\n",
      "Style                     0\n",
      "SKU                       0\n",
      "Category                  0\n",
      "Size                      0\n",
      "ASIN                      0\n",
      "Courier Status         6872\n",
      "Qty                       0\n",
      "currency               7795\n",
      "Amount                 7795\n",
      "ship-city                33\n",
      "ship-state               33\n",
      "ship-postal-code         33\n",
      "ship-country             33\n",
      "promotion-ids         49153\n",
      "B2B                       0\n",
      "fulfilled-by          89698\n",
      "Unnamed: 22           49050\n",
      "dtype: int64\n",
      "\n",
      "5. First few rows of data:\n",
      "   index             Order ID      Date  ...    B2B fulfilled-by Unnamed: 22\n",
      "0      0  405-8078784-5731545  04-30-22  ...  False    Easy Ship         NaN\n",
      "1      1  171-9198151-1101146  04-30-22  ...  False    Easy Ship         NaN\n",
      "2      2  404-0687676-7273146  04-30-22  ...   True          NaN         NaN\n",
      "3      3  403-9615377-8133951  04-30-22  ...  False    Easy Ship         NaN\n",
      "4      4  407-1069790-7240320  04-30-22  ...  False          NaN         NaN\n",
      "\n",
      "[5 rows x 24 columns]\n",
      "Results successfully saved.\n",
      "\u001b[0m\n",
      "2025-05-09 15:03:52.932 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-09 15:03:52.932 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "\n",
      "INFO [src.agents.agents] \u001b[1mToolUse - 도구 실행 결과를 대화에 추가했습니다.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 대화 턴 2 ---\n",
      "\n",
      "\n",
      "데이터의 기본 구조를 살펴본 결과, 다음과 같은 특징들이 발견되었습니다:\n",
      "\n",
      "1. 데이터 크기:\n",
      "   - 총 128,975개의 행과 24개의 열로 구성\n",
      "   - 주문 데이터와 관련된 다양한 정보 포함\n",
      "\n",
      "2. 주요 결측치 현황:\n",
      "   - Courier Status: 6,872개\n",
      "   - currency와 Amount: 각각 7,795개\n",
      "   - promotion-ids: 49,153개\n",
      "   - fulfilled-by: 89,698개\n",
      "   - Unnamed: 22: 49,050개\n",
      "\n",
      "3. 데이터 타입:\n",
      "   - 수치형: Amount(float64), Qty(int64), ship-postal-code(float64)\n",
      "   - 범주형: Status, Category, Size 등\n",
      "   - 날짜: Date (현재 object 타입)\n",
      "   - 불리언: B2B\n",
      "\n",
      "다음으로 데이터 전처리를 진행하겠습니다:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.agents.agents] \u001b[1m\n",
      "ToolUse - Tool Name: python_repl_tool, Input: {'code': 'import pandas as pd\\nimport numpy as np\\nfrom datetime import datetime\\nimport os\\n\\n# Read the CSV file again\\ndf = pd.read_csv(\\'./data/Amazon_Sale_Report.csv\\')\\n\\n# 1. Date 컬럼 datetime 형식으로 변환\\ndf[\\'Date\\'] = pd.to_datetime(df[\\'Date\\'], format=\\'%m-%d-%y\\')\\n\\n# 2. 불필요한 컬럼 제거\\ndf = df.drop([\\'index\\', \\'Unnamed: 22\\'], axis=1)\\n\\n# 3. 결측치 처리\\n# - Courier Status는 \\'Unknown\\'으로 대체\\ndf[\\'Courier Status\\'].fillna(\\'Unknown\\', inplace=True)\\n# - currency와 Amount는 함께 처리 (Amount가 없는 경우는 0으로 처리)\\ndf[\\'currency\\'].fillna(\\'INR\\', inplace=True)\\ndf[\\'Amount\\'].fillna(0, inplace=True)\\n# - 배송 관련 정보는 \\'Unknown\\'으로 대체\\nshipping_cols = [\\'ship-city\\', \\'ship-state\\', \\'ship-postal-code\\', \\'ship-country\\']\\nfor col in shipping_cols:\\n    df[col] = df[col].fillna(\\'Unknown\\')\\n# - promotion-ids는 \\'No Promotion\\'으로 대체\\ndf[\\'promotion-ids\\'].fillna(\\'No Promotion\\', inplace=True)\\n# - fulfilled-by는 \\'Unknown\\'으로 대체\\ndf[\\'fulfilled-by\\'].fillna(\\'Unknown\\', inplace=True)\\n\\n# 4. 데이터 타입 확인 및 변환 후 결과 출력\\nprint(\"1. Updated Data Types:\")\\nprint(df.dtypes)\\nprint(\"\\\\n2. Remaining Missing Values:\")\\nprint(df.isnull().sum())\\nprint(\"\\\\n3. Data Sample After Preprocessing:\")\\nprint(df.head())\\n\\n# Save the preprocessing results\\nstage_name = \"Data Preprocessing\"\\nresult_description = f\"\"\"\\nPreprocessing Steps Completed:\\n1. Date column converted to datetime format\\n2. Removed unnecessary columns (index, Unnamed: 22)\\n3. Missing values handled:\\n   - Courier Status: filled with \\'Unknown\\'\\n   - Currency: filled with \\'INR\\'\\n   - Amount: filled with 0\\n   - Shipping information: filled with \\'Unknown\\'\\n   - Promotion IDs: filled with \\'No Promotion\\'\\n   - Fulfilled by: filled with \\'Unknown\\'\\n\\nUpdated Data Types:\\n{df.dtypes.to_string()}\\n\\nRemaining Missing Values:\\n{df.isnull().sum().to_string()}\\n\"\"\"\\n\\n# Result accumulation storage\\nresults_file = \\'./artifacts/all_results.txt\\'\\ncurrent_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\\ncurrent_result_text = f\"\"\"\\n==================================================\\n## Analysis Stage: {stage_name}\\n## Execution Time: {current_time}\\n--------------------------------------------------\\nResult Description: \\n{result_description}\\n==================================================\\n\"\"\"\\n\\n# Append new results\\ntry:\\n    with open(results_file, \\'a\\', encoding=\\'utf-8\\') as f:\\n        f.write(current_result_text)\\n    print(\"\\\\nResults successfully saved.\")\\nexcept Exception as e:\\n    print(\"Error occurred while saving results: {}\".format(e))'}\u001b[0m\n",
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Executing Python code =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop reason: tool_use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Code execution successful =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[91mCoder - Successfully executed:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import os\n",
      "\n",
      "# Read the CSV file again\n",
      "df = pd.read_csv('./data/Amazon_Sale_Report.csv')\n",
      "\n",
      "# 1. Date 컬럼 datetime 형식으로 변환\n",
      "df['Date'] = pd.to_datetime(df['Date'], format='%m-%d-%y')\n",
      "\n",
      "# 2. 불필요한 컬럼 제거\n",
      "df = df.drop(['index', 'Unnamed: 22'], axis=1)\n",
      "\n",
      "# 3. 결측치 처리\n",
      "# - Courier Status는 'Unknown'으로 대체\n",
      "df['Courier Status'].fillna('Unknown', inplace=True)\n",
      "# - currency와 Amount는 함께 처리 (Amount가 없는 경우는 0으로 처리)\n",
      "df['currency'].fillna('INR', inplace=True)\n",
      "df['Amount'].fillna(0, inplace=True)\n",
      "# - 배송 관련 정보는 'Unknown'으로 대체\n",
      "shipping_cols = ['ship-city', 'ship-state', 'ship-postal-code', 'ship-country']\n",
      "for col in shipping_cols:\n",
      "    df[col] = df[col].fillna('Unknown')\n",
      "# - promotion-ids는 'No Promotion'으로 대체\n",
      "df['promotion-ids'].fillna('No Promotion', inplace=True)\n",
      "# - fulfilled-by는 'Unknown'으로 대체\n",
      "df['fulfilled-by'].fillna('Unknown', inplace=True)\n",
      "\n",
      "# 4. 데이터 타입 확인 및 변환 후 결과 출력\n",
      "print(\"1. Updated Data Types:\")\n",
      "print(df.dtypes)\n",
      "print(\"\\n2. Remaining Missing Values:\")\n",
      "print(df.isnull().sum())\n",
      "print(\"\\n3. Data Sample After Preprocessing:\")\n",
      "print(df.head())\n",
      "\n",
      "# Save the preprocessing results\n",
      "stage_name = \"Data Preprocessing\"\n",
      "result_description = f\"\"\"\n",
      "Preprocessing Steps Completed:\n",
      "1. Date column converted to datetime format\n",
      "2. Removed unnecessary columns (index, Unnamed: 22)\n",
      "3. Missing values handled:\n",
      "   - Courier Status: filled with 'Unknown'\n",
      "   - Currency: filled with 'INR'\n",
      "   - Amount: filled with 0\n",
      "   - Shipping information: filled with 'Unknown'\n",
      "   - Promotion IDs: filled with 'No Promotion'\n",
      "   - Fulfilled by: filled with 'Unknown'\n",
      "\n",
      "Updated Data Types:\n",
      "{df.dtypes.to_string()}\n",
      "\n",
      "Remaining Missing Values:\n",
      "{df.isnull().sum().to_string()}\n",
      "\"\"\"\n",
      "\n",
      "# Result accumulation storage\n",
      "results_file = './artifacts/all_results.txt'\n",
      "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
      "current_result_text = f\"\"\"\n",
      "==================================================\n",
      "## Analysis Stage: {stage_name}\n",
      "## Execution Time: {current_time}\n",
      "--------------------------------------------------\n",
      "Result Description: \n",
      "{result_description}\n",
      "==================================================\n",
      "\"\"\"\n",
      "\n",
      "# Append new results\n",
      "try:\n",
      "    with open(results_file, 'a', encoding='utf-8') as f:\n",
      "        f.write(current_result_text)\n",
      "    print(\"\\nResults successfully saved.\")\n",
      "except Exception as e:\n",
      "    print(\"Error occurred while saving results: {}\".format(e))\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[94m\n",
      "Stdout: 1. Updated Data Types:\n",
      "Order ID                      object\n",
      "Date                  datetime64[ns]\n",
      "Status                        object\n",
      "Fulfilment                    object\n",
      "Sales Channel                 object\n",
      "ship-service-level            object\n",
      "Style                         object\n",
      "SKU                           object\n",
      "Category                      object\n",
      "Size                          object\n",
      "ASIN                          object\n",
      "Courier Status                object\n",
      "Qty                            int64\n",
      "currency                      object\n",
      "Amount                       float64\n",
      "ship-city                     object\n",
      "ship-state                    object\n",
      "ship-postal-code              object\n",
      "ship-country                  object\n",
      "promotion-ids                 object\n",
      "B2B                             bool\n",
      "fulfilled-by                  object\n",
      "dtype: object\n",
      "\n",
      "2. Remaining Missing Values:\n",
      "Order ID              0\n",
      "Date                  0\n",
      "Status                0\n",
      "Fulfilment            0\n",
      "Sales Channel         0\n",
      "ship-service-level    0\n",
      "Style                 0\n",
      "SKU                   0\n",
      "Category              0\n",
      "Size                  0\n",
      "ASIN                  0\n",
      "Courier Status        0\n",
      "Qty                   0\n",
      "currency              0\n",
      "Amount                0\n",
      "ship-city             0\n",
      "ship-state            0\n",
      "ship-postal-code      0\n",
      "ship-country          0\n",
      "promotion-ids         0\n",
      "B2B                   0\n",
      "fulfilled-by          0\n",
      "dtype: int64\n",
      "\n",
      "3. Data Sample After Preprocessing:\n",
      "              Order ID       Date  ...    B2B fulfilled-by\n",
      "0  405-8078784-5731545 2022-04-30  ...  False    Easy Ship\n",
      "1  171-9198151-1101146 2022-04-30  ...  False    Easy Ship\n",
      "2  404-0687676-7273146 2022-04-30  ...   True      Unknown\n",
      "3  403-9615377-8133951 2022-04-30  ...  False    Easy Ship\n",
      "4  407-1069790-7240320 2022-04-30  ...  False      Unknown\n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Results successfully saved.\n",
      "\u001b[0m\n",
      "2025-05-09 15:04:21.882 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "\n",
      "INFO [src.agents.agents] \u001b[1mToolUse - 도구 실행 결과를 대화에 추가했습니다.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 대화 턴 3 ---\n",
      "\n",
      "\n",
      "데이터 전처리가 완료되었습니다. 이제 기술통계 분석을 진행하겠습니다:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.agents.agents] \u001b[1m\n",
      "ToolUse - Tool Name: python_repl_tool, Input: {'code': 'import pandas as pd\\nimport numpy as np\\nfrom datetime import datetime\\nimport os\\n\\n# Read the CSV file and apply previous preprocessing\\ndf = pd.read_csv(\\'./data/Amazon_Sale_Report.csv\\')\\ndf[\\'Date\\'] = pd.to_datetime(df[\\'Date\\'], format=\\'%m-%d-%y\\')\\ndf = df.drop([\\'index\\', \\'Unnamed: 22\\'], axis=1)\\ndf[\\'Courier Status\\'].fillna(\\'Unknown\\', inplace=True)\\ndf[\\'currency\\'].fillna(\\'INR\\', inplace=True)\\ndf[\\'Amount\\'].fillna(0, inplace=True)\\nshipping_cols = [\\'ship-city\\', \\'ship-state\\', \\'ship-postal-code\\', \\'ship-country\\']\\nfor col in shipping_cols:\\n    df[col] = df[col].fillna(\\'Unknown\\')\\ndf[\\'promotion-ids\\'].fillna(\\'No Promotion\\', inplace=True)\\ndf[\\'fulfilled-by\\'].fillna(\\'Unknown\\', inplace=True)\\n\\n# 기술통계 분석\\nprint(\"1. Numerical Variables Statistics:\")\\nprint(df[[\\'Qty\\', \\'Amount\\']].describe())\\n\\nprint(\"\\\\n2. Top Categories:\")\\nprint(df[\\'Category\\'].value_counts().head())\\n\\nprint(\"\\\\n3. Order Status Distribution:\")\\nprint(df[\\'Status\\'].value_counts())\\n\\nprint(\"\\\\n4. Sales Channel Distribution:\")\\nprint(df[\\'Sales Channel \\'].value_counts())\\n\\nprint(\"\\\\n5. Fulfillment Method Distribution:\")\\nprint(df[\\'Fulfilment\\'].value_counts())\\n\\nprint(\"\\\\n6. B2B vs B2C Distribution:\")\\nprint(df[\\'B2B\\'].value_counts(normalize=True) * 100)\\n\\nprint(\"\\\\n7. Top Ship States:\")\\nprint(df[\\'ship-state\\'].value_counts().head())\\n\\n# Monthly sales analysis\\ndf[\\'Month\\'] = df[\\'Date\\'].dt.to_period(\\'M\\')\\nmonthly_sales = df.groupby(\\'Month\\')[\\'Amount\\'].sum()\\nprint(\"\\\\n8. Monthly Sales Summary:\")\\nprint(monthly_sales)\\n\\n# Save the results\\nstage_name = \"Descriptive Statistics Analysis\"\\nresult_description = f\"\"\"\\n1. Numerical Variables Statistics:\\n{df[[\\'Qty\\', \\'Amount\\']].describe().to_string()}\\n\\n2. Top Categories:\\n{df[\\'Category\\'].value_counts().head().to_string()}\\n\\n3. Order Status Distribution:\\n{df[\\'Status\\'].value_counts().to_string()}\\n\\n4. Sales Channel Distribution:\\n{df[\\'Sales Channel \\'].value_counts().to_string()}\\n\\n5. Fulfillment Method Distribution:\\n{df[\\'Fulfilment\\'].value_counts().to_string()}\\n\\n6. B2B vs B2C Distribution (%):\\n{df[\\'B2B\\'].value_counts(normalize=True) * 100}\\n\\n7. Top Ship States:\\n{df[\\'ship-state\\'].value_counts().head().to_string()}\\n\\n8. Monthly Sales Summary:\\n{monthly_sales.to_string()}\\n\"\"\"\\n\\n# Result accumulation storage\\nresults_file = \\'./artifacts/all_results.txt\\'\\ncurrent_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\\ncurrent_result_text = f\"\"\"\\n==================================================\\n## Analysis Stage: {stage_name}\\n## Execution Time: {current_time}\\n--------------------------------------------------\\nResult Description: \\n{result_description}\\n==================================================\\n\"\"\"\\n\\n# Append new results\\ntry:\\n    with open(results_file, \\'a\\', encoding=\\'utf-8\\') as f:\\n        f.write(current_result_text)\\n    print(\"\\\\nResults successfully saved.\")\\nexcept Exception as e:\\n    print(\"Error occurred while saving results: {}\".format(e))'}\u001b[0m\n",
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Executing Python code =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop reason: tool_use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Code execution successful =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[91mCoder - Successfully executed:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import os\n",
      "\n",
      "# Read the CSV file and apply previous preprocessing\n",
      "df = pd.read_csv('./data/Amazon_Sale_Report.csv')\n",
      "df['Date'] = pd.to_datetime(df['Date'], format='%m-%d-%y')\n",
      "df = df.drop(['index', 'Unnamed: 22'], axis=1)\n",
      "df['Courier Status'].fillna('Unknown', inplace=True)\n",
      "df['currency'].fillna('INR', inplace=True)\n",
      "df['Amount'].fillna(0, inplace=True)\n",
      "shipping_cols = ['ship-city', 'ship-state', 'ship-postal-code', 'ship-country']\n",
      "for col in shipping_cols:\n",
      "    df[col] = df[col].fillna('Unknown')\n",
      "df['promotion-ids'].fillna('No Promotion', inplace=True)\n",
      "df['fulfilled-by'].fillna('Unknown', inplace=True)\n",
      "\n",
      "# 기술통계 분석\n",
      "print(\"1. Numerical Variables Statistics:\")\n",
      "print(df[['Qty', 'Amount']].describe())\n",
      "\n",
      "print(\"\\n2. Top Categories:\")\n",
      "print(df['Category'].value_counts().head())\n",
      "\n",
      "print(\"\\n3. Order Status Distribution:\")\n",
      "print(df['Status'].value_counts())\n",
      "\n",
      "print(\"\\n4. Sales Channel Distribution:\")\n",
      "print(df['Sales Channel '].value_counts())\n",
      "\n",
      "print(\"\\n5. Fulfillment Method Distribution:\")\n",
      "print(df['Fulfilment'].value_counts())\n",
      "\n",
      "print(\"\\n6. B2B vs B2C Distribution:\")\n",
      "print(df['B2B'].value_counts(normalize=True) * 100)\n",
      "\n",
      "print(\"\\n7. Top Ship States:\")\n",
      "print(df['ship-state'].value_counts().head())\n",
      "\n",
      "# Monthly sales analysis\n",
      "df['Month'] = df['Date'].dt.to_period('M')\n",
      "monthly_sales = df.groupby('Month')['Amount'].sum()\n",
      "print(\"\\n8. Monthly Sales Summary:\")\n",
      "print(monthly_sales)\n",
      "\n",
      "# Save the results\n",
      "stage_name = \"Descriptive Statistics Analysis\"\n",
      "result_description = f\"\"\"\n",
      "1. Numerical Variables Statistics:\n",
      "{df[['Qty', 'Amount']].describe().to_string()}\n",
      "\n",
      "2. Top Categories:\n",
      "{df['Category'].value_counts().head().to_string()}\n",
      "\n",
      "3. Order Status Distribution:\n",
      "{df['Status'].value_counts().to_string()}\n",
      "\n",
      "4. Sales Channel Distribution:\n",
      "{df['Sales Channel '].value_counts().to_string()}\n",
      "\n",
      "5. Fulfillment Method Distribution:\n",
      "{df['Fulfilment'].value_counts().to_string()}\n",
      "\n",
      "6. B2B vs B2C Distribution (%):\n",
      "{df['B2B'].value_counts(normalize=True) * 100}\n",
      "\n",
      "7. Top Ship States:\n",
      "{df['ship-state'].value_counts().head().to_string()}\n",
      "\n",
      "8. Monthly Sales Summary:\n",
      "{monthly_sales.to_string()}\n",
      "\"\"\"\n",
      "\n",
      "# Result accumulation storage\n",
      "results_file = './artifacts/all_results.txt'\n",
      "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
      "current_result_text = f\"\"\"\n",
      "==================================================\n",
      "## Analysis Stage: {stage_name}\n",
      "## Execution Time: {current_time}\n",
      "--------------------------------------------------\n",
      "Result Description: \n",
      "{result_description}\n",
      "==================================================\n",
      "\"\"\"\n",
      "\n",
      "# Append new results\n",
      "try:\n",
      "    with open(results_file, 'a', encoding='utf-8') as f:\n",
      "        f.write(current_result_text)\n",
      "    print(\"\\nResults successfully saved.\")\n",
      "except Exception as e:\n",
      "    print(\"Error occurred while saving results: {}\".format(e))\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[94m\n",
      "Stdout: 1. Numerical Variables Statistics:\n",
      "                 Qty         Amount\n",
      "count  128975.000000  128975.000000\n",
      "mean        0.904431     609.363662\n",
      "std         0.313354     313.347147\n",
      "min         0.000000       0.000000\n",
      "25%         1.000000     413.000000\n",
      "50%         1.000000     583.000000\n",
      "75%         1.000000     771.000000\n",
      "max        15.000000    5584.000000\n",
      "\n",
      "2. Top Categories:\n",
      "Category\n",
      "Set              50284\n",
      "kurta            49877\n",
      "Western Dress    15500\n",
      "Top              10622\n",
      "Ethnic Dress      1159\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. Order Status Distribution:\n",
      "Status\n",
      "Shipped                          77804\n",
      "Shipped - Delivered to Buyer     28769\n",
      "Cancelled                        18332\n",
      "Shipped - Returned to Seller      1953\n",
      "Shipped - Picked Up                973\n",
      "Pending                            658\n",
      "Pending - Waiting for Pick Up      281\n",
      "Shipped - Returning to Seller      145\n",
      "Shipped - Out for Delivery          35\n",
      "Shipped - Rejected by Buyer         11\n",
      "Shipping                             8\n",
      "Shipped - Lost in Transit            5\n",
      "Shipped - Damaged                    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "4. Sales Channel Distribution:\n",
      "Sales Channel \n",
      "Amazon.in     128851\n",
      "Non-Amazon       124\n",
      "Name: count, dtype: int64\n",
      "\n",
      "5. Fulfillment Method Distribution:\n",
      "Fulfilment\n",
      "Amazon      89698\n",
      "Merchant    39277\n",
      "Name: count, dtype: int64\n",
      "\n",
      "6. B2B vs B2C Distribution:\n",
      "B2B\n",
      "False    99.324675\n",
      "True      0.675325\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "7. Top Ship States:\n",
      "ship-state\n",
      "MAHARASHTRA      22260\n",
      "KARNATAKA        17326\n",
      "TAMIL NADU       11483\n",
      "TELANGANA        11330\n",
      "UTTAR PRADESH    10638\n",
      "Name: count, dtype: int64\n",
      "\n",
      "8. Monthly Sales Summary:\n",
      "Month\n",
      "2022-03      101683.85\n",
      "2022-04    28838708.32\n",
      "2022-05    26226476.75\n",
      "2022-06    23425809.38\n",
      "Freq: M, Name: Amount, dtype: float64\n",
      "\n",
      "Results successfully saved.\n",
      "\u001b[0m\n",
      "2025-05-09 15:04:41.317 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "\n",
      "INFO [src.agents.agents] \u001b[1mToolUse - 도구 실행 결과를 대화에 추가했습니다.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 대화 턴 4 ---\n",
      "\n",
      "\n",
      "기술통계 분석 결과를 바탕으로 주요 시각화를 진행하겠습니다:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.agents.agents] \u001b[1m\n",
      "ToolUse - Tool Name: python_repl_tool, Input: {'code': 'import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport lovelyplots\\nfrom datetime import datetime\\nimport os\\n\\n# Set the style\\nplt.style.use([\\'ipynb\\', \\'use_mathtext\\',\\'colors5-light\\'])\\nplt.rc(\\'font\\', family=\\'NanumGothic\\')\\n\\n# Read and preprocess the data\\ndf = pd.read_csv(\\'./data/Amazon_Sale_Report.csv\\')\\ndf[\\'Date\\'] = pd.to_datetime(df[\\'Date\\'], format=\\'%m-%d-%y\\')\\n\\n# Create artifacts directory\\nos.makedirs(\\'./artifacts\\', exist_ok=True)\\n\\n# 1. 카테고리별 판매량 분포\\nplt.figure(figsize=(12, 6), dpi=150)\\ndf[\\'Category\\'].value_counts().plot(kind=\\'bar\\')\\nplt.title(\\'카테고리별 판매량 분포\\', fontsize=14)\\nplt.xlabel(\\'카테고리\\', fontsize=12)\\nplt.ylabel(\\'판매량\\', fontsize=12)\\nplt.xticks(rotation=45)\\nplt.grid(alpha=0.3)\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/category_distribution.png\\')\\nplt.close()\\n\\n# 2. 월별 판매액 추이\\nmonthly_sales = df.groupby(df[\\'Date\\'].dt.to_period(\\'M\\'))[\\'Amount\\'].sum()\\nplt.figure(figsize=(12, 6), dpi=150)\\nmonthly_sales.plot(kind=\\'line\\', marker=\\'o\\')\\nplt.title(\\'월별 판매액 추이\\', fontsize=14)\\nplt.xlabel(\\'월\\', fontsize=12)\\nplt.ylabel(\\'판매액\\', fontsize=12)\\nplt.grid(alpha=0.3)\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/monthly_sales_trend.png\\')\\nplt.close()\\n\\n# 3. 주문 상태 분포\\nplt.figure(figsize=(12, 6), dpi=150)\\ndf[\\'Status\\'].value_counts().plot(kind=\\'bar\\')\\nplt.title(\\'주문 상태 분포\\', fontsize=14)\\nplt.xlabel(\\'주문 상태\\', fontsize=12)\\nplt.ylabel(\\'주문 수\\', fontsize=12)\\nplt.xticks(rotation=45)\\nplt.grid(alpha=0.3)\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/order_status_distribution.png\\')\\nplt.close()\\n\\n# 4. 상위 5개 주(state)의 판매량\\nplt.figure(figsize=(12, 6), dpi=150)\\ndf[\\'ship-state\\'].value_counts().head().plot(kind=\\'bar\\')\\nplt.title(\\'상위 5개 주(State)의 판매량\\', fontsize=14)\\nplt.xlabel(\\'주(State)\\', fontsize=12)\\nplt.ylabel(\\'판매량\\', fontsize=12)\\nplt.xticks(rotation=45)\\nplt.grid(alpha=0.3)\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/top_states_sales.png\\')\\nplt.close()\\n\\n# 5. 가격대별 판매량 분포 (히스토그램)\\nplt.figure(figsize=(12, 6), dpi=150)\\nplt.hist(df[\\'Amount\\'], bins=50, edgecolor=\\'black\\')\\nplt.title(\\'가격대별 판매량 분포\\', fontsize=14)\\nplt.xlabel(\\'가격\\', fontsize=12)\\nplt.ylabel(\\'판매량\\', fontsize=12)\\nplt.grid(alpha=0.3)\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/price_distribution.png\\')\\nplt.close()\\n\\n# Save visualization results\\nstage_name = \"Data Visualization\"\\nresult_description = \"\"\"\\n다음과 같은 시각화 차트들이 생성되었습니다:\\n\\n1. 카테고리별 판매량 분포 (category_distribution.png)\\n   - Set과 kurta 카테고리가 가장 높은 판매량을 보임\\n   - Western Dress와 Top이 중간 수준의 판매량을 기록\\n\\n2. 월별 판매액 추이 (monthly_sales_trend.png)\\n   - 2022년 4월에 가장 높은 판매액을 기록\\n   - 이후 점진적으로 감소하는 추세를 보임\\n\\n3. 주문 상태 분포 (order_status_distribution.png)\\n   - 대부분의 주문이 \\'Shipped\\' 또는 \\'Shipped - Delivered to Buyer\\' 상태\\n   - 취소된 주문(Cancelled)이 세 번째로 많은 비중을 차지\\n\\n4. 상위 5개 주(State)의 판매량 (top_states_sales.png)\\n   - MAHARASHTRA가 가장 높은 판매량을 기록\\n   - KARNATAKA와 TAMIL NADU가 그 뒤를 이음\\n\\n5. 가격대별 판매량 분포 (price_distribution.png)\\n   - 대부분의 상품이 중저가 범위에 분포\\n   - 오른쪽으로 긴 꼬리를 가진 분포를 보임\\n\"\"\"\\n\\n# Result accumulation storage\\nresults_file = \\'./artifacts/all_results.txt\\'\\ncurrent_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\\ncurrent_result_text = f\"\"\"\\n==================================================\\n## Analysis Stage: {stage_name}\\n## Execution Time: {current_time}\\n--------------------------------------------------\\nResult Description: \\n{result_description}\\n==================================================\\n\"\"\"\\n\\nartifact_files = [\\n    [\"./artifacts/category_distribution.png\", \"카테고리별 판매량 분포 차트\"],\\n    [\"./artifacts/monthly_sales_trend.png\", \"월별 판매액 추이 차트\"],\\n    [\"./artifacts/order_status_distribution.png\", \"주문 상태 분포 차트\"],\\n    [\"./artifacts/top_states_sales.png\", \"상위 5개 주(State)의 판매량 차트\"],\\n    [\"./artifacts/price_distribution.png\", \"가격대별 판매량 분포 차트\"]\\n]\\n\\nif artifact_files:\\n    current_result_text += \"--------------------------------------------------\\\\nGenerated Files:\\\\n\"\\n    for file_path, file_desc in artifact_files:\\n        current_result_text += \"- {} : {}\\\\n\".format(file_path, file_desc)\\n\\n# Append new results\\ntry:\\n    with open(results_file, \\'a\\', encoding=\\'utf-8\\') as f:\\n        f.write(current_result_text)\\n    print(\"Results and visualizations successfully saved.\")\\nexcept Exception as e:\\n    print(\"Error occurred while saving results: {}\".format(e))'}\u001b[0m\n",
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Executing Python code =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop reason: tool_use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Code execution successful =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[91mCoder - Successfully executed:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import lovelyplots\n",
      "from datetime import datetime\n",
      "import os\n",
      "\n",
      "# Set the style\n",
      "plt.style.use(['ipynb', 'use_mathtext','colors5-light'])\n",
      "plt.rc('font', family='NanumGothic')\n",
      "\n",
      "# Read and preprocess the data\n",
      "df = pd.read_csv('./data/Amazon_Sale_Report.csv')\n",
      "df['Date'] = pd.to_datetime(df['Date'], format='%m-%d-%y')\n",
      "\n",
      "# Create artifacts directory\n",
      "os.makedirs('./artifacts', exist_ok=True)\n",
      "\n",
      "# 1. 카테고리별 판매량 분포\n",
      "plt.figure(figsize=(12, 6), dpi=150)\n",
      "df['Category'].value_counts().plot(kind='bar')\n",
      "plt.title('카테고리별 판매량 분포', fontsize=14)\n",
      "plt.xlabel('카테고리', fontsize=12)\n",
      "plt.ylabel('판매량', fontsize=12)\n",
      "plt.xticks(rotation=45)\n",
      "plt.grid(alpha=0.3)\n",
      "plt.tight_layout()\n",
      "plt.savefig('./artifacts/category_distribution.png')\n",
      "plt.close()\n",
      "\n",
      "# 2. 월별 판매액 추이\n",
      "monthly_sales = df.groupby(df['Date'].dt.to_period('M'))['Amount'].sum()\n",
      "plt.figure(figsize=(12, 6), dpi=150)\n",
      "monthly_sales.plot(kind='line', marker='o')\n",
      "plt.title('월별 판매액 추이', fontsize=14)\n",
      "plt.xlabel('월', fontsize=12)\n",
      "plt.ylabel('판매액', fontsize=12)\n",
      "plt.grid(alpha=0.3)\n",
      "plt.tight_layout()\n",
      "plt.savefig('./artifacts/monthly_sales_trend.png')\n",
      "plt.close()\n",
      "\n",
      "# 3. 주문 상태 분포\n",
      "plt.figure(figsize=(12, 6), dpi=150)\n",
      "df['Status'].value_counts().plot(kind='bar')\n",
      "plt.title('주문 상태 분포', fontsize=14)\n",
      "plt.xlabel('주문 상태', fontsize=12)\n",
      "plt.ylabel('주문 수', fontsize=12)\n",
      "plt.xticks(rotation=45)\n",
      "plt.grid(alpha=0.3)\n",
      "plt.tight_layout()\n",
      "plt.savefig('./artifacts/order_status_distribution.png')\n",
      "plt.close()\n",
      "\n",
      "# 4. 상위 5개 주(state)의 판매량\n",
      "plt.figure(figsize=(12, 6), dpi=150)\n",
      "df['ship-state'].value_counts().head().plot(kind='bar')\n",
      "plt.title('상위 5개 주(State)의 판매량', fontsize=14)\n",
      "plt.xlabel('주(State)', fontsize=12)\n",
      "plt.ylabel('판매량', fontsize=12)\n",
      "plt.xticks(rotation=45)\n",
      "plt.grid(alpha=0.3)\n",
      "plt.tight_layout()\n",
      "plt.savefig('./artifacts/top_states_sales.png')\n",
      "plt.close()\n",
      "\n",
      "# 5. 가격대별 판매량 분포 (히스토그램)\n",
      "plt.figure(figsize=(12, 6), dpi=150)\n",
      "plt.hist(df['Amount'], bins=50, edgecolor='black')\n",
      "plt.title('가격대별 판매량 분포', fontsize=14)\n",
      "plt.xlabel('가격', fontsize=12)\n",
      "plt.ylabel('판매량', fontsize=12)\n",
      "plt.grid(alpha=0.3)\n",
      "plt.tight_layout()\n",
      "plt.savefig('./artifacts/price_distribution.png')\n",
      "plt.close()\n",
      "\n",
      "# Save visualization results\n",
      "stage_name = \"Data Visualization\"\n",
      "result_description = \"\"\"\n",
      "다음과 같은 시각화 차트들이 생성되었습니다:\n",
      "\n",
      "1. 카테고리별 판매량 분포 (category_distribution.png)\n",
      "   - Set과 kurta 카테고리가 가장 높은 판매량을 보임\n",
      "   - Western Dress와 Top이 중간 수준의 판매량을 기록\n",
      "\n",
      "2. 월별 판매액 추이 (monthly_sales_trend.png)\n",
      "   - 2022년 4월에 가장 높은 판매액을 기록\n",
      "   - 이후 점진적으로 감소하는 추세를 보임\n",
      "\n",
      "3. 주문 상태 분포 (order_status_distribution.png)\n",
      "   - 대부분의 주문이 'Shipped' 또는 'Shipped - Delivered to Buyer' 상태\n",
      "   - 취소된 주문(Cancelled)이 세 번째로 많은 비중을 차지\n",
      "\n",
      "4. 상위 5개 주(State)의 판매량 (top_states_sales.png)\n",
      "   - MAHARASHTRA가 가장 높은 판매량을 기록\n",
      "   - KARNATAKA와 TAMIL NADU가 그 뒤를 이음\n",
      "\n",
      "5. 가격대별 판매량 분포 (price_distribution.png)\n",
      "   - 대부분의 상품이 중저가 범위에 분포\n",
      "   - 오른쪽으로 긴 꼬리를 가진 분포를 보임\n",
      "\"\"\"\n",
      "\n",
      "# Result accumulation storage\n",
      "results_file = './artifacts/all_results.txt'\n",
      "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
      "current_result_text = f\"\"\"\n",
      "==================================================\n",
      "## Analysis Stage: {stage_name}\n",
      "## Execution Time: {current_time}\n",
      "--------------------------------------------------\n",
      "Result Description: \n",
      "{result_description}\n",
      "==================================================\n",
      "\"\"\"\n",
      "\n",
      "artifact_files = [\n",
      "    [\"./artifacts/category_distribution.png\", \"카테고리별 판매량 분포 차트\"],\n",
      "    [\"./artifacts/monthly_sales_trend.png\", \"월별 판매액 추이 차트\"],\n",
      "    [\"./artifacts/order_status_distribution.png\", \"주문 상태 분포 차트\"],\n",
      "    [\"./artifacts/top_states_sales.png\", \"상위 5개 주(State)의 판매량 차트\"],\n",
      "    [\"./artifacts/price_distribution.png\", \"가격대별 판매량 분포 차트\"]\n",
      "]\n",
      "\n",
      "if artifact_files:\n",
      "    current_result_text += \"--------------------------------------------------\\nGenerated Files:\\n\"\n",
      "    for file_path, file_desc in artifact_files:\n",
      "        current_result_text += \"- {} : {}\\n\".format(file_path, file_desc)\n",
      "\n",
      "# Append new results\n",
      "try:\n",
      "    with open(results_file, 'a', encoding='utf-8') as f:\n",
      "        f.write(current_result_text)\n",
      "    print(\"Results and visualizations successfully saved.\")\n",
      "except Exception as e:\n",
      "    print(\"Error occurred while saving results: {}\".format(e))\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[94m\n",
      "Stdout: Results and visualizations successfully saved.\n",
      "\u001b[0m\n",
      "2025-05-09 15:05:15.968 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "\n",
      "INFO [src.agents.agents] \u001b[1mToolUse - 도구 실행 결과를 대화에 추가했습니다.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 대화 턴 5 ---\n",
      "\n",
      "\n",
      "이제 변수 간 상관관계 분석과 RFM 분석을 진행하겠습니다:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.agents.agents] \u001b[1m\n",
      "ToolUse - Tool Name: python_repl_tool, Input: {'code': 'import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport lovelyplots\\nfrom datetime import datetime\\nimport os\\n\\n# Set the style\\nplt.style.use([\\'ipynb\\', \\'use_mathtext\\',\\'colors5-light\\'])\\nplt.rc(\\'font\\', family=\\'NanumGothic\\')\\n\\n# Read and preprocess the data\\ndf = pd.read_csv(\\'./data/Amazon_Sale_Report.csv\\')\\ndf[\\'Date\\'] = pd.to_datetime(df[\\'Date\\'], format=\\'%m-%d-%y\\')\\n\\n# 1. 상관관계 분석\\n# 수치형 변수만 선택\\nnumeric_cols = [\\'Qty\\', \\'Amount\\']\\ncorrelation = df[numeric_cols].corr()\\n\\n# 상관관계 히트맵 생성\\nplt.figure(figsize=(10, 8), dpi=150)\\nsns.heatmap(correlation, annot=True, cmap=\\'coolwarm\\', center=0)\\nplt.title(\\'변수 간 상관관계 히트맵\\', fontsize=14)\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/correlation_heatmap.png\\')\\nplt.close()\\n\\n# 2. RFM 분석\\n# 분석 기준일 설정 (데이터의 마지막 날짜)\\ntoday = df[\\'Date\\'].max()\\n\\n# RFM 점수 계산\\nrfm = df.groupby(\\'Order ID\\').agg({\\n    \\'Date\\': lambda x: (today - x.max()).days,  # Recency\\n    \\'Order ID\\': \\'count\\',  # Frequency\\n    \\'Amount\\': \\'sum\\'  # Monetary\\n}).rename(columns={\\n    \\'Date\\': \\'Recency\\',\\n    \\'Order ID\\': \\'Frequency\\',\\n    \\'Amount\\': \\'Monetary\\'\\n})\\n\\n# RFM 점수 분포 시각화\\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\\n\\n# Recency 분포\\naxes[0].hist(rfm[\\'Recency\\'], bins=30, edgecolor=\\'black\\')\\naxes[0].set_title(\\'Recency 분포\\')\\naxes[0].set_xlabel(\\'최근 구매일로부터의 일수\\')\\naxes[0].set_ylabel(\\'고객 수\\')\\naxes[0].grid(alpha=0.3)\\n\\n# Frequency 분포\\naxes[1].hist(rfm[\\'Frequency\\'], bins=30, edgecolor=\\'black\\')\\naxes[1].set_title(\\'Frequency 분포\\')\\naxes[1].set_xlabel(\\'구매 빈도\\')\\naxes[1].set_ylabel(\\'고객 수\\')\\naxes[1].grid(alpha=0.3)\\n\\n# Monetary 분포\\naxes[2].hist(rfm[\\'Monetary\\'], bins=30, edgecolor=\\'black\\')\\naxes[2].set_title(\\'Monetary 분포\\')\\naxes[2].set_xlabel(\\'총 구매금액\\')\\naxes[2].set_ylabel(\\'고객 수\\')\\naxes[2].grid(alpha=0.3)\\n\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/rfm_distribution.png\\')\\nplt.close()\\n\\n# RFM 점수화 (1-5점)\\nr_labels = range(4, 0, -1)\\nr_quartiles = pd.qcut(rfm[\\'Recency\\'], q=4, labels=r_labels)\\nf_labels = range(1, 5)\\nf_quartiles = pd.qcut(rfm[\\'Frequency\\'], q=4, labels=f_labels)\\nm_labels = range(1, 5)\\nm_quartiles = pd.qcut(rfm[\\'Monetary\\'], q=4, labels=m_labels)\\n\\nrfm[\\'R\\'] = r_quartiles\\nrfm[\\'F\\'] = f_quartiles\\nrfm[\\'M\\'] = m_quartiles\\n\\n# RFM 세그먼트 정의\\ndef segment_customers(row):\\n    if (row[\\'R\\'] == 4) and (row[\\'F\\'] == 4) and (row[\\'M\\'] == 4):\\n        return \\'Best Customers\\'\\n    elif (row[\\'R\\'] >= 3) and (row[\\'F\\'] >= 3) and (row[\\'M\\'] >= 3):\\n        return \\'Loyal Customers\\'\\n    elif (row[\\'R\\'] >= 3) and (row[\\'F\\'] <= 2) and (row[\\'M\\'] <= 2):\\n        return \\'Lost Customers\\'\\n    elif (row[\\'R\\'] <= 2) and (row[\\'F\\'] <= 2) and (row[\\'M\\'] <= 2):\\n        return \\'Lost Cheap Customers\\'\\n    else:\\n        return \\'Average Customers\\'\\n\\nrfm[\\'Customer_Segment\\'] = rfm.apply(segment_customers, axis=1)\\n\\n# 고객 세그먼트 분포 시각화\\nplt.figure(figsize=(10, 6), dpi=150)\\nrfm[\\'Customer_Segment\\'].value_counts().plot(kind=\\'bar\\')\\nplt.title(\\'고객 세그먼트 분포\\', fontsize=14)\\nplt.xlabel(\\'세그먼트\\', fontsize=12)\\nplt.ylabel(\\'고객 수\\', fontsize=12)\\nplt.xticks(rotation=45)\\nplt.grid(alpha=0.3)\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/customer_segments.png\\')\\nplt.close()\\n\\n# Save analysis results\\nstage_name = \"Correlation and RFM Analysis\"\\nresult_description = f\"\"\"\\n1. 변수 간 상관관계 분석 결과:\\n   - Qty(수량)와 Amount(금액) 간의 상관계수: {correlation.iloc[0,1]:.3f}\\n   - 상관관계는 비교적 약한 양의 관계를 보임\\n\\n2. RFM 분석 결과:\\n   - Recency(최근성) 통계:\\n     * 평균: {rfm[\\'Recency\\'].mean():.2f}일\\n     * 중앙값: {rfm[\\'Recency\\'].median():.2f}일\\n   \\n   - Frequency(빈도) 통계:\\n     * 평균: {rfm[\\'Frequency\\'].mean():.2f}회\\n     * 중앙값: {rfm[\\'Frequency\\'].median():.2f}회\\n   \\n   - Monetary(구매금액) 통계:\\n     * 평균: {rfm[\\'Monetary\\'].mean():.2f}\\n     * 중앙값: {rfm[\\'Monetary\\'].median():.2f}\\n\\n3. 고객 세그먼트 분포:\\n{rfm[\\'Customer_Segment\\'].value_counts().to_string()}\\n\\n분석 인사이트:\\n1. 구매 수량과 금액 간의 상관관계가 약한 것으로 보아, 제품별 가격 차이가 큰 것으로 판단됨\\n2. RFM 분석 결과, 고객층이 다음과 같이 분류됨:\\n   - Best Customers: 최근에 자주 구매하고 구매금액도 높은 VIP 고객\\n   - Loyal Customers: 충성도 높은 단골 고객\\n   - Average Customers: 일반적인 구매 패턴을 보이는 고객\\n   - Lost Customers: 과거 고객이었으나 최근 구매가 없는 고객\\n   - Lost Cheap Customers: 저가 제품 위주로 구매했던 이탈 고객\\n\"\"\"\\n\\n# Result accumulation storage\\nresults_file = \\'./artifacts/all_results.txt\\'\\ncurrent_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\\ncurrent_result_text = f\"\"\"\\n==================================================\\n## Analysis Stage: {stage_name}\\n## Execution Time: {current_time}\\n--------------------------------------------------\\nResult Description: \\n{result_description}\\n==================================================\\n\"\"\"\\n\\nartifact_files = [\\n    [\"./artifacts/correlation_heatmap.png\", \"변수 간 상관관계 히트맵\"],\\n    [\"./artifacts/rfm_distribution.png\", \"RFM 분포 차트\"],\\n    [\"./artifacts/customer_segments.png\", \"고객 세그먼트 분포 차트\"]\\n]\\n\\nif artifact_files:\\n    current_result_text += \"--------------------------------------------------\\\\nGenerated Files:\\\\n\"\\n    for file_path, file_desc in artifact_files:\\n        current_result_text += \"- {} : {}\\\\n\".format(file_path, file_desc)\\n\\n# Append new results\\ntry:\\n    with open(results_file, \\'a\\', encoding=\\'utf-8\\') as f:\\n        f.write(current_result_text)\\n    print(\"Results and visualizations successfully saved.\")\\nexcept Exception as e:\\n    print(\"Error occurred while saving results: {}\".format(e))'}\u001b[0m\n",
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Executing Python code =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop reason: tool_use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Code execution successful =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[91mCoder - Successfully executed:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import lovelyplots\n",
      "from datetime import datetime\n",
      "import os\n",
      "\n",
      "# Set the style\n",
      "plt.style.use(['ipynb', 'use_mathtext','colors5-light'])\n",
      "plt.rc('font', family='NanumGothic')\n",
      "\n",
      "# Read and preprocess the data\n",
      "df = pd.read_csv('./data/Amazon_Sale_Report.csv')\n",
      "df['Date'] = pd.to_datetime(df['Date'], format='%m-%d-%y')\n",
      "\n",
      "# 1. 상관관계 분석\n",
      "# 수치형 변수만 선택\n",
      "numeric_cols = ['Qty', 'Amount']\n",
      "correlation = df[numeric_cols].corr()\n",
      "\n",
      "# 상관관계 히트맵 생성\n",
      "plt.figure(figsize=(10, 8), dpi=150)\n",
      "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0)\n",
      "plt.title('변수 간 상관관계 히트맵', fontsize=14)\n",
      "plt.tight_layout()\n",
      "plt.savefig('./artifacts/correlation_heatmap.png')\n",
      "plt.close()\n",
      "\n",
      "# 2. RFM 분석\n",
      "# 분석 기준일 설정 (데이터의 마지막 날짜)\n",
      "today = df['Date'].max()\n",
      "\n",
      "# RFM 점수 계산\n",
      "rfm = df.groupby('Order ID').agg({\n",
      "    'Date': lambda x: (today - x.max()).days,  # Recency\n",
      "    'Order ID': 'count',  # Frequency\n",
      "    'Amount': 'sum'  # Monetary\n",
      "}).rename(columns={\n",
      "    'Date': 'Recency',\n",
      "    'Order ID': 'Frequency',\n",
      "    'Amount': 'Monetary'\n",
      "})\n",
      "\n",
      "# RFM 점수 분포 시각화\n",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
      "\n",
      "# Recency 분포\n",
      "axes[0].hist(rfm['Recency'], bins=30, edgecolor='black')\n",
      "axes[0].set_title('Recency 분포')\n",
      "axes[0].set_xlabel('최근 구매일로부터의 일수')\n",
      "axes[0].set_ylabel('고객 수')\n",
      "axes[0].grid(alpha=0.3)\n",
      "\n",
      "# Frequency 분포\n",
      "axes[1].hist(rfm['Frequency'], bins=30, edgecolor='black')\n",
      "axes[1].set_title('Frequency 분포')\n",
      "axes[1].set_xlabel('구매 빈도')\n",
      "axes[1].set_ylabel('고객 수')\n",
      "axes[1].grid(alpha=0.3)\n",
      "\n",
      "# Monetary 분포\n",
      "axes[2].hist(rfm['Monetary'], bins=30, edgecolor='black')\n",
      "axes[2].set_title('Monetary 분포')\n",
      "axes[2].set_xlabel('총 구매금액')\n",
      "axes[2].set_ylabel('고객 수')\n",
      "axes[2].grid(alpha=0.3)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('./artifacts/rfm_distribution.png')\n",
      "plt.close()\n",
      "\n",
      "# RFM 점수화 (1-5점)\n",
      "r_labels = range(4, 0, -1)\n",
      "r_quartiles = pd.qcut(rfm['Recency'], q=4, labels=r_labels)\n",
      "f_labels = range(1, 5)\n",
      "f_quartiles = pd.qcut(rfm['Frequency'], q=4, labels=f_labels)\n",
      "m_labels = range(1, 5)\n",
      "m_quartiles = pd.qcut(rfm['Monetary'], q=4, labels=m_labels)\n",
      "\n",
      "rfm['R'] = r_quartiles\n",
      "rfm['F'] = f_quartiles\n",
      "rfm['M'] = m_quartiles\n",
      "\n",
      "# RFM 세그먼트 정의\n",
      "def segment_customers(row):\n",
      "    if (row['R'] == 4) and (row['F'] == 4) and (row['M'] == 4):\n",
      "        return 'Best Customers'\n",
      "    elif (row['R'] >= 3) and (row['F'] >= 3) and (row['M'] >= 3):\n",
      "        return 'Loyal Customers'\n",
      "    elif (row['R'] >= 3) and (row['F'] <= 2) and (row['M'] <= 2):\n",
      "        return 'Lost Customers'\n",
      "    elif (row['R'] <= 2) and (row['F'] <= 2) and (row['M'] <= 2):\n",
      "        return 'Lost Cheap Customers'\n",
      "    else:\n",
      "        return 'Average Customers'\n",
      "\n",
      "rfm['Customer_Segment'] = rfm.apply(segment_customers, axis=1)\n",
      "\n",
      "# 고객 세그먼트 분포 시각화\n",
      "plt.figure(figsize=(10, 6), dpi=150)\n",
      "rfm['Customer_Segment'].value_counts().plot(kind='bar')\n",
      "plt.title('고객 세그먼트 분포', fontsize=14)\n",
      "plt.xlabel('세그먼트', fontsize=12)\n",
      "plt.ylabel('고객 수', fontsize=12)\n",
      "plt.xticks(rotation=45)\n",
      "plt.grid(alpha=0.3)\n",
      "plt.tight_layout()\n",
      "plt.savefig('./artifacts/customer_segments.png')\n",
      "plt.close()\n",
      "\n",
      "# Save analysis results\n",
      "stage_name = \"Correlation and RFM Analysis\"\n",
      "result_description = f\"\"\"\n",
      "1. 변수 간 상관관계 분석 결과:\n",
      "   - Qty(수량)와 Amount(금액) 간의 상관계수: {correlation.iloc[0,1]:.3f}\n",
      "   - 상관관계는 비교적 약한 양의 관계를 보임\n",
      "\n",
      "2. RFM 분석 결과:\n",
      "   - Recency(최근성) 통계:\n",
      "     * 평균: {rfm['Recency'].mean():.2f}일\n",
      "     * 중앙값: {rfm['Recency'].median():.2f}일\n",
      "   \n",
      "   - Frequency(빈도) 통계:\n",
      "     * 평균: {rfm['Frequency'].mean():.2f}회\n",
      "     * 중앙값: {rfm['Frequency'].median():.2f}회\n",
      "   \n",
      "   - Monetary(구매금액) 통계:\n",
      "     * 평균: {rfm['Monetary'].mean():.2f}\n",
      "     * 중앙값: {rfm['Monetary'].median():.2f}\n",
      "\n",
      "3. 고객 세그먼트 분포:\n",
      "{rfm['Customer_Segment'].value_counts().to_string()}\n",
      "\n",
      "분석 인사이트:\n",
      "1. 구매 수량과 금액 간의 상관관계가 약한 것으로 보아, 제품별 가격 차이가 큰 것으로 판단됨\n",
      "2. RFM 분석 결과, 고객층이 다음과 같이 분류됨:\n",
      "   - Best Customers: 최근에 자주 구매하고 구매금액도 높은 VIP 고객\n",
      "   - Loyal Customers: 충성도 높은 단골 고객\n",
      "   - Average Customers: 일반적인 구매 패턴을 보이는 고객\n",
      "   - Lost Customers: 과거 고객이었으나 최근 구매가 없는 고객\n",
      "   - Lost Cheap Customers: 저가 제품 위주로 구매했던 이탈 고객\n",
      "\"\"\"\n",
      "\n",
      "# Result accumulation storage\n",
      "results_file = './artifacts/all_results.txt'\n",
      "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
      "current_result_text = f\"\"\"\n",
      "==================================================\n",
      "## Analysis Stage: {stage_name}\n",
      "## Execution Time: {current_time}\n",
      "--------------------------------------------------\n",
      "Result Description: \n",
      "{result_description}\n",
      "==================================================\n",
      "\"\"\"\n",
      "\n",
      "artifact_files = [\n",
      "    [\"./artifacts/correlation_heatmap.png\", \"변수 간 상관관계 히트맵\"],\n",
      "    [\"./artifacts/rfm_distribution.png\", \"RFM 분포 차트\"],\n",
      "    [\"./artifacts/customer_segments.png\", \"고객 세그먼트 분포 차트\"]\n",
      "]\n",
      "\n",
      "if artifact_files:\n",
      "    current_result_text += \"--------------------------------------------------\\nGenerated Files:\\n\"\n",
      "    for file_path, file_desc in artifact_files:\n",
      "        current_result_text += \"- {} : {}\\n\".format(file_path, file_desc)\n",
      "\n",
      "# Append new results\n",
      "try:\n",
      "    with open(results_file, 'a', encoding='utf-8') as f:\n",
      "        f.write(current_result_text)\n",
      "    print(\"Results and visualizations successfully saved.\")\n",
      "except Exception as e:\n",
      "    print(\"Error occurred while saving results: {}\".format(e))\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[94m\n",
      "Stdout: Error: <string>:14: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 77, in <module>\n",
      "  File \"/home/sagemaker-user/.conda/envs/bedrock-manus/lib/python3.12/site-packages/pandas/core/reshape/tile.py\", line 340, in qcut\n",
      "    fac, bins = _bins_to_cuts(\n",
      "                ^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/bedrock-manus/lib/python3.12/site-packages/pandas/core/reshape/tile.py\", line 443, in _bins_to_cuts\n",
      "    raise ValueError(\n",
      "ValueError: Bin edges must be unique: Index([1.0, 1.0, 1.0, 1.0, 12.0], dtype='float64', name='Frequency').\n",
      "You can drop duplicate edges by setting the 'duplicates' kwarg\n",
      "\u001b[0m\n",
      "2025-05-09 15:06:07.577 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "\n",
      "INFO [src.agents.agents] \u001b[1mToolUse - 도구 실행 결과를 대화에 추가했습니다.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 대화 턴 6 ---\n",
      "\n",
      "\n",
      "죄송합니다. RFM 분석에서 오류가 발생했네요. 데이터의 특성을 고려하여 수정된 분석을 진행하겠습니다:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.agents.agents] \u001b[1m\n",
      "ToolUse - Tool Name: python_repl_tool, Input: {'code': 'import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport lovelyplots\\nfrom datetime import datetime\\nimport os\\n\\n# Set the style\\nplt.style.use([\\'ipynb\\', \\'use_mathtext\\',\\'colors5-light\\'])\\nplt.rc(\\'font\\', family=\\'NanumGothic\\')\\n\\n# Read and preprocess the data\\ndf = pd.read_csv(\\'./data/Amazon_Sale_Report.csv\\', low_memory=False)\\ndf[\\'Date\\'] = pd.to_datetime(df[\\'Date\\'], format=\\'%m-%d-%y\\')\\n\\n# 1. 상관관계 분석\\n# 수치형 변수만 선택\\nnumeric_cols = [\\'Qty\\', \\'Amount\\']\\ncorrelation = df[numeric_cols].corr()\\n\\n# 상관관계 히트맵 생성\\nplt.figure(figsize=(10, 8), dpi=150)\\nsns.heatmap(correlation, annot=True, cmap=\\'coolwarm\\', center=0)\\nplt.title(\\'변수 간 상관관계 히트맵\\', fontsize=14)\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/correlation_heatmap.png\\')\\nplt.close()\\n\\n# 2. 수정된 RFM 분석\\n# 분석 기준일 설정 (데이터의 마지막 날짜)\\ntoday = df[\\'Date\\'].max()\\n\\n# RFM 점수 계산\\nrfm = df.groupby(\\'Order ID\\').agg({\\n    \\'Date\\': lambda x: (today - x.max()).days,  # Recency\\n    \\'Order ID\\': \\'count\\',  # Frequency\\n    \\'Amount\\': \\'sum\\'  # Monetary\\n}).rename(columns={\\n    \\'Date\\': \\'Recency\\',\\n    \\'Order ID\\': \\'Frequency\\',\\n    \\'Amount\\': \\'Monetary\\'\\n})\\n\\n# RFM 점수 분포 시각화\\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\\n\\n# Recency 분포\\naxes[0].hist(rfm[\\'Recency\\'], bins=30, edgecolor=\\'black\\')\\naxes[0].set_title(\\'Recency 분포\\')\\naxes[0].set_xlabel(\\'최근 구매일로부터의 일수\\')\\naxes[0].set_ylabel(\\'고객 수\\')\\naxes[0].grid(alpha=0.3)\\n\\n# Frequency 분포\\naxes[1].hist(rfm[\\'Frequency\\'], bins=30, edgecolor=\\'black\\')\\naxes[1].set_title(\\'Frequency 분포\\')\\naxes[1].set_xlabel(\\'구매 빈도\\')\\naxes[1].set_ylabel(\\'고객 수\\')\\naxes[1].grid(alpha=0.3)\\n\\n# Monetary 분포\\naxes[2].hist(rfm[\\'Monetary\\'], bins=30, edgecolor=\\'black\\')\\naxes[2].set_title(\\'Monetary 분포\\')\\naxes[2].set_xlabel(\\'총 구매금액\\')\\naxes[2].set_ylabel(\\'고객 수\\')\\naxes[2].grid(alpha=0.3)\\n\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/rfm_distribution.png\\')\\nplt.close()\\n\\n# 수정된 RFM 세그먼테이션\\ndef segment_customers(row):\\n    if row[\\'Recency\\'] <= 30:  # 최근 30일 이내 구매\\n        if row[\\'Monetary\\'] > rfm[\\'Monetary\\'].quantile(0.75):\\n            return \\'VIP Customers\\'\\n        else:\\n            return \\'Active Customers\\'\\n    elif row[\\'Recency\\'] <= 90:  # 최근 90일 이내 구매\\n        if row[\\'Monetary\\'] > rfm[\\'Monetary\\'].median():\\n            return \\'Regular Customers\\'\\n        else:\\n            return \\'Occasional Customers\\'\\n    else:  # 90일 이상 구매 없음\\n        return \\'Inactive Customers\\'\\n\\nrfm[\\'Customer_Segment\\'] = rfm.apply(segment_customers, axis=1)\\n\\n# 고객 세그먼트 분포 시각화\\nplt.figure(figsize=(10, 6), dpi=150)\\nsegment_dist = rfm[\\'Customer_Segment\\'].value_counts()\\nsegment_dist.plot(kind=\\'bar\\')\\nplt.title(\\'고객 세그먼트 분포\\', fontsize=14)\\nplt.xlabel(\\'세그먼트\\', fontsize=12)\\nplt.ylabel(\\'고객 수\\', fontsize=12)\\nplt.xticks(rotation=45)\\nplt.grid(alpha=0.3)\\nplt.tight_layout()\\nplt.savefig(\\'./artifacts/customer_segments.png\\')\\nplt.close()\\n\\n# 세그먼트별 평균 구매금액\\nsegment_monetary = rfm.groupby(\\'Customer_Segment\\')[\\'Monetary\\'].agg([\\'mean\\', \\'count\\']).round(2)\\n\\n# Save analysis results\\nstage_name = \"Correlation and Modified RFM Analysis\"\\nresult_description = f\"\"\"\\n1. 변수 간 상관관계 분석 결과:\\n   - Qty(수량)와 Amount(금액) 간의 상관계수: {correlation.iloc[0,1]:.3f}\\n   - 상관관계는 비교적 약한 양의 관계를 보임\\n\\n2. RFM 분석 결과:\\n   - Recency(최근성) 통계:\\n     * 평균: {rfm[\\'Recency\\'].mean():.2f}일\\n     * 중앙값: {rfm[\\'Recency\\'].median():.2f}일\\n   \\n   - Frequency(빈도) 통계:\\n     * 평균: {rfm[\\'Frequency\\'].mean():.2f}회\\n     * 중앙값: {rfm[\\'Frequency\\'].median():.2f}회\\n   \\n   - Monetary(구매금액) 통계:\\n     * 평균: {rfm[\\'Monetary\\'].mean():.2f}\\n     * 중앙값: {rfm[\\'Monetary\\'].median():.2f}\\n\\n3. 고객 세그먼트 분포:\\n{segment_dist.to_string()}\\n\\n4. 세그먼트별 평균 구매금액:\\n{segment_monetary.to_string()}\\n\\n분석 인사이트:\\n1. 구매 수량과 금액 간의 상관관계가 약한 것으로 보아, 제품별 가격 차이가 큰 것으로 판단됨\\n2. 수정된 RFM 분석 결과, 고객층이 다음과 같이 분류됨:\\n   - VIP Customers: 최근 구매하며 높은 구매금액을 보이는 최우수 고객\\n   - Active Customers: 최근 구매는 있으나 구매금액이 상대적으로 낮은 활성 고객\\n   - Regular Customers: 3개월 이내 구매가 있고 중간 이상의 구매금액을 보이는 일반 고객\\n   - Occasional Customers: 3개월 이내 구매가 있으나 구매금액이 낮은 간헐적 고객\\n   - Inactive Customers: 3개월 이상 구매가 없는 비활성 고객\\n\\n3. 주요 시사점:\\n   - 전체 고객 중 비활성 고객의 비중이 높아 재활성화 전략이 필요함\\n   - VIP 고객과 Active 고객을 위한 차별화된 서비스 제공이 필요함\\n   - Regular Customers를 VIP로 전환하기 위한 업셀링 전략 수립이 요구됨\\n\"\"\"\\n\\n# Result accumulation storage\\nresults_file = \\'./artifacts/all_results.txt\\'\\ncurrent_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\\ncurrent_result_text = f\"\"\"\\n==================================================\\n## Analysis Stage: {stage_name}\\n## Execution Time: {current_time}\\n--------------------------------------------------\\nResult Description: \\n{result_description}\\n==================================================\\n\"\"\"\\n\\nartifact_files = [\\n    [\"./artifacts/correlation_heatmap.png\", \"변수 간 상관관계 히트맵\"],\\n    [\"./artifacts/rfm_distribution.png\", \"RFM 분포 차트\"],\\n    [\"./artifacts/customer_segments.png\", \"고객 세그먼트 분포 차트\"]\\n]\\n\\nif artifact_files:\\n    current_result_text += \"--------------------------------------------------\\\\nGenerated Files:\\\\n\"\\n    for file_path, file_desc in artifact_files:\\n        current_result_text += \"- {} : {}\\\\n\".format(file_path, file_desc)\\n\\n# Append new results\\ntry:\\n    with open(results_file, \\'a\\', encoding=\\'utf-8\\') as f:\\n        f.write(current_result_text)\\n    print(\"Results and visualizations successfully saved.\")\\nexcept Exception as e:\\n    print(\"Error occurred while saving results: {}\".format(e))'}\u001b[0m\n",
      "\n",
      "INFO [src.tools.python_repl] \u001b[92m===== Executing Python code =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop reason: tool_use\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def remove_artifact_folder(folder_path=\"./artifacts/\"):\n",
    "    \"\"\"\n",
    "    ./artifact/ 폴더가 존재하면 삭제하는 함수\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): 삭제할 폴더 경로\n",
    "    \"\"\"\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"'{folder_path}' 폴더를 삭제합니다...\")\n",
    "        try:\n",
    "            # 폴더와 그 내용을 모두 삭제\n",
    "            shutil.rmtree(folder_path)\n",
    "            print(f\"'{folder_path}' 폴더가 성공적으로 삭제되었습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")\n",
    "    else:\n",
    "        print(f\"'{folder_path}' 폴더가 존재하지 않습니다.\")\n",
    "\n",
    "\n",
    "#import nest_asyncio\n",
    "#nest_asyncio.apply()\n",
    "\n",
    "remove_artifact_folder()\n",
    "\n",
    "result = run_agent_workflow(\n",
    "    user_input=user_query,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "# Print the conversation history\n",
    "print(\"\\n=== Conversation History ===\")\n",
    "print (\"result\", result)\n",
    "for history in result[\"history\"]:\n",
    "\n",
    "    print (\"===\")\n",
    "    print (f'agent: {history[\"agent\"]}')\n",
    "    print (f'message: {history[\"message\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a3c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock-manus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
