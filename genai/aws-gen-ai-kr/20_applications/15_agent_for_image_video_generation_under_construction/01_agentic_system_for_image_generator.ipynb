{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a15602-4cbd-48b6-a27b-b5e24bc53df3",
   "metadata": {},
   "source": [
    "# Agentic system for image generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e755fec-095b-43ce-8b2f-e46fd002dc00",
   "metadata": {},
   "source": [
    "## Setting\n",
    " - Auto Reload\n",
    " - path for utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beee5ec9-5112-4eda-88ab-43a0ef1721a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dd7fb1d-710f-4345-92a0-b5a794742afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60864ffe-4fe9-4e08-bb49-9b4ecdb8c7db",
   "metadata": {},
   "source": [
    "## 1. Create Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6705c228-f793-47e9-b2da-ade94f209fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock\n",
    "from utils.bedrock import bedrock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197919c-2fbc-4344-a259-4c34dca45a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "- os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "- os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "- os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "- os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67a986c0-60fe-4e77-875c-338a138460fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "  Using profile: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "\u001b[32m\n",
      "== FM lists ==\u001b[0m\n",
      "{'Claude-Instant-V1': 'anthropic.claude-instant-v1',\n",
      " 'Claude-V1': 'anthropic.claude-v1',\n",
      " 'Claude-V2': 'anthropic.claude-v2',\n",
      " 'Claude-V2-1': 'anthropic.claude-v2:1',\n",
      " 'Claude-V3-5-Sonnet': 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
      " 'Claude-V3-5-V-2-Sonnet': 'anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
      " 'Claude-V3-5-V-2-Sonnet-CRI': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
      " 'Claude-V3-Haiku': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
      " 'Claude-V3-Opus': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
      " 'Claude-V3-Sonnet': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
      " 'Cohere-Embeddings-En': 'cohere.embed-english-v3',\n",
      " 'Cohere-Embeddings-Multilingual': 'cohere.embed-multilingual-v3',\n",
      " 'Command': 'cohere.command-text-v14',\n",
      " 'Command-Light': 'cohere.command-light-text-v14',\n",
      " 'Jurassic-2-Mid': 'ai21.j2-mid-v1',\n",
      " 'Jurassic-2-Ultra': 'ai21.j2-ultra-v1',\n",
      " 'Llama2-13b-Chat': 'meta.llama2-13b-chat-v1',\n",
      " 'Nova-Canvas': 'amazon.nova-canvas-v1:0',\n",
      " 'Nova-Lite': 'amazon.nova-lite-v1:0',\n",
      " 'Nova-Micro': 'amazon.nova-micro-v1:0',\n",
      " 'Nova-Pro': 'amazon.nova-pro-v1:0',\n",
      " 'Nova-Pro-CRI': 'us.amazon.nova-pro-v1:0',\n",
      " 'Nova-Reel': 'amazon.nova-reel-v1:0',\n",
      " 'Titan-Embeddings-G1': 'amazon.titan-embed-text-v1',\n",
      " 'Titan-Text-Embeddings-V2': 'amazon.titan-embed-text-v2:0',\n",
      " 'Titan-Text-G1': 'amazon.titan-text-express-v1',\n",
      " 'Titan-Text-G1-Express': 'amazon.titan-text-express-v1',\n",
      " 'Titan-Text-G1-Light': 'amazon.titan-text-lite-v1',\n",
      " 'Titan-Text-G1-Premier': 'amazon.titan-text-premier-v1:0'}\n"
     ]
    }
   ],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "print (colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint (bedrock_info.get_list_fm_models(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e2368-e195-47e4-b981-80f7e6dc4421",
   "metadata": {},
   "source": [
    "## 2. LLM 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b094dc1-26f9-4dcc-b51a-178edadb407e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.bedrock import bedrock_model\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd05a4a4-0ffc-4e29-b1db-0f28ce6374d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = bedrock_model(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-5-V-2-Sonnet-CRI\"),\n",
    "    bedrock_client=boto3_bedrock,\n",
    "    stream=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    inference_config={\n",
    "        'maxTokens': 1024,\n",
    "        'stopSequences': [\"\\n\\nHuman\"],\n",
    "        'temperature': 0.01,\n",
    "        #'topP': ...,\n",
    "    }\n",
    "    #additional_model_request_fields={\"top_k\": 200}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bfe3f-bcdb-4b76-bcfe-df9273733e7a",
   "metadata": {},
   "source": [
    "## 3. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da51dd14-0656-4515-a162-69496260fba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import pprint\n",
    "import base64\n",
    "import traceback\n",
    "from PIL import Image\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from textwrap import dedent\n",
    "from utils.bedrock import bedrock_utils\n",
    "from typing import TypedDict\n",
    "from src.genai_anaysis import llm_call\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a542f11-ce89-4893-ad97-3755aee3c87a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeMeasurement:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.measurements = {}\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def measure(self, section_name):\n",
    "        if self.start_time is None:\n",
    "            raise ValueError(\"start() 메서드를 먼저 호출해야 합니다.\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - self.start_time\n",
    "        self.measurements[section_name] = elapsed_time\n",
    "        self.start_time = end_time  # 다음 구간 측정을 위해 시작 시간 재설정\n",
    "\n",
    "    def reset(self, ):\n",
    "        self.measurements = {}\n",
    "\n",
    "    def print_measurements(self):\n",
    "        for section, elapsed_time in self.measurements.items():\n",
    "            #print(f\"{section}: {elapsed_time:.5f} 초\")\n",
    "            print(colored (f\"\\nelapsed time: {section}: {elapsed_time:.5f} 초\", \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c6f67-b19f-49f6-b809-9afa3633784b",
   "metadata": {},
   "source": [
    "### 3.1 Agent state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4b04f8af-ec22-4a11-a468-231dd6c1ef61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    ask: list[str]\n",
    "    image_model: str\n",
    "    #target_apps: list[str]\n",
    "    ask_refo: str\n",
    "    #code: str\n",
    "    code_err: str\n",
    "    #img_path: str\n",
    "    #img_bytes: str\n",
    "    #chart_desc: str\n",
    "    prev_node: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b42e1ca9-7ce8-4e89-bcd4-952be419506e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class genai_analyzer():\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.llm=kwargs[\"llm\"]\n",
    "        self.state = GraphState\n",
    "\n",
    "        self.llm_caller = llm_call(\n",
    "            llm=self.llm,\n",
    "            verbose=False\n",
    "        ) \n",
    "\n",
    "        self._graph_definition()\n",
    "        self.messages = []\n",
    "        self.img_bytes = \"\"\n",
    "\n",
    "        self.timer = TimeMeasurement()\n",
    "\n",
    "    def _get_string_from_message(self, message):\n",
    "        return message[\"content\"][0][\"text\"]\n",
    "\n",
    "    def _get_message_from_string(self, role, string, imgs=None):\n",
    "        \n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": []\n",
    "        }\n",
    "        \n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_message = {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'png',\n",
    "                        \"source\": {\"bytes\": img}\n",
    "                    }\n",
    "                }\n",
    "                message[\"content\"].append(img_message)\n",
    "        \n",
    "        message[\"content\"].append({\"text\": dedent(string)})\n",
    "\n",
    "        return message\n",
    "    \n",
    "#     def _get_message_from_string(self, role, string, img=None):\n",
    "        \n",
    "#         message = {\n",
    "#             \"role\": role,\n",
    "#             \"content\": [{\"text\": dedent(string)}]\n",
    "#         }\n",
    "        \n",
    "#         if img is not None:\n",
    "#             img_message = {\n",
    "#                 \"image\": {\n",
    "#                     \"format\": 'png',\n",
    "#                     \"source\": {\"bytes\": img}\n",
    "#                 }\n",
    "#             }\n",
    "#             message[\"content\"].append(img_message)\n",
    "\n",
    "#         return message\n",
    "\n",
    "    def _png_to_bytes(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as image_file:\n",
    "                # 파일을 바이너리 모드로 읽기\n",
    "                binary_data = image_file.read()\n",
    "                \n",
    "                # 바이너리 데이터를 base64로 인코딩\n",
    "                base64_encoded = base64.b64encode(binary_data)\n",
    "                \n",
    "                # bytes 타입을 문자열로 디코딩\n",
    "                base64_string = base64_encoded.decode('utf-8')\n",
    "                \n",
    "                return binary_data, base64_string\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            return \"Error: 파일을 찾을 수 없습니다.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def show_image(base64_string):\n",
    "        try:\n",
    "            # base64 문자열을 디코딩하여 바이너리 데이터로 변환\n",
    "            image_data = base64.b64decode(base64_string)\n",
    "            \n",
    "            # 바이너리 데이터를 이미지로 변환\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "            \n",
    "            # matplotlib을 사용하여 이미지 표시\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')  # 축 제거\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: 이미지를 표시하는 데 실패했습니다. {str(e)}\")\n",
    "\n",
    "    def get_messages(self, ):\n",
    "        return self.messages\n",
    "        \n",
    "    def _graph_definition(self, **kwargs):\n",
    "\n",
    "        def intent_analyzer(state):\n",
    "\n",
    "            self.timer.start()\n",
    "            self.timer.reset()\n",
    "\n",
    "            print(\"---CALL AGENT---\")\n",
    "            ask = state[\"ask\"]\n",
    "\n",
    "            \"\"\"\n",
    "            현재 상태를 기반으로 에이전트 모델을 호출하여 응답을 생성합니다. 질문에 따라 검색 도구를 사용하여 검색을 결정하거나 단순히 종료합니다.\n",
    "        \n",
    "            Args:\n",
    "                state (messages): 현재 상태\n",
    "        \n",
    "            Returns:\n",
    "                state (messages): 현재 상태 메시지에 에이전트 응답이 추가된 업데이트된 상태\n",
    "            \"\"\"\n",
    "\n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                <task>\n",
    "                사용자 메시지(ask)를 분석하여 이미지 생성 여부를 결정하는 에이전트 역할 수행\n",
    "                </task>\n",
    "\n",
    "                <instruction>\n",
    "                1. 사용자 메시지를 주의 깊게 분석하세요.\n",
    "                2. 이미지, 그림, 시각화와 관련된 키워드를 찾으세요.\n",
    "                3. 구체적인 시각적 묘사나 이미지 생성 요청의 존재 여부를 확인하세요.\n",
    "                4. 이미지 생성이 가능한 수준의 충분한 정보가 포함되어 있는지 확인하세요.\n",
    "                5. 윤리적 가이드라인과 제한사항을 고려하여 생성 가능 여부를 판단하세요.\n",
    "                6. 불법적이거나 부적절한 이미지 요청은 즉시 거절하세요.\n",
    "                7. 모호하거나 불명확한 요청의 경우, 사용자에게 추가 세부사항을 요청하세요.\n",
    "                </instruction>\n",
    "\n",
    "                <consideration>\n",
    "                - 사용자의 의도와 목적을 정확히 파악하는 것이 중요합니다.\n",
    "                - 명시적인 이미지 생성 요청이 없더라도 시각화가 도움될 수 있는 상황을 고려하세요.\n",
    "                - 단순한 질문이나 일반적인 대화는 이미지 생성이 불필요할 수 있습니다.\n",
    "                - 기술적 제한사항과 윤리적 가이드라인을 항상 준수해야 합니다.\n",
    "                - 이전 요청에 대한 수정사항이라면, 새로운 이미지를 생성하지 말고 \"GENERATE_IMAGE\"를 출력하세요.\n",
    "                </consideration>\n",
    "\n",
    "                <restrictions>\n",
    "                - 폭력적인 내용\n",
    "                - 성인용 콘텐츠\n",
    "                - 저작권이 있는 캐릭터나 로고\n",
    "                - 혐오 표현이나 차별적 내용\n",
    "                - 개인정보가 포함된 이미지\n",
    "                - 기타 불법적이거나 유해한 콘텐츠\n",
    "                </restrictions>\n",
    "\n",
    "                <output_format>\n",
    "                결정에 따라 다음 중 하나를 출력하세요. 반드시 아래 2개중 1개를 선택합니다:\n",
    "                1. \"GENERATE_IMAGE (간단한 이유)\" - 이미지 생성이 적절하고 가능한 경우\n",
    "                2. \"END (간단한 이유)\" - 이미지 생성이 부적절하거나 불가능한 경우\n",
    "\n",
    "                예시:\n",
    "                GENERATE_IMAGE (자연 풍경 이미지 생성 요청)\n",
    "                END (폭력적인 내용 포함으로 생성 거부)\n",
    "                </output_format>\n",
    "                '''\n",
    "            )\n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)\n",
    "                       \n",
    "            \n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                Here is user's ask: <ask>{ask}</ask>\n",
    "                '''\n",
    "            )\n",
    "            context = {\"ask\": ask}\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "            \n",
    "            #ask_string= \"Here is ask: <ask>{ask}</ask>\"\n",
    "            #ask_string = ask_string.format(ask=ask)\n",
    "            \n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts)\n",
    "            self.messages.append(message)\n",
    "            \n",
    "            resp, messages_updated = self.llm_caller.invoke(messages=self.messages, system_prompts=system_prompts)\n",
    "            self.messages = messages_updated\n",
    "            \n",
    "            return self.state(ask=ask, prev_node=\"INTENT_ANALYZER\")\n",
    "\n",
    "        def should_image_generation(state):\n",
    "            \"\"\"\n",
    "            에이전트가 이미지를 생성하는데 있어 추가적으로 고려해야 하는 상황이 있는지 결정합니다.\n",
    "        \n",
    "            이 함수는 상태의 마지막 메시지에서 함수 호출을 확인합니다. 함수 호출이 있으면 정보 검색 프로세스를 계속합니다. 그렇지 않으면 프로세스를 종료합니다.\n",
    "        \n",
    "            Args:\n",
    "                state (messages): 현재 상태\n",
    "        \n",
    "            Returns:\n",
    "                str: 검색 프로세스를 \"계속\"하거나 \"종료\"하는 결정\n",
    "            \"\"\"\n",
    "        \n",
    "            print(\"\\n---DECIDE TO IMAGE GENERATION---\")\n",
    "            #messages = state[\"messages\"]\n",
    "            last_message = self._get_string_from_message(self.messages[-1])\n",
    "            \n",
    "            # 함수 호출이 없으면 종료합니다.\n",
    "            if \"GENERATE_IMAGE\" not in last_message:\n",
    "                print(\"---DECISION: DO NOT IMAGE GENERATION / DONE---\")\n",
    "                return \"end\"\n",
    "            # 그렇지 않으면 함수 호출이 있으므로 계속합니다.\n",
    "            else:\n",
    "                print(\"---DECISION: IMAGE GENERATION---\")\n",
    "                return \"continue\"\n",
    "\n",
    "        def ask_reformulation(state):\n",
    "\n",
    "            print(\"---ASK REFORMULATION---\")\n",
    "            ask = state[\"ask\"]\n",
    "\n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                당신은 사용자의 일반적인 이미지 요청(ask)을 분석하여 더 상세하고 구체적인 이미지 설명으로 재구성하는 전문가입니다.\n",
    "\n",
    "                <task>\n",
    "                1. 사용자의 텍스트 요청에서 이미지와 관련된 모든 시각적 요소를 식별하고 추출하세요.\n",
    "                2. 누락된 중요한 시각적 세부 사항을 파악하세요.\n",
    "                3. 명확하지 않거나 개선이 필요한 부분을 식별하세요.\n",
    "                4. 사용자의 의도를 유지하면서 더 풍부하고 구체적인 설명으로 재구성하세요.\n",
    "                </task>\n",
    "\n",
    "                <output_format>\n",
    "                JSON 형식으로 다음 정보를 포함하여 응답하세요:\n",
    "                {\n",
    "                  \"original_request\": \"원본 요청 내용\",\n",
    "                  \"visual_elements\": {\n",
    "                    \"main_subject\": \"주요 피사체/대상\",\n",
    "                    \"setting\": \"배경/환경\",\n",
    "                    \"style_hints\": \"스타일 관련 단서\",\n",
    "                    \"color_hints\": \"색상 관련 단서\",\n",
    "                    \"composition_hints\": \"구도 관련 단서\"\n",
    "                  },\n",
    "                  \"clarification_needed\": {\n",
    "                    \"unclear_elements\": [\"명확하지 않은 요소들\"],\n",
    "                    \"questions\": [\"각 요소에 대한 구체적인 질문들\"],\n",
    "                    \"suggestions\": [\"개선을 위한 제안사항들\"]\n",
    "                  },\n",
    "                  \"missing_details\": [\"명시되지 않은 중요 요소들\"],\n",
    "                  \"reformulated_request\": \"재구성된 상세 설명\",\n",
    "                  \"interactive_guidance\": {\n",
    "                    \"priority_questions\": [\"가장 중요한 확인이 필요한 질문 1-2개\"],\n",
    "                    \"enhancement_options\": [\"사용자가 선택할 수 있는 개선 옵션들\"]\n",
    "                  }\n",
    "                }\n",
    "                </output_format>\n",
    "\n",
    "                <instruction>\n",
    "                - 원본 요청의 핵심 의도를 유지하세요.\n",
    "                - 불명확하거나 모호한 표현을 식별하고 적절한 질문을 준비하세요.\n",
    "                - 시각적으로 중요하지만 명시되지 않은 요소들을 식별하세요.\n",
    "                - 감정이나 분위기와 관련된 단서를 포함하세요.\n",
    "                - 부적절하거나 유해한 내용이 포함된 요청은 \"INAPPROPRIATE_REQUEST\"를 반환하세요.\n",
    "                - 사용자와의 상호작용이 필요한 부분을 명확히 구분하세요.\n",
    "                </instruction>\n",
    "\n",
    "                <consideration>\n",
    "                - 시각적 명확성(Visual Clarity)\n",
    "                - 세부 사항의 구체성(Specificity)\n",
    "                - 의도의 보존(Intent Preservation)\n",
    "                - 이미지 생성 가능성(Feasibility)\n",
    "                - 윤리적 가이드라인(Ethical Guidelines)\n",
    "                - 사용자 상호작용의 효율성(Interactive Efficiency)\n",
    "                </consideration>\n",
    "\n",
    "                <interaction_guidelines>\n",
    "                - 질문은 간단명료하게 작성하세요.\n",
    "                - 한 번에 너무 많은 질문을 하지 마세요(최대 2개 권장).\n",
    "                - 우선순위가 높은 질문부터 제시하세요.\n",
    "                - 선택지가 있는 경우 명확한 옵션을 제시하세요.\n",
    "                - 개선 제안은 구체적인 예시와 함께 제시하세요.\n",
    "                </interaction_guidelines>\n",
    "\n",
    "                이 정보를 바탕으로 LLM이 고품질의 이미지 생성 프롬프트를 작성할 수 있도록 충분히 상세하고 구체적인 설명을 제공하는 것이 중요합니다.\n",
    "                '''\n",
    "            )\n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)\n",
    "\n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                Here is user's ask: <ask>{ask}</ask>\n",
    "                '''\n",
    "            )\n",
    "            context = {\"ask\": ask}\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "            \n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts)            \n",
    "            self.messages.append(message)\n",
    "\n",
    "            resp, messages_updated = self.llm_caller.invoke(messages=self.messages, system_prompts=system_prompts)\n",
    "\n",
    "            results = eval(resp['text'])\n",
    "            \n",
    "            ask_reformulation, clarification_needed = results[\"reformulated_request\"], results[\"clarification_needed\"]\n",
    "            self.messages=messages_updated\n",
    "\n",
    "            return self.state(ask_refo=ask_reformulation, prev_node=\"ASK_REFORMULATION\")\n",
    "\n",
    "        def prompt_generation_for_image(state):\n",
    "\n",
    "            print(\"---PROMPT GENERATION FOR IMAGE---\")\n",
    "            ask_reformulation = state[\"ask_refo\"]\n",
    "            image_model = state[\"image_model\"]\n",
    "            previous_node = state[\"prev_node\"]\n",
    "            #code_error = state[\"code_err\"]\n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                당신은 이미지 생성 프롬프트 엔지니어링 전문가입니다.\n",
    "                사용자의 이미지 생성 요청과 선택된 이미지 생성 모델 {image_model}을 바탕으로 최적화된 프롬프트를 생성하는 것이 당신의 임무입니다.\n",
    "\n",
    "                <task>\n",
    "                재구성된 사용자 요청(ask_refo)을 바탕으로 선택된 이미지 생성 AI에 최적화된 프롬프트 생성\n",
    "                </task>\n",
    "\n",
    "                <input>\n",
    "                1. ask_reformulation: 재구성된 사용자의 이미지 생성 요청\n",
    "                2. model_name: 사용할 이미지 생성 AI 모델명\n",
    "                </input>\n",
    "\n",
    "                <output_format>\n",
    "                JSON 형식으로 다음 정보를 포함하여 응답하세요. 절대 JSON 포멧 외 텍스트는 넣지 마세요.:\n",
    "                {{\n",
    "                   \"prompt\": {{\n",
    "                       \"main_prompt\": \"주요 프롬프트\",\n",
    "                       \"negative_prompt\": \"제외할 요소들\",\n",
    "                       \"additional_params\": {{\n",
    "                           \"model_specific_params\": \"모델별 특수 파라미터\",\n",
    "                           \"style_params\": \"스타일 관련 파라미터\",\n",
    "                           \"quality_params\": \"품질 관련 파라미터\"\n",
    "                       }}\n",
    "                   }},\n",
    "                   \"model_config\": {{\n",
    "                       \"model\": \"사용할 모델명\",\n",
    "                       \"version\": \"모델 버전(해당되는 경우)\",\n",
    "                       \"specific_settings\": \"모델별 특수 설정\"\n",
    "                   }}\n",
    "                }}\n",
    "                </output_format>\n",
    "\n",
    "                <instruction>\n",
    "                1. 선택된 모델의 특성과 제한사항을 고려하세요.\n",
    "                2. 각 모델의 프롬프트 작성 best practice를 따르세요.\n",
    "                3. 시각적 요소들을 모델의 문법과 스타일에 맞게 변환하세요.\n",
    "                4. 주요 키워드의 강조나 가중치를 적절히 설정하세요.\n",
    "                5. 부적절하거나 금지된 내용이 포함되지 않도록 하세요.\n",
    "                6. 이미지의 품질과 일관성을 높이는 파라미터를 포함하세요.\n",
    "                7. 모델별 특수 기능이나 옵션을 활용하세요.\n",
    "                8. 프롬프트는 모두 영어로 작성하세요.\n",
    "                </instruction>\n",
    "\n",
    "                <model_specific_guidelines>\n",
    "                {{\n",
    "                   \"stable_diffusion\": {{\n",
    "                       \"prompt_format\": \"detailed description, style keywords, quality parameters\",\n",
    "                       \"weights_syntax\": \"(keyword:weight)\",\n",
    "                       \"negative_prompt_support\": true,\n",
    "                       \"max_length\": 500\n",
    "                   }},\n",
    "                   \"dalle\": {{\n",
    "                       \"prompt_format\": \"clear, natural language description\",\n",
    "                       \"negative_prompt_support\": false,\n",
    "                       \"max_length\": 400\n",
    "                   }},\n",
    "                   \"midjourney\": {{\n",
    "                       \"prompt_format\": \"description, style parameters, special commands\",\n",
    "                       \"parameter_syntax\": \"--parameter value\",\n",
    "                       \"negative_prompt_support\": true,\n",
    "                       \"max_length\": 600\n",
    "                   }}\n",
    "                }}\n",
    "                </model_specific_guidelines>\n",
    "\n",
    "                <consideration>\n",
    "                1. 모델별 프롬프트 최적화 전략을 적용하세요.\n",
    "                2. 이미지 품질을 높이는 일반적인 키워드를 적절히 활용하세요.\n",
    "                3. 부적절하거나 유해한 콘텐츠 생성을 방지하세요.\n",
    "                4. 저작권 관련 이슈를 고려하세요.\n",
    "                5. 모델의 한계와 제한사항을 고려하세요.\n",
    "                6. 프롬프트의 명확성과 구체성을 유지하세요.\n",
    "                </consideration>\n",
    "\n",
    "                <restrictions>\n",
    "                1. 폭력적이거나 유해한 내용\n",
    "                2. 성인용 콘텐츠\n",
    "                3. 혐오 표현\n",
    "                4. 개인정보가 포함된 내용\n",
    "                5. 저작권이 있는 캐릭터나 브랜드\n",
    "                6. 기타 불법적이거나 비윤리적인 내용\n",
    "                </restrictions>\n",
    "\n",
    "                이 정보를 바탕으로 선택된 이미지 생성 AI 모델에 최적화된 프롬프트를 생성하세요.\n",
    "                '''\n",
    "            )\n",
    "            print (image_model)\n",
    "            context = {\n",
    "                \"image_model\": image_model,\n",
    "            }\n",
    "            print (context)\n",
    "            print (system_prompts)\n",
    "            system_prompts = system_prompts.format(**context)\n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)\n",
    "                \n",
    "            # user_prompts = dedent(\n",
    "            #     '''\n",
    "            #     Here is the reformulated ask: <ask_reformulation>{ask_reformulation}</ask_reformulation>\n",
    "            #     Here is the error log: <error_log>{error_log}</error_log>\n",
    "            #     '''\n",
    "            # )\n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                Here is the reformulated ask: <ask_reformulation>{ask_reformulation}</ask_reformulation>\n",
    "                '''\n",
    "            )\n",
    "            context = {\n",
    "                \"ask_reformulation\": ask_reformulation,\n",
    "                #\"error_log\": \"None\" if code_error == \"None\" else code_error\n",
    "            }\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "\n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts)            \n",
    "            self.messages.append(message)\n",
    "\n",
    "            resp, messages_updated = self.llm_caller.invoke(messages=self.messages, system_prompts=system_prompts)\n",
    "            self.messages = messages_updated\n",
    "\n",
    "            results = eval(resp['text'])\n",
    "            code, img_path = results[\"code\"], results[\"img_path\"]\n",
    "\n",
    "            self.timer.measure(\"node: code_generation_for_chart\")\n",
    "            self.timer.print_measurements()\n",
    "\n",
    "            return self.state(code=code, img_path=img_path, prev_node=\"CODE_GENERATION\")\n",
    "\n",
    "        def chart_generation(state):\n",
    "\n",
    "            print(\"---CHART GENERATION---\")\n",
    "            df, code = self.df, state[\"code\"]\n",
    "\n",
    "            try:\n",
    "                results = exec(code, {\"df\": df})\n",
    "                return self.state(code_err=\"None\", prev_node=\"CHART_GENERATION\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_type = type(e).__name__\n",
    "                error_message = str(e)\n",
    "                error_traceback = traceback.format_exc()\n",
    "\n",
    "                error = f\"Error Type: {error_type}\\nError Message: {error_message}\\n\\nTraceback:\\n{error_traceback}\"\n",
    "                print (f\"error: {error}\")\n",
    "                return self.state(code_err=error, prev_node=\"CHART_GENERATION\")\n",
    "\n",
    "        def code_checker(state):\n",
    "\n",
    "            print(\"---CODE CHECKER---\")\n",
    "            code_error = state[\"code_err\"]\n",
    "\n",
    "            if code_error == \"None\":\n",
    "                print (\"---GO TO CHART DESCRIPTION---\")\n",
    "                return \"continue\"\n",
    "            else:\n",
    "                print (\"---[ERROR] GO TO CODE REWRITE---\")\n",
    "                return \"rewrite\"\n",
    "            \n",
    "        def chart_description(state):\n",
    "\n",
    "            print(\"---CHART DESCRIPTION---\")\n",
    "            img_path = state[\"img_path\"] # PNG 파일 경로\n",
    "\n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                <task>\n",
    "                 사용자의 요청(ask)에 따라 생성된 차트(PNG 형식)를 분석하고 설명합니다. 사용자의 원래 요청을 고려하여 차트의 내용을 정확하고 상세하게 해석하고, 관련 인사이트를 제공합니다.\n",
    "                 </task>\n",
    "                 \n",
    "                <output_format>\n",
    "                다음 정보를 포함하여 응답하세요:\n",
    "                1. 차트 개요: 차트 유형과 전반적인 구조 설명\n",
    "                2. 데이터 분석: 주요 데이터 포인트, 추세, 패턴 설명\n",
    "                3. 사용자 요청 연관성: 차트가 사용자의 요청을 어떻게 충족시키는지 설명\n",
    "                4. 주요 인사이트: 차트에서 도출할 수 있는 중요한 결론이나 통찰\n",
    "                5. 한계점 및 추가 고려사항: 차트의 제한사항이나 추가 분석 필요성\n",
    "                6. 요약 및 결론: 분석의 핵심 포인트와 사용자 요청에 대한 직접적인 답변\n",
    "                </output_format>\n",
    "                \n",
    "                <instruction>\n",
    "                1. 사용자의 요청(ask) 분석:\n",
    "                    - 사용자가 얻고자 하는 정보와 주요 키워드 파악\n",
    "                2. 차트 유형 식별:\n",
    "                    - 차트 유형 파악 및 사용자 요청과의 적절성 평가\n",
    "                3. 데이터 분석:\n",
    "                    - 주요 데이터 포인트, 추세, 패턴, 이상치 관찰\n",
    "                    - 관련 통계 정보 파악 (최대값, 최소값, 평균 등)\n",
    "                4. 차트 구성 요소 설명:\n",
    "                    - x축, y축, 범례, 제목, 라벨 등의 의미 해석\n",
    "                5. 사용자 요청과의 연관성 설명:\n",
    "                    - 차트가 사용자 요청을 어떻게 충족시키는지 구체적으로 설명\n",
    "                6. 인사이트 도출:\n",
    "                    - 차트에서 볼 수 있는 주요 인사이트나 결론 제시\n",
    "                    - 데이터의 의미를 사용자 요청 맥락에서 해석\n",
    "                7. 한계점 및 추가 고려사항 언급:\n",
    "                    - 차트의 한계점이나 누락된 정보 지적\n",
    "                    - 추가 분석이나 데이터 필요성 제안\n",
    "                8. 요약 및 결론 제시:\n",
    "                    - 분석의 핵심 포인트 요약\n",
    "                    - 사용자의 원래 요청에 대한 직접적인 답변 제공\n",
    "                </instruction>\n",
    "                \n",
    "                <consideration>\n",
    "                1. 객관적이고 중립적인 톤을 유지하며, 데이터에 기반한 설명 제공\n",
    "                2. 전문 용어 사용 시 필요에 따라 간단한 설명 추가\n",
    "                3. 사용자의 추가 질문 가능성을 고려하여 상세한 설명이 필요한 부분 명시\n",
    "                4. 차트나 데이터의 품질 문제가 있을 경우 적절히 지적\n",
    "                5. 사용자의 요청과 관련성이 낮은 차트 세부사항은 간략히 다루거나 생략\n",
    "                6. 시각적 요소(색상, 크기 등)가 데이터 해석에 중요한 경우 이를 언급\n",
    "                7. 가능한 경우, 차트에서 얻은 정보를 실제 상황이나 의사결정에 적용하는 방법 제안\n",
    "                8. 차트가 표현하는 데이터의 출처나 시간 범위가 중요한 경우 이를 강조\n",
    "                9. chart description 생성 시 '\"' 사용하지 말 것. \n",
    "                </consideration>\n",
    "                '''\n",
    "             )\n",
    "\n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)\n",
    "\n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                Here is the question: <ask>{ask}</ask>\n",
    "                Here is chart: \n",
    "                '''\n",
    "            )\n",
    "\n",
    "            context = {\n",
    "                \"ask\": ask_reformulation\n",
    "            }\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "            \n",
    "            self.img_bytes, img_base64 = self._png_to_bytes(img_path)\n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts, img=self.img_bytes)\n",
    "            self.messages.append(message)\n",
    "\n",
    "            resp, messages_updated = self.llm_caller.invoke(messages=self.messages, system_prompts=system_prompts, llm_name=\"sonnet\")\n",
    "            self.messages = messages_updated\n",
    "            chart_description = self._get_string_from_message(self.messages[-1])\n",
    "\n",
    "            self.timer.measure(\"node: chart_description\")\n",
    "            self.timer.print_measurements()\n",
    "             \n",
    "            return self.state(chart_desc=chart_description, prev_node=\"CHART_DESCRIPTION\")\n",
    "            \n",
    "        # langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "        workflow = StateGraph(self.state)\n",
    "\n",
    "        # Todo 를 작성합니다.\n",
    "        workflow.add_node(\"intent_analyzer\", intent_analyzer)  # 에이전트 노드를 추가합니다.\n",
    "        workflow.add_node(\"ask_reformulation\", ask_reformulation)  # 요청을 차트생성에 용이하게 수정하는 노드를 추가합니다.\n",
    "        workflow.add_node(\"prompt_generation_for_image\", prompt_generation_for_image)  # 차트 생성을 위한 코드 생성 노드를 추가합니다.\n",
    "        #workflow.add_node(\"chart_generation\", chart_generation)  # 생성된 코드를 실행하여 노드를 생성하는 노드를 추가합니다.\n",
    "        #workflow.add_node(\"chart_description\", chart_description)  # 생성된 코드를 설명하는 노드를 추가합니다.\n",
    "        \n",
    "        # 각 노드들을 연결합니다.\n",
    "        workflow.add_conditional_edges(\n",
    "            \"intent_analyzer\",\n",
    "            # 에이전트 결정 평가\n",
    "            should_image_generation,\n",
    "            {\n",
    "                # 도구 노드 호출\n",
    "                \"continue\": \"ask_reformulation\",\n",
    "                \"end\": END,\n",
    "            },\n",
    "        )\n",
    "        workflow.add_edge(\"ask_reformulation\", \"prompt_generation_for_image\")\n",
    "        # workflow.add_edge(\"code_generation_for_chart\", \"chart_generation\")\n",
    "        # workflow.add_conditional_edges(\n",
    "        #     \"chart_generation\",\n",
    "        #     # 에이전트 결정 평가\n",
    "        #     code_checker,\n",
    "        #     {\n",
    "        #         # 도구 노드 호출\n",
    "        #         \"continue\": \"chart_description\",\n",
    "        #         \"rewrite\": \"code_generation_for_chart\",\n",
    "        #     },\n",
    "        # )\n",
    "        # #workflow.add_edge(\"chart_generation\", \"chart_description\")\n",
    "        # workflow.add_edge(\"chart_description\", END)\n",
    "        #workflow.add_edge(\"intent_analyzer\", END)\n",
    "\n",
    "        # 시작점을 설정합니다.\n",
    "        workflow.set_entry_point(\"intent_analyzer\")\n",
    "\n",
    "        # 기록을 위한 메모리 저장소를 설정합니다.\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        # 그래프를 컴파일합니다.\n",
    "        self.app = workflow.compile(checkpointer=memory)        \n",
    "        self.config = RunnableConfig(recursion_limit=100, configurable={\"thread_id\": \"Text2Image\"})\n",
    "\n",
    "    def invoke(self, **kwargs):\n",
    "        \n",
    "        inputs = self.state(ask=kwargs[\"ask\"], image_model=kwargs[\"image_model\"])\n",
    "        # app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "        for output in self.app.stream(inputs, self.config):\n",
    "            # 출력된 결과에서 키와 값을 순회합니다.\n",
    "            for key, value in output.items():\n",
    "                # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "                pprint.pprint(f\"\\nOutput from node '{key}':\")\n",
    "                pprint.pprint(\"---\")\n",
    "                # 출력 값을 예쁘게 출력합니다.\n",
    "                pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "            # 각 출력 사이에 구분선을 추가합니다.\n",
    "            pprint.pprint(\"\\n---\\n\")\n",
    "    \n",
    "    def show_graph(self, ):\n",
    "        \n",
    "        from IPython.display import Image, display\n",
    "\n",
    "        try:\n",
    "            display(\n",
    "                Image(self.app.get_graph(xray=True).draw_mermaid_png())\n",
    "            )  # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. \n",
    "            # xray=True는 추가적인 세부 정보를 포함합니다.\n",
    "        except:\n",
    "            # 이 부분은 추가적인 의존성이 필요하며 선택적으로 실행됩니다.\n",
    "            pass\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b35c6aca-a29e-4272-9130-b01d1d350227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "24713621-3cb2-4640-b345-4fdf35c9a84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer = genai_analyzer(\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "87c2ec8e-81f7-4603-9fd4-5aaade4fe8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyzer.show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d6efed19-b6de-464c-8932-cb9bb8ceb9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "GENERATE_IMAGE (하늘의 구름을 시각화하는 자연 이미지 생성 요청)\n",
      "---DECIDE TO IMAGE GENERATION---\n",
      "---DECISION: IMAGE GENERATION---\n",
      "\"\\nOutput from node 'intent_analyzer':\"\n",
      "'---'\n",
      "{'ask': '하늘의 구름을 그려줘', 'prev_node': 'INTENT_ANALYZER'}\n",
      "'\\n---\\n'\n",
      "---ASK REFORMULATION---\n",
      "{\n",
      "  \"original_request\": \"하늘의 구름을 그려줘\",\n",
      "  \"visual_elements\": {\n",
      "    \"main_subject\": \"구름\",\n",
      "    \"setting\": \"하늘\",\n",
      "    \"style_hints\": \"명시되지 않음\",\n",
      "    \"color_hints\": \"명시되지 않음\",\n",
      "    \"composition_hints\": \"명시되지 않음\"\n",
      "  },\n",
      "  \"clarification_needed\": {\n",
      "    \"unclear_elements\": [\n",
      "      \"구름의 유형\",\n",
      "      \"시간대\",\n",
      "      \"날씨 상태\",\n",
      "      \"구름의 밀도\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"어떤 종류의 구름을 원하시나요? (적운, 층운, 권운 등)\",\n",
      "      \"하루 중 어느 시간대의 하늘인가요? (일출, 정오, 일몰 등)\"\n",
      "    ],\n",
      "    \"suggestions\": [\n",
      "      \"구름의 색상과 형태를 구체적으로 지정\",\n",
      "      \"하늘의 전반적인 분위기 설정\",\n",
      "      \"구도와 시점 추가\"\n",
      "    ]\n",
      "  },\n",
      "  \"missing_details\": [\n",
      "    \"구름의 형태와 패턴\",\n",
      "    \"하늘의 색상\",\n",
      "    \"날씨 조건\",\n",
      "    \"시점과 구도\",\n",
      "    \"빛의 방향과 강도\"\n",
      "  ],\n",
      "  \"reformulated_request\": \"맑고 푸른 하늘을 배경으로, 포근하고 솜털같은 흰 적운이 떠있는 평화로운 낮하늘의 모습. 구름은 자연스러운 층을 이루며 하늘에 둥둥 떠있고, 따스한 햇살이 구름을 부드럽게 비추고 있는 모습\",\n",
      "  \"interactive_guidance\": {\n",
      "    \"priority_questions\": [\n",
      "      \"어떤 분위기의 하늘을 원하시나요? (맑은/흐린/극적인)\",\n",
      "      \"구름의 양은 어느 정도로 하시겠습니까? (듬성듬성/빽빽한)\"\n",
      "    ],\n",
      "    \"enhancement_options\": [\n",
      "      \"일출/일몰의 극적인 하늘\",\n",
      "      \"맑은 날의 포근한 하늘\",\n",
      "      \"폭풍 전의 극적인 하늘\",\n",
      "      \"저녁 노을이 물든 하늘\"\n",
      "    ]\n",
      "  }\n",
      "}\"\\nOutput from node 'ask_reformulation':\"\n",
      "'---'\n",
      "{ 'ask_refo': '맑고 푸른 하늘을 배경으로, 포근하고 솜털같은 흰 적운이 떠있는 평화로운 낮하늘의 모습. 구름은 자연스러운 층을 '\n",
      "              '이루며 하늘에 둥둥 떠있고, 따스한 햇살이 구름을 부드럽게 비추고 있는 모습',\n",
      "  'prev_node': 'ASK_REFORMULATION'}\n",
      "'\\n---\\n'\n",
      "---PROMPT GENERATION FOR IMAGE---\n",
      "stable_diffusion\n",
      "{'image_model': 'stable_diffusion'}\n",
      "\n",
      "당신은 이미지 생성 프롬프트 엔지니어링 전문가입니다.\n",
      "사용자의 이미지 생성 요청과 선택된 이미지 생성 모델 {image_model}을 바탕으로 최적화된 프롬프트를 생성하는 것이 당신의 임무입니다.\n",
      "\n",
      "<task>\n",
      "재구성된 사용자 요청(ask_refo)을 바탕으로 선택된 이미지 생성 AI에 최적화된 프롬프트 생성\n",
      "</task>\n",
      "\n",
      "<input>\n",
      "1. ask_reformulation: 재구성된 사용자의 이미지 생성 요청\n",
      "2. model_name: 사용할 이미지 생성 AI 모델명\n",
      "</input>\n",
      "\n",
      "<output_format>\n",
      "JSON 형식으로 다음 정보를 포함하여 응답하세요. 절대 JSON 포멧 외 텍스트는 넣지 마세요.:\n",
      "{{\n",
      "   \"prompt\": {{\n",
      "       \"main_prompt\": \"주요 프롬프트\",\n",
      "       \"negative_prompt\": \"제외할 요소들\",\n",
      "       \"additional_params\": {{\n",
      "           \"model_specific_params\": \"모델별 특수 파라미터\",\n",
      "           \"style_params\": \"스타일 관련 파라미터\",\n",
      "           \"quality_params\": \"품질 관련 파라미터\"\n",
      "       }}\n",
      "   }},\n",
      "   \"model_config\": {{\n",
      "       \"model\": \"사용할 모델명\",\n",
      "       \"version\": \"모델 버전(해당되는 경우)\",\n",
      "       \"specific_settings\": \"모델별 특수 설정\"\n",
      "   }}\n",
      "}}\n",
      "</output_format>\n",
      "\n",
      "<instruction>\n",
      "1. 선택된 모델의 특성과 제한사항을 고려하세요.\n",
      "2. 각 모델의 프롬프트 작성 best practice를 따르세요.\n",
      "3. 시각적 요소들을 모델의 문법과 스타일에 맞게 변환하세요.\n",
      "4. 주요 키워드의 강조나 가중치를 적절히 설정하세요.\n",
      "5. 부적절하거나 금지된 내용이 포함되지 않도록 하세요.\n",
      "6. 이미지의 품질과 일관성을 높이는 파라미터를 포함하세요.\n",
      "7. 모델별 특수 기능이나 옵션을 활용하세요.\n",
      "8. 프롬프트는 모두 영어로 작성하세요.\n",
      "</instruction>\n",
      "\n",
      "<model_specific_guidelines>\n",
      "{{\n",
      "   \"stable_diffusion\": {{\n",
      "       \"prompt_format\": \"detailed description, style keywords, quality parameters\",\n",
      "       \"weights_syntax\": \"(keyword:weight)\",\n",
      "       \"negative_prompt_support\": true,\n",
      "       \"max_length\": 500\n",
      "   }},\n",
      "   \"dalle\": {{\n",
      "       \"prompt_format\": \"clear, natural language description\",\n",
      "       \"negative_prompt_support\": false,\n",
      "       \"max_length\": 400\n",
      "   }},\n",
      "   \"midjourney\": {{\n",
      "       \"prompt_format\": \"description, style parameters, special commands\",\n",
      "       \"parameter_syntax\": \"--parameter value\",\n",
      "       \"negative_prompt_support\": true,\n",
      "       \"max_length\": 600\n",
      "   }}\n",
      "}}\n",
      "</model_specific_guidelines>\n",
      "\n",
      "<consideration>\n",
      "1. 모델별 프롬프트 최적화 전략을 적용하세요.\n",
      "2. 이미지 품질을 높이는 일반적인 키워드를 적절히 활용하세요.\n",
      "3. 부적절하거나 유해한 콘텐츠 생성을 방지하세요.\n",
      "4. 저작권 관련 이슈를 고려하세요.\n",
      "5. 모델의 한계와 제한사항을 고려하세요.\n",
      "6. 프롬프트의 명확성과 구체성을 유지하세요.\n",
      "</consideration>\n",
      "\n",
      "<restrictions>\n",
      "1. 폭력적이거나 유해한 내용\n",
      "2. 성인용 콘텐츠\n",
      "3. 혐오 표현\n",
      "4. 개인정보가 포함된 내용\n",
      "5. 저작권이 있는 캐릭터나 브랜드\n",
      "6. 기타 불법적이거나 비윤리적인 내용\n",
      "</restrictions>\n",
      "\n",
      "이 정보를 바탕으로 선택된 이미지 생성 AI 모델에 최적화된 프롬프트를 생성하세요.\n",
      "\n",
      "{\n",
      "   \"prompt\": {\n",
      "       \"main_prompt\": \"serene daytime sky with fluffy white cumulus clouds (RAW:1.2), crystal clear blue sky background, soft sunlight illuminating clouds, natural cloud layers floating peacefully, photorealistic cloudscape, volumetric clouds with cotton-like texture, perfect weather, high altitude perspective, heavenly atmosphere, soft lighting (RAW:1.1), atmospheric depth\",\n",
      "       \"negative_prompt\": \"dark clouds, storm, rain, lightning, oversaturation, unrealistic colors, distortion, blurriness, artificial looking, low quality, grainy, overexposed\",\n",
      "       \"additional_params\": {\n",
      "           \"model_specific_params\": \"steps: 30, cfg_scale: 7.5, sampler: Euler a\",\n",
      "           \"style_params\": \"masterpiece, high quality, detailed, photorealistic, volumetric lighting\",\n",
      "           \"quality_params\": \"8k, high resolution, detailed, sharp focus\"\n",
      "       }\n",
      "   },\n",
      "   \"model_config\": {\n",
      "       \"model\": \"stable_diffusion\",\n",
      "       \"version\": \"v1.5\",\n",
      "       \"specific_settings\": {\n",
      "           \"width\": 1024,\n",
      "           \"height\": 768,\n",
      "           \"guidance_scale\": 7.5,\n",
      "           \"num_inference_steps\": 30\n",
      "       }\n",
      "   }\n",
      "}"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdedent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m하늘의 구름을 그려줘\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstable_diffusion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#ask=dedent(\"오늘 날씨는?\")\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[151], line 603\u001b[0m, in \u001b[0;36mgenai_analyzer.invoke\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate(ask\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask\u001b[39m\u001b[38;5;124m\"\u001b[39m], image_model\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_model\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39mstream(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;66;03m# 출력된 결과에서 키와 값을 순회합니다.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;66;03m# 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m         pprint\u001b[38;5;241m.\u001b[39mpprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput from node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1670\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1670\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1671\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1672\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1673\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1674\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1675\u001b[0m         ):\n\u001b[1;32m   1676\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langgraph/pregel/runner.py:171\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    169\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langgraph/utils/runnable.py:448\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    445\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m )\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langgraph/utils/runnable.py:219\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 219\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[151], line 430\u001b[0m, in \u001b[0;36mgenai_analyzer._graph_definition.<locals>.prompt_generation_for_image\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages \u001b[38;5;241m=\u001b[39m messages_updated\n\u001b[1;32m    429\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 430\u001b[0m code, img_path \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39mmeasure(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode: code_generation_for_chart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39mprint_measurements()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'code'"
     ]
    }
   ],
   "source": [
    "analyzer.invoke(\n",
    "    ask=dedent(\"하늘의 구름을 그려줘\"),\n",
    "    image_model=\"stable_diffusion\"\n",
    "    #ask=dedent(\"오늘 날씨는?\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18909f1d-8123-4024-b7ce-93ad0478d99c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.invoke(\n",
    "    ask=dedent(\"너무 많다. 2주일만  보여줘\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e9ef9a-94e1-4297-bb61-575975d8ca0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict, Annotated, Sequence\n",
    "from langgraph.graph import Graph, StateGraph\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import operator\n",
    "\n",
    "# Define the state structure\n",
    "class AgentState(TypedDict):\n",
    "    messages: Sequence[HumanMessage | AIMessage]\n",
    "    current_step: str\n",
    "    user_feedback: str | None\n",
    "    approved: bool | None\n",
    "\n",
    "# Initialize LLM\n",
    "llm = llm\n",
    "\n",
    "# Define node functions\n",
    "def generate_initial_response(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate initial response to user query.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": [*messages, response],\n",
    "        \"current_step\": \"awaiting_feedback\"\n",
    "    }\n",
    "\n",
    "def process_feedback(state: AgentState) -> AgentState:\n",
    "    \"\"\"Process user feedback and decide next steps.\"\"\"\n",
    "    feedback = state[\"user_feedback\"]\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if feedback.lower() in [\"yes\", \"approve\", \"good\"]:\n",
    "        return {**state, \"approved\": True, \"current_step\": \"end\"}\n",
    "    \n",
    "    # Add feedback to context and generate new response\n",
    "    feedback_msg = HumanMessage(content=f\"Please revise based on this feedback: {feedback}\")\n",
    "    messages.append(feedback_msg)\n",
    "    \n",
    "    new_response = llm.invoke(messages)\n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": [*messages, new_response],\n",
    "        \"current_step\": \"awaiting_feedback\",\n",
    "        \"user_feedback\": None\n",
    "    }\n",
    "\n",
    "def get_user_feedback() -> str:\n",
    "    \"\"\"Get feedback from user.\"\"\"\n",
    "    return input(\"Is this response satisfactory? (yes/no + feedback): \")\n",
    "\n",
    "# Define conditional edges\n",
    "def should_continue(state: AgentState) -> bool:\n",
    "    \"\"\"Determine if we need another iteration.\"\"\"\n",
    "    return not state.get(\"approved\", False)\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"generate_response\", generate_initial_response)\n",
    "workflow.add_node(\"get_feedback\", lambda x: {**x, \"user_feedback\": get_user_feedback()})\n",
    "workflow.add_node(\"process_feedback\", process_feedback)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"generate_response\", \"get_feedback\")\n",
    "workflow.add_edge(\"get_feedback\", \"process_feedback\")\n",
    "workflow.add_edge(\"process_feedback\", \"get_feedback\", condition=should_continue)\n",
    "\n",
    "# Set entry and end points\n",
    "workflow.set_entry_point(\"generate_response\")\n",
    "workflow.add_edge(\"process_feedback\", \"end\", condition=lambda x: not should_continue(x))\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Example usage\n",
    "def run_interactive_session(initial_query: str):\n",
    "    \"\"\"Run an interactive session with the agent.\"\"\"\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=initial_query)],\n",
    "        \"current_step\": \"start\",\n",
    "        \"user_feedback\": None,\n",
    "        \"approved\": False\n",
    "    }\n",
    "    \n",
    "    for state in app.stream(initial_state):\n",
    "        if state[\"current_step\"] == \"awaiting_feedback\":\n",
    "            print(\"\\nCurrent Response:\", state[\"messages\"][-1].content)\n",
    "            print(\"\\nProvide feedback or type 'yes' to approve:\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Write a short marketing copy for a new smartphone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed8ac1-1cf4-4947-9851-53dbbfd40e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.invoke(\n",
    "    ask=dedent(\"비교가 어렵네. 막대 그래프로 변환해 줄래?\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c74e0-ac8e-43a0-8ef3-ea8306909299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.invoke(\n",
    "    ask=dedent(\"전력 사용량이 가장 큰 지점을 표시해줘\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5e4a7-6243-4d76-9a94-a095b7595e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.invoke(\n",
    "    ask=dedent(\"가장 큰 전력을 쓴 날짜도 표시해줘\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16c7ba40-cacd-4760-9127-941c005ff0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "GENERATE_CHART (사용자가 app_e를 추가로 요청했으므로 새로운 시각화가 필요함)\n",
      "---DECIDE TO CHART GENERATION---\n",
      "---DECISION: CHART GENERATION---\n",
      "\"\\nOutput from node 'agent':\"\n",
      "'---'\n",
      "{'ask': '앱 e도 추가해 줄래?', 'prev_node': 'AGENT'}\n",
      "'\\n---\\n'\n",
      "---ASK REFORMULATION---\n",
      "{\n",
      "  \"target_apps\": [\"app_a\", \"app_b\", \"app_c\", \"app_e\"],\n",
      "  \"ask_reformulation\": \"app_a, app_b, app_c, app_e의 최근 2주일 동안의 전력사용량을 비교하는 막대 그래프를 생성하고, 각 앱의 최대 전력 사용량 지점과 전체적으로 가장 큰 전력을 사용한 날짜를 표시\"\n",
      "}\"\\nOutput from node 'ask_reformulation':\"\n",
      "'---'\n",
      "{ 'ask_refo': 'app_a, app_b, app_c, app_e의 최근 2주일 동안의 전력사용량을 비교하는 막대 그래프를 '\n",
      "              '생성하고, 각 앱의 최대 전력 사용량 지점과 전체적으로 가장 큰 전력을 사용한 날짜를 표시',\n",
      "  'prev_node': 'ASK_REFORMULATION',\n",
      "  'target_apps': ['app_a', 'app_b', 'app_c', 'app_e']}\n",
      "'\\n---\\n'\n",
      "---CODE GENERATION FOR CHART---\n",
      "{\n",
      "    \"code\": \"\"\"\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# 날짜를 datetime 형식으로 변환\n",
      "df['날짜'] = pd.to_datetime(df['날짜'])\n",
      "\n",
      "# 가장 최근 날짜 찾기\n",
      "latest_date = df['날짜'].max()\n",
      "\n",
      "# 2주 전 날짜 계산\n",
      "two_weeks_ago = latest_date - timedelta(days=13)\n",
      "\n",
      "# 최근 2주 데이터 필터링\n",
      "recent_data = df[(df['날짜'] >= two_weeks_ago) & (df['날짜'] <= latest_date)]\n",
      "\n",
      "# 그래프 크기 설정\n",
      "plt.figure(figsize=(16, 8))\n",
      "\n",
      "# 막대 그래프 그리기\n",
      "bar_width = 0.2\n",
      "index = np.arange(len(recent_data))\n",
      "\n",
      "plt.bar(index, recent_data['app_a'], bar_width, label='App A', alpha=0.8)\n",
      "plt.bar(index + bar_width, recent_data['app_b'], bar_width, label='App B', alpha=0.8)\n",
      "plt.bar(index + 2*bar_width, recent_data['app_c'], bar_width, label='App C', alpha=0.8)\n",
      "plt.bar(index + 3*bar_width, recent_data['app_e'], bar_width, label='App E', alpha=0.8)\n",
      "\n",
      "# 각 앱의 최대 전력 사용량 지점 찾기 및 표시\n",
      "apps = ['app_a', 'app_b', 'app_c', 'app_e']\n",
      "colors = ['red', 'green', 'blue', 'purple']\n",
      "for i, app in enumerate(apps):\n",
      "    max_value = recent_data[app].max()\n",
      "    max_index = recent_data[app].idxmax()\n",
      "    max_date = recent_data.loc[max_index, '날짜']\n",
      "    plt.scatter(index[recent_data.index.get_loc(max_index)] + i * bar_width, max_value, \\\n",
      "                color=colors[i], s=100, zorder=3)\n",
      "    plt.annotate(f'Max: {max_value}', \\\n",
      "                 (index[recent_data.index.get_loc(max_index)] + i * bar_width, max_value), \\\n",
      "                 xytext=(0, 10), textcoords='offset points', ha='center', va='bottom', \\\n",
      "                 color=colors[i], fontweight='bold')\n",
      "\n",
      "# 전체적으로 가장 큰 전력을 사용한 날짜 찾기 및 표시\n",
      "total_power = recent_data[['app_a', 'app_b', 'app_c', 'app_e']].sum(axis=1)\n",
      "max_total_power_index = total_power.idxmax()\n",
      "max_total_power_date = recent_data.loc[max_total_power_index, '날짜']\n",
      "max_total_power = total_power.max()\n",
      "\n",
      "plt.axvline(x=index[recent_data.index.get_loc(max_total_power_index)] + 1.5*bar_width, \\\n",
      "            color='orange', linestyle='--', linewidth=2)\n",
      "plt.text(index[recent_data.index.get_loc(max_total_power_index)] + 1.5*bar_width, \\\n",
      "         plt.ylim()[1], f'Max Total: {max_total_power:.0f}\\\\n{max_total_power_date.date()}', \\\n",
      "         color='orange', fontweight='bold', ha='center', va='bottom')\n",
      "\n",
      "# 그래프 제목과 레이블 설정\n",
      "plt.title('Power Consumption Comparison of App A, B, C, and E (Last 2 Weeks)')\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel('Power Consumption')\n",
      "\n",
      "# x축 레이블 설정\n",
      "plt.xticks(index + 1.5*"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 64) (<string>, line 2)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[38], line 1\u001b[0m\n    analyzer.invoke(\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[28], line 500\u001b[0m in \u001b[1;35minvoke\u001b[0m\n    for output in self.app.stream(inputs, self.config):\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1029\u001b[0m in \u001b[1;35mstream\u001b[0m\n    _panic_or_proceed(done, inflight, loop.step)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1456\u001b[0m in \u001b[1;35m_panic_or_proceed\u001b[0m\n    raise exc\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.10/site-packages/langgraph/pregel/executor.py:60\u001b[0m in \u001b[1;35mdone\u001b[0m\n    task.result()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m in \u001b[1;35mresult\u001b[0m\n    return self.__get_result()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m in \u001b[1;35m__get_result\u001b[0m\n    raise self._exception\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m in \u001b[1;35mrun\u001b[0m\n    result = self.fn(*self.args, **self.kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.10/site-packages/langgraph/pregel/retry.py:25\u001b[0m in \u001b[1;35mrun_with_retry\u001b[0m\n    task.proc.invoke(task.input, task.config)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:2876\u001b[0m in \u001b[1;35minvoke\u001b[0m\n    input = context.run(step.invoke, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.10/site-packages/langgraph/utils.py:102\u001b[0m in \u001b[1;35minvoke\u001b[0m\n    ret = context.run(self.func, input, **kwargs)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[28], line 327\u001b[0;36m in \u001b[0;35mcode_generation_for_chart\u001b[0;36m\n\u001b[0;31m    results = eval(resp['text'])\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"code\": \"\"\"\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 64)\n"
     ]
    }
   ],
   "source": [
    "analyzer.invoke(\n",
    "    ask=dedent(\"앱 e도 추가해 줄래?\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548e81b-cce8-4500-a3a3-0cc1701212b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9cc8b-71a6-41da-992c-da08ab39eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. 코드에 주석을 달아 각 단계를 설명하세요. 주석은 \"#####\"를 이용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792a4ee-4cb0-418e-9b6c-6baccf780060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드설명 백업\n",
    "'''\n",
    "<task>\n",
    " 사용자의 요청(ask)에 따라 생성된 차트(PNG 형식)를 분석하고 설명합니다. 사용자의 원래 요청을 고려하여 차트의 내용을 정확하고 상세하게 해석하고, 관련 인사이트를 제공합니다.\n",
    " </task>\n",
    " \n",
    "<output_format>\n",
    "다음 정보를 포함하여 응답하세요:\n",
    "1. 주요 인사이트: 차트에서 도출할 수 있는 중요한 결론이나 통찰\n",
    "2. 한계점 및 추가 고려사항: 차트의 제한사항이나 추가 분석 필요성\n",
    "</output_format>\n",
    "\n",
    "<instruction>\n",
    "1. 사용자의 요청(ask) 분석:\n",
    "    - 사용자가 얻고자 하는 정보와 주요 키워드 파악\n",
    "2. 차트 유형 식별:\n",
    "    - 차트 유형 파악 및 사용자 요청과의 적절성 평가\n",
    "3. 데이터 분석:\n",
    "    - 주요 데이터 포인트, 추세, 패턴, 이상치 관찰\n",
    "    - 관련 통계 정보 파악 (최대값, 최소값, 평균 등)\n",
    "4. 차트 구성 요소 설명:\n",
    "    - x축, y축, 범례, 제목, 라벨 등의 의미 해석\n",
    "5. 사용자 요청과의 연관성 설명:\n",
    "    - 차트가 사용자 요청을 어떻게 충족시키는지 구체적으로 설명\n",
    "6. 인사이트 도출:\n",
    "    - 차트에서 볼 수 있는 주요 인사이트나 결론 제시\n",
    "    - 데이터의 의미를 사용자 요청 맥락에서 해석\n",
    "7. 한계점 및 추가 고려사항 언급:\n",
    "    - 차트의 한계점이나 누락된 정보 지적\n",
    "    - 추가 분석이나 데이터 필요성 제안\n",
    "8. 요약 및 결론 제시:\n",
    "    - 분석의 핵심 포인트 요약\n",
    "    - 사용자의 원래 요청에 대한 직접적인 답변 제공\n",
    "</instruction>\n",
    "\n",
    "<consideration>\n",
    "1. 객관적이고 중립적인 톤을 유지하며, 데이터에 기반한 설명 제공\n",
    "2. 전문 용어 사용 시 필요에 따라 간단한 설명 추가\n",
    "3. 사용자의 추가 질문 가능성을 고려하여 상세한 설명이 필요한 부분 명시\n",
    "4. 차트나 데이터의 품질 문제가 있을 경우 적절히 지적\n",
    "5. 사용자의 요청과 관련성이 낮은 차트 세부사항은 간략히 다루거나 생략\n",
    "6. 시각적 요소(색상, 크기 등)가 데이터 해석에 중요한 경우 이를 언급\n",
    "7. 가능한 경우, 차트에서 얻은 정보를 실제 상황이나 의사결정에 적용하는 방법 제안\n",
    "8. 차트가 표현하는 데이터의 출처나 시간 범위가 중요한 경우 이를 강조\n",
    "9. chart description 생성 시 '\"' 사용하지 말 것. \n",
    "</consideration>\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b8e647a79df62bf31906a725b05de775d285962ac600487339d38c51a5c07b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
