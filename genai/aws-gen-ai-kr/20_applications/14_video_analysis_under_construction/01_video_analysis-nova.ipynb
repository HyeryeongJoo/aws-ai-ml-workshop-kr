{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4eb837-ef78-4b45-9e2c-dbd81c3b763d",
   "metadata": {},
   "source": [
    "# Video analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e755fec-095b-43ce-8b2f-e46fd002dc00",
   "metadata": {},
   "source": [
    "## Setting\n",
    " - Auto Reload\n",
    " - path for utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee5ec9-5112-4eda-88ab-43a0ef1721a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7fb1d-710f-4345-92a0-b5a794742afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60864ffe-4fe9-4e08-bb49-9b4ecdb8c7db",
   "metadata": {},
   "source": [
    "## 1. Create Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705c228-f793-47e9-b2da-ade94f209fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock\n",
    "from utils.bedrock import bedrock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197919c-2fbc-4344-a259-4c34dca45a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "- os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "- os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "- os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "- os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a986c0-60fe-4e77-875c-338a138460fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "print (colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint (bedrock_info.get_list_fm_models(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e2368-e195-47e4-b981-80f7e6dc4421",
   "metadata": {},
   "source": [
    "## 2. LLM 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b094dc1-26f9-4dcc-b51a-178edadb407e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.bedrock import bedrock_model\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05a4a4-0ffc-4e29-b1db-0f28ce6374d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = bedrock_model(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Nova-Pro-CRI\"),\n",
    "    #model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-5-V-2-Sonnet-CRI\"),\n",
    "    bedrock_client=boto3_bedrock,\n",
    "    stream=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    inference_config={\n",
    "        'maxTokens': 1024,\n",
    "        'stopSequences': [\"\\n\\nHuman\"],\n",
    "        'temperature': 0.01,\n",
    "        #'topP': ...,\n",
    "    }\n",
    "    #additional_model_request_fields={\"top_k\": 200}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bfe3f-bcdb-4b76-bcfe-df9273733e7a",
   "metadata": {},
   "source": [
    "## 3. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c3f3c-0acf-4db0-b0db-27cb903655c8",
   "metadata": {},
   "source": [
    "### 3.1 LLM caller 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e2397-0314-43f4-8bcd-004ba6f6d7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from utils.bedrock import bedrock_utils, bedrock_chain\n",
    "\n",
    "class llm_call():\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.llm=kwargs[\"llm\"]\n",
    "        self.verbose = kwargs.get(\"verbose\", False)\n",
    "        self.chain = bedrock_chain(bedrock_utils.converse_api) | bedrock_chain(bedrock_utils.outputparser)\n",
    "\n",
    "    def _message_format(self, role, message):\n",
    "\n",
    "        if role == \"user\":\n",
    "             message_format = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": dedent(message)}]\n",
    "            }\n",
    "        elif role == \"assistant\":\n",
    "            \n",
    "            message_format = {\n",
    "                \"role\": \"assistant\",\n",
    "                'content': [{'text': dedent(message)}]\n",
    "            }\n",
    "\n",
    "        return message_format\n",
    "            \n",
    "    def invoke(self, **kwargs):\n",
    "\n",
    "        system_prompts = kwargs.get(\"system_prompts\", None)\n",
    "        messages = kwargs[\"messages\"]\n",
    "        #llm_name = kwargs[\"llm_name\"]\n",
    "    \n",
    "        response = self.chain( ## pipeline의 제일 처음 func의 argument를 입력으로 한다. 여기서는 converse_api의 arg를 쓴다.\n",
    "            llm=self.llm,\n",
    "            system_prompts=system_prompts,\n",
    "            messages=messages,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        ai_message = self._message_format(role=\"assistant\", message=response[\"text\"])\n",
    "        messages.append(ai_message)\n",
    "        return response, messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e968d-2b35-4006-b2ed-73f25c16c516",
   "metadata": {},
   "source": [
    "### 3.2 Timer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c634d-6166-4159-86c7-aea8813cf5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf0ca9-c1ad-430b-9b37-008d8d032fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeMeasurement:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.measurements = {}\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def measure(self, section_name):\n",
    "        if self.start_time is None:\n",
    "            raise ValueError(\"start() 메서드를 먼저 호출해야 합니다.\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - self.start_time\n",
    "        self.measurements[section_name] = elapsed_time\n",
    "        self.start_time = end_time  # 다음 구간 측정을 위해 시작 시간 재설정\n",
    "\n",
    "    def reset(self, ):\n",
    "        self.measurements = {}\n",
    "\n",
    "    def print_measurements(self):\n",
    "        for section, elapsed_time in self.measurements.items():\n",
    "            #print(f\"{section}: {elapsed_time:.5f} 초\")\n",
    "            print(colored (f\"\\nelapsed time: {section}: {elapsed_time:.5f} 초\", \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255c4f6-c9da-45fa-821d-7275d0cabce8",
   "metadata": {},
   "source": [
    "### 3.3 Video analyzer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31a24d-3698-4e09-b781-58556c143eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import shutil\n",
    "import pickle\n",
    "import botocore\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import TypedDict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.common_utils import retry\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453fae87-661d-430b-9ee8-921e32857a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    video_path: str\n",
    "    video_info: dict\n",
    "    analysis_config: dict\n",
    "    summary:str\n",
    "    target_apps: list[str]\n",
    "    ask_refo: str\n",
    "    code: str\n",
    "    code_err: str\n",
    "    img_path: str\n",
    "    img_bytes: str\n",
    "    chart_desc: str\n",
    "    prev_node: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9c7d3-b5bc-414b-a9a4-a9e084e34952",
   "metadata": {},
   "source": [
    "- **approach 1**: 각각의 프레임에 대한 desc를 생성한 후, 생성된 desc를 모아서 요약하여 상황설명 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0879f8-e7f1-4b42-819b-a4ae113feb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class video_analyzer():\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.llm=kwargs[\"llm\"]\n",
    "        self.state = GraphState\n",
    "\n",
    "        self.llm_caller = llm_call(\n",
    "            llm=self.llm,\n",
    "            verbose=True ## To show token usage\n",
    "        ) \n",
    "\n",
    "        self._graph_definition()\n",
    "        self.messages = []\n",
    "        self.img_bytes = \"\"\n",
    "        \n",
    "        self.timer = TimeMeasurement()\n",
    "        \n",
    "    def _get_price(self, tokens):\n",
    "        \n",
    "        input_price = tokens[\"input\"] * 0.0008 / 1000\n",
    "        output_price = tokens[\"output\"] * 0.0032 / 1000\n",
    "        total = input_price + output_price\n",
    "        \n",
    "        print (\"======= Cost Calculator =======\")\n",
    "        print (f'Token Usage, input: {tokens[\"input\"]}, Output: {tokens[\"output\"]}')\n",
    "        print (f'Price: {total} USD')\n",
    "        print (\"===============================\")\n",
    "        \n",
    "    def _get_string_from_message(self, message):\n",
    "        return message[\"content\"][0][\"text\"]\n",
    "    \n",
    "    def _get_message_from_string(self, role, string, imgs=None):\n",
    "        \n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": []\n",
    "        }\n",
    "        \n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_message = {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'png',\n",
    "                        \"source\": {\"bytes\": img}\n",
    "                    }\n",
    "                }\n",
    "                message[\"content\"].append(img_message)\n",
    "        \n",
    "        message[\"content\"].append({\"text\": dedent(string)})\n",
    "\n",
    "        return message\n",
    "        \n",
    "    def _frame_to_bytes(self, frame, format='.png'):\n",
    "        \"\"\"\n",
    "        cv2 frame을 bytes로 변환\n",
    "\n",
    "        Args:\n",
    "            frame: cv2로 읽은 이미지/프레임\n",
    "            format: 이미지 포맷 (예: '.jpg', '.png')\n",
    "\n",
    "        Returns:\n",
    "            bytes: 이미지의 바이트 데이터\n",
    "        \"\"\"\n",
    "        # imencode() 함수로 프레임을 지정된 포맷의 이미지로 인코딩\n",
    "        # 반환값: (success, encoded_image)\n",
    "        success, buffer = cv2.imencode(format, frame)\n",
    "\n",
    "        if not success:\n",
    "            raise ValueError(\"이미지 인코딩 실패\")\n",
    "\n",
    "        # numpy array를 bytes로 변환\n",
    "        return buffer.tobytes()\n",
    "\n",
    "    def _save_pickle(self, data: Any, file_path: str | Path) -> None:\n",
    "        \"\"\"\n",
    "        데이터를 pickle 파일로 저장합니다.\n",
    "\n",
    "        Args:\n",
    "            data: 저장할 데이터 (Any type)\n",
    "            file_path: 저장할 파일 경로 (확장자 .pkl 권장)\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        # 디렉토리가 없으면 생성\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            print(f\"Successfully saved to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving pickle file: {e}\")\n",
    "\n",
    "    def _load_pickle(self, file_path: str | Path) -> Any:\n",
    "        \"\"\"\n",
    "        pickle 파일을 로드합니다.\n",
    "\n",
    "        Args:\n",
    "            file_path: 로드할 파일 경로\n",
    "\n",
    "        Returns:\n",
    "            저장된 데이터 객체\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: 파일이 존재하지 않을 경우\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"No pickle file found at {file_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pickle file: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_messages(self, ):\n",
    "        return self.messages\n",
    "        \n",
    "    def _graph_definition(self, **kwargs):\n",
    "        \n",
    "        def agent(state):\n",
    "            \n",
    "            self.timer.start()\n",
    "            self.timer.reset()\n",
    "            \n",
    "            print(\"---CALL AGENT---\")\n",
    "            video_path = state[\"video_path\"]\n",
    "            analysis_config = state[\"analysis_config\"]\n",
    "            \n",
    "            self.tokens = {\"input\": 0, \"output\": 0, \"total\": 0}\n",
    "            self.pricing = {\"input\": 0, \"output\": 0}\n",
    "            \n",
    "            print (analysis_config)\n",
    " \n",
    "            return self.state(\n",
    "                video_path=video_path,\n",
    "                analysis_config=analysis_config,\n",
    "                rev_node=\"AGENT\"\n",
    "            )\n",
    "\n",
    "        def sample_video_frames(state):\n",
    "            \n",
    "            def setup_directory(dir_path):\n",
    "    \n",
    "                print (\"===setup_directory===\")\n",
    "                # 디렉토리가 존재하는지 확인\n",
    "                if os.path.exists(dir_path):\n",
    "                    # 존재하면 삭제\n",
    "                    shutil.rmtree(dir_path)\n",
    "                    print(f\"기존 디렉토리 삭제됨: {dir_path}\")\n",
    "\n",
    "                # 디렉토리 생성\n",
    "                os.makedirs(dir_path)\n",
    "                print(f\"디렉토리 생성됨: {dir_path}\")\n",
    "                print (\"=====================\")\n",
    "    \n",
    "            \n",
    "            print(\"---SAMPLE VIDEO FRAMES---\")\n",
    "            \n",
    "            video_path = state[\"video_path\"]\n",
    "            sample_msec = state[\"analysis_config\"][\"sample_msec\"]\n",
    "            resize_ratio = state[\"analysis_config\"][\"resize_ratio\"]\n",
    "            sample_output_dir = state[\"analysis_config\"][\"sample_output_dir\"]\n",
    "            \n",
    "            \n",
    "            print (\"===sample_video_frames===\")\n",
    "            \"\"\"\n",
    "            비디오에서 특정 시간 간격으로 프레임을 샘플링하는 함수\n",
    "\n",
    "            Args:\n",
    "                video_path (str): 비디오 파일 경로\n",
    "                sample_msec (int): 샘플링 간격 (밀리초)\n",
    "                sample_output_dir (Optional[str]): 프레임 저장 디렉토리. None이면 저장하지 않음\n",
    "\n",
    "            Returns:\n",
    "                Tuple[int, int]: (총 프레임 수, 샘플링된 프레임 수)\n",
    "\n",
    "            Raises:\n",
    "                FileNotFoundError: 비디오 파일이 없는 경우\n",
    "                ValueError: 샘플링 간격이 잘못된 경우\n",
    "            \"\"\"\n",
    "            # 입력값 검증\n",
    "            if not os.path.exists(video_path):\n",
    "                raise FileNotFoundError(f\"비디오 파일을 찾을 수 없습니다: {video_path}\")\n",
    "\n",
    "            if sample_msec <= 0:\n",
    "                raise ValueError(\"샘플링 간격은 0보다 커야 합니다\")\n",
    "\n",
    "            # 비디오 캡처 객체 생성\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                raise RuntimeError(\"비디오 파일을 열 수 없습니다\")\n",
    "\n",
    "            # 비디오 정보 가져오기\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "            # 샘플링 간격(프레임 단위) 계산\n",
    "            frame_interval = max(1, int(sample_msec / 1000 * fps))\n",
    "\n",
    "            # 출력 디렉토리 생성 (지정된 경우)\n",
    "            if sample_output_dir is not None:\n",
    "                setup_directory(sample_output_dir)\n",
    "\n",
    "            sampled_count = 0\n",
    "            frame_count = 0\n",
    "            sampled_frame = {\"frame\": [], \"seq\": []}\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                \n",
    "                # resize\n",
    "                if resize_ratio != 1:\n",
    "                    re_width, re_height = int(frame.shape[1]*resize_ratio), int(frame.shape[0]*resize_ratio)\n",
    "                    frame = cv2.resize(frame, (re_width, re_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # frame_interval마다 프레임 처리\n",
    "                if frame_count % frame_interval == 0:\n",
    "                    if sample_output_dir is not None:\n",
    "                        # 프레임 저장\n",
    "                        frame_path = os.path.join(sample_output_dir, f\"frame_{frame_count:06d}.jpg\")\n",
    "                        cv2.imwrite(frame_path, frame)\n",
    "                    sampled_count += 1\n",
    "                    sampled_frame[\"frame\"].append(frame)\n",
    "                    sampled_frame[\"seq\"].append(frame_count)\n",
    "                frame_count += 1\n",
    "\n",
    "                # 진행상황 출력 (10% 단위)\n",
    "                if frame_count % (total_frames // 10) == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"진행률: {progress:.1f}%\")\n",
    "\n",
    "            # 자원 해제\n",
    "            cap.release()\n",
    "\n",
    "            print(f\"\\n처리 완료:\")\n",
    "            print(f\"총 프레임 수: {total_frames}\")\n",
    "            print(f\"프레임 크기: {sampled_frame['frame'][0].shape[1]}X{sampled_frame['frame'][0].shape[0]}\")\n",
    "            print(f\"샘플링된 프레임 수: {sampled_count}\")\n",
    "            print(f\"샘플링 간격: {frame_interval}프레임 ({sample_msec}ms)\")\n",
    "            if sample_output_dir is not None:\n",
    "                print(f\"저장 위치: {sample_output_dir}\")\n",
    "\n",
    "            print (\"=========================\")\n",
    "            \n",
    "            sampled_frames_path = os.path.join(sample_output_dir, \"pickle\", \"sampled_frames.pickle\")\n",
    "            self._save_pickle(sampled_frame, sampled_frames_path)\n",
    "            \n",
    "            video_info = {\n",
    "                \"sampled_frames_path\": sampled_frames_path,\n",
    "                \"total_frame_cnt\": total_frames,\n",
    "                \"sampled_cnt\": sampled_count\n",
    "            }\n",
    "            \n",
    "            self.timer.measure(\"node: sample_video_frames\")\n",
    "            self.timer.print_measurements()\n",
    "            \n",
    "            return self.state(\n",
    "                video_info=video_info,\n",
    "                rev_node=\"SAMPLE_VIDEO_FRAMES\"\n",
    "            )\n",
    "        \n",
    "        def video_description(state):\n",
    "            \n",
    "            @retry(total_try_cnt=5, sleep_in_sec=10, retryable_exceptions=(botocore.exceptions.EventStreamError))\n",
    "            def frame_description(**kwargs):\n",
    "\n",
    "                sampled_frame = kwargs[\"sampled_frame\"]\n",
    "                sampled_frame_idx = kwargs[\"sampled_frame_idx\"]\n",
    "                total_frame_cnt = kwargs[\"total_frame_cnt\"]\n",
    "                prev_frame_desc = kwargs.get(\"prev_frame_desc\", \"None\")\n",
    "                messages = kwargs[\"messages\"] \n",
    "\n",
    "                system_prompts = dedent(\n",
    "                    '''\n",
    "                    System:\n",
    "                    1. You are a CCTV Video Analysis Expert specialized in analyzing surveillance footage frames and describing situations in natural language.\n",
    "                    2. You must analyze the given CCTV frame images objectively and comprehensively.\n",
    "\n",
    "                    Model Instructions:\n",
    "                    - You MUST maintain objectivity in all observations\n",
    "                    - You MUST describe only observable facts, avoiding speculation\n",
    "                    - You MUST focus on moving objects and people activity, ignoring static elements                    \n",
    "                    - You MUST write all descriptions in Korean\n",
    "                    - You MUST consider the previous frame description for context\n",
    "                    - You MUST highlight any significant changes or anomalies\n",
    "                    - DO NOT describe stationary objects or background elements\n",
    "                    - DO NOT make subjective interpretations\n",
    "                    - DO NOT speculate about intentions or motivations\n",
    "                    - DO NOT use ```json``` in response\n",
    "\n",
    "                    Input Format:\n",
    "                    - frame: Captured CCTV frame image\n",
    "                    - frame_info:\n",
    "                        - frame_number: Current frame number in sequence\n",
    "                        - total_frame_number: Total number of frames\n",
    "                        - prev_frame_desc: Description of previous frame\n",
    "\n",
    "                    Output Schema:\n",
    "                    {\n",
    "                        \"scene_description\": string,  // Objective description of current frame situation\n",
    "                        \"person_description\": string  // Distinctive features of moving persons for identification\n",
    "                    }\n",
    "\n",
    "                    Observation Guidelines:\n",
    "                    1. Movement Analysis:\n",
    "                       - Track position changes of people and objects\n",
    "                       - Note entry/exit of subjects from frame\n",
    "                       - Document any sudden or unusual movements\n",
    "\n",
    "                    2. Visual Clarity:\n",
    "                       - Report any visual obstructions\n",
    "                       - Note lighting conditions affecting visibility\n",
    "                       - Mention any quality issues impacting analysis\n",
    "\n",
    "                    3. Subject Differentiation:\n",
    "                       - Distinguish between multiple subjects\n",
    "                       - Note identifying characteristics\n",
    "                       - Track individual movements separately\n",
    "\n",
    "                    4. Critical Observations:\n",
    "                       - Highlight suspicious activities\n",
    "                       - Note any safety concerns\n",
    "                       - Document unusual patterns\n",
    "\n",
    "                    Remember: Your descriptions must be concise, clear, and factual, focusing only on observable changes and movements within the frame.\n",
    "                    '''\n",
    "                )\n",
    "                user_prompts = dedent(\n",
    "                    '''\n",
    "                    Frame_number:\n",
    "                    {frame_number}\n",
    "                    Total_frame_number:\n",
    "                    {total_frame_number}\n",
    "                    Prev_frame_desc:\n",
    "                    {prev_frame_desc}\n",
    "\n",
    "                    '''\n",
    "                )\n",
    "\n",
    "                img_bytes = self._frame_to_bytes(sampled_frame)\n",
    "                system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)  \n",
    "\n",
    "                context = {\n",
    "                    \"frame_number\": sampled_frame_idx,\n",
    "                    \"total_frame_number\": total_frame_cnt,\n",
    "                    \"prev_frame_desc\": prev_frame_desc\n",
    "                }\n",
    "                user_prompts = user_prompts.format(**context)\n",
    "\n",
    "                message = self._get_message_from_string(role=\"user\", string=user_prompts, imgs=[img_bytes])\n",
    "                messages.append(message)\n",
    "                \n",
    "                 # 이미지 표시\n",
    "                rgb_frame = cv2.cvtColor(sampled_frame, cv2.COLOR_BGR2RGB)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(rgb_frame)\n",
    "                plt.axis('off')  # 축 숨기기\n",
    "                plt.show()\n",
    "                \n",
    "                # Call LLM\n",
    "                resp, messages_updated = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "                \n",
    "                if self.llm_caller.verbose:\n",
    "                    self.tokens[\"input\"] += resp[\"token_usage\"][\"inputTokens\"]\n",
    "                    self.tokens[\"output\"] += resp[\"token_usage\"][\"outputTokens\"]\n",
    "                    self.tokens[\"total\"] += resp[\"token_usage\"][\"totalTokens\"]\n",
    "                    print (f'total token usage: {self.tokens}')    \n",
    "                \n",
    "                messages = messages_updated\n",
    "                results = eval(resp[\"text\"])\n",
    "\n",
    "                return results\n",
    "            \n",
    "            print(\"---VIDEO DESCRIPTION---\")\n",
    "            sampled_frames= self._load_pickle(state[\"video_info\"][\"sampled_frames_path\"])\n",
    "            total_frame_cnt=state[\"video_info\"][\"total_frame_cnt\"]\n",
    "            messages=kwargs.get(\"messages\", [])\n",
    "\n",
    "            frame_desc = \"None\"\n",
    "            frame_descs = []\n",
    "            for sampled_frame_idx, sampled_frame in zip(sampled_frames[\"seq\"], sampled_frames[\"frame\"]):\n",
    "\n",
    "                res = frame_description(\n",
    "                    sampled_frame=sampled_frame,\n",
    "                    sampled_frame_idx=sampled_frame_idx,\n",
    "                    total_frame_cnt=total_frame_cnt,\n",
    "                    frame_desc=frame_desc,\n",
    "                    messages=[]\n",
    "                )\n",
    "                frame_desc = res[\"scene_description\"]\n",
    "                frame_descs.append([res[\"scene_description\"], res[\"person_description\"]])\n",
    "\n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                System:\n",
    "                1. You are an Advanced CCTV Analysis Expert specialized in synthesizing frame-by-frame descriptions to provide comprehensive situation analysis and event extraction, particularly for after-business-hours surveillance footage.\n",
    "                2. Your role is to analyze sequential frame descriptions and identify meaningful patterns and events while maintaining privacy and security considerations.\n",
    "\n",
    "                Model Instructions:\n",
    "                1. Analysis Requirements:\n",
    "                  - You MUST review frame descriptions chronologically\n",
    "                  - You MUST identify and extract meaningful events\n",
    "                  - You MUST write all content in Korean\n",
    "                  - You MUST focus on behaviors, not person counts\n",
    "                  - You MUST consider after-business-hours context when assessing behavior\n",
    "                  - DO NOT include personally identifiable information\n",
    "                  - DO NOT speculate about unclear situations\n",
    "                  - DO NOT include routine movements as key events\n",
    "\n",
    "                2. Event Assessment Criteria:\n",
    "                  - Security risks\n",
    "                  - Abnormal behavior patterns\n",
    "                  - Property or facility risks\n",
    "                  - Pattern repetition\n",
    "                  - Contextual significance\n",
    "                  - Unauthorized access or presence\n",
    "                  - After-hours activity legitimacy\n",
    "\n",
    "                Input Format:\n",
    "                frame_descriptions: [\n",
    "                   {\n",
    "                       \"scene_description\": string,\n",
    "                       \"person_description\": string\n",
    "                   }\n",
    "                ]\n",
    "\n",
    "                Output Schema:\n",
    "                {\n",
    "                   \"summary\": string,  // Comprehensive situation analysis\n",
    "                   \"key_events\": [\n",
    "                       {\n",
    "                           \"description\": string,  // Event description\n",
    "                           \"significance\": \"HIGH/MEDIUM/LOW\"  // Event importance\n",
    "                       }\n",
    "                   ],\n",
    "                   \"objects_involved\": {\n",
    "                       \"items\": [string]    // Key objects\n",
    "                   },\n",
    "                   \"analysis\": {\n",
    "                       \"pattern\": string,   // Identified behavior patterns\n",
    "                       \"anomalies\": [string], // Unusual activities\n",
    "                       \"risk_assessment\": string  // Potential risk evaluation\n",
    "                   }\n",
    "                }\n",
    "\n",
    "                Analysis Guidelines:\n",
    "                1. After-Hours Context Evaluation:\n",
    "                  - Consider normal vs. abnormal after-hours presence\n",
    "                  - Evaluate authorization level of observed personnel\n",
    "                  - Pay special attention to access patterns\n",
    "                  - Monitor unusual equipment or area usage\n",
    "\n",
    "                2. Context Evaluation:\n",
    "                  - Review descriptions chronologically\n",
    "                  - Consider full context for pattern identification\n",
    "                  - Verify consistency across frames\n",
    "                  - Identify person consistency across frames\n",
    "\n",
    "                3. Event Prioritization:\n",
    "                  - Focus on security-relevant events\n",
    "                  - Identify behavioral patterns\n",
    "                  - Assess potential risks\n",
    "                  - Exclude authorized routine activities\n",
    "                  - Flag any unauthorized after-hours activities\n",
    "\n",
    "                4. Risk Assessment:\n",
    "                  - Evaluate security implications\n",
    "                  - Identify potential threats\n",
    "                  - Assess pattern abnormalities\n",
    "                  - Consider after-hours security protocols\n",
    "                  - Evaluate facility access and usage\n",
    "\n",
    "                Remember:\n",
    "                - Maintain objectivity in all analyses\n",
    "                - Prioritize security and privacy\n",
    "                - Focus on behavioral patterns\n",
    "                - Exclude non-essential details\n",
    "                - Report only observed facts\n",
    "                - Consider elevated risk during after-hours\n",
    "                - Pay special attention to authorization and access\n",
    "                '''\n",
    "            )\n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                Frame_descriptions:\n",
    "                {frame_descriptions}\n",
    "\n",
    "                '''\n",
    "            )\n",
    "\n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)  \n",
    "\n",
    "            context = {\n",
    "                \"frame_descriptions\": str(frame_descs)\n",
    "            }\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "\n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts)\n",
    "            messages.append(message)\n",
    "\n",
    "            resp, messages_updated = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "            \n",
    "            if self.llm_caller.verbose:\n",
    "                self.tokens[\"input\"] += resp[\"token_usage\"][\"inputTokens\"]\n",
    "                self.tokens[\"output\"] += resp[\"token_usage\"][\"outputTokens\"]\n",
    "                self.tokens[\"total\"] += resp[\"token_usage\"][\"totalTokens\"]            \n",
    "                self._get_price(self.tokens)\n",
    "                \n",
    "            messages = messages_updated\n",
    "            results = eval(resp[\"text\"])\n",
    "            \n",
    "            self.timer.measure(\"node: video_description\")\n",
    "            self.timer.print_measurements()\n",
    "            \n",
    "            return self.state(\n",
    "                summary=results,\n",
    "                rev_node=\"VIDEO_DESCRIPTION\"\n",
    "            )\n",
    "        \n",
    "        def cleanup(state):\n",
    "                        \n",
    "            print(\"---CLEAN UP---\")\n",
    "            sample_output_dir = state[\"analysis_config\"][\"sample_output_dir\"]\n",
    "            \n",
    "            if os.path.exists(sample_output_dir):\n",
    "                # 존재하면 삭제\n",
    "                shutil.rmtree(sample_output_dir)\n",
    "                print(f\"디렉토리 삭제됨: {sample_output_dir}\")\n",
    "                        \n",
    "        # langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "        workflow = StateGraph(self.state)\n",
    "\n",
    "        # Todo 를 작성합니다.\n",
    "        workflow.add_node(\"agent\", agent)  # 에이전트 노드를 추가합니다.\n",
    "        workflow.add_node(\"sample_video_frames\", sample_video_frames)  # 비디오를 샘플링하여 프레임(이미지)변환 노드를 추가합니다.\n",
    "        workflow.add_node(\"video_description\", video_description)  # 비디오 영상 설명 노드를 추가합니다.\n",
    "        workflow.add_node(\"cleanup\", cleanup)  # 정리 노드를 추가합니다.\n",
    "\n",
    "        workflow.add_edge(\"agent\", \"sample_video_frames\")\n",
    "        workflow.add_edge(\"sample_video_frames\", \"video_description\")\n",
    "        workflow.add_edge(\"video_description\", \"cleanup\")\n",
    "        workflow.add_edge(\"cleanup\", END)\n",
    "\n",
    "        # 시작점을 설정합니다.\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "\n",
    "        # 기록을 위한 메모리 저장소를 설정합니다.\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        # 그래프를 컴파일합니다.\n",
    "        self.app = workflow.compile(checkpointer=memory)        \n",
    "        self.config = RunnableConfig(recursion_limit=100, configurable={\"thread_id\": \"VideoAnalysis\"})\n",
    "\n",
    "    def invoke(self, **kwargs):\n",
    "        \n",
    "        inputs = self.state(\n",
    "            video_path=kwargs[\"video_path\"],\n",
    "            analysis_config=kwargs[\"analysis_config\"]\n",
    "        )\n",
    "        # app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "        for output in self.app.stream(inputs, self.config):\n",
    "            # 출력된 결과에서 키와 값을 순회합니다.\n",
    "            for key, value in output.items():\n",
    "                # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "                pprint(f\"\\nOutput from node '{key}':\")\n",
    "                pprint(\"---\")\n",
    "                # 출력 값을 예쁘게 출력합니다.\n",
    "                pprint(value, indent=2, width=80, depth=None)\n",
    "            # 각 출력 사이에 구분선을 추가합니다.\n",
    "            pprint(\"\\n---\\n\")\n",
    "    \n",
    "    def show_graph(self, ):\n",
    "        \n",
    "        from IPython.display import Image, display\n",
    "\n",
    "        try:\n",
    "            display(\n",
    "                Image(self.app.get_graph(xray=True).draw_mermaid_png())\n",
    "            )  # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. \n",
    "            # xray=True는 추가적인 세부 정보를 포함합니다.\n",
    "        except:\n",
    "            # 이 부분은 추가적인 의존성이 필요하며 선택적으로 실행됩니다.\n",
    "            pass\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e6ac3-d647-4a84-9d22-e2584b016403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer = video_analyzer(\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a956e-a9f5-4267-8502-5e7797caa855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2787cb-d8b9-41a9-950e-81cb7109c0b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.invoke(\n",
    "    video_path=\"./video/video_sample_2.mkv\",\n",
    "    analysis_config={\n",
    "        \"sample_msec\": 10000,\n",
    "        \"resize_ratio\": 0.7,\n",
    "        \"sample_output_dir\": \"./workspace\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd6105-b5e6-4385-9342-2deed36e5ad0",
   "metadata": {},
   "source": [
    "- **approach 2**: n개 연속 프레임 제공 후 상황에 대한 desc를 생성, 생성된 desc를 모아서 요약하여 상황설명 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ae3e2-f17e-4a61-bf81-ff649260b604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class video_analyzer():\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.llm=kwargs[\"llm\"]\n",
    "        self.state = GraphState\n",
    "\n",
    "        self.llm_caller = llm_call(\n",
    "            llm=self.llm,\n",
    "            verbose=True ## To show token usage\n",
    "        ) \n",
    "\n",
    "        self._graph_definition()\n",
    "        self.messages = []\n",
    "        self.img_bytes = \"\"\n",
    "        \n",
    "        self.timer = TimeMeasurement()\n",
    "        \n",
    "    def _get_price(self, tokens):\n",
    "        \n",
    "        input_price = tokens[\"input\"] * 0.003 / 1000\n",
    "        output_price = tokens[\"output\"] * 0.015 / 1000\n",
    "        total = input_price + output_price\n",
    "        \n",
    "        print (\"======= Cost Calculator =======\")\n",
    "        print (f'Token Usage, input: {tokens[\"input\"]}, Output: {tokens[\"output\"]}')\n",
    "        print (f'Price: {total} USD')\n",
    "        print (\"===============================\")\n",
    "        \n",
    "    def _get_string_from_message(self, message):\n",
    "        return message[\"content\"][0][\"text\"]\n",
    "    \n",
    "    def _get_message_from_string(self, role, string, imgs=None):\n",
    "        \n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": []\n",
    "        }\n",
    "        \n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_message = {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'png',\n",
    "                        \"source\": {\"bytes\": img}\n",
    "                    }\n",
    "                }\n",
    "                message[\"content\"].append(img_message)\n",
    "        \n",
    "        message[\"content\"].append({\"text\": dedent(string)})\n",
    "\n",
    "        return message\n",
    "    \n",
    "    def _frame_to_bytes(self, frame, format='.png'):\n",
    "        \"\"\"\n",
    "        cv2 frame을 bytes로 변환\n",
    "\n",
    "        Args:\n",
    "            frame: cv2로 읽은 이미지/프레임\n",
    "            format: 이미지 포맷 (예: '.jpg', '.png')\n",
    "\n",
    "        Returns:\n",
    "            bytes: 이미지의 바이트 데이터\n",
    "        \"\"\"\n",
    "        # imencode() 함수로 프레임을 지정된 포맷의 이미지로 인코딩\n",
    "        # 반환값: (success, encoded_image)\n",
    "        success, buffer = cv2.imencode(format, frame)\n",
    "\n",
    "        if not success:\n",
    "            raise ValueError(\"이미지 인코딩 실패\")\n",
    "\n",
    "        # numpy array를 bytes로 변환\n",
    "        return buffer.tobytes()\n",
    "\n",
    "    def _save_pickle(self, data: Any, file_path: str | Path) -> None:\n",
    "        \"\"\"\n",
    "        데이터를 pickle 파일로 저장합니다.\n",
    "\n",
    "        Args:\n",
    "            data: 저장할 데이터 (Any type)\n",
    "            file_path: 저장할 파일 경로 (확장자 .pkl 권장)\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        # 디렉토리가 없으면 생성\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            print(f\"Successfully saved to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving pickle file: {e}\")\n",
    "\n",
    "    def _load_pickle(self, file_path: str | Path) -> Any:\n",
    "        \"\"\"\n",
    "        pickle 파일을 로드합니다.\n",
    "\n",
    "        Args:\n",
    "            file_path: 로드할 파일 경로\n",
    "\n",
    "        Returns:\n",
    "            저장된 데이터 객체\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: 파일이 존재하지 않을 경우\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"No pickle file found at {file_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pickle file: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_messages(self, ):\n",
    "        return self.messages\n",
    "        \n",
    "    def _graph_definition(self, **kwargs):\n",
    "        \n",
    "        def agent(state):\n",
    "            \n",
    "            self.timer.start()\n",
    "            self.timer.reset()\n",
    "            \n",
    "            print(\"---CALL AGENT---\")\n",
    "            video_path = state[\"video_path\"]\n",
    "            analysis_config = state[\"analysis_config\"]\n",
    "            \n",
    "            self.tokens = {\"input\": 0, \"output\": 0, \"total\": 0}\n",
    "            self.pricing = {\"input\": 0, \"output\": 0}\n",
    "            \n",
    "            print (analysis_config)\n",
    " \n",
    "            return self.state(\n",
    "                video_path=video_path,\n",
    "                analysis_config=analysis_config,\n",
    "                rev_node=\"AGENT\"\n",
    "            )\n",
    "\n",
    "        def sample_video_frames(state):\n",
    "        \n",
    "            def setup_directory(dir_path):\n",
    "    \n",
    "                print (\"===setup_directory===\")\n",
    "                # 디렉토리가 존재하는지 확인\n",
    "                if os.path.exists(dir_path):\n",
    "                    # 존재하면 삭제\n",
    "                    shutil.rmtree(dir_path)\n",
    "                    print(f\"기존 디렉토리 삭제됨: {dir_path}\")\n",
    "\n",
    "                # 디렉토리 생성\n",
    "                os.makedirs(dir_path)\n",
    "                print(f\"디렉토리 생성됨: {dir_path}\")\n",
    "                print (\"=====================\")\n",
    "    \n",
    "            video_path = state[\"video_path\"]\n",
    "            sample_msec = state[\"analysis_config\"][\"sample_msec\"]\n",
    "            resize_ratio = state[\"analysis_config\"][\"resize_ratio\"]\n",
    "            sample_output_dir = state[\"analysis_config\"][\"sample_output_dir\"]\n",
    "            \n",
    "            print(\"---SAMPLE VIDEO FRAMES---\")\n",
    "            \"\"\"\n",
    "            비디오에서 특정 시간 간격으로 프레임을 샘플링하는 함수\n",
    "\n",
    "            Args:\n",
    "                video_path (str): 비디오 파일 경로\n",
    "                sample_msec (int): 샘플링 간격 (밀리초)\n",
    "                sample_output_dir (Optional[str]): 프레임 저장 디렉토리. None이면 저장하지 않음\n",
    "\n",
    "            Returns:\n",
    "                Tuple[int, int]: (총 프레임 수, 샘플링된 프레임 수)\n",
    "\n",
    "            Raises:\n",
    "                FileNotFoundError: 비디오 파일이 없는 경우\n",
    "                ValueError: 샘플링 간격이 잘못된 경우\n",
    "            \"\"\"\n",
    "            # 입력값 검증\n",
    "            if not os.path.exists(video_path):\n",
    "                raise FileNotFoundError(f\"비디오 파일을 찾을 수 없습니다: {video_path}\")\n",
    "\n",
    "            if sample_msec <= 0:\n",
    "                raise ValueError(\"샘플링 간격은 0보다 커야 합니다\")\n",
    "\n",
    "            # 비디오 캡처 객체 생성\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                raise RuntimeError(\"비디오 파일을 열 수 없습니다\")\n",
    "\n",
    "            # 비디오 정보 가져오기\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "            # 샘플링 간격(프레임 단위) 계산\n",
    "            frame_interval = max(1, int(sample_msec / 1000 * fps))\n",
    "\n",
    "            # 출력 디렉토리 생성 (지정된 경우)\n",
    "            if sample_output_dir is not None:\n",
    "                setup_directory(sample_output_dir)\n",
    "\n",
    "            sampled_count = 0\n",
    "            frame_count = 0\n",
    "            sampled_frame = {\"frame\": [], \"seq\": []}\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                \n",
    "                if resize_ratio != 1: \n",
    "                    re_width, re_height = int(frame.shape[1]*resize_ratio), int(frame.shape[0]*resize_ratio)\n",
    "                    frame = cv2.resize(frame, (re_width, re_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # frame_interval마다 프레임 처리\n",
    "                if frame_count % frame_interval == 0:\n",
    "                    if sample_output_dir is not None:\n",
    "                        # 프레임 저장\n",
    "                        frame_path = os.path.join(sample_output_dir, f\"frame_{frame_count:06d}.jpg\")\n",
    "                        cv2.imwrite(frame_path, frame)\n",
    "                    sampled_count += 1\n",
    "                    sampled_frame[\"frame\"].append(frame)\n",
    "                    sampled_frame[\"seq\"].append(frame_count)\n",
    "                frame_count += 1\n",
    "\n",
    "                # 진행상황 출력 (10% 단위)\n",
    "                if frame_count % (total_frames // 10) == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"진행률: {progress:.1f}%\")\n",
    "\n",
    "            # 자원 해제\n",
    "            cap.release()\n",
    "\n",
    "            print(f\"\\n처리 완료:\")\n",
    "            print(f\"총 프레임 수: {total_frames}\")\n",
    "            print(f\"프레임 크기: {sampled_frame['frame'][0].shape[1]}X{sampled_frame['frame'][0].shape[0]}\")\n",
    "            print(f\"샘플링된 프레임 수: {sampled_count}\")\n",
    "            print(f\"샘플링 간격: {frame_interval}프레임 ({sample_msec}ms)\")\n",
    "            if sample_output_dir is not None:\n",
    "                print(f\"저장 위치: {sample_output_dir}\")\n",
    "\n",
    "            print (\"=========================\")\n",
    "            \n",
    "            sampled_frames_path = os.path.join(sample_output_dir, \"pickle\", \"sampled_frames.pickle\")\n",
    "            self._save_pickle(sampled_frame, sampled_frames_path)\n",
    "            \n",
    "            video_info = {\n",
    "                \"sampled_frames_path\": sampled_frames_path,\n",
    "                \"total_frame_cnt\": total_frames,\n",
    "                \"sampled_cnt\": sampled_count\n",
    "            }\n",
    "            \n",
    "            self.timer.measure(\"node: sample_video_frames\")\n",
    "            self.timer.print_measurements()\n",
    "            \n",
    "            return self.state(\n",
    "                video_info=video_info,\n",
    "                rev_node=\"SAMPLE_VIDEO_FRAMES\"\n",
    "            )\n",
    "        \n",
    "        def video_description(state):\n",
    "            \n",
    "            @retry(total_try_cnt=5, sleep_in_sec=10, retryable_exceptions=(botocore.exceptions.EventStreamError))\n",
    "            def frame_description(**kwargs):\n",
    "    \n",
    "                sampled_frame = kwargs[\"sampled_frame\"]\n",
    "                sampled_frame_idx = kwargs[\"sampled_frame_idx\"]\n",
    "                frame_batch_size = kwargs[\"frame_batch_size\"]\n",
    "                prev_frame_desc = kwargs.get(\"prev_frame_desc\", \"None\")\n",
    "                messages = kwargs[\"messages\"]\n",
    "\n",
    "                system_prompts = dedent(\n",
    "                    '''\n",
    "                    System:\n",
    "                    1. You are a CCTV Video Analysis Expert specialized in analyzing sequences of surveillance footage frames and describing situations in natural language.\n",
    "                    2. Your role is to observe multiple consecutive frames and provide comprehensive situation analysis while maintaining objectivity and focus on relevant activities.\n",
    "\n",
    "                    Model Instructions:\n",
    "                    - You MUST analyze frames chronologically \n",
    "                    - You MUST focus on task, movement and behavioral patterns\n",
    "                    - You MUST write all descriptions in Korean\n",
    "                    - You MUST highlight significant changes or anomalies\n",
    "                    - DO NOT describe static objects or background elements\n",
    "                    - DO NOT make subjective interpretations\n",
    "                    - DO NOT focus on counting or identifying specific individuals\n",
    "                    - DO NOT speculate about unclear situations\n",
    "\n",
    "                    Input Format:\n",
    "                    - frames: Array of consecutive CCTV frame images\n",
    "                    - frame_count: Number of provided frames\n",
    "                    - prev_frame_desc: Description of previous frame sequence\n",
    "\n",
    "                    Output Schema:\n",
    "                    {\n",
    "                        \"sequence_summary\": string,  // Objective description of observed situation\n",
    "                        \"key_events\": [\n",
    "                            {\n",
    "                                \"frame_range\": [start_frame, end_frame],\n",
    "                                \"event_description\": string  // Description of significant event\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    Analysis Guidelines:\n",
    "                    1. Movement Tracking:\n",
    "                       - Track continuous movement patterns\n",
    "                       - Note entry/exit from frame\n",
    "                       - Document significant position changes\n",
    "                       - Identify reappearing subjects\n",
    "\n",
    "                    2. Sequence Understanding:\n",
    "                       - Review frames chronologically\n",
    "                       - Consider previous sequence context\n",
    "                       - Maintain continuity in descriptions\n",
    "                       - Focus on action progression\n",
    "\n",
    "                    3. Visual Quality Assessment:\n",
    "                       - Report visual obstructions\n",
    "                       - Note lighting conditions\n",
    "                       - Mention quality limitations\n",
    "                       - Identify unclear areas\n",
    "\n",
    "                    4. Critical Observations:\n",
    "                       - Highlight unusual activities\n",
    "                       - Note behavioral anomalies\n",
    "                       - Document significant changes\n",
    "                       - Mark suspicious patterns\n",
    "\n",
    "                    5. Behavioral Focus:\n",
    "                       - Emphasize actions over identities\n",
    "                       - Track behavioral patterns\n",
    "                       - Note interactions\n",
    "                       - Document activity sequences\n",
    "\n",
    "                    Remember:\n",
    "                    - Maintain chronological continuity\n",
    "                    - Focus on observable actions\n",
    "                    - Be objective and clear\n",
    "                    - Highlight significant changes\n",
    "                    - Note technical limitations\n",
    "                    - Track subject reappearances\n",
    "\n",
    "                    '''\n",
    "                )\n",
    "                user_prompts = dedent(\n",
    "                    '''\n",
    "                     Frame_count:\n",
    "                    {frame_count}\n",
    "                    Prev_frame_desc>\n",
    "                    {prev_frame_desc}\n",
    "\n",
    "                    '''\n",
    "                )\n",
    "\n",
    "                img_bytes = []\n",
    "                for frame in sampled_frame:                    \n",
    "                    img_bytes.append(self._frame_to_bytes(frame))\n",
    "                \n",
    "                system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)  \n",
    "\n",
    "                context = {\n",
    "                    \"frame_count\": sampled_frame_idx,\n",
    "                    \"prev_frame_desc\": prev_frame_desc\n",
    "                }\n",
    "                user_prompts = user_prompts.format(**context)\n",
    "\n",
    "                message = self._get_message_from_string(role=\"user\", string=user_prompts, imgs=img_bytes)\n",
    "                messages.append(message)\n",
    "                \n",
    "                # 이미지 표시\n",
    "                num_frames = len(sampled_frames)\n",
    "                num_cols = frame_batch_size # row에 몇개의 사진을 보이게 할 것 인가\n",
    "                num_rows = (num_frames + num_cols - 1) // num_cols  # 올림 나눗셈\n",
    "\n",
    "                # 전체 figure 크기 설정\n",
    "                plt.figure(figsize=(4*num_cols, 4*num_rows))\n",
    "\n",
    "                # 각 프레임을 subplot으로 표시\n",
    "                for idx, frame in enumerate(sampled_frame):\n",
    "                    plt.subplot(num_rows, num_cols, idx + 1)\n",
    "\n",
    "                    # BGR to RGB 변환\n",
    "                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # 이미지 표시\n",
    "                    plt.imshow(rgb_frame)\n",
    "                    plt.title(f'Frame {idx+1}')\n",
    "                    plt.axis('off')\n",
    "\n",
    "                # subplot 간 간격 조정\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                ## Call LLM\n",
    "                resp, messages_updated = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "                messages = messages_updated\n",
    "                results = eval(resp[\"text\"])\n",
    "                \n",
    "                if self.llm_caller.verbose:\n",
    "                    self.tokens[\"input\"] += resp[\"token_usage\"][\"inputTokens\"]\n",
    "                    self.tokens[\"output\"] += resp[\"token_usage\"][\"outputTokens\"]\n",
    "                    self.tokens[\"total\"] += resp[\"token_usage\"][\"totalTokens\"]\n",
    "                    print (f'total token usage: {self.tokens}')    \n",
    "\n",
    "                return results\n",
    "            \n",
    "            print(\"---VIDEO DESCRIPTION---\")\n",
    "            \n",
    "            frame_batch_size = state[\"analysis_config\"][\"frame_numbers_at_one\"] \n",
    "            sliding_interval = state[\"analysis_config\"][\"sliding_window_size\"] \n",
    "            sampled_frames= self._load_pickle(state[\"video_info\"][\"sampled_frames_path\"])\n",
    "            total_frame_cnt=state[\"video_info\"][\"total_frame_cnt\"]\n",
    "            total_sample_frame_cnt = state[\"video_info\"][\"sampled_cnt\"]\n",
    "            messages=kwargs.get(\"messages\", [])\n",
    "            \n",
    "            # 제약 조건 체크\n",
    "            if frame_batch_size > total_sample_frame_cnt:\n",
    "                raise ValueError(\"Window size cannot be larger than total frames\")\n",
    "\n",
    "            # 마지막 프레임까지 커버하기 위한 조건 체크\n",
    "            remaining_frames = (total_sample_frame_cnt - frame_batch_size) % sliding_interval\n",
    "            if remaining_frames != 0:\n",
    "                print(f\"Warning: Last {remaining_frames} frames might be skipped with current parameters\")\n",
    "                print(f\"Recommended sliding_interval: {[i for i in range(1, frame_batch_size) if (total_sample_frame_cnt - frame_batch_size) % i == 0]}\")\n",
    "                #raise ValueError(\"In this settingm you could not use all frames. Read recommendations\")\n",
    "\n",
    "            # 마지막 시작점 계산\n",
    "            last_start = total_sample_frame_cnt - frame_batch_size\n",
    "\n",
    "            frame_desc = \"None\"\n",
    "            frame_descs = []\n",
    "            for i in range(0, last_start + 1, sliding_interval):\n",
    "                # frame_batch_size 개의 연속된 프레임 슬라이스 추출\n",
    "                sampled_frame_idx = sampled_frames[\"seq\"][i:i+frame_batch_size]\n",
    "                sampled_frame = sampled_frames[\"frame\"][i:i+frame_batch_size]\n",
    "\n",
    "                # 마지막 윈도우가 전체 프레임을 커버하지 못하는 경우, 마지막 윈도우 추가\n",
    "                # if i + sliding_interval > last_start and i != last_start:\n",
    "                #     sampled_frame_idx = sampled_frames[\"seq\"][last_start:last_start+frame_batch_size]\n",
    "                #     sampled_frame = sampled_frames[\"frame\"][last_start:last_start+frame_batch_size]\n",
    "                    #break\n",
    "                \n",
    "                res = frame_description(\n",
    "                    sampled_frame=sampled_frame,\n",
    "                    sampled_frame_idx=sampled_frame_idx,\n",
    "                    frame_batch_size=frame_batch_size,\n",
    "                    total_frame_cnt=total_frame_cnt,\n",
    "                    frame_desc=frame_desc, \n",
    "                    messages=[]\n",
    "                )\n",
    "\n",
    "                frame_desc = res[\"sequence_summary\"]\n",
    "                frame_descs.append(res[\"sequence_summary\"])\n",
    "\n",
    "\n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                System:\n",
    "                1. You are a CCTV Surveillance Analysis Expert specialized in synthesizing frame-by-frame descriptions into comprehensive situational analysis.\n",
    "                2. Your role is to analyze sequential frame descriptions, identify meaningful patterns, extract significant events, and assess security risks while maintaining privacy considerations.\n",
    "\n",
    "                Model Instructions:\n",
    "                1. Analysis Requirements:\n",
    "                  - You MUST review frame descriptions chronologically\n",
    "                  - You MUST identify and extract meaningful events\n",
    "                  - You MUST write all content in Korean\n",
    "                  - You MUST evaluate event significance based on security impact\n",
    "                  - DO NOT include personally identifiable information\n",
    "                  - DO NOT speculate about unclear situations\n",
    "                  - DO NOT include routine movements as key events\n",
    "\n",
    "                2. Event Assessment Criteria:\n",
    "                  - Security risks and threats\n",
    "                  - Abnormal behavior patterns\n",
    "                  - Property/facility risks\n",
    "                  - Pattern repetition\n",
    "                  - Contextual significance\n",
    "\n",
    "                Input Format:\n",
    "                frame_descriptions: [\n",
    "                   string  // Sequential frame descriptions\n",
    "                ]\n",
    "\n",
    "                Output Schema:\n",
    "                {\n",
    "                   \"summary\": string,  // Comprehensive situation analysis\n",
    "                   \"key_events\": [\n",
    "                       {\n",
    "                           \"description\": string,  // Event description\n",
    "                           \"significance\": \"HIGH/MEDIUM/LOW\"  // Event importance\n",
    "                       }\n",
    "                   ],\n",
    "                   \"objects_involved\": {\n",
    "                       \"people\": [string],  // Roles without specific identifiers\n",
    "                       \"items\": [string]    // Key objects\n",
    "                   },\n",
    "                   \"analysis\": {\n",
    "                       \"pattern\": string,   // Identified behavior patterns\n",
    "                       \"anomalies\": [string], // Unusual activities\n",
    "                       \"risk_assessment\": string  // Potential risk evaluation\n",
    "                   }\n",
    "                }\n",
    "\n",
    "                Analysis Guidelines:\n",
    "                1. Sequence Analysis:\n",
    "                  - Review descriptions chronologically\n",
    "                  - Verify sequence consistency\n",
    "                  - Identify behavioral patterns\n",
    "                  - Track subject continuity\n",
    "                  - Note temporal relationships\n",
    "\n",
    "                2. Event Prioritization:\n",
    "                  - Focus on security-relevant events\n",
    "                  - Evaluate behavior patterns\n",
    "                  - Assess potential risks\n",
    "                  - Exclude routine activities\n",
    "                  - Identify repeated patterns\n",
    "\n",
    "                3. Risk Assessment:\n",
    "                  - Evaluate security implications\n",
    "                  - Identify potential threats\n",
    "                  - Assess pattern abnormalities\n",
    "                  - Consider contextual factors\n",
    "                  - Flag suspicious activities\n",
    "\n",
    "                4. Pattern Recognition:\n",
    "                  - Identify behavioral trends\n",
    "                  - Note recurring events\n",
    "                  - Track movement patterns\n",
    "                  - Document unusual sequences\n",
    "                  - Compare with normal activity\n",
    "\n",
    "                Remember:\n",
    "                - Maintain objectivity in analysis\n",
    "                - Prioritize security and privacy\n",
    "                - Focus on significant patterns\n",
    "                - Exclude non-essential details\n",
    "                - Report only observed facts\n",
    "                - Consider full context\n",
    "                - Flag potential security risks\n",
    "                '''\n",
    "            )\n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                Frame_descriptions:\n",
    "                {frame_descriptions}\n",
    "\n",
    "                '''\n",
    "            )\n",
    "\n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)  \n",
    "\n",
    "            context = {\n",
    "                \"frame_descriptions\": str(frame_descs)\n",
    "            }\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "\n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts)\n",
    "            messages.append(message)\n",
    "\n",
    "            resp, messages_updated = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "            messages = messages_updated\n",
    "            results = eval(resp[\"text\"])\n",
    "            \n",
    "            if self.llm_caller.verbose:\n",
    "                self.tokens[\"input\"] += resp[\"token_usage\"][\"inputTokens\"]\n",
    "                self.tokens[\"output\"] += resp[\"token_usage\"][\"outputTokens\"]\n",
    "                self.tokens[\"total\"] += resp[\"token_usage\"][\"totalTokens\"]            \n",
    "                self._get_price(self.tokens)\n",
    "            \n",
    "            self.timer.measure(\"node: video_description\")\n",
    "            self.timer.print_measurements()\n",
    "            \n",
    "            return self.state(\n",
    "                summary=results,\n",
    "                rev_node=\"VIDEO_DESCRIPTION\"\n",
    "            )\n",
    "        \n",
    "        def cleanup(state):\n",
    "                        \n",
    "            print(\"---CLEAN UP---\")\n",
    "            analysis_config = state[\"analysis_config\"]\n",
    "            \n",
    "            sample_output_dir = state[\"analysis_config\"][\"sample_output_dir\"]\n",
    "            \n",
    "            if os.path.exists(sample_output_dir):\n",
    "                # 존재하면 삭제\n",
    "                shutil.rmtree(sample_output_dir)\n",
    "                print(f\"디렉토리 삭제됨: {sample_output_dir}\")\n",
    "                        \n",
    "        # langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "        workflow = StateGraph(self.state)\n",
    "\n",
    "        # Todo 를 작성합니다.\n",
    "        workflow.add_node(\"agent\", agent)  # 에이전트 노드를 추가합니다.\n",
    "        workflow.add_node(\"sample_video_frames\", sample_video_frames)  # 비디오를 샘플링하여 프레임(이미지)변환 노드를 추가합니다.\n",
    "        workflow.add_node(\"video_description\", video_description)  # 비디오 영상 설명 노드를 추가합니다.\n",
    "        workflow.add_node(\"cleanup\", cleanup)  # 정리 노드를 추가합니다.\n",
    "\n",
    "        workflow.add_edge(\"agent\", \"sample_video_frames\")\n",
    "        workflow.add_edge(\"sample_video_frames\", \"video_description\")\n",
    "        workflow.add_edge(\"video_description\", \"cleanup\")\n",
    "        workflow.add_edge(\"cleanup\", END)\n",
    "\n",
    "        # 시작점을 설정합니다.\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "\n",
    "        # 기록을 위한 메모리 저장소를 설정합니다.\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        # 그래프를 컴파일합니다.\n",
    "        self.app = workflow.compile(checkpointer=memory)        \n",
    "        self.config = RunnableConfig(recursion_limit=100, configurable={\"thread_id\": \"VideoAnalysis\"})\n",
    "\n",
    "    def invoke(self, **kwargs):\n",
    "        \n",
    "        inputs = self.state(\n",
    "            video_path=kwargs[\"video_path\"],\n",
    "            analysis_config=kwargs[\"analysis_config\"]\n",
    "        )\n",
    "        # app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "        for output in self.app.stream(inputs, self.config):\n",
    "            # 출력된 결과에서 키와 값을 순회합니다.\n",
    "            for key, value in output.items():\n",
    "                # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "                pprint(f\"\\nOutput from node '{key}':\")\n",
    "                pprint(\"---\")\n",
    "                # 출력 값을 예쁘게 출력합니다.\n",
    "                pprint(value, indent=2, width=80, depth=None)\n",
    "            # 각 출력 사이에 구분선을 추가합니다.\n",
    "            pprint(\"\\n---\\n\")\n",
    "    \n",
    "    def show_graph(self, ):\n",
    "        \n",
    "        from IPython.display import Image, display\n",
    "\n",
    "        try:\n",
    "            display(\n",
    "                Image(self.app.get_graph(xray=True).draw_mermaid_png())\n",
    "            )  # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. \n",
    "            # xray=True는 추가적인 세부 정보를 포함합니다.\n",
    "        except:\n",
    "            # 이 부분은 추가적인 의존성이 필요하며 선택적으로 실행됩니다.\n",
    "            pass\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e19db-56c1-451a-a82a-5f445466bbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer = video_analyzer(\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0ad70-7d83-4f0e-b2cd-2a3cfc2b1a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f35aa6-46ab-45ac-8a52-75b1e8523919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.invoke(\n",
    "    video_path=\"./video/video_sample_2.mkv\",\n",
    "    analysis_config={\n",
    "        \"sample_msec\": 3000,\n",
    "        \"resize_ratio\": 0.7,\n",
    "        \"frame_numbers_at_one\": 7,\n",
    "        \"sliding_window_size\": 7,\n",
    "        \"sample_output_dir\": \"./workspace\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae971d2f-3d46-4a62-ac51-1fd30a65d1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101ea2e-32f8-4509-b832-d27859e65da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666aacf-792b-44ac-98a1-5ab623ba25c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ace7b-a522-47ac-946b-b53ec80eab31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456ffcf-b17c-4702-9413-54f20c1bd68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376f922-4c06-4f63-83bb-d82b7eff481c",
   "metadata": {},
   "source": [
    "- cv2.CAP_PROP_FPS: 초당 프레임 수\n",
    "- cv2.CAP_PROP_FRAME_COUNT: 총 프레임 수\n",
    "- cv2.CAP_PROP_FRAME_WIDTH: 프레임 너비\n",
    "- cv2.CAP_PROP_FRAME_HEIGHT: 프레임 높이\n",
    "- cv2.CAP_PROP_POS_FRAMES: 현재 프레임 번호\n",
    "- cv2.CAP_PROP_POS_MSEC: 현재 위치(밀리초)\n",
    "- cv2.CAP_PROP_FOURCC: 비디오 코덱\n",
    "- cv2.CAP_PROP_BRIGHTNESS: 밝기\n",
    "- cv2.CAP_PROP_CONTRAST: 대비"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b8e647a79df62bf31906a725b05de775d285962ac600487339d38c51a5c07b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
