{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4eb837-ef78-4b45-9e2c-dbd81c3b763d",
   "metadata": {},
   "source": [
    "# Video analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e755fec-095b-43ce-8b2f-e46fd002dc00",
   "metadata": {},
   "source": [
    "## Setting\n",
    " - Auto Reload\n",
    " - path for utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee5ec9-5112-4eda-88ab-43a0ef1721a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7fb1d-710f-4345-92a0-b5a794742afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60864ffe-4fe9-4e08-bb49-9b4ecdb8c7db",
   "metadata": {},
   "source": [
    "## 1. Create Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705c228-f793-47e9-b2da-ade94f209fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock\n",
    "from utils.bedrock import bedrock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197919c-2fbc-4344-a259-4c34dca45a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "- os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "- os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "- os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "- os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a986c0-60fe-4e77-875c-338a138460fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "print (colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint (bedrock_info.get_list_fm_models(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e2368-e195-47e4-b981-80f7e6dc4421",
   "metadata": {},
   "source": [
    "## 2. LLM 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b094dc1-26f9-4dcc-b51a-178edadb407e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.bedrock import bedrock_model\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05a4a4-0ffc-4e29-b1db-0f28ce6374d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = bedrock_model(\n",
    "    #model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-5-Sonnet\"),\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Nova-Pro\"),\n",
    "    bedrock_client=boto3_bedrock,\n",
    "    stream=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    inference_config={\n",
    "        'maxTokens': 1024,\n",
    "        'stopSequences': [\"\\n\\nHuman\"],\n",
    "        'temperature': 0.01,\n",
    "        #'topP': ...,\n",
    "    }\n",
    "    #additional_model_request_fields={\"top_k\": 200}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bfe3f-bcdb-4b76-bcfe-df9273733e7a",
   "metadata": {},
   "source": [
    "## 3. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c3f3c-0acf-4db0-b0db-27cb903655c8",
   "metadata": {},
   "source": [
    "### 3.1 LLM caller 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e2397-0314-43f4-8bcd-004ba6f6d7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from utils.bedrock import bedrock_utils, bedrock_chain\n",
    "\n",
    "class llm_call():\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.llm=kwargs[\"llm\"]\n",
    "        self.verbose = kwargs.get(\"verbose\", False)\n",
    "        self.chain = bedrock_chain(bedrock_utils.converse_api) | bedrock_chain(bedrock_utils.outputparser)\n",
    "\n",
    "    def _message_format(self, role, message):\n",
    "\n",
    "        if role == \"user\":\n",
    "             message_format = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": dedent(message)}]\n",
    "            }\n",
    "        elif role == \"assistant\":\n",
    "            \n",
    "            message_format = {\n",
    "                \"role\": \"assistant\",\n",
    "                'content': [{'text': dedent(message)}]\n",
    "            }\n",
    "\n",
    "        return message_format\n",
    "            \n",
    "    def invoke(self, **kwargs):\n",
    "\n",
    "        system_prompts = kwargs.get(\"system_prompts\", None)\n",
    "        messages = kwargs[\"messages\"]\n",
    "        #llm_name = kwargs[\"llm_name\"]\n",
    "    \n",
    "        response = self.chain( ## pipeline의 제일 처음 func의 argument를 입력으로 한다. 여기서는 converse_api의 arg를 쓴다.\n",
    "            llm=self.llm,\n",
    "            system_prompts=system_prompts,\n",
    "            messages=messages,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        ai_message = self._message_format(role=\"assistant\", message=response[\"text\"])\n",
    "        messages.append(ai_message)\n",
    "        return response, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e61c6-db06-48cb-bb87-44c9ecd8aaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef806f87-26f5-4935-8614-26b7e34cbb82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"./video/video_sample.mp4\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    video_bytes = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba4de7-53d7-4dd4-bd56-23429ec59e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb91d5-b0b4-4fef-8299-bd69fbf4504f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_message_from_string(role, string, imgs=None, videos=None):\n",
    "\n",
    "    message = {\n",
    "        \"role\": role,\n",
    "        \"content\": []\n",
    "    }\n",
    "\n",
    "    if imgs is not None:\n",
    "        for img in imgs:\n",
    "            img_message = {\n",
    "                \"image\": {\n",
    "                    \"format\": 'png',\n",
    "                    \"source\": {\"bytes\": img}\n",
    "                }\n",
    "            }\n",
    "            message[\"content\"].append(img_message)\n",
    "    \n",
    "    if videos is not None:\n",
    "        for video in videos:\n",
    "            video_message = {\n",
    "                \"video\": {\n",
    "                    \"format\": 'mp4',\n",
    "                    \"source\": {\"bytes\": video_bytes}\n",
    "                }\n",
    "            }\n",
    "            message[\"content\"].append(video_message)\n",
    "\n",
    "    message[\"content\"].append({\"text\": dedent(string)})\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ba2db-b940-424b-bae7-75294ffc585b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_caller = llm_call(\n",
    "    llm=llm,\n",
    "    verbose=True ## To show token usage\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947aff8-99e4-4324-84b3-4c47cb0275ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompts = dedent(\n",
    "    '''\n",
    "    You are an expert in recipe videos. Describe this video following these guidelines:\n",
    "    \n",
    "    <output_format>\n",
    "    {\n",
    "       \"summary\": \"전체 시퀀스에서 발생한 상황에 대한 종합적 설명\",\n",
    "       \"key_events\": [\n",
    "           {\n",
    "               \"description\": \"이벤트 설명\",\n",
    "               \"significance\": \"이벤트의 중요도 (HIGH/MEDIUM/LOW)\"\n",
    "           }\n",
    "       ],\n",
    "       \"objects_involved\": {\n",
    "           \"people\": [\"식별된 사람 수 및 역할\"],\n",
    "           \"items\": [\"관련된 주요 물체들\"]\n",
    "       },\n",
    "       \"analysis\": {\n",
    "           \"pattern\": \"발견된 행동 패턴\",\n",
    "           \"anomalies\": [\"비정상적인 활동이나 특이사항\"],\n",
    "           \"risk_assessment\": \"잠재적 위험 평가\"\n",
    "       }\n",
    "    }\n",
    "    </output_format>\n",
    "\n",
    "    <instruction>\n",
    "    1. 영상을 시간순으로 검토하여 전체 맥락을 파악하세요.\n",
    "    3. 프레임 설명은 하나의 상황을 반영하고 있음을 인지하세요.\n",
    "    4. 영상에서 의미있는 이벤트를 식별하세요.\n",
    "    5. 이벤트의 중요도를 평가할 때는 다음을 고려하세요:\n",
    "      - 보안상의 위험성\n",
    "      - 비정상적인 행동 패턴\n",
    "      - 시설물이나 재산에 대한 위험\n",
    "    6. 전체 상황에서 패턴이나 특이사항을 분석하세요.\n",
    "    7. 설명은 간결하고 객관적으로 작성하세요.\n",
    "    8. 설명은 한글로 작성하세요.\n",
    "    </instruction>\n",
    "\n",
    "    <consideration>\n",
    "    1. 단순한 움직임이나 일상적인 활동은 중요 이벤트에서 제외하세요.\n",
    "    2. 동일한 이벤트가 반복되는 경우 패턴으로 분류하세요.\n",
    "    3. 시간대별 일반적인 활동 패턴과 비교하여 이상 징후를 판단하세요.\n",
    "    4. 보안과 프라이버시를 고려하여 개인을 특정할 수 있는 정보는 제외하세요.\n",
    "    5. 불확실한 상황에 대해서는 추측을 피하고 관찰된 사실만 보고하세요.\n",
    "    6. 잠재적 위험이 감지되면 이를 반드시 포함하세요.\n",
    "    7. 전체 맥락에서 중요하지 않은 세부사항은 생략하세요.\n",
    "    8. 움직이는 사람 설명을 바탕으로 frame에 등장하는 사람의 동일인 여부를 판단하세요.\n",
    "    </consideration>\n",
    "\n",
    "    '''\n",
    ")\n",
    "user_prompts = dedent(\n",
    "    '''\n",
    "    Describe given video in less than 200 words. 한글로 설명하세요\n",
    "\n",
    "    '''\n",
    ")\n",
    "system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts) \n",
    "\n",
    "file_path = \"./video/video_sample.mp4\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    video_bytes = file.read()\n",
    "    \n",
    "messages = []\n",
    "message = _get_message_from_string(role=\"user\", string=user_prompts, videos=[video_bytes])\n",
    "messages.append(message)\n",
    "                                \n",
    "# Call LLM\n",
    "resp, messages_updated = llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "                \n",
    "#                 if self.llm_caller.verbose:\n",
    "#                     self.tokens[\"input\"] += resp[\"token_usage\"][\"inputTokens\"]\n",
    "#                     self.tokens[\"output\"] += resp[\"token_usage\"][\"outputTokens\"]\n",
    "#                     self.tokens[\"total\"] += resp[\"token_usage\"][\"totalTokens\"]\n",
    "#                     print (f'total token usage: {self.tokens}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749e81c-15f3-4ae4-b048-a7faa5ab4061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533b9a7-2819-436e-8e61-d0220c3abbb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce6932-41e8-41dd-84b2-b7c84a2a6e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa6c5d-18ec-4c67-930f-0edd017a3591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076940b-548c-4237-9428-9ba57dfdbabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfab966-dbfd-4f95-99e6-d076144d8d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e1e968d-2b35-4006-b2ed-73f25c16c516",
   "metadata": {},
   "source": [
    "### 3.2 Timer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c634d-6166-4159-86c7-aea8813cf5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf0ca9-c1ad-430b-9b37-008d8d032fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeMeasurement:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.measurements = {}\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def measure(self, section_name):\n",
    "        if self.start_time is None:\n",
    "            raise ValueError(\"start() 메서드를 먼저 호출해야 합니다.\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - self.start_time\n",
    "        self.measurements[section_name] = elapsed_time\n",
    "        self.start_time = end_time  # 다음 구간 측정을 위해 시작 시간 재설정\n",
    "\n",
    "    def reset(self, ):\n",
    "        self.measurements = {}\n",
    "\n",
    "    def print_measurements(self):\n",
    "        for section, elapsed_time in self.measurements.items():\n",
    "            #print(f\"{section}: {elapsed_time:.5f} 초\")\n",
    "            print(colored (f\"\\nelapsed time: {section}: {elapsed_time:.5f} 초\", \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255c4f6-c9da-45fa-821d-7275d0cabce8",
   "metadata": {},
   "source": [
    "### 3.3 Video analyzer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31a24d-3698-4e09-b781-58556c143eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import shutil\n",
    "import pickle\n",
    "import botocore\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import TypedDict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.common_utils import retry\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453fae87-661d-430b-9ee8-921e32857a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    video_path: str\n",
    "    video_info: dict\n",
    "    analysis_config: dict\n",
    "    summary:str\n",
    "    target_apps: list[str]\n",
    "    ask_refo: str\n",
    "    code: str\n",
    "    code_err: str\n",
    "    img_path: str\n",
    "    img_bytes: str\n",
    "    chart_desc: str\n",
    "    prev_node: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9c7d3-b5bc-414b-a9a4-a9e084e34952",
   "metadata": {},
   "source": [
    "- **approach 1**: 각각의 프레임에 대한 desc를 생성한 후, 생성된 desc를 모아서 요약하여 상황설명 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0879f8-e7f1-4b42-819b-a4ae113feb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class video_analyzer():\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.llm=kwargs[\"llm\"]\n",
    "        self.state = GraphState\n",
    "\n",
    "        self.llm_caller = llm_call(\n",
    "            llm=self.llm,\n",
    "            verbose=True ## To show token usage\n",
    "        ) \n",
    "\n",
    "        self._graph_definition()\n",
    "        self.messages = []\n",
    "        self.img_bytes = \"\"\n",
    "        \n",
    "        self.timer = TimeMeasurement()\n",
    "        \n",
    "    def _get_price(self, tokens):\n",
    "        \n",
    "        input_price = tokens[\"input\"] * 0.003 / 1000\n",
    "        output_price = tokens[\"output\"] * 0.015 / 1000\n",
    "        total = input_price + output_price\n",
    "        \n",
    "        print (\"======= Cost Calculator =======\")\n",
    "        print (f'Token Usage, input: {tokens[\"input\"]}, Output: {tokens[\"output\"]}')\n",
    "        print (f'Price: {total} USD')\n",
    "        print (\"===============================\")\n",
    "        \n",
    "    def _get_string_from_message(self, message):\n",
    "        return message[\"content\"][0][\"text\"]\n",
    "        \n",
    "    def _get_message_from_string(self, role, string, imgs=None):\n",
    "        \n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": []\n",
    "        }\n",
    "        \n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_message = {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'png',\n",
    "                        \"source\": {\"bytes\": img}\n",
    "                    }\n",
    "                }\n",
    "                message[\"content\"].append(img_message)\n",
    "        \n",
    "        message[\"content\"].append({\"text\": dedent(string)})\n",
    "\n",
    "        return message\n",
    "    \n",
    "    def _frame_to_bytes(self, frame, format='.png'):\n",
    "        \"\"\"\n",
    "        cv2 frame을 bytes로 변환\n",
    "\n",
    "        Args:\n",
    "            frame: cv2로 읽은 이미지/프레임\n",
    "            format: 이미지 포맷 (예: '.jpg', '.png')\n",
    "\n",
    "        Returns:\n",
    "            bytes: 이미지의 바이트 데이터\n",
    "        \"\"\"\n",
    "        # imencode() 함수로 프레임을 지정된 포맷의 이미지로 인코딩\n",
    "        # 반환값: (success, encoded_image)\n",
    "        success, buffer = cv2.imencode(format, frame)\n",
    "\n",
    "        if not success:\n",
    "            raise ValueError(\"이미지 인코딩 실패\")\n",
    "\n",
    "        # numpy array를 bytes로 변환\n",
    "        return buffer.tobytes()\n",
    "\n",
    "    def _save_pickle(self, data: Any, file_path: str | Path) -> None:\n",
    "        \"\"\"\n",
    "        데이터를 pickle 파일로 저장합니다.\n",
    "\n",
    "        Args:\n",
    "            data: 저장할 데이터 (Any type)\n",
    "            file_path: 저장할 파일 경로 (확장자 .pkl 권장)\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        # 디렉토리가 없으면 생성\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            print(f\"Successfully saved to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving pickle file: {e}\")\n",
    "\n",
    "    def _load_pickle(self, file_path: str | Path) -> Any:\n",
    "        \"\"\"\n",
    "        pickle 파일을 로드합니다.\n",
    "\n",
    "        Args:\n",
    "            file_path: 로드할 파일 경로\n",
    "\n",
    "        Returns:\n",
    "            저장된 데이터 객체\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: 파일이 존재하지 않을 경우\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"No pickle file found at {file_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pickle file: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_messages(self, ):\n",
    "        return self.messages\n",
    "        \n",
    "    def _graph_definition(self, **kwargs):\n",
    "        \n",
    "        def agent(state):\n",
    "            \n",
    "            self.timer.start()\n",
    "            self.timer.reset()\n",
    "            \n",
    "            print(\"---CALL AGENT---\")\n",
    "            video_path = state[\"video_path\"]\n",
    "            analysis_config = state[\"analysis_config\"]\n",
    "            \n",
    "            self.tokens = {\"input\": 0, \"output\": 0, \"total\": 0}\n",
    "            self.pricing = {\"input\": 0, \"output\": 0}\n",
    "            \n",
    "            print (analysis_config)\n",
    " \n",
    "            return self.state(\n",
    "                video_path=video_path,\n",
    "                analysis_config=analysis_config,\n",
    "                rev_node=\"AGENT\"\n",
    "            )\n",
    "\n",
    "        def sample_video_frames(state):\n",
    "            \n",
    "            def setup_directory(dir_path):\n",
    "    \n",
    "                print (\"===setup_directory===\")\n",
    "                # 디렉토리가 존재하는지 확인\n",
    "                if os.path.exists(dir_path):\n",
    "                    # 존재하면 삭제\n",
    "                    shutil.rmtree(dir_path)\n",
    "                    print(f\"기존 디렉토리 삭제됨: {dir_path}\")\n",
    "\n",
    "                # 디렉토리 생성\n",
    "                os.makedirs(dir_path)\n",
    "                print(f\"디렉토리 생성됨: {dir_path}\")\n",
    "                print (\"=====================\")\n",
    "    \n",
    "            \n",
    "            print(\"---SAMPLE VIDEO FRAMES---\")\n",
    "            \n",
    "            video_path = state[\"video_path\"]\n",
    "            analysis_config = state[\"analysis_config\"]\n",
    "            sample_msec = state[\"analysis_config\"][\"sample_msec\"]\n",
    "            sample_output_dir = state[\"analysis_config\"][\"sample_output_dir\"]\n",
    "            \n",
    "            \n",
    "            print (\"===sample_video_frames===\")\n",
    "            \"\"\"\n",
    "            비디오에서 특정 시간 간격으로 프레임을 샘플링하는 함수\n",
    "\n",
    "            Args:\n",
    "                video_path (str): 비디오 파일 경로\n",
    "                sample_msec (int): 샘플링 간격 (밀리초)\n",
    "                sample_output_dir (Optional[str]): 프레임 저장 디렉토리. None이면 저장하지 않음\n",
    "\n",
    "            Returns:\n",
    "                Tuple[int, int]: (총 프레임 수, 샘플링된 프레임 수)\n",
    "\n",
    "            Raises:\n",
    "                FileNotFoundError: 비디오 파일이 없는 경우\n",
    "                ValueError: 샘플링 간격이 잘못된 경우\n",
    "            \"\"\"\n",
    "            # 입력값 검증\n",
    "            if not os.path.exists(video_path):\n",
    "                raise FileNotFoundError(f\"비디오 파일을 찾을 수 없습니다: {video_path}\")\n",
    "\n",
    "            if sample_msec <= 0:\n",
    "                raise ValueError(\"샘플링 간격은 0보다 커야 합니다\")\n",
    "\n",
    "            # 비디오 캡처 객체 생성\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                raise RuntimeError(\"비디오 파일을 열 수 없습니다\")\n",
    "\n",
    "            # 비디오 정보 가져오기\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "            # 샘플링 간격(프레임 단위) 계산\n",
    "            frame_interval = max(1, int(sample_msec / 1000 * fps))\n",
    "\n",
    "            # 출력 디렉토리 생성 (지정된 경우)\n",
    "            if sample_output_dir is not None:\n",
    "                setup_directory(sample_output_dir)\n",
    "\n",
    "            sampled_count = 0\n",
    "            frame_count = 0\n",
    "            sampled_frame = {\"frame\": [], \"seq\": []}\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "\n",
    "                # frame_interval마다 프레임 처리\n",
    "                if frame_count % frame_interval == 0:\n",
    "                    if sample_output_dir is not None:\n",
    "                        # 프레임 저장\n",
    "                        frame_path = os.path.join(sample_output_dir, f\"frame_{frame_count:06d}.jpg\")\n",
    "                        cv2.imwrite(frame_path, frame)\n",
    "                    sampled_count += 1\n",
    "                    sampled_frame[\"frame\"].append(frame)\n",
    "                    sampled_frame[\"seq\"].append(frame_count)\n",
    "                frame_count += 1\n",
    "\n",
    "                # 진행상황 출력 (10% 단위)\n",
    "                if frame_count % (total_frames // 10) == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"진행률: {progress:.1f}%\")\n",
    "\n",
    "            # 자원 해제\n",
    "            cap.release()\n",
    "\n",
    "            print(f\"\\n처리 완료:\")\n",
    "            print(f\"총 프레임 수: {total_frames}\")\n",
    "            print(f\"프레임 크기: {sampled_frame['frame'][0].shape[1]}X{sampled_frame['frame'][0].shape[0]}\")\n",
    "            print(f\"샘플링된 프레임 수: {sampled_count}\")\n",
    "            print(f\"샘플링 간격: {frame_interval}프레임 ({sample_msec}ms)\")\n",
    "            if sample_output_dir is not None:\n",
    "                print(f\"저장 위치: {sample_output_dir}\")\n",
    "\n",
    "            print (\"=========================\")\n",
    "            \n",
    "            sampled_frames_path = os.path.join(sample_output_dir, \"pickle\", \"sampled_frames.pickle\")\n",
    "            self._save_pickle(sampled_frame, sampled_frames_path)\n",
    "            \n",
    "            video_info = {\n",
    "                \"sampled_frames_path\": sampled_frames_path,\n",
    "                \"total_frame_cnt\": total_frames,\n",
    "                \"sampled_cnt\": sampled_count\n",
    "            }\n",
    "            \n",
    "            self.timer.measure(\"node: sample_video_frames\")\n",
    "            self.timer.print_measurements()\n",
    "            \n",
    "            return self.state(\n",
    "                video_info=video_info,\n",
    "                rev_node=\"SAMPLE_VIDEO_FRAMES\"\n",
    "            )\n",
    "        \n",
    "        def video_description(state):\n",
    "            \n",
    "            @retry(total_try_cnt=5, sleep_in_sec=10, retryable_exceptions=(botocore.exceptions.EventStreamError))\n",
    "            def frame_description(**kwargs):\n",
    "\n",
    "                sampled_frame = kwargs[\"sampled_frame\"]\n",
    "                sampled_frame_idx = kwargs[\"sampled_frame_idx\"]\n",
    "                total_frame_cnt = kwargs[\"total_frame_cnt\"]\n",
    "                prev_frame_desc = kwargs.get(\"prev_frame_desc\", \"None\"), \n",
    "                messages = kwargs[\"messages\"] \n",
    "                \n",
    "                system_prompts = dedent(\n",
    "                    '''\n",
    "                    ##ROLE##\n",
    "                    - CCTV 영상 분석 전문가\n",
    "                    \n",
    "                    ##TASK##\n",
    "                    - 주어진 CCTV 프레임 이미지를 분석하고, 해당 장면에서 발생하는 상황을 자연어로 설명합니다.\n",
    "                    - You must strictly follow ##Model Instructions## and ##Consideration##\n",
    "                    \n",
    "                    ##CONTEXT IMFORMATION##\n",
    "                    - frame: CCTV에서 캡처된 단일 프레임 이미지\n",
    "                    - frame_info:\n",
    "                        - frame_number: 전체 시퀀스에서 현재 프레임의 순서\n",
    "                        - total_frame_number: 전체 프레임 수\n",
    "                        - prev_frame_desc: 이전 프레임 설명\n",
    "                      \n",
    "                    ##RESPONSE STYLE AND FORMAT REQUIREMENT##\n",
    "                    - {\n",
    "                            \"scene_description\": \"현재 프레임에서 관찰되는 상황에 대한 객관적 설명\",\n",
    "                            \"person_description\": \"움직이는 사람의 경우, 그 사람을 인식/구분하기 위한 특징 설명\"\n",
    "                      }\n",
    "                    \n",
    "                    ##MODEL INSTRUCTION##\n",
    "                    - 주어진 프레임을 객관적으로 관찰하세요.\n",
    "                    - 이전 프레임 설명을 고려하여 현재 프레임을 관찰하세요.\n",
    "                    - 움직이는 물체나 사람들의 활동을 식별하세요.\n",
    "                    - 움직임이 없는 물체/사람에 대해서는 설명하지 마세요.\n",
    "                    - 주변 사물에 대해서는 설명하지 마세요.\n",
    "                    - 특이사항이나 중요한 변화가 있다면 이를 강조하세요.\n",
    "                    - 추측이나 주관적 해석은 최소화하고 관찰 가능한 사실만 설명하세요.\n",
    "                    - 설명은 간결하고 명확하게 작성하세요.\n",
    "                    - 설명은 한글로 작성하세요.\n",
    "                    \n",
    "                    ##CONSIDERATION##\n",
    "                    - 사람이나 물체의 위치 변화에 특히 주의를 기울이세요.\n",
    "                    - 시야가 가려지거나 불명확한 부분이 있다면 이를 명시하세요.\n",
    "                    - 여러 물체나 사람이 있는 경우, 각각을 구분하여 설명하세요.\n",
    "                    - 비정상적이거나 특이한 활동이 관찰되면 이를 강조하세요.\n",
    "                    - 조명 상태나 화질로 인한 제약사항이 있다면 이를 언급하세요.\n",
    "\n",
    "                    '''\n",
    "                )\n",
    "                user_prompts = dedent(\n",
    "                    '''\n",
    "                    This is <frame_number>{frame_number}</frame_number>\n",
    "                    This is <total_frame_number>{total_frame_number}</total_frame_number>\n",
    "                    This is <prev_frame_desc>{prev_frame_desc}</prev_frame_desc>\n",
    "                    This is the frame.\n",
    "\n",
    "                    '''\n",
    "                )\n",
    "\n",
    "                img_bytes = self._frame_to_bytes(sampled_frame)\n",
    "                system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)  \n",
    "\n",
    "                context = {\n",
    "                    \"frame_number\": sampled_frame_idx,\n",
    "                    \"total_frame_number\": total_frame_cnt,\n",
    "                    \"prev_frame_desc\": prev_frame_desc\n",
    "                }\n",
    "                user_prompts = user_prompts.format(**context)\n",
    "\n",
    "                message = self._get_message_from_string(role=\"user\", string=user_prompts, imgs=[img_bytes])\n",
    "                messages.append(message)\n",
    "                \n",
    "                 # 이미지 표시\n",
    "                rgb_frame = cv2.cvtColor(sampled_frame, cv2.COLOR_BGR2RGB)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(rgb_frame)\n",
    "                plt.axis('off')  # 축 숨기기\n",
    "                plt.show()\n",
    "                \n",
    "                # Call LLM\n",
    "                resp, messages_updated = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "                \n",
    "                if self.llm_caller.verbose:\n",
    "                    self.tokens[\"input\"] += resp[\"token_usage\"][\"inputTokens\"]\n",
    "                    self.tokens[\"output\"] += resp[\"token_usage\"][\"outputTokens\"]\n",
    "                    self.tokens[\"total\"] += resp[\"token_usage\"][\"totalTokens\"]\n",
    "                    print (f'total token usage: {self.tokens}')    \n",
    "                \n",
    "                messages = messages_updated\n",
    "                results = eval(resp[\"text\"])\n",
    "\n",
    "                return results\n",
    "            \n",
    "            print(\"---VIDEO DESCRIPTION---\")\n",
    "            sampled_frames= self._load_pickle(state[\"video_info\"][\"sampled_frames_path\"])\n",
    "            total_frame_cnt=state[\"video_info\"][\"total_frame_cnt\"]\n",
    "            messages=kwargs.get(\"messages\", [])\n",
    "\n",
    "            frame_desc = \"None\"\n",
    "            frame_descs = []\n",
    "            for sampled_frame_idx, sampled_frame in zip(sampled_frames[\"seq\"], sampled_frames[\"frame\"]):\n",
    "\n",
    "                res = frame_description(\n",
    "                    sampled_frame=sampled_frame,\n",
    "                    sampled_frame_idx=sampled_frame_idx,\n",
    "                    total_frame_cnt=total_frame_cnt,\n",
    "                    frame_desc=frame_desc,\n",
    "                    messages=[]\n",
    "                )\n",
    "                frame_desc = res[\"scene_description\"]\n",
    "                frame_descs.append([res[\"scene_description\"], res[\"person_description\"]])\n",
    "\n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                ##ROLE##\n",
    "                - 당신은 CCTV 영상의 전체 상황을 분석하는 전문가입니다.\n",
    "                - 개별 프레임 설명들을 종합하여 전체 시퀀스에서 발생한 상황을 요약하고 의미있는 이벤트를 추출하는 것이 당신의 임무입니다.\n",
    "\n",
    "                ##TASK##\n",
    "                - 시간 순서대로 정렬된 프레임별 설명을 분석하여 전체 상황을 요약하고 중요 이벤트를 추출\n",
    "\n",
    "                ##CONTEXT IMFORMATION##\n",
    "                - frame_descriptions: 각 프레임별 설명 ([상황 설명, 움직이는 사람 설명])이 담긴 텍스트형태의 리스트\n",
    "\n",
    "                ##RESPONSE STYLE AND FORMAT REQUIREMENT##\n",
    "                {\n",
    "                   \"summary\": \"전체 시퀀스에서 발생한 상황에 대한 종합적 설명\",\n",
    "                   \"key_events\": [\n",
    "                       {\n",
    "                           \"description\": \"이벤트 설명\",\n",
    "                           \"significance\": \"이벤트의 중요도 (HIGH/MEDIUM/LOW)\"\n",
    "                       }\n",
    "                   ],\n",
    "                   \"objects_involved\": {\n",
    "                       \"people\": [\"식별된 사람 수 및 역할\"],\n",
    "                       \"items\": [\"관련된 주요 물체들\"]\n",
    "                   },\n",
    "                   \"analysis\": {\n",
    "                       \"pattern\": \"발견된 행동 패턴\",\n",
    "                       \"anomalies\": [\"비정상적인 활동이나 특이사항\"],\n",
    "                       \"risk_assessment\": \"잠재적 위험 평가\"\n",
    "                   }\n",
    "                }\n",
    "                \n",
    "                ##MODEL INSTRUCTION##\n",
    "                - 개별 프레임 설명들을 시간순으로 검토하여 전체 맥락을 파악하세요.\n",
    "                - 전체적인 맥락을 고려하여 프레임 설명이 잘못되어 있다면 수정하여 결과에 반영하세요.\n",
    "                - 프레임 설명은 하나의 상황을 반영하고 있음을 인지하세요.\n",
    "                - 연속된 프레임들에서 의미있는 이벤트를 식별하세요.\n",
    "                - 이벤트의 중요도를 평가할 때는 다음을 고려하세요:\n",
    "                  - 보안상의 위험성\n",
    "                  - 비정상적인 행동 패턴\n",
    "                  - 시설물이나 재산에 대한 위험\n",
    "                - 전체 상황에서 패턴이나 특이사항을 분석하세요.\n",
    "                - 설명은 간결하고 객관적으로 작성하세요.\n",
    "                - 설명은 한글로 작성하세요.\n",
    "                - 설명에는 사람수에 대해서는 언급하지말고 사람 행동에 집중하세요.\n",
    "                </instruction>\n",
    "\n",
    "                ##CONSIDERATION##\n",
    "                - 단순한 움직임이나 일상적인 활동은 중요 이벤트에서 제외하세요.\n",
    "                - 동일한 이벤트가 반복되는 경우 패턴으로 분류하세요.\n",
    "                - 시간대별 일반적인 활동 패턴과 비교하여 이상 징후를 판단하세요.\n",
    "                - 보안과 프라이버시를 고려하여 개인을 특정할 수 있는 정보는 제외하세요.\n",
    "                - 불확실한 상황에 대해서는 추측을 피하고 관찰된 사실만 보고하세요.\n",
    "                - 잠재적 위험이 감지되면 이를 반드시 포함하세요.\n",
    "                - 전체 맥락에서 중요하지 않은 세부사항은 생략하세요.\n",
    "                - 움직이는 사람 설명을 바탕으로 frame에 등장하는 사람의 동일인 여부를 판단하세요.\n",
    "                '''\n",
    "            )\n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                This is <frame_descriptions>{frame_descriptions}</frame_descriptions>\n",
    "\n",
    "                '''\n",
    "            )\n",
    "\n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)  \n",
    "\n",
    "            context = {\n",
    "                \"frame_descriptions\": str(frame_descs)\n",
    "            }\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "\n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts)\n",
    "            messages.append(message)\n",
    "\n",
    "            resp, messages_updated = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "            \n",
    "            if self.llm_caller.verbose:\n",
    "                self.tokens[\"input\"] += resp[\"token_usage\"][\"inputTokens\"]\n",
    "                self.tokens[\"output\"] += resp[\"token_usage\"][\"outputTokens\"]\n",
    "                self.tokens[\"total\"] += resp[\"token_usage\"][\"totalTokens\"]            \n",
    "                self._get_price(self.tokens)\n",
    "                \n",
    "            messages = messages_updated\n",
    "            results = eval(resp[\"text\"])\n",
    "            \n",
    "            self.timer.measure(\"node: video_description\")\n",
    "            self.timer.print_measurements()\n",
    "            \n",
    "            return self.state(\n",
    "                summary=results,\n",
    "                rev_node=\"VIDEO_DESCRIPTION\"\n",
    "            )\n",
    "        \n",
    "        def cleanup(state):\n",
    "                        \n",
    "            print(\"---CLEAN UP---\")\n",
    "            analysis_config = state[\"analysis_config\"]\n",
    "            \n",
    "            sample_output_dir = state[\"analysis_config\"][\"sample_output_dir\"]\n",
    "            \n",
    "            if os.path.exists(sample_output_dir):\n",
    "                # 존재하면 삭제\n",
    "                shutil.rmtree(sample_output_dir)\n",
    "                print(f\"디렉토리 삭제됨: {sample_output_dir}\")\n",
    "                        \n",
    "        # langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "        workflow = StateGraph(self.state)\n",
    "\n",
    "        # Todo 를 작성합니다.\n",
    "        workflow.add_node(\"agent\", agent)  # 에이전트 노드를 추가합니다.\n",
    "        workflow.add_node(\"sample_video_frames\", sample_video_frames)  # 비디오를 샘플링하여 프레임(이미지)변환 노드를 추가합니다.\n",
    "        workflow.add_node(\"video_description\", video_description)  # 비디오 영상 설명 노드를 추가합니다.\n",
    "        workflow.add_node(\"cleanup\", cleanup)  # 정리 노드를 추가합니다.\n",
    "\n",
    "        workflow.add_edge(\"agent\", \"sample_video_frames\")\n",
    "        workflow.add_edge(\"sample_video_frames\", \"video_description\")\n",
    "        workflow.add_edge(\"video_description\", \"cleanup\")\n",
    "        workflow.add_edge(\"cleanup\", END)\n",
    "\n",
    "        # 시작점을 설정합니다.\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "\n",
    "        # 기록을 위한 메모리 저장소를 설정합니다.\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        # 그래프를 컴파일합니다.\n",
    "        self.app = workflow.compile(checkpointer=memory)        \n",
    "        self.config = RunnableConfig(recursion_limit=100, configurable={\"thread_id\": \"VideoAnalysis\"})\n",
    "\n",
    "    def invoke(self, **kwargs):\n",
    "        \n",
    "        inputs = self.state(\n",
    "            video_path=kwargs[\"video_path\"],\n",
    "            analysis_config=kwargs[\"analysis_config\"]\n",
    "        )\n",
    "        # app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "        for output in self.app.stream(inputs, self.config):\n",
    "            # 출력된 결과에서 키와 값을 순회합니다.\n",
    "            for key, value in output.items():\n",
    "                # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "                pprint(f\"\\nOutput from node '{key}':\")\n",
    "                pprint(\"---\")\n",
    "                # 출력 값을 예쁘게 출력합니다.\n",
    "                pprint(value, indent=2, width=80, depth=None)\n",
    "            # 각 출력 사이에 구분선을 추가합니다.\n",
    "            pprint(\"\\n---\\n\")\n",
    "    \n",
    "    def show_graph(self, ):\n",
    "        \n",
    "        from IPython.display import Image, display\n",
    "\n",
    "        try:\n",
    "            display(\n",
    "                Image(self.app.get_graph(xray=True).draw_mermaid_png())\n",
    "            )  # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. \n",
    "            # xray=True는 추가적인 세부 정보를 포함합니다.\n",
    "        except:\n",
    "            # 이 부분은 추가적인 의존성이 필요하며 선택적으로 실행됩니다.\n",
    "            pass\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e6ac3-d647-4a84-9d22-e2584b016403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer = video_analyzer(\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a956e-a9f5-4267-8502-5e7797caa855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2787cb-d8b9-41a9-950e-81cb7109c0b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.invoke(\n",
    "    video_path=\"./video/video_sample.mp4\",\n",
    "    analysis_config={\n",
    "        \"sample_msec\": 3000,\n",
    "        \"frame_numbers\": 3,\n",
    "        \"window_size\": 2,\n",
    "        \"sample_output_dir\": \"./workspace\"\n",
    "    }\n",
    ")\n",
    "\n",
    "#임시 Framefile 지우기 넣기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd6105-b5e6-4385-9342-2deed36e5ad0",
   "metadata": {},
   "source": [
    "- **approach 2**: n개 연속 프레임 제공 후 상황에 대한 desc를 생성, 생성된 desc를 모아서 요약하여 상황설명 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ae3e2-f17e-4a61-bf81-ff649260b604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class video_analyzer():\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.llm=kwargs[\"llm\"]\n",
    "        self.state = GraphState\n",
    "\n",
    "        self.llm_caller = llm_call(\n",
    "            llm=self.llm,\n",
    "            verbose=True ## To show token usage\n",
    "        ) \n",
    "\n",
    "        self._graph_definition()\n",
    "        self.messages = []\n",
    "        self.img_bytes = \"\"\n",
    "        \n",
    "        self.timer = TimeMeasurement()\n",
    "        \n",
    "    def _get_price(self, tokens):\n",
    "        \n",
    "        input_price = tokens[\"input\"] * 0.003 / 1000\n",
    "        output_price = tokens[\"output\"] * 0.015 / 1000\n",
    "        total = input_price + output_price\n",
    "        \n",
    "        print (\"======= Cost Calculator =======\")\n",
    "        print (f'Token Usage, input: {tokens[\"input\"]}, Output: {tokens[\"output\"]}')\n",
    "        print (f'Price: {total} USD')\n",
    "        print (\"===============================\")\n",
    "        \n",
    "    def _get_string_from_message(self, message):\n",
    "        return message[\"content\"][0][\"text\"]\n",
    "        \n",
    "    def _get_message_from_string(self, role, string, imgs=None):\n",
    "        \n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": []\n",
    "        }\n",
    "        \n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_message = {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'png',\n",
    "                        \"source\": {\"bytes\": img}\n",
    "                    }\n",
    "                }\n",
    "                message[\"content\"].append(img_message)\n",
    "        \n",
    "        message[\"content\"].append({\"text\": dedent(string)})\n",
    "\n",
    "        return message\n",
    "    \n",
    "    def _frame_to_bytes(self, frame, format='.png'):\n",
    "        \"\"\"\n",
    "        cv2 frame을 bytes로 변환\n",
    "\n",
    "        Args:\n",
    "            frame: cv2로 읽은 이미지/프레임\n",
    "            format: 이미지 포맷 (예: '.jpg', '.png')\n",
    "\n",
    "        Returns:\n",
    "            bytes: 이미지의 바이트 데이터\n",
    "        \"\"\"\n",
    "        # imencode() 함수로 프레임을 지정된 포맷의 이미지로 인코딩\n",
    "        # 반환값: (success, encoded_image)\n",
    "        success, buffer = cv2.imencode(format, frame)\n",
    "\n",
    "        if not success:\n",
    "            raise ValueError(\"이미지 인코딩 실패\")\n",
    "\n",
    "        # numpy array를 bytes로 변환\n",
    "        return buffer.tobytes()\n",
    "\n",
    "    def _save_pickle(self, data: Any, file_path: str | Path) -> None:\n",
    "        \"\"\"\n",
    "        데이터를 pickle 파일로 저장합니다.\n",
    "\n",
    "        Args:\n",
    "            data: 저장할 데이터 (Any type)\n",
    "            file_path: 저장할 파일 경로 (확장자 .pkl 권장)\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        # 디렉토리가 없으면 생성\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            print(f\"Successfully saved to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving pickle file: {e}\")\n",
    "\n",
    "    def _load_pickle(self, file_path: str | Path) -> Any:\n",
    "        \"\"\"\n",
    "        pickle 파일을 로드합니다.\n",
    "\n",
    "        Args:\n",
    "            file_path: 로드할 파일 경로\n",
    "\n",
    "        Returns:\n",
    "            저장된 데이터 객체\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: 파일이 존재하지 않을 경우\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"No pickle file found at {file_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pickle file: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_messages(self, ):\n",
    "        return self.messages\n",
    "        \n",
    "    def _graph_definition(self, **kwargs):\n",
    "        \n",
    "        def agent(state):\n",
    "            \n",
    "            self.timer.start()\n",
    "            self.timer.reset()\n",
    "            \n",
    "            print(\"---CALL AGENT---\")\n",
    "            video_path = state[\"video_path\"]\n",
    "            analysis_config = state[\"analysis_config\"]\n",
    "            \n",
    "            self.tokens = {\"input\": 0, \"output\": 0, \"total\": 0}\n",
    "            self.pricing = {\"input\": 0, \"output\": 0}\n",
    "            \n",
    "            print (analysis_config)\n",
    " \n",
    "            return self.state(\n",
    "                video_path=video_path,\n",
    "                analysis_config=analysis_config,\n",
    "                rev_node=\"AGENT\"\n",
    "            )\n",
    "\n",
    "        def sample_video_frames(state):\n",
    "        \n",
    "            def setup_directory(dir_path):\n",
    "    \n",
    "                print (\"===setup_directory===\")\n",
    "                # 디렉토리가 존재하는지 확인\n",
    "                if os.path.exists(dir_path):\n",
    "                    # 존재하면 삭제\n",
    "                    shutil.rmtree(dir_path)\n",
    "                    print(f\"기존 디렉토리 삭제됨: {dir_path}\")\n",
    "\n",
    "                # 디렉토리 생성\n",
    "                os.makedirs(dir_path)\n",
    "                print(f\"디렉토리 생성됨: {dir_path}\")\n",
    "                print (\"=====================\")\n",
    "    \n",
    "            video_path = state[\"video_path\"]\n",
    "            analysis_config = state[\"analysis_config\"]\n",
    "            sample_msec = state[\"analysis_config\"][\"sample_msec\"]\n",
    "            sample_output_dir = state[\"analysis_config\"][\"sample_output_dir\"]\n",
    "            \n",
    "            print(\"---SAMPLE VIDEO FRAMES---\")\n",
    "            \"\"\"\n",
    "            비디오에서 특정 시간 간격으로 프레임을 샘플링하는 함수\n",
    "\n",
    "            Args:\n",
    "                video_path (str): 비디오 파일 경로\n",
    "                sample_msec (int): 샘플링 간격 (밀리초)\n",
    "                sample_output_dir (Optional[str]): 프레임 저장 디렉토리. None이면 저장하지 않음\n",
    "\n",
    "            Returns:\n",
    "                Tuple[int, int]: (총 프레임 수, 샘플링된 프레임 수)\n",
    "\n",
    "            Raises:\n",
    "                FileNotFoundError: 비디오 파일이 없는 경우\n",
    "                ValueError: 샘플링 간격이 잘못된 경우\n",
    "            \"\"\"\n",
    "            # 입력값 검증\n",
    "            if not os.path.exists(video_path):\n",
    "                raise FileNotFoundError(f\"비디오 파일을 찾을 수 없습니다: {video_path}\")\n",
    "\n",
    "            if sample_msec <= 0:\n",
    "                raise ValueError(\"샘플링 간격은 0보다 커야 합니다\")\n",
    "\n",
    "            # 비디오 캡처 객체 생성\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                raise RuntimeError(\"비디오 파일을 열 수 없습니다\")\n",
    "\n",
    "            # 비디오 정보 가져오기\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "            # 샘플링 간격(프레임 단위) 계산\n",
    "            frame_interval = max(1, int(sample_msec / 1000 * fps))\n",
    "\n",
    "            # 출력 디렉토리 생성 (지정된 경우)\n",
    "            if sample_output_dir is not None:\n",
    "                setup_directory(sample_output_dir)\n",
    "\n",
    "            sampled_count = 0\n",
    "            frame_count = 0\n",
    "            sampled_frame = {\"frame\": [], \"seq\": []}\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "\n",
    "                # frame_interval마다 프레임 처리\n",
    "                if frame_count % frame_interval == 0:\n",
    "                    if sample_output_dir is not None:\n",
    "                        # 프레임 저장\n",
    "                        frame_path = os.path.join(sample_output_dir, f\"frame_{frame_count:06d}.jpg\")\n",
    "                        cv2.imwrite(frame_path, frame)\n",
    "                    sampled_count += 1\n",
    "                    sampled_frame[\"frame\"].append(frame)\n",
    "                    sampled_frame[\"seq\"].append(frame_count)\n",
    "                frame_count += 1\n",
    "\n",
    "                # 진행상황 출력 (10% 단위)\n",
    "                if frame_count % (total_frames // 10) == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"진행률: {progress:.1f}%\")\n",
    "\n",
    "            # 자원 해제\n",
    "            cap.release()\n",
    "\n",
    "            print(f\"\\n처리 완료:\")\n",
    "            print(f\"총 프레임 수: {total_frames}\")\n",
    "            print(f\"프레임 크기: {sampled_frame['frame'][0].shape[1]}X{sampled_frame['frame'][0].shape[0]}\")\n",
    "            print(f\"샘플링된 프레임 수: {sampled_count}\")\n",
    "            print(f\"샘플링 간격: {frame_interval}프레임 ({sample_msec}ms)\")\n",
    "            if sample_output_dir is not None:\n",
    "                print(f\"저장 위치: {sample_output_dir}\")\n",
    "\n",
    "            print (\"=========================\")\n",
    "            \n",
    "            sampled_frames_path = os.path.join(sample_output_dir, \"pickle\", \"sampled_frames.pickle\")\n",
    "            self._save_pickle(sampled_frame, sampled_frames_path)\n",
    "            \n",
    "            video_info = {\n",
    "                \"sampled_frames_path\": sampled_frames_path,\n",
    "                \"total_frame_cnt\": total_frames,\n",
    "                \"sampled_cnt\": sampled_count\n",
    "            }\n",
    "            \n",
    "            self.timer.measure(\"node: sample_video_frames\")\n",
    "            self.timer.print_measurements()\n",
    "            \n",
    "            return self.state(\n",
    "                video_info=video_info,\n",
    "                rev_node=\"SAMPLE_VIDEO_FRAMES\"\n",
    "            )\n",
    "        \n",
    "        def video_description(state):\n",
    "            \n",
    "            @retry(total_try_cnt=5, sleep_in_sec=10, retryable_exceptions=(botocore.exceptions.EventStreamError))\n",
    "            def frame_description(**kwargs):\n",
    "    \n",
    "                sampled_frame = kwargs[\"sampled_frame\"]\n",
    "                sampled_frame_idx = kwargs[\"sampled_frame_idx\"]\n",
    "                prev_frame_desc = kwargs.get(\"prev_frame_desc\", \"None\"), \n",
    "                messages = kwargs[\"messages\"] \n",
    "\n",
    "                system_prompts = dedent(\n",
    "                    '''\n",
    "                    당신은 CCTV 영상 분석 전문가입니다.\n",
    "                    연속된 n개의 CCTV 프레임 이미지를 분석하고 전체 상황을 자연어로 설명하는 것이 당신의 임무입니다.\n",
    "\n",
    "                    <task>\n",
    "                    연속된 CCTV 프레임들을 관찰하여 전체 시퀀스에서 발생하는 상황을 자연어로 설명\n",
    "                    </task>\n",
    "\n",
    "                    <input>\n",
    "                    1. frames: CCTV에서 캡처된 n개의 연속된 프레임 이미지들\n",
    "                    2. frame_count: 제공된 프레임의 수\n",
    "                    3. prev_frame_desc: 이전 프레임셋의 상황 설명\n",
    "                    </input>\n",
    "\n",
    "                    <output_format>\n",
    "                    {\n",
    "                       \"sequence_summary\": \"전체 시퀀스에서 관찰되는 상황에 대한 객관적 설명\",\n",
    "                       \"key_events\": [\n",
    "                           {\n",
    "                               \"frame_range\": [시작_프레임, 종료_프레임],\n",
    "                               \"event_description\": \"주요 이벤트 설명\"\n",
    "                           }\n",
    "                       ]\n",
    "                    }\n",
    "                    </output_format>\n",
    "\n",
    "                    <instruction>\n",
    "                    1. 프레임들을 연속성을 검토하고 그 속에서 물체/사람의 움직임을 파악하세요.\n",
    "                    2. 이전 상황 설명을 참고하여 상황의 연속성을 파악하세요.\n",
    "                    3. 동일 사람 여부 및 사람 수 보다는 행동 및 상황 설명에 집중하세요.\n",
    "                    3. 움직임이 없는 물체/사람에 대해서는 설명하지 마세요.\n",
    "                    4. 주변 사물에 대해서는 설명하지 마세요.\n",
    "                    5. 특이사항이나 중요한 변화가 있다면 이를 강조하세요.\n",
    "                    6. 추측이나 주관적 해석은 최소화하고 관찰 가능한 사실만 설명하세요.\n",
    "                    7. 설명은 간결하고 명확하게 작성하세요.\n",
    "                    8. 설명은 한글로 작성하세요.\n",
    "                    </instruction>\n",
    "\n",
    "                    <consideration>\n",
    "                    1. 주어진 프레임들은 시간 순서대로 나열되어 있습니다.\n",
    "                    1. 사람이나 물체의 위치 변화에 특히 주의를 기울이세요.\n",
    "                    2. 시야가 가려지거나 불명확한 부분이 있다면 이를 명시하세요.\n",
    "                    3. 사람 구분 보다는 행위에 집중해서 설명하세요.\n",
    "                    4. 비정상적이거나 특이한 활동이 관찰되면 이를 강조하세요.\n",
    "                    5. 조명 상태나 화질로 인한 제약사항이 있다면 이를 언급하세요.\n",
    "                    6. 한 사람이 시야에서 사라졌다가 다시 나타나는 경우, 동일 인물임을 파악하여 설명하세요.\n",
    "                    </consideration>\n",
    "\n",
    "                    '''\n",
    "                )\n",
    "                user_prompts = dedent(\n",
    "                    '''\n",
    "                    This is <frame_count>{frame_count}</frame_count>\n",
    "                    This is <prev_frame_desc>{prev_frame_desc}</prev_frame_desc>\n",
    "                    This is the frame.\n",
    "\n",
    "                    '''\n",
    "                )\n",
    "\n",
    "                img_bytes = []\n",
    "                for frame in sampled_frame:                    \n",
    "                    img_bytes.append(self._frame_to_bytes(frame))\n",
    "                \n",
    "                system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)  \n",
    "\n",
    "                context = {\n",
    "                    \"frame_count\": sampled_frame_idx,\n",
    "                    \"prev_frame_desc\": prev_frame_desc\n",
    "                }\n",
    "                user_prompts = user_prompts.format(**context)\n",
    "\n",
    "                message = self._get_message_from_string(role=\"user\", string=user_prompts, imgs=img_bytes)\n",
    "                messages.append(message)\n",
    "                \n",
    "                # 이미지 표시\n",
    "                num_frames = len(sampled_frames)\n",
    "                num_cols = 5 # row에 몇개의 사진을 보이게 할 것 인가\n",
    "                num_rows = (num_frames + num_cols - 1) // num_cols  # 올림 나눗셈\n",
    "\n",
    "                # 전체 figure 크기 설정\n",
    "                plt.figure(figsize=(4*num_cols, 4*num_rows))\n",
    "\n",
    "                # 각 프레임을 subplot으로 표시\n",
    "                for idx, frame in enumerate(sampled_frame):\n",
    "                    plt.subplot(num_rows, num_cols, idx + 1)\n",
    "\n",
    "                    # BGR to RGB 변환\n",
    "                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # 이미지 표시\n",
    "                    plt.imshow(rgb_frame)\n",
    "                    plt.title(f'Frame {idx+1}')\n",
    "                    plt.axis('off')\n",
    "\n",
    "                # subplot 간 간격 조정\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                ## Call LLM\n",
    "                resp, messages_updated = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "                messages = messages_updated\n",
    "                results = eval(resp[\"text\"])\n",
    "                \n",
    "                if self.llm_caller.verbose:\n",
    "                    self.tokens[\"input\"] += resp[\"token_usage\"][\"inputTokens\"]\n",
    "                    self.tokens[\"output\"] += resp[\"token_usage\"][\"outputTokens\"]\n",
    "                    self.tokens[\"total\"] += resp[\"token_usage\"][\"totalTokens\"]\n",
    "                    print (f'total token usage: {self.tokens}')    \n",
    "\n",
    "                return results\n",
    "            \n",
    "            print(\"---VIDEO DESCRIPTION---\")\n",
    "            \n",
    "            frame_batch_size = state[\"analysis_config\"][\"frame_numbers_at_one\"] \n",
    "            sliding_interval = state[\"analysis_config\"][\"sliding_window_size\"] \n",
    "            sampled_frames= self._load_pickle(state[\"video_info\"][\"sampled_frames_path\"])\n",
    "            total_frame_cnt=state[\"video_info\"][\"total_frame_cnt\"]\n",
    "            total_sample_frame_cnt = state[\"video_info\"][\"sampled_cnt\"]\n",
    "            messages=kwargs.get(\"messages\", [])\n",
    "            \n",
    "            # 제약 조건 체크\n",
    "            if frame_batch_size > total_sample_frame_cnt:\n",
    "                raise ValueError(\"Window size cannot be larger than total frames\")\n",
    "\n",
    "            # 마지막 프레임까지 커버하기 위한 조건 체크\n",
    "            remaining_frames = (total_sample_frame_cnt - frame_batch_size) % sliding_interval\n",
    "            if remaining_frames != 0:\n",
    "                print(f\"Warning: Last {remaining_frames} frames might be skipped with current parameters\")\n",
    "                print(f\"Recommended sliding_interval: {[i for i in range(1, frame_batch_size) if (total_sample_frame_cnt - frame_batch_size) % i == 0]}\")\n",
    "                #raise ValueError(\"In this settingm you could not use all frames. Read recommendations\")\n",
    "\n",
    "            # 마지막 시작점 계산\n",
    "            last_start = total_sample_frame_cnt - frame_batch_size\n",
    "\n",
    "            frame_desc = \"None\"\n",
    "            frame_descs = []\n",
    "            for i in range(0, last_start + 1, sliding_interval):\n",
    "                # frame_batch_size 개의 연속된 프레임 슬라이스 추출\n",
    "                sampled_frame_idx = sampled_frames[\"seq\"][i:i+frame_batch_size]\n",
    "                sampled_frame = sampled_frames[\"frame\"][i:i+frame_batch_size]\n",
    "\n",
    "                # 마지막 윈도우가 전체 프레임을 커버하지 못하는 경우, 마지막 윈도우 추가\n",
    "                # if i + sliding_interval > last_start and i != last_start:\n",
    "                #     sampled_frame_idx = sampled_frames[\"seq\"][last_start:last_start+frame_batch_size]\n",
    "                #     sampled_frame = sampled_frames[\"frame\"][last_start:last_start+frame_batch_size]\n",
    "                    #break\n",
    "\n",
    "                res = frame_description(\n",
    "                    sampled_frame=sampled_frame,\n",
    "                    sampled_frame_idx=sampled_frame_idx,\n",
    "                    total_frame_cnt=total_frame_cnt,\n",
    "                    frame_desc=frame_desc,\n",
    "                    messages=messages\n",
    "                )\n",
    "\n",
    "                frame_desc = res[\"sequence_summary\"]\n",
    "                frame_descs.append(res[\"sequence_summary\"])\n",
    "\n",
    "\n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                당신은 CCTV 영상의 전체 상황을 분석하는 전문가입니다.\n",
    "                개별 프레임 설명들을 종합하여 전체 시퀀스에서 발생한 상황을 요약하고 의미있는 이벤트를 추출하는 것이 당신의 임무입니다.\n",
    "\n",
    "                <task>\n",
    "                시간 순서대로 정렬된 프레임별 설명을 분석하여 전체 상황을 요약하고 중요 이벤트를 추출\n",
    "                </task>\n",
    "\n",
    "                <input>\n",
    "                - frame_descriptions: 각 프레임별 설명(상황 설명)이 담긴 텍스트형태의 리스트\n",
    "                </input>\n",
    "\n",
    "                <output_format>\n",
    "                {\n",
    "                   \"summary\": \"전체 시퀀스에서 발생한 상황에 대한 종합적 설명\",\n",
    "                   \"key_events\": [\n",
    "                       {\n",
    "                           \"description\": \"이벤트 설명\",\n",
    "                           \"significance\": \"이벤트의 중요도 (HIGH/MEDIUM/LOW)\"\n",
    "                       }\n",
    "                   ],\n",
    "                   \"objects_involved\": {\n",
    "                       \"people\": [\"식별된 사람 수 및 역할\"],\n",
    "                       \"items\": [\"관련된 주요 물체들\"]\n",
    "                   },\n",
    "                   \"analysis\": {\n",
    "                       \"pattern\": \"발견된 행동 패턴\",\n",
    "                       \"anomalies\": [\"비정상적인 활동이나 특이사항\"],\n",
    "                       \"risk_assessment\": \"잠재적 위험 평가\"\n",
    "                   }\n",
    "                }\n",
    "                </output_format>\n",
    "\n",
    "                <instruction>\n",
    "                1. 개별 프레임 설명들을 시간순으로 검토하여 전체 맥락을 파악하세요.\n",
    "                2. 전체적인 맥락을 고려하여 프레임 설명이 잘못되어 있다면 수정하여 결과에 반영하세요.\n",
    "                3. 프레임 설명은 하나의 상황을 반영하고 있음을 인지하세요.\n",
    "                4. 연속된 프레임들에서 의미있는 이벤트를 식별하세요.\n",
    "                5. 이벤트의 중요도를 평가할 때는 다음을 고려하세요:\n",
    "                  - 보안상의 위험성\n",
    "                  - 비정상적인 행동 패턴\n",
    "                  - 시설물이나 재산에 대한 위험\n",
    "                6. 전체 상황에서 패턴이나 특이사항을 분석하세요.\n",
    "                7. 설명은 간결하고 객관적으로 작성하세요.\n",
    "                8. 설명은 한글로 작성하세요.\n",
    "                </instruction>\n",
    "\n",
    "                <consideration>\n",
    "                1. 단순한 움직임이나 일상적인 활동은 중요 이벤트에서 제외하세요.\n",
    "                2. 동일한 이벤트가 반복되는 경우 패턴으로 분류하세요.\n",
    "                3. 시간대별 일반적인 활동 패턴과 비교하여 이상 징후를 판단하세요.\n",
    "                4. 보안과 프라이버시를 고려하여 개인을 특정할 수 있는 정보는 제외하세요.\n",
    "                5. 불확실한 상황에 대해서는 추측을 피하고 관찰된 사실만 보고하세요.\n",
    "                6. 잠재적 위험이 감지되면 이를 반드시 포함하세요.\n",
    "                7. 전체 맥락에서 중요하지 않은 세부사항은 생략하세요.\n",
    "                8. 움직이는 사람 설명을 바탕으로 frame에 등장하는 사람의 동일인 여부를 판단하세요.\n",
    "                </consideration>\n",
    "                '''\n",
    "            )\n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                This is <frame_descriptions>{frame_descriptions}</frame_descriptions>\n",
    "\n",
    "                '''\n",
    "            )\n",
    "\n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)  \n",
    "\n",
    "            context = {\n",
    "                \"frame_descriptions\": str(frame_descs)\n",
    "            }\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "\n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts)\n",
    "            messages.append(message)\n",
    "\n",
    "            resp, messages_updated = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "            messages = messages_updated\n",
    "            results = eval(resp[\"text\"])\n",
    "            \n",
    "            if self.llm_caller.verbose:\n",
    "                self.tokens[\"input\"] += resp[\"token_usage\"][\"inputTokens\"]\n",
    "                self.tokens[\"output\"] += resp[\"token_usage\"][\"outputTokens\"]\n",
    "                self.tokens[\"total\"] += resp[\"token_usage\"][\"totalTokens\"]            \n",
    "                self._get_price(self.tokens)\n",
    "            \n",
    "            self.timer.measure(\"node: video_description\")\n",
    "            self.timer.print_measurements()\n",
    "            \n",
    "            return self.state(\n",
    "                summary=results,\n",
    "                rev_node=\"VIDEO_DESCRIPTION\"\n",
    "            )\n",
    "        \n",
    "        def cleanup(state):\n",
    "                        \n",
    "            print(\"---CLEAN UP---\")\n",
    "            analysis_config = state[\"analysis_config\"]\n",
    "            \n",
    "            sample_output_dir = state[\"analysis_config\"][\"sample_output_dir\"]\n",
    "            \n",
    "            if os.path.exists(sample_output_dir):\n",
    "                # 존재하면 삭제\n",
    "                shutil.rmtree(sample_output_dir)\n",
    "                print(f\"디렉토리 삭제됨: {sample_output_dir}\")\n",
    "                        \n",
    "        # langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "        workflow = StateGraph(self.state)\n",
    "\n",
    "        # Todo 를 작성합니다.\n",
    "        workflow.add_node(\"agent\", agent)  # 에이전트 노드를 추가합니다.\n",
    "        workflow.add_node(\"sample_video_frames\", sample_video_frames)  # 비디오를 샘플링하여 프레임(이미지)변환 노드를 추가합니다.\n",
    "        workflow.add_node(\"video_description\", video_description)  # 비디오 영상 설명 노드를 추가합니다.\n",
    "        workflow.add_node(\"cleanup\", cleanup)  # 정리 노드를 추가합니다.\n",
    "\n",
    "        workflow.add_edge(\"agent\", \"sample_video_frames\")\n",
    "        workflow.add_edge(\"sample_video_frames\", \"video_description\")\n",
    "        workflow.add_edge(\"video_description\", \"cleanup\")\n",
    "        workflow.add_edge(\"cleanup\", END)\n",
    "\n",
    "        # 시작점을 설정합니다.\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "\n",
    "        # 기록을 위한 메모리 저장소를 설정합니다.\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        # 그래프를 컴파일합니다.\n",
    "        self.app = workflow.compile(checkpointer=memory)        \n",
    "        self.config = RunnableConfig(recursion_limit=100, configurable={\"thread_id\": \"VideoAnalysis\"})\n",
    "\n",
    "    def invoke(self, **kwargs):\n",
    "        \n",
    "        inputs = self.state(\n",
    "            video_path=kwargs[\"video_path\"],\n",
    "            analysis_config=kwargs[\"analysis_config\"]\n",
    "        )\n",
    "        # app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "        for output in self.app.stream(inputs, self.config):\n",
    "            # 출력된 결과에서 키와 값을 순회합니다.\n",
    "            for key, value in output.items():\n",
    "                # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "                pprint(f\"\\nOutput from node '{key}':\")\n",
    "                pprint(\"---\")\n",
    "                # 출력 값을 예쁘게 출력합니다.\n",
    "                pprint(value, indent=2, width=80, depth=None)\n",
    "            # 각 출력 사이에 구분선을 추가합니다.\n",
    "            pprint(\"\\n---\\n\")\n",
    "    \n",
    "    def show_graph(self, ):\n",
    "        \n",
    "        from IPython.display import Image, display\n",
    "\n",
    "        try:\n",
    "            display(\n",
    "                Image(self.app.get_graph(xray=True).draw_mermaid_png())\n",
    "            )  # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. \n",
    "            # xray=True는 추가적인 세부 정보를 포함합니다.\n",
    "        except:\n",
    "            # 이 부분은 추가적인 의존성이 필요하며 선택적으로 실행됩니다.\n",
    "            pass\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e19db-56c1-451a-a82a-5f445466bbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer = video_analyzer(\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0ad70-7d83-4f0e-b2cd-2a3cfc2b1a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f35aa6-46ab-45ac-8a52-75b1e8523919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer.invoke(\n",
    "    video_path=\"./video/video_sample.mp4\",\n",
    "    analysis_config={\n",
    "        \"sample_msec\": 3000,\n",
    "        \"frame_numbers_at_one\": 3,\n",
    "        \"sliding_window_size\": 2,\n",
    "        \"sample_output_dir\": \"./workspace\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae971d2f-3d46-4a62-ac51-1fd30a65d1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101ea2e-32f8-4509-b832-d27859e65da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666aacf-792b-44ac-98a1-5ab623ba25c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ace7b-a522-47ac-946b-b53ec80eab31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456ffcf-b17c-4702-9413-54f20c1bd68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376f922-4c06-4f63-83bb-d82b7eff481c",
   "metadata": {},
   "source": [
    "- cv2.CAP_PROP_FPS: 초당 프레임 수\n",
    "- cv2.CAP_PROP_FRAME_COUNT: 총 프레임 수\n",
    "- cv2.CAP_PROP_FRAME_WIDTH: 프레임 너비\n",
    "- cv2.CAP_PROP_FRAME_HEIGHT: 프레임 높이\n",
    "- cv2.CAP_PROP_POS_FRAMES: 현재 프레임 번호\n",
    "- cv2.CAP_PROP_POS_MSEC: 현재 위치(밀리초)\n",
    "- cv2.CAP_PROP_FOURCC: 비디오 코덱\n",
    "- cv2.CAP_PROP_BRIGHTNESS: 밝기\n",
    "- cv2.CAP_PROP_CONTRAST: 대비"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b8e647a79df62bf31906a725b05de775d285962ac600487339d38c51a5c07b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
