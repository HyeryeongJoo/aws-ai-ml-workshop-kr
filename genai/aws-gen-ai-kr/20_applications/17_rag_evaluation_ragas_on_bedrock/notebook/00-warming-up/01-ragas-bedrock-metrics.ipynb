{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Metrics 이해하기\n",
    "- [필수 사항] 이 노트북을 실행하기 이전에 setup/README.md 를 참고하여 \"가상환경\" 을 먼저 설치하시고, 이 가상 환경을 커널로 설정 후에 진행 하세요.\n",
    "- 참고 : RAGAS Metrics\n",
    "    - [Context Precision](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_precision/)\n",
    "    - [Context Recall](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_recall/)\n",
    "    - [Response Relevancy](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/answer_relevance/)\n",
    "    - [Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/)\n",
    "    - [Factual Correctness](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/factual_correctness/)\n",
    "    - [SQL](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/sql/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RAGAS 래핑 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datasets import Dataset\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from ragas import evaluate\n",
    "\n",
    "# Bedrock 클라이언트 설정\n",
    "bedrock_client = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-5-haiku-20241022-v1:0\", \n",
    "    client=bedrock_client,\n",
    "))\n",
    "\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "# from ragas.embeddings import LangchainEmbeddingWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# Bedrock Embeddings 설정\n",
    "evaluator_embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"  # 또는 다른 임베딩 모델\n",
    ")\n",
    "\n",
    "# RAGAS Wrapper로 감싸기\n",
    "embeddings_wrapper = LangchainEmbeddingsWrapper(evaluator_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Context Precision\n",
    "- Context Precision은 retrieved_contexts에서 관련된 청크들의 비율을 측정하는 지표입\n",
    "- 문서나 텍스트를 검색할 때, 시스템은 여러 개의 작은 조각(청크)들을 가져옵니다. 이때 우리가 원하는 정보와 관련 있는 조각들을 얼마나 정확하게 가져왔는지를 측정하는 것이 Context Precision입니다.\n",
    "    - 예를 들어 설명하면: 당신이 \"한국의 전통 음식\"에 대해 검색했다고 가정해봅시다. \n",
    "    - 시스템이 10개의 텍스트 조각을 가져왔는데, 그 중 7개는 실제로 한국 음식에 대한 내용이고,  3개는 다른 나라의 음식이나 관련 없는 내용이라면 이 경우의 Context Precision은 7/10 = 0.7 또는 70%가 됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Precision without reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
    "\n",
    "context_precision = LLMContextPrecisionWithoutReference(llm=evaluator_llm)\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    response=\"The Eiffel Tower is located in Paris.\",\n",
    "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"], \n",
    ")\n",
    "\n",
    "\n",
    "await context_precision.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Precision with reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "\n",
    "context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    reference=\"The Eiffel Tower is located in Paris.\",\n",
    "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"], \n",
    ")\n",
    "\n",
    "await context_precision.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non LLM Based Context Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import NonLLMContextPrecisionWithReference\n",
    "\n",
    "context_precision = NonLLMContextPrecisionWithReference()\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"], \n",
    "    reference_contexts=[\"Paris is the capital of France.\", \"The Eiffel Tower is one of the most famous landmarks in Paris.\"]\n",
    ")\n",
    "\n",
    "await context_precision.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Context Recall\n",
    "Context Recall은 관련된 문서(또는 정보)들이 얼마나 성공적으로 검색되었는지를 측정합니다. 이는 중요한 결과들을 놓치지 않는 것에 초점을 맞춥니다. 높은 recall은 관련된 문서들이 더 적게 누락되었다는 것을 의미합니다. 간단히 말해서, recall은 중요한 정보를 놓치지 않는 것에 관한 것입니다. 이는 누락된 것이 없는지를 측정하는 것이기 때문에, context recall을 계산하기 위해서는 항상 비교할 수 있는 기준이 필요합니다.\n",
    "\n",
    "더 쉽게 설명하면:\n",
    "- Precision이 \"가져온 정보가 얼마나 정확한가\"를 측정한다면\n",
    "- Recall은 \"필요한 정보를 얼마나 빠짐없이 가져왔는가\"를 측정합니다\n",
    "\n",
    "예를 들어:\n",
    "- 도서관에 한국 요리에 대한 책이 총 100권이 있다고 가정했을 때\n",
    "- 검색 시스템이 80권을 찾아냈다면\n",
    "- Recall은 80/100 = 0.8 또는 80%가 됩니다\n",
    "- 즉, 필요한 정보의 80%를 찾아냈다는 의미입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Based Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import LLMContextRecall\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    response=\"The Eiffel Tower is located in Paris.\",\n",
    "    reference=\"The Eiffel Tower is located in Paris.\",\n",
    "    retrieved_contexts=[\"Paris is the capital of France.\"], \n",
    ")\n",
    "\n",
    "context_recall = LLMContextRecall(llm=evaluator_llm)\n",
    "await context_recall.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Response Relevancy\n",
    "`ResponseRelevancy` 지표는 응답이 사용자 입력과 얼마나 관련이 있는지를 측정합니다. 높은 점수는 사용자 입력과의 더 나은 일치를 나타내며, 응답이 불완전하거나 불필요한 정보를 포함할 경우 낮은 점수가 주어집니다.\n",
    "\n",
    "이 지표는 `user_input`과 `response`를 사용하여 다음과 같이 계산됩니다:\n",
    "1. 응답을 기반으로 인공적인 질문들(기본값 3개)을 생성합니다. 이 질문들은 응답의 내용을 반영하도록 설계됩니다.\n",
    "2. 사용자 입력의 임베딩(Eo)과 각 생성된 질문의 임베딩(Egi) 사이의 코사인 유사도를 계산합니다.\n",
    "3. 이러한 코사인 유사도 점수들의 평균을 계산하여 **응답 관련성(Answer Relevancy)**을 구합니다.\n",
    "\n",
    "더 쉽게 설명하면:\n",
    "- 이 지표는 \"시스템의 응답이 사용자의 질문이나 요청에 얼마나 잘 부합하는가\"를 측정합니다\n",
    "- 예를 들어, 사용자가 \"김치 만드는 방법\"을 물었는데:\n",
    "  - 김치 만드는 과정을 자세히 설명하면 높은 점수\n",
    "  - 다른 한식 요리법을 설명하거나 불필요한 정보를 포함하면 낮은 점수를 받게 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9486586568274785"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import SingleTurnSample \n",
    "from ragas.metrics import ResponseRelevancy\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "        user_input=\"When was the first super bowl?\",\n",
    "        response=\"The first superbowl was held on Jan 15, 1967\",\n",
    "        retrieved_contexts=[\n",
    "            \"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Faithfulness\n",
    "**Faithfulness**(충실도) 지표는 `응답`이 `검색된 컨텍스트`와 얼마나 사실적으로 일치하는지를 측정합니다. 0에서 1 사이의 값을 가지며, 높은 점수일수록 더 나은 일관성을 나타냅니다.\n",
    "\n",
    "응답이 **충실하다(faithful)**고 판단되는 것은 응답의 모든 주장이 검색된 컨텍스트로부터 뒷받침될 수 있을 때입니다.\n",
    "\n",
    "계산 방법:\n",
    "1. 응답에 있는 모든 주장들을 식별합니다\n",
    "2. 각 주장이 검색된 컨텍스트로부터 추론될 수 있는지 확인합니다\n",
    "3. 공식을 사용하여 충실도 점수를 계산합니다\n",
    "\n",
    "더 쉽게 설명하면:\n",
    "- 이는 \"LLM이 제공한 응답이 Retrieved Context의 내용과 얼마나 일치하는가\"를 측정합니다\n",
    "- 예를 들어:\n",
    "  - Retrieved Context에 \"김치는 배추, 고춧가루, 마늘이 들어갑니다\"라고 되어있는데\n",
    "  - LLM 이 \"김치의 주재료는 배추, 고춧가루, 마늘입니다\"라고 응답하면 높은 충실도 점수\n",
    "  - 반면 \"김치에는 당근이 들어갑니다\"라고 응답하면 낮은 충실도 점수를 받게 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"The first Super Bowl was held on January 15, 1967.\", \"reason\": \"The context directly states that the First AFL\\u2013NFL World Championship Game was played on January 15, 1967, which matches the statement's details precisely.\", \"verdict\": 1}, {\"statement\": \"The first Super Bowl took place in the year 1967 during the month of January.\", \"reason\": \"The context directly mentions the game was played on January 15, 1967, which aligns perfectly with the statement's details about the year and month.\", \"verdict\": 1}]}. Got: 1 validation error for StringIO\ntext\n  Field required [type=missing, input_value={'statements': [{'stateme...month.\", 'verdict': 1}]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/langchain_core/output_parsers/pydantic.py:28\u001b[0m, in \u001b[0;36mPydanticOutputParser._parse_obj\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object, pydantic\u001b[38;5;241m.\u001b[39mBaseModel):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object, pydantic\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mBaseModel):\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/pydantic/main.py:627\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    626\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StringIO\ntext\n  Field required [type=missing, input_value={'statements': [{'stateme...month.\", 'verdict': 1}]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m sample \u001b[38;5;241m=\u001b[39m SingleTurnSample(\n\u001b[1;32m      5\u001b[0m         user_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen was the first super bowl?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first superbowl was held on Jan 15, 1967\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m         ]\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m scorer \u001b[38;5;241m=\u001b[39m Faithfulness(llm\u001b[38;5;241m=\u001b[39mevaluator_llm)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m scorer\u001b[38;5;241m.\u001b[39msingle_turn_ascore(sample)\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/metrics/base.py:541\u001b[0m, in \u001b[0;36mSingleTurnMetric.single_turn_ascore\u001b[0;34m(self, sample, callbacks, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    540\u001b[0m         rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/metrics/base.py:534\u001b[0m, in \u001b[0;36mSingleTurnMetric.single_turn_ascore\u001b[0;34m(self, sample, callbacks, timeout)\u001b[0m\n\u001b[1;32m    527\u001b[0m rm, group_cm \u001b[38;5;241m=\u001b[39m new_group(\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    529\u001b[0m     inputs\u001b[38;5;241m=\u001b[39msample\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m    530\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    531\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: ChainType\u001b[38;5;241m.\u001b[39mMETRIC},\n\u001b[1;32m    532\u001b[0m )\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_turn_ascore(sample\u001b[38;5;241m=\u001b[39msample, callbacks\u001b[38;5;241m=\u001b[39mgroup_cm),\n\u001b[1;32m    536\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    537\u001b[0m     )\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/asyncio/tasks.py:408\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    405\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    411\u001b[0m     fut \u001b[38;5;241m=\u001b[39m ensure_future(fut, loop\u001b[38;5;241m=\u001b[39mloop)\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/metrics/_faithfulness.py:200\u001b[0m, in \u001b[0;36mFaithfulness._single_turn_ascore\u001b[0;34m(self, sample, callbacks)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_single_turn_ascore\u001b[39m(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m, sample: SingleTurnSample, callbacks: Callbacks\n\u001b[1;32m    198\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    199\u001b[0m     row \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ascore(row, callbacks)\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/metrics/_faithfulness.py:213\u001b[0m, in \u001b[0;36mFaithfulness._ascore\u001b[0;34m(self, row, callbacks)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m statements \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m--> 213\u001b[0m verdicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_verdicts(row, statements, callbacks)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_score(verdicts)\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/metrics/_faithfulness.py:158\u001b[0m, in \u001b[0;36mFaithfulness._create_verdicts\u001b[0;34m(self, row, statements, callbacks)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm must be set to compute score\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m contexts_str: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieved_contexts\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 158\u001b[0m verdicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnli_statements_prompt\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    159\u001b[0m     data\u001b[38;5;241m=\u001b[39mNLIStatementInput(context\u001b[38;5;241m=\u001b[39mcontexts_str, statements\u001b[38;5;241m=\u001b[39mstatements),\n\u001b[1;32m    160\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm,\n\u001b[1;32m    161\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m verdicts\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/prompt/pydantic_prompt.py:127\u001b[0m, in \u001b[0;36mPydanticPrompt.generate\u001b[0;34m(self, llm, data, temperature, stop, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    124\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m callbacks \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# this is just a special case of generate_multiple\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m output_single \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_multiple(\n\u001b[1;32m    128\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    129\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    130\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    131\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    132\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    133\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    134\u001b[0m     retries_left\u001b[38;5;241m=\u001b[39mretries_left,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_single[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/prompt/pydantic_prompt.py:201\u001b[0m, in \u001b[0;36mPydanticPrompt.generate_multiple\u001b[0;34m(self, llm, data, n, temperature, stop, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    199\u001b[0m output_string \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse_output_string(\n\u001b[1;32m    202\u001b[0m         output_string\u001b[38;5;241m=\u001b[39moutput_string,\n\u001b[1;32m    203\u001b[0m         prompt_value\u001b[38;5;241m=\u001b[39mprompt_value,\n\u001b[1;32m    204\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    205\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mprompt_cb,\n\u001b[1;32m    206\u001b[0m         retries_left\u001b[38;5;241m=\u001b[39mretries_left,\n\u001b[1;32m    207\u001b[0m     )\n\u001b[1;32m    208\u001b[0m     processed_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_output(answer, data)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     output_models\u001b[38;5;241m.\u001b[39mappend(processed_output)\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/prompt/pydantic_prompt.py:409\u001b[0m, in \u001b[0;36mRagasOutputParser.parse_output_string\u001b[0;34m(self, output_string, prompt_value, llm, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries_left \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    404\u001b[0m     retry_rm, retry_cb \u001b[38;5;241m=\u001b[39m new_group(\n\u001b[1;32m    405\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfix_output_format\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    406\u001b[0m         inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_string\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_string},\n\u001b[1;32m    407\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 409\u001b[0m     fixed_output_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fix_output_format_prompt\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    410\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    411\u001b[0m         data\u001b[38;5;241m=\u001b[39mOutputStringAndPrompt(\n\u001b[1;32m    412\u001b[0m             output_string\u001b[38;5;241m=\u001b[39moutput_string,\n\u001b[1;32m    413\u001b[0m             prompt_value\u001b[38;5;241m=\u001b[39mprompt_value\u001b[38;5;241m.\u001b[39mto_string(),\n\u001b[1;32m    414\u001b[0m         ),\n\u001b[1;32m    415\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mretry_cb,\n\u001b[1;32m    416\u001b[0m         retries_left\u001b[38;5;241m=\u001b[39mretries_left \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     retry_rm\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed_output_string\u001b[39m\u001b[38;5;124m\"\u001b[39m: fixed_output_string})\n\u001b[1;32m    419\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse(fixed_output_string\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/prompt/pydantic_prompt.py:127\u001b[0m, in \u001b[0;36mPydanticPrompt.generate\u001b[0;34m(self, llm, data, temperature, stop, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    124\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m callbacks \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# this is just a special case of generate_multiple\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m output_single \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_multiple(\n\u001b[1;32m    128\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    129\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    130\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    131\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    132\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    133\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    134\u001b[0m     retries_left\u001b[38;5;241m=\u001b[39mretries_left,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_single[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/prompt/pydantic_prompt.py:201\u001b[0m, in \u001b[0;36mPydanticPrompt.generate_multiple\u001b[0;34m(self, llm, data, n, temperature, stop, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    199\u001b[0m output_string \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse_output_string(\n\u001b[1;32m    202\u001b[0m         output_string\u001b[38;5;241m=\u001b[39moutput_string,\n\u001b[1;32m    203\u001b[0m         prompt_value\u001b[38;5;241m=\u001b[39mprompt_value,\n\u001b[1;32m    204\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    205\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mprompt_cb,\n\u001b[1;32m    206\u001b[0m         retries_left\u001b[38;5;241m=\u001b[39mretries_left,\n\u001b[1;32m    207\u001b[0m     )\n\u001b[1;32m    208\u001b[0m     processed_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_output(answer, data)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     output_models\u001b[38;5;241m.\u001b[39mappend(processed_output)\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/prompt/pydantic_prompt.py:409\u001b[0m, in \u001b[0;36mRagasOutputParser.parse_output_string\u001b[0;34m(self, output_string, prompt_value, llm, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries_left \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    404\u001b[0m     retry_rm, retry_cb \u001b[38;5;241m=\u001b[39m new_group(\n\u001b[1;32m    405\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfix_output_format\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    406\u001b[0m         inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_string\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_string},\n\u001b[1;32m    407\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 409\u001b[0m     fixed_output_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fix_output_format_prompt\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    410\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    411\u001b[0m         data\u001b[38;5;241m=\u001b[39mOutputStringAndPrompt(\n\u001b[1;32m    412\u001b[0m             output_string\u001b[38;5;241m=\u001b[39moutput_string,\n\u001b[1;32m    413\u001b[0m             prompt_value\u001b[38;5;241m=\u001b[39mprompt_value\u001b[38;5;241m.\u001b[39mto_string(),\n\u001b[1;32m    414\u001b[0m         ),\n\u001b[1;32m    415\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mretry_cb,\n\u001b[1;32m    416\u001b[0m         retries_left\u001b[38;5;241m=\u001b[39mretries_left \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     retry_rm\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed_output_string\u001b[39m\u001b[38;5;124m\"\u001b[39m: fixed_output_string})\n\u001b[1;32m    419\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse(fixed_output_string\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/prompt/pydantic_prompt.py:127\u001b[0m, in \u001b[0;36mPydanticPrompt.generate\u001b[0;34m(self, llm, data, temperature, stop, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    124\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m callbacks \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# this is just a special case of generate_multiple\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m output_single \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_multiple(\n\u001b[1;32m    128\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    129\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    130\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    131\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    132\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    133\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    134\u001b[0m     retries_left\u001b[38;5;241m=\u001b[39mretries_left,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_single[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/prompt/pydantic_prompt.py:201\u001b[0m, in \u001b[0;36mPydanticPrompt.generate_multiple\u001b[0;34m(self, llm, data, n, temperature, stop, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    199\u001b[0m output_string \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse_output_string(\n\u001b[1;32m    202\u001b[0m         output_string\u001b[38;5;241m=\u001b[39moutput_string,\n\u001b[1;32m    203\u001b[0m         prompt_value\u001b[38;5;241m=\u001b[39mprompt_value,\n\u001b[1;32m    204\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    205\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mprompt_cb,\n\u001b[1;32m    206\u001b[0m         retries_left\u001b[38;5;241m=\u001b[39mretries_left,\n\u001b[1;32m    207\u001b[0m     )\n\u001b[1;32m    208\u001b[0m     processed_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_output(answer, data)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     output_models\u001b[38;5;241m.\u001b[39mappend(processed_output)\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/ragas/prompt/pydantic_prompt.py:419\u001b[0m, in \u001b[0;36mRagasOutputParser.parse_output_string\u001b[0;34m(self, output_string, prompt_value, llm, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    409\u001b[0m     fixed_output_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fix_output_format_prompt\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    410\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    411\u001b[0m         data\u001b[38;5;241m=\u001b[39mOutputStringAndPrompt(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m         retries_left\u001b[38;5;241m=\u001b[39mretries_left \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     retry_rm\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed_output_string\u001b[39m\u001b[38;5;124m\"\u001b[39m: fixed_output_string})\n\u001b[0;32m--> 419\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed_output_string\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RagasOutputParserException()\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/langchain_core/output_parsers/pydantic.py:83\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TBaseModel:\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        The parsed pydantic object.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:97\u001b[0m, in \u001b[0;36mJsonOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call to a JSON object.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m        The parsed JSON object.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/langchain_core/output_parsers/pydantic.py:68\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse_result(result)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m partial:\n",
      "File \u001b[0;32m~/.conda/envs/ragas_bedrock/lib/python3.10/site-packages/langchain_core/output_parsers/pydantic.py:36\u001b[0m, in \u001b[0;36mPydanticOutputParser._parse_obj\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (pydantic\u001b[38;5;241m.\u001b[39mValidationError, pydantic\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parser_exception(e, obj) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pydantic v1\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"The first Super Bowl was held on January 15, 1967.\", \"reason\": \"The context directly states that the First AFL\\u2013NFL World Championship Game was played on January 15, 1967, which matches the statement's details precisely.\", \"verdict\": 1}, {\"statement\": \"The first Super Bowl took place in the year 1967 during the month of January.\", \"reason\": \"The context directly mentions the game was played on January 15, 1967, which aligns perfectly with the statement's details about the year and month.\", \"verdict\": 1}]}. Got: 1 validation error for StringIO\ntext\n  Field required [type=missing, input_value={'statements': [{'stateme...month.\", 'verdict': 1}]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample \n",
    "from ragas.metrics import Faithfulness\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "        user_input=\"When was the first super bowl?\",\n",
    "        response=\"The first superbowl was held on Jan 15, 1967\",\n",
    "        retrieved_contexts=[\n",
    "            \"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"\n",
    "        ]\n",
    "    )\n",
    "scorer = Faithfulness(llm=evaluator_llm)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Factual Correctness\n",
    "`FactualCorrectness`(사실적 정확도) 지표는 생성된 `응답`의 사실적 정확성을 `참조`와 비교하고 평가합니다. 이 지표는 생성된 응답이 참조와 얼마나 일치하는지를 판단하는 데 사용됩니다. 사실적 정확도 점수는 0에서 1 사이의 값을 가지며, 높은 값일수록 더 나은 성능을 나타냅니다.\n",
    "\n",
    "응답과 참조 간의 일치도를 측정하기 위해, 이 지표는 다음과 같은 과정을 거칩니다:\n",
    "1. LLM(대규모 언어 모델)을 사용하여 응답과 참조를 각각의 주장들로 분해합니다\n",
    "2. 자연어 추론을 사용하여 응답과 참조 사이의 사실적 중복을 판단합니다\n",
    "3. 사실적 중복은 정밀도(precision), 재현율(recall), F1 점수를 사용하여 수치화됩니다\n",
    "\n",
    "더 쉽게 설명하면:\n",
    "- 이 지표는 \"LLM의 응답이 정답과 얼마나 사실적으로 일치하는가\"를 측정합니다\n",
    "- 예를 들어:\n",
    "  - 정답(참조)이 \"서울은 대한민국의 수도이며 약 1000만 명의 인구가 있습니다\"일 때\n",
    "  - LLM 응답이 \"서울은 대한민국의 수도이고 인구가 약 1000만 명입니다\"라면 높은 점수\n",
    "  - 반면 \"서울은 대한민국의 수도이고 인구가 500만 명입니다\"라면 낮은 점수를 받게 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics._factual_correctness import FactualCorrectness\n",
    "\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"The Eiffel Tower is located in Paris.\",\n",
    "    reference=\"The Eiffel Tower is located in Paris. I has a height of 1000ft.\"\n",
    ")\n",
    "\n",
    "scorer = FactualCorrectness(llm = evaluator_llm)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. SQL\n",
    "## 7.1. Execution based metrics\n",
    "이러한 지표들에서는 SQL 쿼리를 데이터베이스에서 실행한 후 그 `응답`을 예상 결과와 비교하여 평가합니다.\n",
    "\n",
    "DataCompy 점수:\n",
    "`DataCompyScore` 지표는 두 개의 pandas DataFrame을 비교하는 파이썬 라이브러리인 DataCompy를 사용합니다. 이는 두 DataFrame을 비교하고 차이점에 대한 상세한 보고서를 제공하는 간단한 인터페이스를 제공합니다. 이 지표에서는 `응답`을 데이터베이스에서 실행하고 그 결과 데이터를 예상 데이터(`reference`)와 비교합니다. 비교를 가능하게 하기 위해 `응답`과 `reference` 모두 예시와 같이 쉼표로 구분된 값(CSV) 형태여야 합니다.\n",
    "\n",
    "DataFrame은 행 또는 열 단위로 비교할 수 있습니다. 이는 `mode` 매개변수를 사용하여 설정할 수 있습니다:\n",
    "- `mode`가 `row`이면 행 단위로 비교가 수행됩니다\n",
    "- `mode`가 `column`이면 열 단위로 비교가 수행됩니다\n",
    "\n",
    "더 쉽게 설명하면:\n",
    "- 이는 \"SQL 쿼리 실행 결과가 예상했던 결과와 얼마나 일치하는지\"를 측정합니다\n",
    "- 예를 들어:\n",
    "  - 학생 성적 데이터베이스에서 \"A학점 받은 학생 목록\"을 조회하는 경우\n",
    "  - 정답 쿼리의 결과와 시스템이 생성한 쿼리의 결과를 비교하여\n",
    "  - 동일한 학생들이 나오는지, 정보가 정확한지 등을 검증합니다\n",
    "  - 이때 행 단위로 비교하면 각 학생의 전체 정보가 일치하는지를\n",
    "  - 열 단위로 비교하면 특정 항목(예: 이름, 학점 등)별로 일치하는지를 확인할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import DataCompyScore\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "data1 = \"\"\"acct_id,dollar_amt,name,float_fld,date_fld\n",
    "10000001234,123.45,George Maharis,14530.1555,2017-01-01\n",
    "10000001235,0.45,Michael Bluth,1,2017-01-01\n",
    "10000001236,1345,George Bluth,,2017-01-01\n",
    "10000001237,123456,Bob Loblaw,345.12,2017-01-01\n",
    "10000001238,1.05,Lucille Bluth,,2017-01-01\n",
    "10000001238,1.05,Loose Seal Bluth,,2017-01-01\n",
    "\"\"\"\n",
    "\n",
    "data2 = \"\"\"acct_id,dollar_amt,name,float_fld\n",
    "10000001234,123.4,George Michael Bluth,14530.155\n",
    "10000001235,0.45,Michael Bluth,\n",
    "10000001236,1345,George Bluth,1\n",
    "10000001237,123456,Robert Loblaw,345.12\n",
    "10000001238,1.05,Loose Seal Bluth,111\n",
    "\"\"\"\n",
    "sample = SingleTurnSample(response=data1, reference=data2)\n",
    "scorer = DataCompyScore()\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Non Execution based metrics\n",
    "SQL 쿼리를 데이터베이스에서 실행하는 것은 시간이 많이 걸리고 때로는 실행이 어려울 수 있습니다. 이러한 경우에는 실행하지 않고 평가하는 지표들을 사용할 수 있습니다. 이러한 지표들은 SQL 쿼리를 데이터베이스에서 실행하지 않고 직접 비교합니다.\n",
    "\n",
    "SQL 쿼리 의미적 동등성:\n",
    "`LLMSQLEquivalence`는 `응답` 쿼리와 `참조` 쿼리의 동등성을 평가하는데 사용되는 지표입니다. 이 지표는 쿼리를 비교할 때 사용할 데이터베이스 스키마도 필요하며, 이는 `reference_contexts`에 입력됩니다. 이는 이진 지표로, 1은 SQL 쿼리들이 의미적으로 동등함을, 0은 SQL 쿼리들이 의미적으로 동등하지 않음을 나타냅니다.\n",
    "\n",
    "더 쉽게 설명하면:\n",
    "- 이는 \"두 SQL 쿼리가 실제로 실행하지 않고도 같은 결과를 만들어낼 것인지\"를 평가합니다\n",
    "- 예를 들어:\n",
    "  - \"학생들의 평균 성적을 구하는\" 두 개의 다른 SQL 쿼리가 있을 때\n",
    "  - 쿼리의 작성 방식은 다르더라도\n",
    "  - 같은 결과를 얻을 수 있다면 의미적으로 동등(점수 1)\n",
    "  - 다른 결과가 나온다면 동등하지 않음(점수 0)으로 판단합니다\n",
    "- 이는 실제로 쿼리를 실행하지 않고도 빠르게 평가할 수 있는 장점이 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMSQLEquivalence\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"\"\"\n",
    "        SELECT p.product_name, SUM(oi.quantity) AS total_quantity\n",
    "        FROM order_items oi\n",
    "        JOIN products p ON oi.product_id = p.product_id\n",
    "        GROUP BY p.product_name;\n",
    "    \"\"\",\n",
    "    reference=\"\"\"\n",
    "        SELECT p.product_name, COUNT(oi.quantity) AS total_quantity\n",
    "        FROM order_items oi\n",
    "        JOIN products p ON oi.product_id = p.product_id\n",
    "        GROUP BY p.product_name;\n",
    "    \"\"\",\n",
    "    reference_contexts=[\n",
    "        \"\"\"\n",
    "        Table order_items:\n",
    "        - order_item_id: INT\n",
    "        - order_id: INT\n",
    "        - product_id: INT\n",
    "        - quantity: INT\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Table products:\n",
    "        - product_id: INT\n",
    "        - product_name: VARCHAR\n",
    "        - price: DECIMAL\n",
    "        \"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "scorer = LLMSQLEquivalence()\n",
    "scorer.llm = evaluator_llm\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas_bedrock",
   "language": "python",
   "name": "ragas_bedrock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
