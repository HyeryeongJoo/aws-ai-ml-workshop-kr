{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Preprocessing for Fine-tuning Korean ReRanker\n",
    " - **한국어 ReRanker 모델 파인튜닝 예시는 [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/tree/master?tab=readme-ov-file)을 기반으로 합니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. [Data format](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#data-format)\n",
    "- `{\"query\": str, \"pos\": List[str], \"neg\":List[str]}`\n",
    "    - `query` 및 `pos`는 **1개 이상의 문장이 필요**하며, `neg`는 복수개의 문장도 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download dataset\n",
    "- [msmarco-triplets](https://github.com/microsoft/MSMARCO-Passage-Ranking)\n",
    "    - (Question, Answer, Negative)-Triplets from MS MARCO Passages dataset, 499,184 samples\n",
    "    - 해당 데이터 셋은 영문으로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -O ./dataset/msmarco/msmarco-triplets.jsonl.gz https://huggingface.co/datasets/sentence-transformers/embedding-training-data/resolve/main/msmarco-triplets.jsonl.gz \n",
    "!gunzip ./dataset/msmarco/msmarco-triplets.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. [Hard negatives](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#hard-negatives) (optional)\n",
    "- Hard Negatives는 문장 임베딩의 품질을 개선하기 위해 널리 사용되는 방법입니다.\n",
    "- 다음 명령에 따라 Hard Negatives 생성 할 수 있습니다\n",
    "```\n",
    "!python ./src/preprocess/hn_mine.py \\\n",
    "    --model_name_or_path BAAI/bge-base-en-v1.5 \\\n",
    "    --input_file ./dataset/toy_finetune_data.jsonl \\\n",
    "    --output_file ./dataset/toy_finetune_data_minedHN.jsonl \\\n",
    "    --range_for_sampling 2-200 \\\n",
    "    --use_gpu_for_searching\n",
    "```\n",
    "\n",
    "- `input_file`: json data for finetuning. This script will retrieve top-k documents for each query, and random sample negatives from the top-k documents (not including the positive documents).\n",
    "- `output_file`: path to save JSON data with mined hard negatives for finetuning\n",
    "- `range_for_sampling`: where to sample negative. For example, 2-100 means sampling negative from top2-top200 documents. You can set larger value to reduce the difficulty of negatives (e.g., set it 60-300 to sample negatives from top50-300 passages)\n",
    "- `candidate_pool`: The pool to retrieval. The default value is None, and this script will retrieve from the combination of all neg in input_file. The format of this file is the same as pretrain data. If input a candidate_pool, this script will retrieve negatives from this file.\n",
    " - `use_gpu_for_searching`: whether use faiss-gpu to retrieve negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 4*GPUs----------\n",
      "inferencing embedding for corpus (number=80)--------------\n",
      "corpus: 80 || ['Two males are performing.', 'The family was falling apart.', 'The people are watching a funeral procession.', 'A boy is sitting outside playing in the sand.', 'It is boring and mundane.', 'Conrad was being plotted against, to be hit on the head.', 'A group of people plays volleyball.', 'Some people are playing a tune.', 'They sold their home because they were retiring and not because of the loan.', 'The man sits at the table and eats food.', 'Mother Teresa is an easy choice.', 'Person in black clothing, with white bandanna and sunglasses waits at a bus stop.', \"She's not going to court to clear her record.\", 'a cat is running', 'A man is in a city.', 'A group of women watch soap operas.', 'A girl sits beside a boy.', 'A woman is riding her bike.', 'Some women with flip-flops on, are walking along the beach', 'Two people jumped off the dock.', 'Two men watching a magic show.', 'The Commission notes that no significant alternatives were considered.', 'Two children is sleeping.', 'The Spring Creek facility is old and outdated.', 'People watching a spaceship launch.', 'The street was lined with white-painted houses.', 'The man is talking about hawaii.', 'Several chefs are sitting down and talking about food.', 'There was a reform in 1996.', 'The rule discourages people to pay their child support.', 'Critical factors for essential activities are set out.', 'The girl is standing, leaning against the archway.', 'Meanwhile, the mainland was empty of population.', 'A girl is wearing blue.', \"It's worth being able to go at a pace you prefer.\", 'People are assembled in protest.', 'George Bush told the Republicans there was no way he would let them even consider this foolish idea, against his top advisors advice.', 'The morning sunlight was shining brightly and it was warm. ', 'The seal of Missouri is perfect.', 'A person gives a speech.', 'A group of Indians are having a gathering with food and drinks', 'A man is a pilot of an airplane.', 'a dog is running', 'A woman is jogging in the park.', \"Neither the Globe or Mail had comments on the current state of Canada's road system. \", 'Financing is an issue for us in public schools.', 'The 4 women are sitting on the beach.', 'A woman is standing outside.', 'No matter how old people get they never forget. ', 'The child is wearing black.', 'The battle was over. ', 'A woman is standing on a cliff.', 'The fatal dose was not taken when the murderer thought it would be.', 'A child is reading in her bedroom.', 'An athlete is competing in the 1500 meter swimming competition.', 'Ended as soon as I received the wire.', 'no one showed up to the meeting', 'A man is skiing down a mountain.', 'A group watches a movie inside.', 'A woman sits on a chair.', 'This is definitely not an endorsement.', \"It is only staged on Winter afternoons in Palma's large bullring.\", 'Right information can empower the legal service practices and the justice system. ', 'It is calming to be assaulted.', 'A man in a vest sits in a car.', 'A girl is with three cats.', 'Steele did not keep her original story.', 'The state would prefer for you to do that.', 'A group of Indians are having a funeral', 'We ran out of firewood and had to use pine needles for the fire.', 'Someone is raising their hand.', 'I face a serious problem at eighteen years old. ', 'Person on bike', 'Kids at a pool.', 'man at picnics cut steak', 'a fisherman is trying to catch a monkey', 'It lays out critical activities but makes no provision for critical factors related to those activities.', 'the people are in a train', 'Two women are playing a guitar and drums.', 'Nobody is jumping']\n",
      "p_vecs: 80 || [[-0.003918  0.01176  -0.009186 ...  0.01648   0.02963  -0.02628 ]\n",
      " [-0.03278  -0.02037   0.003534 ... -0.01279   0.004154  0.04303 ]\n",
      " [-0.01212   0.01588   0.0316   ...  0.02736   0.009865  0.0265  ]\n",
      " ...\n",
      " [-0.02028   0.01348   0.01312  ...  0.02898   0.0366    0.01545 ]\n",
      " [ 0.01555   0.02768  -0.004913 ...  0.008194  0.02267  -0.03223 ]\n",
      " [-0.03009   0.005642 -0.005672 ...  0.01417   0.0377    0.06104 ]]\n",
      "==\n",
      "inferencing embedding for queries (number=10)--------------\n",
      "queries: 10 || ['Five women walk along a beach wearing flip-flops.', 'A woman standing on a high cliff on one leg looking over a river.', 'Two woman are playing instruments; one a clarinet, the other a violin.', 'A girl with a blue tank top sitting watching three dogs.', 'A yellow dog running along a forest path.', 'It sets out essential activities in each phase along with critical factors related to those activities.', 'A man giving a speech in a restaurant.', 'Indians having a gathering with coats and food and drinks.', 'A woman with violet hair rides her bicycle outside.', 'A man pulls two women down a city street in a rickshaw.']\n",
      "q_vecs: 10 || [[-0.01567   -0.04654   -0.0002033 ... -0.008484   0.0179    -0.02641  ]\n",
      " [-0.00594   -0.002182   0.006577  ...  0.03062    0.013275   0.01305  ]\n",
      " [-0.002188  -0.00785   -0.00743   ...  0.042      0.02531   -0.0224   ]\n",
      " ...\n",
      " [-0.01033    0.002497  -0.004513  ...  0.009705  -0.02043   -0.005196 ]\n",
      " [ 0.02231   -0.05637   -0.02306   ...  0.003408   0.0197     0.03436  ]\n",
      " [-0.0418    -0.03635    0.00647   ... -0.04413    0.005924   0.03577  ]]\n",
      "==\n",
      "create index and search------------------\n"
     ]
    }
   ],
   "source": [
    "!python ./src/preprocess/hn_mine.py \\\n",
    "    --model_name_or_path BAAI/bge-base-en-v1.5 \\\n",
    "    --input_file ./dataset/toy_finetune_data.jsonl \\\n",
    "    --output_file ./dataset/toy_finetune_data_minedHN.jsonl \\\n",
    "    --range_for_sampling 2-200 \\\n",
    "    #--use_gpu_for_searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translatation (en -> ko)\n",
    " - Amazon Translate를 활용하여 영문을 국문으로 번역합니다.\n",
    " - **[주의] Amazon Translate를 사용할 경우 비용이 발생합니다!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import datetime\n",
    "import threading\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file = \"./dataset/msmarco/msmarco-triplets.jsonl\"\n",
    "out_file = \"./dataset/translated/msmarco/msmarco-triplets-trans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translate = boto3.client(\"translate\")\n",
    "def trans(text, target=\"ko\"):\n",
    "    try:\n",
    "        response=translate.translate_text(\n",
    "            Text=text,\n",
    "            SourceLanguageCode=\"Auto\",\n",
    "            TargetLanguageCode=target\n",
    "        )\n",
    "\n",
    "        text_translate = response[\"TranslatedText\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        text_translate = \"err\"\n",
    "\n",
    "    return text_translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TIP] 처리속도 향상을 위해 1/ `Multi-thread`, 2/ `Multi-processing with multi-thread` 두 가지 옵션을 제공합니다.\n",
    "- 둘 중 하나 선택하면 됩니다. (2번 방식이 더 빠릅니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Multi-thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(processes=10)\n",
    "write_freq = 10\n",
    "trans_data_chuck = []\n",
    "start = time.time()\n",
    "\n",
    "# error callback function\n",
    "def custom_error_callback(error):\n",
    "    print(f'Got error: {error}')\n",
    "\n",
    "for idx, line in enumerate(open(input_file)):\n",
    "\n",
    "    line = json.loads(line.strip())\n",
    "    query, pos, neg = line[\"query\"], line[\"pos\"], line[\"neg\"]\n",
    "\n",
    "    query_ko = trans(query)\n",
    "\n",
    "    task_pos = []\n",
    "    for pos_ in pos:\n",
    "        trans_pos= partial(trans, text=pos_)\n",
    "        task_pos.append(pool.apply_async(trans_pos, error_callback=custom_error_callback))\n",
    "\n",
    "    task_neg = []\n",
    "    for neg_ in neg:\n",
    "        trans_neg= partial(trans, text=neg_)\n",
    "        task_neg.append(pool.apply_async(trans_neg, error_callback=custom_error_callback))\n",
    "\n",
    "    pos_ko = [task.get() for task in task_pos if task.get() != \"err\"]\n",
    "    neg_ko = [task.get() for task in task_neg if task.get() != \"err\"]\n",
    "\n",
    "    trans_data = {}\n",
    "    trans_data[\"query\"], trans_data[\"pos\"], trans_data[\"neg\"] = query_ko, pos_ko, neg_ko\n",
    "\n",
    "    trans_data_chuck.append(trans_data)\n",
    "\n",
    "    if len(trans_data_chuck) == write_freq:\n",
    "        with open(f'{out_file}.jsonl', \"a+\", encoding=\"utf-8\") as f:\n",
    "            for trans_data in trans_data_chuck:\n",
    "                json.dump(trans_data, f, ensure_ascii=False) # ensure_ascii로 한글이 깨지지 않게 저장\n",
    "                f.write(\"\\n\") # json을 쓰는 것과 같지만, 여러 줄을 써주는 것이므로 \"\\n\"을 붙여준다.\n",
    "        trans_data_chuck = []\n",
    "\n",
    "    if idx % write_freq == 0:\n",
    "        elapsed = time.time() - start\n",
    "        elapsed = datetime.timedelta(seconds=elapsed)\n",
    "        print (f'{idx}/499184, Elapsed: {elapsed}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multi-processing with multi-thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# error callback function\n",
    "def custom_error_callback(error):\n",
    "    print(f'Got error: {error}')\n",
    "    \n",
    "def tranlation(input_file, out_file, start_idx, end_idx, write_freq):\n",
    "\n",
    "    pool = ThreadPool(processes=7)\n",
    "    trans_data_chuck = []\n",
    "    start = time.time()\n",
    "\n",
    "    for idx, line in enumerate(open(input_file)):\n",
    "\n",
    "        if idx >= start_idx and idx < end_idx:\n",
    "\n",
    "            line = json.loads(line.strip())\n",
    "            query, pos, neg = line[\"query\"], line[\"pos\"], line[\"neg\"]\n",
    "\n",
    "            query_ko = trans(query)\n",
    "\n",
    "            task_pos = []\n",
    "            for pos_ in pos:\n",
    "                trans_pos= partial(trans, text=pos_)\n",
    "                task_pos.append(pool.apply_async(trans_pos,))\n",
    "\n",
    "            task_neg = []\n",
    "            for neg_ in neg:\n",
    "                trans_neg= partial(trans, text=neg_)\n",
    "                task_neg.append(pool.apply_async(trans_neg,))\n",
    "\n",
    "            pos_ko = [task.get() for task in task_pos if task.get() != \"err\"]\n",
    "            neg_ko = [task.get() for task in task_neg if task.get() != \"err\"]\n",
    "\n",
    "            trans_data = {}\n",
    "            trans_data[\"query\"], trans_data[\"pos\"], trans_data[\"neg\"] = query_ko, pos_ko, neg_ko\n",
    "\n",
    "            trans_data_chuck.append(trans_data)\n",
    "\n",
    "            if len(trans_data_chuck) == write_freq:\n",
    "                with open(f'{out_file}-{start_idx}.jsonl', \"a+\", encoding=\"utf-8\") as f:\n",
    "                    for trans_data in trans_data_chuck:\n",
    "                        json.dump(trans_data, f, ensure_ascii=False) # ensure_ascii로 한글이 깨지지 않게 저장\n",
    "                        f.write(\"\\n\") # json을 쓰는 것과 같지만, 여러 줄을 써주는 것이므로 \"\\n\"을 붙여준다.\n",
    "                trans_data_chuck = []\n",
    "\n",
    "            if (idx-start_idx) % write_freq == 0:\n",
    "                elapsed = time.time() - start\n",
    "                elapsed = datetime.timedelta(seconds=elapsed)\n",
    "                print (f'{idx-start_idx}/{end_idx-start_idx}, Elapsed: {elapsed}')\n",
    "\n",
    "        if idx >= end_idx:\n",
    "            with open(f'{out_file}-{start_idx}.jsonl', \"a+\", encoding=\"utf-8\") as f:\n",
    "                for trans_data in trans_data_chuck:\n",
    "                    json.dump(trans_data, f, ensure_ascii=False) # ensure_ascii로 한글이 깨지지 않게 저장\n",
    "                    f.write(\"\\n\") # json을 쓰는 것과 같지만, 여러 줄을 써주는 것이므로 \"\\n\"을 붙여준다.\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_row = 499184\n",
    "worker_size = 6\n",
    "interval = int(total_row/worker_size)\n",
    "mp_pool = Pool(worker_size)\n",
    "\n",
    "for i in range(0, total_row, interval):\n",
    "    start_idx = i\n",
    "    end_idx = start_idx + interval\n",
    "    if end_idx > total_row:\n",
    "        end_idx = total_row\n",
    "\n",
    "    print (start_idx, end_idx)\n",
    "\n",
    "    trans_jobs= partial(\n",
    "        tranlation,\n",
    "        input_file=input_file,\n",
    "        out_file=out_file,\n",
    "        start_idx=start_idx,\n",
    "        end_idx=end_idx,\n",
    "        write_freq=50\n",
    "    )\n",
    "\n",
    "    mp_pool.apply_async(trans_jobs)\n",
    "    \n",
    "mp_pool.close()\n",
    "mp_pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check files\n",
    "Data format에 맞지 않는 샘플들은 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"./dataset/translated/msmarco/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cnt = 0\n",
    "for input_file in glob(os.path.join(dir_path, \"msmarco-triplets-trans-*.jsonl\")):\n",
    "    cnt = 0\n",
    "    for idx, line in enumerate(open(input_file)): cnt += 1\n",
    "    total_cnt += cnt\n",
    "    print (f'{input_file}: currently {cnt} lines')\n",
    "print (f'total: {total_cnt} lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in glob(os.path.join(dir_path, \"msmarco-triplets-trans-*.jsonl\")):\n",
    "    out_file = input_file.replace(\"msmarco-triplets-trans\", \"msmarco-triplets-trans-processed\")\n",
    "    \n",
    "    print (\"==========\")\n",
    "    print (f'input_file: {input_file}')\n",
    "    print (f'out_file: {out_file}')\n",
    "    \n",
    "    processed_data = []\n",
    "    for idx, line in enumerate(open(input_file)):\n",
    "        line = json.loads(line.strip())\n",
    "        query, pos, neg = line[\"query\"], line[\"pos\"], line[\"neg\"]\n",
    "\n",
    "        if len(query) > 0 and len(pos) > 0 and len(neg) > 0: processed_data.append(line)\n",
    "        else: print (f'Skip line {idx}: query: {len(query)}, pos: {len(pos)}, neg: {len(neg)}')\n",
    "\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for data in processed_data:\n",
    "            json.dump(data, f, ensure_ascii=False) # ensure_ascii로 한글이 깨지지 않게 저장\n",
    "            f.write(\"\\n\") # json을 쓰는 것과 같지만, 여러 줄을 써주는 것이므로 \"\\n\"을 붙여준다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Check each files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"./dataset/translated/msmarco/\"\n",
    "\n",
    "total_cnt = 0\n",
    "for input_file in glob(os.path.join(dir_path, \"msmarco-triplets-trans-processed-*.jsonl\")):\n",
    "    cnt = 0\n",
    "    for idx, line in enumerate(open(input_file)): cnt += 1\n",
    "    total_cnt += cnt\n",
    "    print (f'{input_file}: currently {cnt} lines')\n",
    "print (f'total: {total_cnt} lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.2 Merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = os.path.join(dir_path, \"msmarco-triplets-trans-processed-*.jsonl\")\n",
    "dst = \"./dataset/translated/merged/msmarco-triplets-trans-processed-merged.jsonl\"\n",
    "!cat $src > $dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for idx, line in enumerate(open(dst)): cnt += 1\n",
    "print (f'{dst}: {cnt} lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Store data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "print (f'bucket_name: {bucket_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path = f\"s3://{bucket_name}/fine-tune-reranker-kr/dataset\"\n",
    "local_data_Path = os.path.join(os.getcwd(), \"dataset\", \"translated\", \"merged\")\n",
    "file_name = \"msmarco-triplets-trans-processed-merged.jsonl\"\n",
    "\n",
    "print (f's3_data_path: {s3_data_path}')\n",
    "print (f'local_data_Path: {local_data_Path}')\n",
    "print (f'file_name: {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws configure set default.s3.max_concurrent_requests 100\n",
    "aws configure set default.s3.max_queue_size 10000\n",
    "aws configure set default.s3.multipart_threshold 1GB\n",
    "aws configure set default.s3.multipart_chunksize 64MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync $local_data_Path $s3_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data back-up (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path = f\"s3://{bucket_name}/reranker-dataset-ko/\"\n",
    "local_data_Path = os.path.join(os.getcwd(), \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync $local_data_Path $s3_data_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b8e647a79df62bf31906a725b05de775d285962ac600487339d38c51a5c07b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
