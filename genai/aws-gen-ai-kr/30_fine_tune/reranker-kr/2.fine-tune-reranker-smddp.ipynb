{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune for Korean ReRanker based on SMDDP\n",
    " - **한국어 ReRanker 모델 파인튜닝 예시는 [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/tree/master?tab=readme-ov-file)을 기반으로 합니다.**\n",
    " - Fine-tuning은 SageMaker 기반 Distributed Learning으로 진행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/SageMaker/.xdg/config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket_name: sagemaker-us-east-1-419974056037\n"
     ]
    }
   ],
   "source": [
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "print (f'bucket_name: {bucket_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [확인] `1.data-preprocessing.ipynb`에서 데이터를 저장한 경로를 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_data_path: s3://sagemaker-us-east-1-419974056037/fine-tune-reranker-kr/dataset/\n",
      "local_data_Path: /home/ec2-user/SageMaker/fine-tune-reranker-kr/dataset/translated/merged\n",
      "file_name: msmarco-triplets-trans-processed-merged.jsonl\n"
     ]
    }
   ],
   "source": [
    "s3_data_path = f\"s3://{bucket_name}/fine-tune-reranker-kr/dataset/\" \n",
    "local_data_Path = os.path.join(os.getcwd(), \"dataset\", \"translated\", \"merged\")\n",
    "file_name = \"msmarco-triplets-trans-processed-merged.jsonl\"\n",
    "\n",
    "print (f's3_data_path: {s3_data_path}')\n",
    "print (f'local_data_Path: {local_data_Path}')\n",
    "print (f'file_name: {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Training-job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 params for training job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = False\n",
    "\n",
    "channel = \"train\"\n",
    "fast_file = lambda x: TrainingInput(x, input_mode=\"FastFile\")\n",
    "\n",
    "if local_mode:\n",
    "    \n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    instance_type = \"local_gpu\"\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "            \n",
    "    data_channel = {\n",
    "        #channel: f'file:///home/ec2-user/SageMaker/fine-tune-reranker-kr/dataset/translated/merged/msmarco-triplets-trans-processed-merged-sample.jsonl',\n",
    "        channel: f'file://{os.path.join(local_data_Path, file_name)}',\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    instance_type = \"ml.p3.8xlarge\"# \"ml.p3.8xlarge\", \"ml.g5.12xlarge\", \"ml.p3dn.24xlarge\"\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "\n",
    "    data_channel = {\n",
    "        channel: os.path.join(s3_data_path, file_name),\n",
    "    }\n",
    "\n",
    "role = get_execution_role().rsplit('/', 1)[-1]\n",
    "\n",
    "instance_count = 1\n",
    "\n",
    "spot_training = False\n",
    "if spot_training:\n",
    "    max_wait = 1*60*60\n",
    "    max_run = 1*60*60\n",
    "    \n",
    "else:\n",
    "    max_wait = None\n",
    "    max_run = 1*60*60\n",
    "    \n",
    "\n",
    "use_train_warm_pool = True ## training image 다운받지 않음, 속도 빨라진다\n",
    "if use_train_warm_pool: keep_alive_seconds = 3600 ## 최대 1시간 동안!!, service quota에서 warmpool을 위한 request 필요\n",
    "else: keep_alive_seconds = None\n",
    "if spot_training:\n",
    "    use_train_warm_pool = False # warmpool은 spot instance 사용시 활용 할 수 없음\n",
    "    keep_alive_seconds = None\n",
    "\n",
    "prefix = \"fine-tune-reranker-kr\"\n",
    "job_name = \"-\".join([prefix, \"training\"])\n",
    "\n",
    "output_path = os.path.join(\n",
    "    \"s3://{}\".format(bucket_name),\n",
    "    prefix,\n",
    "    \"training\",\n",
    "    \"model-output\"\n",
    ")\n",
    "\n",
    "code_location = os.path.join(\n",
    "    \"s3://{}\".format(bucket_name),\n",
    "    prefix,\n",
    "    \"training\",\n",
    "    \"backup-codes\"\n",
    ")\n",
    "\n",
    "s3_chkpt_path = os.path.join(\n",
    "    \"s3://{}\".format(bucket_name),\n",
    "    prefix,\n",
    "    \"training\",\n",
    "    \"checkpoints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role Name: AmazonSageMaker-ExecutionRole-20221206T163436\n",
      "job_name: fine-tune-reranker-kr-training\n",
      "instance_type: ml.p3.8xlarge\n",
      "instance_count: 1\n",
      "sagemaker_session: <sagemaker.session.Session object at 0x7fa1d52a4f70>\n",
      "spot_training: False\n",
      "data_channel: {'train': 's3://sagemaker-us-east-1-419974056037/fine-tune-reranker-kr/dataset/msmarco-triplets-trans-processed-merged.jsonl'}\n",
      "output_path: s3://sagemaker-us-east-1-419974056037/fine-tune-reranker-kr/training/model-output\n",
      "code_location: s3://sagemaker-us-east-1-419974056037/fine-tune-reranker-kr/training/backup-codes\n",
      "use_train_warm_pool: True/3600\n",
      "s3_chkpt_path: s3://sagemaker-us-east-1-419974056037/fine-tune-reranker-kr/training/checkpoints/g3_b1_gas32\n"
     ]
    }
   ],
   "source": [
    "print (f\"SageMaker Execution Role Name: {role}\")\n",
    "print (f\"job_name: {job_name}\")\n",
    "print (f'instance_type: {instance_type}')\n",
    "print (f'instance_count: {instance_count}')\n",
    "print (f'sagemaker_session: {sagemaker_session}')\n",
    "print (f'spot_training: {spot_training}')\n",
    "print (f'data_channel: {data_channel}')\n",
    "print (f'output_path: {output_path}')\n",
    "print (f'code_location: {code_location}')\n",
    "print (f'use_train_warm_pool: {use_train_warm_pool}/{keep_alive_seconds}')\n",
    "print (f's3_chkpt_path: {s3_chkpt_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define training job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"output_dir\": \"/opt/ml/model\",\n",
    "    \"model_name_or_path\": \"BAAI/bge-reranker-large\",\n",
    "    #\"train_data\": os.path.join(f'/opt/ml/input/data/train/msmarco-triplets-trans-processed-merged-sample.jsonl'),\n",
    "    \"train_data\": os.path.join(f'/opt/ml/input/data/{channel}/{file_name}'),\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"fp16\": True,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 32,\n",
    "    \"dataloader_drop_last\": True,\n",
    "    \"train_group_size\": 3,\n",
    "    \"max_len\": 512,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"logging_steps\": 30,\n",
    "    #\"save_strategy\": \"no\"\n",
    "    \"save_steps\": 1000,\n",
    "    \"save_total_limit\": 1,\n",
    "}\n",
    "\n",
    "# enable torchrun\n",
    "distribution = {\"torch_distributed\": {\"enabled\": True}} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [SageMaker built-in images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run.py',\n",
    "    source_dir='./src/fine-tune/',\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    volume_size=500,\n",
    "    role=role,\n",
    "    job_name=job_name,\n",
    "    transformers_version='4.28.1',\n",
    "    pytorch_version='2.0.0',\n",
    "    py_version=\"py310\",\n",
    "    hyperparameters = hyperparameters,\n",
    "    distribution=distribution,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    keep_alive_period_in_seconds=keep_alive_seconds,\n",
    "    output_path=output_path,\n",
    "    code_location=code_location,\n",
    "    #input_mode='FastFile',\n",
    "    checkpoint_s3_uri=s3_chkpt_path if instance_type not in ['local', 'local_gpu'] else None,\n",
    "    checkpoint_local_path='/opt/checkpoints' if instance_type not in ['local', 'local_gpu'] else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Start Training job\n",
    "S3에서 훈련 인스턴스로 복사될 데이터를 지정한 후 SageMaker 훈련 job을 시작합니다. 모델 크기, 데이터 세트 크기에 따라서 몇십 분에서 몇 시간까지 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-12-21-07-22-38-356\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(\n",
    "    inputs=data_channel,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 View job information and logs\n",
    "훈련 로그는 CloudWatch Logs를 통해서 확인할 수 있습니다. 만약 다른 코드 셀을 실행하고 싶다면 이 코드 셀의 실행을 중단하셔도 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b> [Fine-tuning] Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/huggingface-pytorch-training-2023-12-21-07-22-38-356\">Training Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b> [Fine-tuning] Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=huggingface-pytorch-training-2023-12-21-07-22-38-356;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_console_link(region, train_job_name, train_task='[Training]'):\n",
    "    train_job_link = f'<b> {train_task} Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/jobs/{train_job_name}\">Training Job</a></b>'   \n",
    "    cloudwatch_link = f'<b> {train_task} Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={region}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={train_job_name};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>'\n",
    "    return train_job_link, cloudwatch_link  \n",
    "        \n",
    "region = boto3.Session().region_name\n",
    "train_job_name = huggingface_estimator.latest_training_job.job_name\n",
    "train_job_link, cloudwatch_link = make_console_link(region, train_job_name, '[Fine-tuning]')\n",
    "\n",
    "display(HTML(train_job_link))\n",
    "display(HTML(cloudwatch_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-21 07:22:40 Starting - Starting the training job......\n",
      "2023-12-21 07:23:33 Starting - Preparing the instances for training......\n",
      "2023-12-21 07:24:40 Downloading - Downloading input data........................\n",
      "2023-12-21 07:28:35 Downloading - Downloading the training image...\n",
      "2023-12-21 07:29:05 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-12-21 07:29:07,698 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-12-21 07:29:07,734 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-21 07:29:07,744 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-12-21 07:29:07,746 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2023-12-21 07:29:07,747 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-12-21 07:29:09,543 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Collecting FlagEmbedding (from -r requirements.txt (line 1))\n",
      "Downloading FlagEmbedding-1.1.8.tar.gz (26 kB)\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.28.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from FlagEmbedding->-r requirements.txt (line 1)) (2.0.0)\n",
      "Collecting transformers (from -r requirements.txt (line 2))\n",
      "Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 75.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from FlagEmbedding->-r requirements.txt (line 1)) (2.12.0)\n",
      "Collecting accelerate>=0.20.1 (from FlagEmbedding->-r requirements.txt (line 1))\n",
      "Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 265.7/265.7 kB 46.6 MB/s eta 0:00:00\n",
      "Collecting sentence_transformers (from FlagEmbedding->-r requirements.txt (line 1))\n",
      "Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.0/86.0 kB 18.9 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers->-r requirements.txt (line 2))\n",
      "Downloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 330.1/330.1 kB 46.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (2023.5.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (2.28.2)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers->-r requirements.txt (line 2))\n",
      "Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 85.7 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 2))\n",
      "Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 80.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->FlagEmbedding->-r requirements.txt (line 1)) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers->-r requirements.txt (line 2)) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers->-r requirements.txt (line 2)) (4.5.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers->-r requirements.txt (line 2))\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 48.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding->-r requirements.txt (line 1)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->FlagEmbedding->-r requirements.txt (line 1)) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->FlagEmbedding->-r requirements.txt (line 1)) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->FlagEmbedding->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->FlagEmbedding->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->FlagEmbedding->-r requirements.txt (line 1)) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->FlagEmbedding->-r requirements.txt (line 1)) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->FlagEmbedding->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 2)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->FlagEmbedding->-r requirements.txt (line 1)) (0.15.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->FlagEmbedding->-r requirements.txt (line 1)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->FlagEmbedding->-r requirements.txt (line 1)) (1.10.1)\n",
      "Collecting nltk (from sentence_transformers->FlagEmbedding->-r requirements.txt (line 1))\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 79.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->FlagEmbedding->-r requirements.txt (line 1)) (0.1.99)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->FlagEmbedding->-r requirements.txt (line 1)) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->FlagEmbedding->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->FlagEmbedding->-r requirements.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->FlagEmbedding->-r requirements.txt (line 1)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->FlagEmbedding->-r requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->FlagEmbedding->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->FlagEmbedding->-r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers->FlagEmbedding->-r requirements.txt (line 1)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers->FlagEmbedding->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->FlagEmbedding->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->FlagEmbedding->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->FlagEmbedding->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->FlagEmbedding->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->FlagEmbedding->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers->FlagEmbedding->-r requirements.txt (line 1)) (9.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->FlagEmbedding->-r requirements.txt (line 1)) (1.16.0)\n",
      "Building wheels for collected packages: FlagEmbedding, sentence_transformers\n",
      "Building wheel for FlagEmbedding (setup.py): started\n",
      "Building wheel for FlagEmbedding (setup.py): finished with status 'done'\n",
      "Created wheel for FlagEmbedding: filename=FlagEmbedding-1.1.8-py3-none-any.whl size=34687 sha256=6ffc4212b39865ac05b8261f5547cf0bcf04dda1f12080f79bbd7f4ff7169e85\n",
      "Stored in directory: /root/.cache/pip/wheels/30/2d/31/05df164c74bcd8573644a9df3a843a06cc50ff482affb14651\n",
      "Building wheel for sentence_transformers (setup.py): started\n",
      "Building wheel for sentence_transformers (setup.py): finished with status 'done'\n",
      "Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=e1b45acea687f083afc9b13f1518a173ba53640dc2d5fc311bc389eaf3098867\n",
      "Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built FlagEmbedding sentence_transformers\n",
      "Installing collected packages: safetensors, nltk, huggingface-hub, tokenizers, accelerate, transformers, sentence_transformers, FlagEmbedding\n",
      "Attempting uninstall: huggingface-hub\n",
      "Found existing installation: huggingface-hub 0.14.1\n",
      "Uninstalling huggingface-hub-0.14.1:\n",
      "Successfully uninstalled huggingface-hub-0.14.1\n",
      "Attempting uninstall: tokenizers\n",
      "Found existing installation: tokenizers 0.13.3\n",
      "Uninstalling tokenizers-0.13.3:\n",
      "Successfully uninstalled tokenizers-0.13.3\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.19.0\n",
      "Uninstalling accelerate-0.19.0:\n",
      "Successfully uninstalled accelerate-0.19.0\n",
      "Attempting uninstall: transformers\n",
      "Found existing installation: transformers 4.28.1\n",
      "Uninstalling transformers-4.28.1:\n",
      "Successfully uninstalled transformers-4.28.1\n",
      "Successfully installed FlagEmbedding-1.1.8 accelerate-0.25.0 huggingface-hub-0.17.3 nltk-3.8.1 safetensors-0.4.1 sentence_transformers-2.2.2 tokenizers-0.14.1 transformers-4.34.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2023-12-21 07:29:26,341 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-12-21 07:29:26,341 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-12-21 07:29:26,395 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-21 07:29:26,441 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-21 07:29:26,452 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2023-12-21 07:29:26,487 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-21 07:29:26,500 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p3.8xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.8xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataloader_drop_last\": true,\n",
      "        \"fp16\": true,\n",
      "        \"gradient_accumulation_steps\": 32,\n",
      "        \"learning_rate\": 5e-06,\n",
      "        \"logging_steps\": 30,\n",
      "        \"max_len\": 512,\n",
      "        \"model_name_or_path\": \"BAAI/bge-reranker-large\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"save_steps\": 1000,\n",
      "        \"save_total_limit\": 1,\n",
      "        \"train_data\": \"/opt/ml/input/data/train/msmarco-triplets-trans-processed-merged.jsonl\",\n",
      "        \"train_group_size\": 3,\n",
      "        \"weight_decay\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.8xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-12-21-07-22-38-356\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-419974056037/fine-tune-reranker-kr/training/backup-codes/huggingface-pytorch-training-2023-12-21-07-22-38-356/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.8xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.8xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"dataloader_drop_last\":true,\"fp16\":true,\"gradient_accumulation_steps\":32,\"learning_rate\":5e-06,\"logging_steps\":30,\"max_len\":512,\"model_name_or_path\":\"BAAI/bge-reranker-large\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":1,\"save_steps\":1000,\"save_total_limit\":1,\"train_data\":\"/opt/ml/input/data/train/msmarco-triplets-trans-processed-merged.jsonl\",\"train_group_size\":3,\"weight_decay\":0.01}\n",
      "SM_USER_ENTRY_POINT=run.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.p3.8xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=32\n",
      "SM_NUM_GPUS=4\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-419974056037/fine-tune-reranker-kr/training/backup-codes/huggingface-pytorch-training-2023-12-21-07-22-38-356/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p3.8xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.8xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataloader_drop_last\":true,\"fp16\":true,\"gradient_accumulation_steps\":32,\"learning_rate\":5e-06,\"logging_steps\":30,\"max_len\":512,\"model_name_or_path\":\"BAAI/bge-reranker-large\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":1,\"save_steps\":1000,\"save_total_limit\":1,\"train_data\":\"/opt/ml/input/data/train/msmarco-triplets-trans-processed-merged.jsonl\",\"train_group_size\":3,\"weight_decay\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-12-21-07-22-38-356\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-419974056037/fine-tune-reranker-kr/training/backup-codes/huggingface-pytorch-training-2023-12-21-07-22-38-356/source/sourcedir.tar.gz\",\"module_name\":\"run\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run.py\"}\n",
      "SM_USER_ARGS=[\"--dataloader_drop_last\",\"True\",\"--fp16\",\"True\",\"--gradient_accumulation_steps\",\"32\",\"--learning_rate\",\"5e-06\",\"--logging_steps\",\"30\",\"--max_len\",\"512\",\"--model_name_or_path\",\"BAAI/bge-reranker-large\",\"--num_train_epochs\",\"3\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_train_batch_size\",\"1\",\"--save_steps\",\"1000\",\"--save_total_limit\",\"1\",\"--train_data\",\"/opt/ml/input/data/train/msmarco-triplets-trans-processed-merged.jsonl\",\"--train_group_size\",\"3\",\"--weight_decay\",\"0.01\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_DATALOADER_DROP_LAST=true\n",
      "SM_HP_FP16=true\n",
      "SM_HP_GRADIENT_ACCUMULATION_STEPS=32\n",
      "SM_HP_LEARNING_RATE=5e-06\n",
      "SM_HP_LOGGING_STEPS=30\n",
      "SM_HP_MAX_LEN=512\n",
      "SM_HP_MODEL_NAME_OR_PATH=BAAI/bge-reranker-large\n",
      "SM_HP_NUM_TRAIN_EPOCHS=3\n",
      "SM_HP_OUTPUT_DIR=/opt/ml/model\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\n",
      "SM_HP_SAVE_STEPS=1000\n",
      "SM_HP_SAVE_TOTAL_LIMIT=1\n",
      "SM_HP_TRAIN_DATA=/opt/ml/input/data/train/msmarco-triplets-trans-processed-merged.jsonl\n",
      "SM_HP_TRAIN_GROUP_SIZE=3\n",
      "SM_HP_WEIGHT_DECAY=0.01\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 1 --nproc_per_node 4 run.py --dataloader_drop_last True --fp16 True --gradient_accumulation_steps 32 --learning_rate 5e-06 --logging_steps 30 --max_len 512 --model_name_or_path BAAI/bge-reranker-large --num_train_epochs 3 --output_dir /opt/ml/model --per_device_train_batch_size 1 --save_steps 1000 --save_total_limit 1 --train_data /opt/ml/input/data/train/msmarco-triplets-trans-processed-merged.jsonl --train_group_size 3 --weight_decay 0.01\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "12/21/2023 07:29:32 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "12/21/2023 07:29:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=True,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=32,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/opt/ml/model/runs/Dec21_07-29-32_algo-1,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=30,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/opt/ml/model,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/opt/ml/model,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=1000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n",
      "12/21/2023 07:29:32 - INFO - __main__ -   Model parameters ModelArguments(model_name_or_path='BAAI/bge-reranker-large', config_name=None, tokenizer_name=None, cache_dir=None)\n",
      "12/21/2023 07:29:32 - INFO - __main__ -   Data parameters DataArguments(train_data='/opt/ml/input/data/train/msmarco-triplets-trans-processed-merged.jsonl', train_group_size=3, max_len=512)\n",
      "12/21/2023 07:29:32 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "12/21/2023 07:29:32 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "12/21/2023 07:29:32 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "Downloading tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 443/443 [00:00<00:00, 3.27MB/s]\n",
      "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]\n",
      "Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 314MB/s]\n",
      "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 279/279 [00:00<00:00, 2.39MB/s]\n",
      "Downloading tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 17.1M/17.1M [00:00<00:00, 433MB/s]\n",
      "Downloading config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]\n",
      "Downloading config.json: 100%|██████████| 801/801 [00:00<00:00, 6.30MB/s]\n",
      "Downloading model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]\n",
      "Downloading model.safetensors:   2%|▏         | 41.9M/2.24G [00:00<00:06, 343MB/s]\n",
      "Downloading model.safetensors:   4%|▎         | 83.9M/2.24G [00:00<00:06, 338MB/s]\n",
      "Downloading model.safetensors:   6%|▌         | 136M/2.24G [00:00<00:05, 386MB/s]\n",
      "Downloading model.safetensors:   8%|▊         | 178M/2.24G [00:00<00:05, 375MB/s]\n",
      "Downloading model.safetensors:  10%|▉         | 220M/2.24G [00:00<00:05, 357MB/s]\n",
      "Downloading model.safetensors:  12%|█▏        | 262M/2.24G [00:00<00:05, 364MB/s]\n",
      "Downloading model.safetensors:  14%|█▎        | 304M/2.24G [00:00<00:05, 373MB/s]\n",
      "Downloading model.safetensors:  15%|█▌        | 346M/2.24G [00:00<00:05, 354MB/s]\n",
      "Downloading model.safetensors:  17%|█▋        | 388M/2.24G [00:01<00:05, 335MB/s]\n",
      "Downloading model.safetensors:  19%|█▉        | 430M/2.24G [00:01<00:05, 333MB/s]\n",
      "Downloading model.safetensors:  22%|██▏       | 482M/2.24G [00:01<00:04, 363MB/s]\n",
      "Downloading model.safetensors:  24%|██▍       | 535M/2.24G [00:01<00:04, 395MB/s]\n",
      "Downloading model.safetensors:  26%|██▌       | 577M/2.24G [00:01<00:04, 390MB/s]\n",
      "Downloading model.safetensors:  28%|██▊       | 619M/2.24G [00:01<00:04, 374MB/s]\n",
      "Downloading model.safetensors:  29%|██▉       | 661M/2.24G [00:01<00:04, 384MB/s]\n",
      "Downloading model.safetensors:  32%|███▏      | 713M/2.24G [00:01<00:03, 407MB/s]\n",
      "Downloading model.safetensors:  34%|███▎      | 755M/2.24G [00:02<00:03, 385MB/s]\n",
      "Downloading model.safetensors:  36%|███▌      | 807M/2.24G [00:02<00:03, 402MB/s]\n",
      "Downloading model.safetensors:  38%|███▊      | 849M/2.24G [00:02<00:03, 381MB/s]\n",
      "Downloading model.safetensors:  40%|███▉      | 891M/2.24G [00:02<00:03, 376MB/s]\n",
      "Downloading model.safetensors:  42%|████▏     | 933M/2.24G [00:02<00:03, 377MB/s]\n",
      "Downloading model.safetensors:  44%|████▍     | 986M/2.24G [00:02<00:03, 401MB/s]\n",
      "Downloading model.safetensors:  46%|████▌     | 1.03G/2.24G [00:02<00:03, 402MB/s]\n",
      "Downloading model.safetensors:  48%|████▊     | 1.07G/2.24G [00:02<00:03, 358MB/s]\n",
      "Downloading model.safetensors:  50%|████▉     | 1.11G/2.24G [00:02<00:03, 350MB/s]\n",
      "Downloading model.safetensors:  52%|█████▏    | 1.15G/2.24G [00:03<00:03, 354MB/s]\n",
      "Downloading model.safetensors:  54%|█████▍    | 1.21G/2.24G [00:03<00:02, 382MB/s]\n",
      "Downloading model.safetensors:  56%|█████▌    | 1.26G/2.24G [00:03<00:02, 399MB/s]\n",
      "Downloading model.safetensors:  58%|█████▊    | 1.30G/2.24G [00:03<00:02, 382MB/s]\n",
      "Downloading model.safetensors:  60%|█████▉    | 1.34G/2.24G [00:03<00:02, 365MB/s]\n",
      "Downloading model.safetensors:  62%|██████▏   | 1.38G/2.24G [00:03<00:02, 362MB/s]\n",
      "Downloading model.safetensors:  64%|██████▍   | 1.44G/2.24G [00:03<00:02, 380MB/s]\n",
      "Downloading model.safetensors:  66%|██████▌   | 1.48G/2.24G [00:03<00:02, 356MB/s]\n",
      "Downloading model.safetensors:  68%|██████▊   | 1.52G/2.24G [00:04<00:01, 365MB/s]\n",
      "Downloading model.safetensors:  70%|██████▉   | 1.56G/2.24G [00:04<00:01, 351MB/s]\n",
      "Downloading model.safetensors:  72%|███████▏  | 1.60G/2.24G [00:04<00:01, 361MB/s]\n",
      "Downloading model.safetensors:  74%|███████▍  | 1.66G/2.24G [00:04<00:01, 369MB/s]\n",
      "Downloading model.safetensors:  76%|███████▌  | 1.70G/2.24G [00:04<00:01, 367MB/s]\n",
      "Downloading model.safetensors:  78%|███████▊  | 1.74G/2.24G [00:04<00:01, 364MB/s]\n",
      "Downloading model.safetensors:  80%|███████▉  | 1.78G/2.24G [00:04<00:01, 367MB/s]\n",
      "Downloading model.safetensors:  82%|████████▏ | 1.84G/2.24G [00:04<00:01, 393MB/s]\n",
      "Downloading model.safetensors:  84%|████████▍ | 1.88G/2.24G [00:05<00:00, 390MB/s]\n",
      "Downloading model.safetensors:  86%|████████▌ | 1.92G/2.24G [00:05<00:00, 387MB/s]\n",
      "Downloading model.safetensors:  88%|████████▊ | 1.96G/2.24G [00:05<00:00, 380MB/s]\n",
      "Downloading model.safetensors:  89%|████████▉ | 2.00G/2.24G [00:05<00:00, 376MB/s]\n",
      "Downloading model.safetensors:  91%|█████████▏| 2.04G/2.24G [00:05<00:00, 358MB/s]\n",
      "Downloading model.safetensors:  93%|█████████▎| 2.09G/2.24G [00:05<00:00, 356MB/s]\n",
      "Downloading model.safetensors:  95%|█████████▌| 2.13G/2.24G [00:05<00:00, 354MB/s]\n",
      "Downloading model.safetensors:  97%|█████████▋| 2.17G/2.24G [00:05<00:00, 363MB/s]\n",
      "Downloading model.safetensors:  99%|█████████▉| 2.21G/2.24G [00:05<00:00, 341MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 2.24G/2.24G [00:06<00:00, 369MB/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-21a6ce2a9a206269/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n",
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 7061.12it/s]\n",
      "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1214.68it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 1062 examples [00:00, 7574.23 examples/s]\n",
      "Generating train split: 2117 examples [00:00, 8917.97 examples/s]\n",
      "Generating train split: 3167 examples [00:00, 9535.38 examples/s]\n",
      "Generating train split: 4210 examples [00:00, 9675.67 examples/s]\n",
      "Generating train split: 5245 examples [00:00, 9867.35 examples/s]\n",
      "Generating train split: 6319 examples [00:00, 10106.01 examples/s]\n",
      "Generating train split: 7358 examples [00:00, 10131.10 examples/s]\n",
      "Generating train split: 8404 examples [00:00, 10170.22 examples/s]\n",
      "Generating train split: 9453 examples [00:00, 10211.73 examples/s]\n",
      "Generating train split: 10497 examples [00:01, 10241.87 examples/s]\n",
      "Generating train split: 12045 examples [00:01, 10197.80 examples/s]\n",
      "Generating train split: 13100 examples [00:01, 10265.78 examples/s]\n",
      "Generating train split: 14147 examples [00:01, 10271.66 examples/s]\n",
      "Generating train split: 15184 examples [00:01, 10220.43 examples/s]\n",
      "Generating train split: 16243 examples [00:01, 10130.67 examples/s]\n",
      "Generating train split: 17283 examples [00:01, 10175.61 examples/s]\n",
      "Generating train split: 18326 examples [00:01, 10209.88 examples/s]\n",
      "Generating train split: 19362 examples [00:01, 10215.18 examples/s]\n",
      "Generating train split: 20401 examples [00:02, 10215.29 examples/s]\n",
      "Generating train split: 21937 examples [00:02, 10161.89 examples/s]\n",
      "Generating train split: 22980 examples [00:02, 10201.98 examples/s]\n",
      "Generating train split: 24002 examples [00:02, 10181.22 examples/s]\n",
      "Generating train split: 25516 examples [00:02, 9967.17 examples/s]\n",
      "Generating train split: 26546 examples [00:02, 9826.70 examples/s]\n",
      "Generating train split: 27589 examples [00:02, 9947.26 examples/s]\n",
      "Generating train split: 28612 examples [00:02, 9987.93 examples/s]\n",
      "Generating train split: 29647 examples [00:02, 10014.73 examples/s]\n",
      "Generating train split: 30702 examples [00:03, 10147.52 examples/s]\n",
      "Generating train split: 31734 examples [00:03, 10137.48 examples/s]\n",
      "Generating train split: 32765 examples [00:03, 10162.44 examples/s]\n",
      "Generating train split: 33809 examples [00:03, 10191.13 examples/s]\n",
      "Generating train split: 34845 examples [00:03, 10130.74 examples/s]\n",
      "Generating train split: 35879 examples [00:03, 10081.55 examples/s]\n",
      "Generating train split: 36926 examples [00:03, 9990.82 examples/s]\n",
      "Generating train split: 37977 examples [00:03, 10089.96 examples/s]\n",
      "Generating train split: 39027 examples [00:03, 10159.60 examples/s]\n",
      "Generating train split: 40082 examples [00:03, 10202.17 examples/s]\n",
      "Generating train split: 41626 examples [00:04, 10066.96 examples/s]\n",
      "Generating train split: 42666 examples [00:04, 10115.87 examples/s]\n",
      "Generating train split: 43703 examples [00:04, 10130.46 examples/s]\n",
      "Generating train split: 44719 examples [00:04, 10083.62 examples/s]\n",
      "Generating train split: 45778 examples [00:04, 10192.44 examples/s]\n",
      "Generating train split: 46827 examples [00:04, 10204.56 examples/s]\n",
      "Generating train split: 47888 examples [00:04, 10219.24 examples/s]\n",
      "Generating train split: 48930 examples [00:04, 10194.17 examples/s]\n",
      "Generating train split: 49989 examples [00:04, 10263.10 examples/s]\n",
      "Generating train split: 51045 examples [00:05, 10234.89 examples/s]\n",
      "Generating train split: 52084 examples [00:05, 10232.67 examples/s]\n",
      "Generating train split: 53134 examples [00:05, 10095.47 examples/s]\n",
      "Generating train split: 54191 examples [00:05, 10200.93 examples/s]\n",
      "Generating train split: 55243 examples [00:05, 10164.38 examples/s]\n",
      "Generating train split: 56303 examples [00:05, 10251.65 examples/s]\n",
      "Generating train split: 57363 examples [00:05, 10158.06 examples/s]\n",
      "Generating train split: 58399 examples [00:05, 10076.22 examples/s]\n",
      "Generating train split: 59442 examples [00:05, 10155.88 examples/s]\n",
      "Generating train split: 60479 examples [00:05, 10173.41 examples/s]\n",
      "Generating train split: 62012 examples [00:06, 10081.79 examples/s]\n",
      "Generating train split: 63028 examples [00:06, 9974.08 examples/s]\n",
      "Generating train split: 64072 examples [00:06, 10075.33 examples/s]\n",
      "Generating train split: 65116 examples [00:06, 10097.58 examples/s]\n",
      "Generating train split: 66153 examples [00:06, 10105.78 examples/s]\n",
      "Generating train split: 67187 examples [00:06, 9958.07 examples/s]\n",
      "Generating train split: 68233 examples [00:06, 10056.16 examples/s]\n",
      "Generating train split: 69275 examples [00:06, 10109.94 examples/s]\n",
      "Generating train split: 70320 examples [00:06, 10191.78 examples/s]\n",
      "Generating train split: 71352 examples [00:07, 10135.39 examples/s]\n",
      "Generating train split: 72386 examples [00:07, 10025.45 examples/s]\n",
      "Generating train split: 73429 examples [00:07, 10113.89 examples/s]\n",
      "Generating train split: 74469 examples [00:07, 10121.69 examples/s]\n",
      "Generating train split: 75517 examples [00:07, 10149.29 examples/s]\n",
      "Generating train split: 76549 examples [00:07, 10146.42 examples/s]\n",
      "Generating train split: 77589 examples [00:07, 10166.08 examples/s]\n",
      "Generating train split: 78648 examples [00:07, 10263.90 examples/s]\n",
      "Generating train split: 79697 examples [00:07, 10293.88 examples/s]\n",
      "Generating train split: 80738 examples [00:07, 10306.83 examples/s]\n",
      "Generating train split: 81773 examples [00:08, 10235.30 examples/s]\n",
      "Generating train split: 82829 examples [00:08, 10299.89 examples/s]\n",
      "Generating train split: 83876 examples [00:08, 10296.86 examples/s]\n",
      "Generating train split: 85417 examples [00:08, 10190.61 examples/s]\n",
      "Generating train split: 86476 examples [00:08, 10253.07 examples/s]\n",
      "Generating train split: 88020 examples [00:08, 10182.93 examples/s]\n",
      "Generating train split: 89069 examples [00:08, 10210.11 examples/s]\n",
      "Generating train split: 90110 examples [00:08, 10198.09 examples/s]\n",
      "Generating train split: 91180 examples [00:09, 10259.20 examples/s]\n",
      "Generating train split: 92210 examples [00:09, 10021.42 examples/s]\n",
      "Generating train split: 93267 examples [00:09, 10128.46 examples/s]\n",
      "Generating train split: 94307 examples [00:09, 10143.28 examples/s]\n",
      "Generating train split: 95355 examples [00:09, 10199.70 examples/s]\n",
      "Generating train split: 96894 examples [00:09, 10048.02 examples/s]\n",
      "Generating train split: 97948 examples [00:09, 10055.04 examples/s]\n",
      "Generating train split: 98975 examples [00:09, 9940.60 examples/s]\n",
      "Generating train split: 100016 examples [00:09, 10036.11 examples/s]\n",
      "Generating train split: 101054 examples [00:10, 9781.91 examples/s]\n",
      "Generating train split: 102080 examples [00:10, 9805.38 examples/s]\n",
      "Generating train split: 103129 examples [00:10, 9951.71 examples/s]\n",
      "Generating train split: 104170 examples [00:10, 9908.68 examples/s]\n",
      "Generating train split: 105207 examples [00:10, 9863.62 examples/s]\n",
      "Generating train split: 106246 examples [00:10, 9833.87 examples/s]\n",
      "Generating train split: 107272 examples [00:10, 9750.50 examples/s]\n",
      "Generating train split: 108321 examples [00:10, 9810.36 examples/s]\n",
      "Generating train split: 109355 examples [00:10, 9836.25 examples/s]\n",
      "Generating train split: 110415 examples [00:10, 10023.05 examples/s]\n",
      "Generating train split: 111458 examples [00:11, 10105.42 examples/s]\n",
      "Generating train split: 112522 examples [00:11, 10078.63 examples/s]\n",
      "Generating train split: 113566 examples [00:11, 10169.08 examples/s]\n",
      "Generating train split: 114613 examples [00:11, 10217.55 examples/s]\n",
      "Generating train split: 115693 examples [00:11, 10352.44 examples/s]\n",
      "Generating train split: 117237 examples [00:11, 10193.74 examples/s]\n",
      "Generating train split: 118265 examples [00:11, 10193.52 examples/s]\n",
      "Generating train split: 119297 examples [00:11, 10031.46 examples/s]\n",
      "Generating train split: 120330 examples [00:11, 10061.88 examples/s]\n",
      "Generating train split: 121365 examples [00:12, 10073.82 examples/s]\n",
      "Generating train split: 122418 examples [00:12, 10014.65 examples/s]\n",
      "Generating train split: 123470 examples [00:12, 10083.45 examples/s]\n",
      "Generating train split: 124510 examples [00:12, 10068.36 examples/s]\n",
      "Generating train split: 125569 examples [00:12, 10123.48 examples/s]\n",
      "Generating train split: 126589 examples [00:12, 10067.63 examples/s]\n",
      "Generating train split: 127623 examples [00:12, 10074.33 examples/s]\n",
      "Generating train split: 128646 examples [00:12, 10042.92 examples/s]\n",
      "Generating train split: 129687 examples [00:12, 10080.34 examples/s]\n",
      "Generating train split: 130738 examples [00:12, 10058.57 examples/s]\n",
      "Generating train split: 131777 examples [00:13, 10121.92 examples/s]\n",
      "Generating train split: 132813 examples [00:13, 10057.51 examples/s]\n",
      "Generating train split: 133849 examples [00:13, 10064.92 examples/s]\n",
      "Generating train split: 134908 examples [00:13, 10106.47 examples/s]\n",
      "Generating train split: 135956 examples [00:13, 10142.36 examples/s]\n",
      "Generating train split: 136997 examples [00:13, 10027.51 examples/s]\n",
      "Generating train split: 138024 examples [00:13, 9941.58 examples/s]\n",
      "Generating train split: 139057 examples [00:13, 9840.67 examples/s]\n",
      "Generating train split: 140119 examples [00:13, 9892.03 examples/s]\n",
      "Generating train split: 141175 examples [00:14, 9934.88 examples/s]\n",
      "Generating train split: 142199 examples [00:14, 9890.89 examples/s]\n",
      "Generating train split: 143224 examples [00:14, 9847.16 examples/s]\n",
      "Generating train split: 144287 examples [00:14, 9891.52 examples/s]\n",
      "Generating train split: 145322 examples [00:14, 9901.07 examples/s]\n",
      "Generating train split: 146360 examples [00:14, 9890.14 examples/s]\n",
      "Generating train split: 147426 examples [00:14, 9841.68 examples/s]\n",
      "Generating train split: 148490 examples [00:14, 9895.55 examples/s]\n",
      "Generating train split: 149525 examples [00:14, 9852.61 examples/s]\n",
      "Generating train split: 150572 examples [00:14, 9822.41 examples/s]\n",
      "Generating train split: 151594 examples [00:15, 9504.68 examples/s]\n",
      "Generating train split: 152612 examples [00:15, 9515.49 examples/s]\n",
      "Generating train split: 153664 examples [00:15, 9590.68 examples/s]\n",
      "Generating train split: 154707 examples [00:15, 9570.79 examples/s]\n",
      "Generating train split: 155736 examples [00:15, 9632.28 examples/s]\n",
      "Generating train split: 156761 examples [00:15, 9699.79 examples/s]\n",
      "Generating train split: 157816 examples [00:15, 9861.64 examples/s]\n",
      "Generating train split: 158848 examples [00:15, 9905.70 examples/s]\n",
      "Generating train split: 159899 examples [00:15, 9996.10 examples/s]\n",
      "Generating train split: 160939 examples [00:16, 9988.59 examples/s]\n",
      "Generating train split: 161985 examples [00:16, 10059.13 examples/s]\n",
      "Generating train split: 163023 examples [00:16, 10069.34 examples/s]\n",
      "Generating train split: 164053 examples [00:16, 10002.57 examples/s]\n",
      "Generating train split: 165087 examples [00:16, 9845.06 examples/s]\n",
      "Generating train split: 166160 examples [00:16, 10024.04 examples/s]\n",
      "Generating train split: 167196 examples [00:16, 10025.97 examples/s]\n",
      "Generating train split: 168242 examples [00:16, 10075.73 examples/s]\n",
      "Generating train split: 169278 examples [00:16, 10093.69 examples/s]\n",
      "Generating train split: 170309 examples [00:16, 10084.15 examples/s]\n",
      "Generating train split: 171367 examples [00:17, 10134.32 examples/s]\n",
      "Generating train split: 172417 examples [00:17, 10178.60 examples/s]\n",
      "Generating train split: 173450 examples [00:17, 10117.33 examples/s]\n",
      "Generating train split: 174517 examples [00:17, 10151.06 examples/s]\n",
      "Generating train split: 175577 examples [00:17, 10161.89 examples/s]\n",
      "Generating train split: 176623 examples [00:17, 10162.14 examples/s]\n",
      "Generating train split: 177655 examples [00:17, 10131.67 examples/s]\n",
      "Generating train split: 178721 examples [00:17, 10084.78 examples/s]\n",
      "Generating train split: 179773 examples [00:17, 10091.01 examples/s]\n",
      "Generating train split: 180835 examples [00:17, 10191.35 examples/s]\n",
      "Generating train split: 181866 examples [00:18, 10040.51 examples/s]\n",
      "Generating train split: 182917 examples [00:18, 9983.52 examples/s]\n",
      "Generating train split: 183940 examples [00:18, 10021.33 examples/s]\n",
      "Generating train split: 184979 examples [00:18, 10014.59 examples/s]\n",
      "Generating train split: 186021 examples [00:18, 10079.62 examples/s]\n",
      "Generating train split: 187553 examples [00:18, 10052.98 examples/s]\n",
      "Generating train split: 188615 examples [00:18, 10158.93 examples/s]\n",
      "Generating train split: 189641 examples [00:18, 10145.82 examples/s]\n",
      "Generating train split: 190657 examples [00:18, 10111.68 examples/s]\n",
      "Generating train split: 191709 examples [00:19, 10107.91 examples/s]\n",
      "Generating train split: 192742 examples [00:19, 10119.07 examples/s]\n",
      "Generating train split: 193815 examples [00:19, 10267.57 examples/s]\n",
      "Generating train split: 195358 examples [00:19, 10150.78 examples/s]\n",
      "Generating train split: 196396 examples [00:19, 10180.89 examples/s]\n",
      "Generating train split: 197435 examples [00:19, 10220.28 examples/s]\n",
      "Generating train split: 198476 examples [00:19, 10094.19 examples/s]\n",
      "Generating train split: 199495 examples [00:19, 10044.20 examples/s]\n",
      "Generating train split: 200549 examples [00:19, 10100.25 examples/s]\n",
      "Generating train split: 201597 examples [00:20, 10165.84 examples/s]\n",
      "Generating train split: 202660 examples [00:20, 10248.11 examples/s]\n",
      "Generating train split: 203686 examples [00:20, 10196.79 examples/s]\n",
      "Generating train split: 204730 examples [00:20, 10197.28 examples/s]\n",
      "Generating train split: 205762 examples [00:20, 10184.90 examples/s]\n",
      "Generating train split: 206786 examples [00:20, 10134.93 examples/s]\n",
      "Generating train split: 207803 examples [00:20, 10060.19 examples/s]\n",
      "Generating train split: 208854 examples [00:20, 10136.53 examples/s]\n",
      "Generating train split: 209895 examples [00:20, 10135.96 examples/s]\n",
      "Generating train split: 210924 examples [00:20, 10059.88 examples/s]\n",
      "Generating train split: 211980 examples [00:21, 10016.98 examples/s]\n",
      "Generating train split: 213011 examples [00:21, 10053.37 examples/s]\n",
      "Generating train split: 214053 examples [00:21, 10086.78 examples/s]\n",
      "Generating train split: 215090 examples [00:21, 9944.46 examples/s]\n",
      "Generating train split: 216136 examples [00:21, 10009.64 examples/s]\n",
      "Generating train split: 217170 examples [00:21, 9996.59 examples/s]\n",
      "Generating train split: 218227 examples [00:21, 10037.09 examples/s]\n",
      "Generating train split: 219271 examples [00:21, 10054.47 examples/s]\n",
      "Generating train split: 220317 examples [00:21, 9908.52 examples/s]\n",
      "Generating train split: 221354 examples [00:22, 9968.81 examples/s]\n",
      "Generating train split: 222416 examples [00:22, 10104.72 examples/s]\n",
      "Generating train split: 223454 examples [00:22, 10123.80 examples/s]\n",
      "Generating train split: 224508 examples [00:22, 10076.88 examples/s]\n",
      "Generating train split: 225560 examples [00:22, 9818.84 examples/s]\n",
      "Generating train split: 226592 examples [00:22, 9819.11 examples/s]\n",
      "Generating train split: 227632 examples [00:22, 9917.77 examples/s]\n",
      "Generating train split: 228692 examples [00:22, 9938.60 examples/s]\n",
      "Generating train split: 229743 examples [00:22, 10009.45 examples/s]\n",
      "Generating train split: 230792 examples [00:22, 9933.71 examples/s]\n",
      "Generating train split: 231816 examples [00:23, 9979.27 examples/s]\n",
      "Generating train split: 232875 examples [00:23, 10087.59 examples/s]\n",
      "Generating train split: 233899 examples [00:23, 10033.65 examples/s]\n",
      "Generating train split: 234936 examples [00:23, 9985.97 examples/s]\n",
      "Generating train split: 235952 examples [00:23, 9945.31 examples/s]\n",
      "Generating train split: 236979 examples [00:23, 9953.94 examples/s]\n",
      "Generating train split: 238022 examples [00:23, 10015.55 examples/s]\n",
      "Generating train split: 239069 examples [00:23, 10067.58 examples/s]\n",
      "Generating train split: 240103 examples [00:23, 10087.96 examples/s]\n",
      "Generating train split: 241172 examples [00:23, 10057.96 examples/s]\n",
      "Generating train split: 242224 examples [00:24, 10020.97 examples/s]\n",
      "Generating train split: 243261 examples [00:24, 10020.69 examples/s]\n",
      "Generating train split: 244302 examples [00:24, 10061.71 examples/s]\n",
      "Generating train split: 245334 examples [00:24, 10019.24 examples/s]\n",
      "Generating train split: 246410 examples [00:24, 10100.45 examples/s]\n",
      "Generating train split: 247470 examples [00:24, 10130.12 examples/s]\n",
      "Generating train split: 248518 examples [00:24, 10155.14 examples/s]\n",
      "Generating train split: 249563 examples [00:24, 10138.12 examples/s]\n",
      "Generating train split: 250601 examples [00:24, 10102.89 examples/s]\n",
      "Generating train split: 251656 examples [00:25, 10004.03 examples/s]\n",
      "Generating train split: 252721 examples [00:25, 10051.42 examples/s]\n",
      "Generating train split: 253754 examples [00:25, 10074.32 examples/s]\n",
      "Generating train split: 254792 examples [00:25, 10087.11 examples/s]\n",
      "Generating train split: 255830 examples [00:25, 10114.87 examples/s]\n",
      "Generating train split: 256868 examples [00:25, 10080.85 examples/s]\n",
      "Generating train split: 257898 examples [00:25, 10037.70 examples/s]\n",
      "Generating train split: 258944 examples [00:25, 10085.70 examples/s]\n",
      "Generating train split: 259998 examples [00:25, 10129.62 examples/s]\n",
      "Generating train split: 261030 examples [00:25, 10110.96 examples/s]\n",
      "Generating train split: 262057 examples [00:26, 9936.26 examples/s]\n",
      "Generating train split: 263102 examples [00:26, 9961.31 examples/s]\n",
      "Generating train split: 264133 examples [00:26, 9959.83 examples/s]\n",
      "Generating train split: 265162 examples [00:26, 9926.78 examples/s]\n",
      "Generating train split: 266190 examples [00:26, 9901.38 examples/s]\n",
      "Generating train split: 267234 examples [00:26, 9935.36 examples/s]\n",
      "Generating train split: 268259 examples [00:26, 9843.68 examples/s]\n",
      "Generating train split: 269299 examples [00:26, 9903.64 examples/s]\n",
      "Generating train split: 270320 examples [00:26, 9918.45 examples/s]\n",
      "Generating train split: 271349 examples [00:27, 9712.11 examples/s]\n",
      "Generating train split: 272411 examples [00:27, 9813.94 examples/s]\n",
      "Generating train split: 273439 examples [00:27, 9837.37 examples/s]\n",
      "Generating train split: 274461 examples [00:27, 9869.78 examples/s]\n",
      "Generating train split: 275520 examples [00:27, 9927.33 examples/s]\n",
      "Generating train split: 276548 examples [00:27, 9886.85 examples/s]\n",
      "Generating train split: 277595 examples [00:27, 9959.25 examples/s]\n",
      "Generating train split: 278649 examples [00:27, 9969.82 examples/s]\n",
      "Generating train split: 279704 examples [00:27, 10025.63 examples/s]\n",
      "Generating train split: 280731 examples [00:27, 9959.62 examples/s]\n",
      "Generating train split: 281765 examples [00:28, 9854.38 examples/s]\n",
      "Generating train split: 282830 examples [00:28, 9874.37 examples/s]\n",
      "Generating train split: 283880 examples [00:28, 9982.04 examples/s]\n",
      "Generating train split: 284924 examples [00:28, 9999.84 examples/s]\n",
      "Generating train split: 285950 examples [00:28, 9965.79 examples/s]\n",
      "Generating train split: 286981 examples [00:28, 9975.77 examples/s]\n",
      "Generating train split: 288028 examples [00:28, 10015.47 examples/s]\n",
      "Generating train split: 289092 examples [00:28, 10088.80 examples/s]\n",
      "Generating train split: 290138 examples [00:28, 10082.46 examples/s]\n",
      "Generating train split: 291176 examples [00:28, 10074.82 examples/s]\n",
      "Generating train split: 292213 examples [00:29, 10058.93 examples/s]\n",
      "Generating train split: 293252 examples [00:29, 10025.00 examples/s]\n",
      "Generating train split: 294300 examples [00:29, 9882.34 examples/s]\n",
      "Generating train split: 295319 examples [00:29, 9813.08 examples/s]\n",
      "Generating train split: 296352 examples [00:29, 9891.21 examples/s]\n",
      "Generating train split: 297405 examples [00:29, 9815.85 examples/s]\n",
      "Generating train split: 298448 examples [00:29, 9872.97 examples/s]\n",
      "Generating train split: 299465 examples [00:29, 9851.04 examples/s]\n",
      "Generating train split: 300515 examples [00:29, 9903.72 examples/s]\n",
      "Generating train split: 301580 examples [00:30, 10035.33 examples/s]\n",
      "Generating train split: 302611 examples [00:30, 10019.61 examples/s]\n",
      "Generating train split: 303667 examples [00:30, 10062.06 examples/s]\n",
      "Generating train split: 304700 examples [00:30, 10028.67 examples/s]\n",
      "Generating train split: 305752 examples [00:30, 10033.73 examples/s]\n",
      "Generating train split: 306786 examples [00:30, 10053.87 examples/s]\n",
      "Generating train split: 307822 examples [00:30, 9941.64 examples/s]\n",
      "Generating train split: 308901 examples [00:30, 9974.28 examples/s]\n",
      "Generating train split: 309925 examples [00:30, 9996.37 examples/s]\n",
      "Generating train split: 310959 examples [00:30, 9942.09 examples/s]\n",
      "Generating train split: 312004 examples [00:31, 9886.31 examples/s]\n",
      "Generating train split: 313053 examples [00:31, 10003.22 examples/s]\n",
      "Generating train split: 314115 examples [00:31, 10101.91 examples/s]\n",
      "Generating train split: 315186 examples [00:31, 10219.62 examples/s]\n",
      "Generating train split: 316212 examples [00:31, 10139.04 examples/s]\n",
      "Generating train split: 317731 examples [00:31, 9988.22 examples/s]\n",
      "Generating train split: 318796 examples [00:31, 10087.54 examples/s]\n",
      "Generating train split: 319824 examples [00:31, 10066.43 examples/s]\n",
      "Generating train split: 320871 examples [00:31, 10118.70 examples/s]\n",
      "Generating train split: 321928 examples [00:32, 10167.22 examples/s]\n",
      "Generating train split: 323452 examples [00:32, 10079.91 examples/s]\n",
      "Generating train split: 324489 examples [00:32, 10082.16 examples/s]\n",
      "Generating train split: 325534 examples [00:32, 10009.06 examples/s]\n",
      "Generating train split: 326588 examples [00:32, 10069.16 examples/s]\n",
      "Generating train split: 327620 examples [00:32, 10055.33 examples/s]\n",
      "Generating train split: 328644 examples [00:32, 10025.70 examples/s]\n",
      "Generating train split: 329706 examples [00:32, 10065.77 examples/s]\n",
      "Generating train split: 330733 examples [00:32, 10059.00 examples/s]\n",
      "Generating train split: 331781 examples [00:33, 10097.60 examples/s]\n",
      "Generating train split: 332802 examples [00:33, 10069.47 examples/s]\n",
      "Generating train split: 333858 examples [00:33, 10132.33 examples/s]\n",
      "Generating train split: 334911 examples [00:33, 10136.07 examples/s]\n",
      "Generating train split: 335966 examples [00:33, 9888.32 examples/s]\n",
      "Generating train split: 337007 examples [00:33, 9929.59 examples/s]\n",
      "Generating train split: 338078 examples [00:33, 10108.77 examples/s]\n",
      "Generating train split: 339120 examples [00:33, 10118.15 examples/s]\n",
      "Generating train split: 340189 examples [00:33, 10220.09 examples/s]\n",
      "Generating train split: 341253 examples [00:33, 10240.48 examples/s]\n",
      "Generating train split: 342288 examples [00:34, 9919.72 examples/s]\n",
      "Generating train split: 343320 examples [00:34, 9965.66 examples/s]\n",
      "Generating train split: 344359 examples [00:34, 9855.60 examples/s]\n",
      "Generating train split: 345406 examples [00:34, 9871.73 examples/s]\n",
      "Generating train split: 346461 examples [00:34, 10008.90 examples/s]\n",
      "Generating train split: 347518 examples [00:34, 9989.99 examples/s]\n",
      "Generating train split: 348547 examples [00:34, 9876.28 examples/s]\n",
      "Generating train split: 349594 examples [00:34, 9963.45 examples/s]\n",
      "Generating train split: 350656 examples [00:34, 10051.34 examples/s]\n",
      "Generating train split: 351698 examples [00:35, 10075.01 examples/s]\n",
      "Generating train split: 352729 examples [00:35, 10098.74 examples/s]\n",
      "Generating train split: 353787 examples [00:35, 10190.04 examples/s]\n",
      "Generating train split: 354845 examples [00:35, 10258.19 examples/s]\n",
      "Generating train split: 355875 examples [00:35, 10161.37 examples/s]\n",
      "Generating train split: 356903 examples [00:35, 10128.35 examples/s]\n",
      "Generating train split: 357928 examples [00:35, 10087.18 examples/s]\n",
      "Generating train split: 358978 examples [00:35, 10050.37 examples/s]\n",
      "Generating train split: 360021 examples [00:35, 10055.58 examples/s]\n",
      "Generating train split: 361079 examples [00:35, 10108.91 examples/s]\n",
      "Generating train split: 362111 examples [00:36, 9967.82 examples/s]\n",
      "Generating train split: 363160 examples [00:36, 10084.57 examples/s]\n",
      "Generating train split: 364194 examples [00:36, 9987.32 examples/s]\n",
      "Generating train split: 365236 examples [00:36, 10058.17 examples/s]\n",
      "Generating train split: 366280 examples [00:36, 10118.83 examples/s]\n",
      "Generating train split: 367354 examples [00:36, 10239.33 examples/s]\n",
      "Generating train split: 368421 examples [00:36, 10294.76 examples/s]\n",
      "Generating train split: 369454 examples [00:36, 10154.00 examples/s]\n",
      "Generating train split: 370500 examples [00:36, 10179.68 examples/s]\n",
      "Generating train split: 371540 examples [00:36, 10132.19 examples/s]\n",
      "Generating train split: 372567 examples [00:37, 9820.88 examples/s]\n",
      "Generating train split: 373607 examples [00:37, 9939.33 examples/s]\n",
      "Generating train split: 374650 examples [00:37, 10039.87 examples/s]\n",
      "Generating train split: 375675 examples [00:37, 10020.29 examples/s]\n",
      "Generating train split: 376716 examples [00:37, 10011.16 examples/s]\n",
      "Generating train split: 377759 examples [00:37, 9999.00 examples/s]\n",
      "Generating train split: 378792 examples [00:37, 9998.23 examples/s]\n",
      "Generating train split: 379825 examples [00:37, 10010.32 examples/s]\n",
      "Generating train split: 380862 examples [00:37, 9910.80 examples/s]\n",
      "Generating train split: 381917 examples [00:38, 9913.42 examples/s]\n",
      "Generating train split: 382940 examples [00:38, 9908.43 examples/s]\n",
      "Generating train split: 383983 examples [00:38, 10014.56 examples/s]\n",
      "Generating train split: 385034 examples [00:38, 10123.54 examples/s]\n",
      "Generating train split: 386091 examples [00:38, 10203.73 examples/s]\n",
      "Generating train split: 387630 examples [00:38, 10138.53 examples/s]\n",
      "Generating train split: 388663 examples [00:38, 10148.14 examples/s]\n",
      "Generating train split: 389718 examples [00:38, 10214.48 examples/s]\n",
      "Generating train split: 390746 examples [00:38, 10037.02 examples/s]\n",
      "Generating train split: 391795 examples [00:39, 9971.18 examples/s]\n",
      "Generating train split: 392841 examples [00:39, 10070.23 examples/s]\n",
      "Generating train split: 393861 examples [00:39, 10072.28 examples/s]\n",
      "Generating train split: 394928 examples [00:39, 10200.79 examples/s]\n",
      "Generating train split: 396453 examples [00:39, 10083.94 examples/s]\n",
      "Generating train split: 397470 examples [00:39, 10065.62 examples/s]\n",
      "Generating train split: 398501 examples [00:39, 10083.33 examples/s]\n",
      "Generating train split: 399526 examples [00:39, 10075.94 examples/s]\n",
      "Generating train split: 400555 examples [00:39, 9933.53 examples/s]\n",
      "Generating train split: 401629 examples [00:39, 10059.43 examples/s]\n",
      "Generating train split: 402678 examples [00:40, 10141.34 examples/s]\n",
      "Generating train split: 403707 examples [00:40, 10148.50 examples/s]\n",
      "Generating train split: 404762 examples [00:40, 10095.93 examples/s]\n",
      "Generating train split: 405803 examples [00:40, 10143.67 examples/s]\n",
      "Generating train split: 406822 examples [00:40, 10105.16 examples/s]\n",
      "Generating train split: 407873 examples [00:40, 10128.39 examples/s]\n",
      "Generating train split: 408911 examples [00:40, 10152.33 examples/s]\n",
      "Generating train split: 409983 examples [00:40, 10269.91 examples/s]\n",
      "Generating train split: 411021 examples [00:40, 10202.56 examples/s]\n",
      "Generating train split: 412061 examples [00:41, 10216.18 examples/s]\n",
      "Generating train split: 413096 examples [00:41, 10214.66 examples/s]\n",
      "Generating train split: 414146 examples [00:41, 10258.45 examples/s]\n",
      "Generating train split: 415192 examples [00:41, 10286.33 examples/s]\n",
      "Generating train split: 416224 examples [00:41, 10027.95 examples/s]\n",
      "Generating train split: 417270 examples [00:41, 10095.36 examples/s]\n",
      "Generating train split: 418299 examples [00:41, 10050.73 examples/s]\n",
      "Generating train split: 419336 examples [00:41, 10094.50 examples/s]\n",
      "Generating train split: 420387 examples [00:41, 10152.46 examples/s]\n",
      "Generating train split: 421424 examples [00:41, 10179.75 examples/s]\n",
      "Generating train split: 422467 examples [00:42, 10220.36 examples/s]\n",
      "Generating train split: 423508 examples [00:42, 10204.05 examples/s]\n",
      "Generating train split: 424550 examples [00:42, 10077.52 examples/s]\n",
      "Generating train split: 425571 examples [00:42, 9905.81 examples/s]\n",
      "Generating train split: 426598 examples [00:42, 9973.31 examples/s]\n",
      "Generating train split: 427643 examples [00:42, 10010.05 examples/s]\n",
      "Generating train split: 428680 examples [00:42, 10065.89 examples/s]\n",
      "Generating train split: 429730 examples [00:42, 10074.02 examples/s]\n",
      "Generating train split: 430771 examples [00:42, 10123.56 examples/s]\n",
      "Generating train split: 431808 examples [00:42, 10131.87 examples/s]\n",
      "Generating train split: 432854 examples [00:43, 10167.50 examples/s]\n",
      "Generating train split: 433898 examples [00:43, 10190.87 examples/s]\n",
      "Generating train split: 434939 examples [00:43, 10220.46 examples/s]\n",
      "Generating train split: 435986 examples [00:43, 10097.47 examples/s]\n",
      "Generating train split: 437045 examples [00:43, 10192.62 examples/s]\n",
      "Generating train split: 438094 examples [00:43, 10181.92 examples/s]\n",
      "Generating train split: 439149 examples [00:43, 10216.39 examples/s]\n",
      "Generating train split: 440188 examples [00:43, 10172.63 examples/s]\n",
      "Generating train split: 441237 examples [00:43, 10147.10 examples/s]\n",
      "Generating train split: 442293 examples [00:43, 10157.09 examples/s]\n",
      "Generating train split: 443341 examples [00:44, 10179.49 examples/s]\n",
      "Generating train split: 444385 examples [00:44, 10140.63 examples/s]\n",
      "Generating train split: 445419 examples [00:44, 10094.29 examples/s]\n",
      "Generating train split: 446441 examples [00:44, 9928.42 examples/s]\n",
      "Generating train split: 447482 examples [00:44, 9993.16 examples/s]\n",
      "Generating train split: 448510 examples [00:44, 10030.66 examples/s]\n",
      "Generating train split: 449567 examples [00:44, 9993.00 examples/s]\n",
      "Generating train split: 450624 examples [00:44, 10084.13 examples/s]\n",
      "Generating train split: 451643 examples [00:44, 10046.99 examples/s]\n",
      "Generating train split: 452692 examples [00:45, 10116.63 examples/s]\n",
      "Generating train split: 453734 examples [00:45, 10166.08 examples/s]\n",
      "Generating train split: 454779 examples [00:45, 10208.88 examples/s]\n",
      "Generating train split: 455823 examples [00:45, 10191.07 examples/s]\n",
      "Generating train split: 456864 examples [00:45, 10162.28 examples/s]\n",
      "Generating train split: 457903 examples [00:45, 10164.00 examples/s]\n",
      "Generating train split: 458961 examples [00:45, 10216.80 examples/s]\n",
      "Generating train split: 460501 examples [00:45, 10130.69 examples/s]\n",
      "Generating train split: 461550 examples [00:45, 10179.40 examples/s]\n",
      "Generating train split: 462605 examples [00:46, 10076.24 examples/s]\n",
      "Generating train split: 463640 examples [00:46, 10119.28 examples/s]\n",
      "Generating train split: 464655 examples [00:46, 10089.01 examples/s]\n",
      "Generating train split: 465695 examples [00:46, 10143.72 examples/s]\n",
      "Generating train split: 466754 examples [00:46, 10240.52 examples/s]\n",
      "Generating train split: 467805 examples [00:46, 10269.93 examples/s]\n",
      "Generating train split: 469350 examples [00:46, 10218.55 examples/s]\n",
      "Generating train split: 470860 examples [00:46, 10093.00 examples/s]\n",
      "Generating train split: 471912 examples [00:46, 10170.93 examples/s]\n",
      "Generating train split: 472966 examples [00:47, 10186.30 examples/s]\n",
      "Generating train split: 474002 examples [00:47, 10153.21 examples/s]\n",
      "Generating train split: 475523 examples [00:47, 9973.52 examples/s]\n",
      "Generating train split: 476573 examples [00:47, 10007.20 examples/s]\n",
      "Generating train split: 477627 examples [00:47, 10097.78 examples/s]\n",
      "Generating train split: 478664 examples [00:47, 10097.67 examples/s]\n",
      "Generating train split: 479684 examples [00:47, 9969.69 examples/s]\n",
      "Generating train split: 480720 examples [00:47, 9999.14 examples/s]\n",
      "Generating train split: 481749 examples [00:47, 9986.53 examples/s]\n",
      "Generating train split: 482801 examples [00:48, 10096.62 examples/s]\n",
      "Generating train split: 483862 examples [00:48, 10138.13 examples/s]\n",
      "Generating train split: 484904 examples [00:48, 10168.55 examples/s]\n",
      "Generating train split: 485949 examples [00:48, 10205.13 examples/s]\n",
      "Generating train split: 486979 examples [00:48, 10175.43 examples/s]\n",
      "Generating train split: 488025 examples [00:48, 10192.79 examples/s]\n",
      "Generating train split: 489064 examples [00:48, 10171.64 examples/s]\n",
      "Generating train split: 490132 examples [00:48, 10132.07 examples/s]\n",
      "Generating train split: 491165 examples [00:48, 10116.00 examples/s]\n",
      "Generating train split: 492220 examples [00:48, 10184.57 examples/s]\n",
      "Generating train split: 493758 examples [00:49, 10133.02 examples/s]\n",
      "Generating train split: 494810 examples [00:49, 10191.81 examples/s]\n",
      "Generating train split: 495867 examples [00:49, 10256.97 examples/s]\n",
      "Generating train split: 496927 examples [00:49, 10262.73 examples/s]\n",
      "Generating train split: 497973 examples [00:49, 10276.88 examples/s]\n",
      "Generating train split: 499008 examples [00:49, 10272.09 examples/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-21a6ce2a9a206269/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
      "12/21/2023 07:30:37 - WARNING - datasets.builder -   Found cached dataset json (/root/.cache/huggingface/datasets/json/default-21a6ce2a9a206269/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "12/21/2023 07:30:37 - WARNING - datasets.builder -   Found cached dataset json (/root/.cache/huggingface/datasets/json/default-21a6ce2a9a206269/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "12/21/2023 07:30:37 - WARNING - datasets.builder -   Found cached dataset json (/root/.cache/huggingface/datasets/json/default-21a6ce2a9a206269/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "NCCL version 2.16.2+cuda11.8\n",
      "algo-1:66:1157 [0] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "algo-1:66:1157 [0] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "algo-1:68:1158 [2] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "algo-1:68:1158 [2] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "algo-1:67:1159 [1] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "algo-1:67:1159 [1] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "algo-1:69:1160 [3] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "algo-1:69:1160 [3] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "0%|          | 0/11697 [00:00<?, ?it/s]\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "0%|          | 1/11697 [00:03<12:57:03,  3.99s/it]\n",
      "0%|          | 2/11697 [00:07<12:20:20,  3.80s/it]\n",
      "0%|          | 3/11697 [00:11<11:50:55,  3.65s/it]\n",
      "0%|          | 4/11697 [00:14<11:40:22,  3.59s/it]\n",
      "0%|          | 5/11697 [00:18<11:34:14,  3.56s/it]\n",
      "0%|          | 6/11697 [00:21<11:34:02,  3.56s/it]\n",
      "0%|          | 7/11697 [00:25<11:36:21,  3.57s/it]\n",
      "0%|          | 8/11697 [00:28<11:32:59,  3.56s/it]\n",
      "0%|          | 9/11697 [00:32<11:33:32,  3.56s/it]\n",
      "0%|          | 10/11697 [00:35<11:33:16,  3.56s/it]\n",
      "0%|          | 11/11697 [00:39<11:37:08,  3.58s/it]\n",
      "0%|          | 12/11697 [00:43<11:35:54,  3.57s/it]\n",
      "0%|          | 13/11697 [00:46<11:35:11,  3.57s/it]\n",
      "0%|          | 14/11697 [00:50<11:34:38,  3.57s/it]\n",
      "0%|          | 15/11697 [00:53<11:34:26,  3.57s/it]\n",
      "0%|          | 16/11697 [00:57<11:36:31,  3.58s/it]\n",
      "0%|          | 17/11697 [01:00<11:35:30,  3.57s/it]\n",
      "0%|          | 18/11697 [01:04<11:37:29,  3.58s/it]\n",
      "0%|          | 19/11697 [01:08<11:34:38,  3.57s/it]\n",
      "0%|          | 20/11697 [01:11<11:31:12,  3.55s/it]\n",
      "0%|          | 21/11697 [01:15<11:27:06,  3.53s/it]\n",
      "0%|          | 22/11697 [01:18<11:27:19,  3.53s/it]\n",
      "0%|          | 23/11697 [01:22<11:27:52,  3.54s/it]\n",
      "0%|          | 24/11697 [01:25<11:31:37,  3.56s/it]\n",
      "0%|          | 25/11697 [01:29<11:31:28,  3.55s/it]\n",
      "0%|          | 26/11697 [01:32<11:30:33,  3.55s/it]\n",
      "0%|          | 27/11697 [01:36<11:34:48,  3.57s/it]\n",
      "0%|          | 28/11697 [01:40<11:33:08,  3.56s/it]\n",
      "0%|          | 29/11697 [01:43<11:32:19,  3.56s/it]\n",
      "0%|          | 30/11697 [01:47<11:31:13,  3.55s/it]\n",
      "{'loss': 0.3436, 'learning_rate': 4.9888860391553396e-06, 'epoch': 0.01}\n",
      "0%|          | 30/11697 [01:47<11:31:13,  3.55s/it]\n",
      "0%|          | 31/11697 [01:50<11:28:59,  3.54s/it]\n",
      "0%|          | 32/11697 [01:54<11:28:06,  3.54s/it]\n",
      "0%|          | 33/11697 [01:57<11:27:12,  3.54s/it]\n",
      "0%|          | 34/11697 [02:01<11:25:26,  3.53s/it]\n",
      "0%|          | 35/11697 [02:04<11:24:29,  3.52s/it]\n",
      "0%|          | 36/11697 [02:08<11:24:44,  3.52s/it]\n",
      "0%|          | 37/11697 [02:11<11:24:51,  3.52s/it]\n",
      "0%|          | 38/11697 [02:15<11:28:21,  3.54s/it]\n",
      "0%|          | 39/11697 [02:18<11:27:10,  3.54s/it]\n",
      "0%|          | 40/11697 [02:22<11:34:47,  3.58s/it]\n",
      "0%|          | 41/11697 [02:26<11:32:06,  3.56s/it]\n",
      "0%|          | 42/11697 [02:29<11:28:57,  3.55s/it]\n",
      "0%|          | 43/11697 [02:33<11:27:13,  3.54s/it]\n",
      "0%|          | 44/11697 [02:36<11:25:43,  3.53s/it]\n",
      "0%|          | 45/11697 [02:40<11:24:46,  3.53s/it]\n",
      "0%|          | 46/11697 [02:43<11:24:46,  3.53s/it]\n",
      "0%|          | 47/11697 [02:47<11:24:19,  3.52s/it]\n",
      "0%|          | 48/11697 [02:50<11:24:17,  3.52s/it]\n",
      "0%|          | 49/11697 [02:54<11:24:15,  3.52s/it]\n",
      "0%|          | 50/11697 [02:57<11:24:25,  3.53s/it]\n",
      "0%|          | 51/11697 [03:01<11:27:37,  3.54s/it]\n",
      "0%|          | 52/11697 [03:04<11:25:49,  3.53s/it]\n",
      "0%|          | 53/11697 [03:08<11:26:15,  3.54s/it]\n",
      "0%|          | 54/11697 [03:11<11:25:15,  3.53s/it]\n",
      "0%|          | 55/11697 [03:15<11:24:47,  3.53s/it]\n",
      "0%|          | 56/11697 [03:19<11:27:48,  3.55s/it]\n",
      "0%|          | 57/11697 [03:22<11:28:19,  3.55s/it]\n",
      "0%|          | 58/11697 [03:26<11:28:45,  3.55s/it]\n",
      "1%|          | 59/11697 [03:29<11:26:59,  3.54s/it]\n",
      "1%|          | 60/11697 [03:33<11:25:24,  3.53s/it]\n",
      "{'loss': 0.2967, 'learning_rate': 4.976062238180731e-06, 'epoch': 0.02}\n",
      "1%|          | 60/11697 [03:33<11:25:24,  3.53s/it]\n",
      "1%|          | 61/11697 [03:36<11:32:19,  3.57s/it]\n",
      "1%|          | 62/11697 [03:40<11:29:12,  3.55s/it]\n",
      "1%|          | 63/11697 [03:43<11:28:52,  3.55s/it]\n",
      "1%|          | 64/11697 [03:47<11:27:52,  3.55s/it]\n",
      "1%|          | 65/11697 [03:50<11:25:49,  3.54s/it]\n",
      "1%|          | 66/11697 [03:54<11:24:37,  3.53s/it]\n",
      "1%|          | 67/11697 [03:58<11:30:24,  3.56s/it]\n",
      "1%|          | 68/11697 [04:01<11:27:15,  3.55s/it]\n",
      "1%|          | 69/11697 [04:05<11:25:10,  3.54s/it]\n",
      "1%|          | 70/11697 [04:08<11:23:50,  3.53s/it]\n",
      "1%|          | 71/11697 [04:12<11:31:10,  3.57s/it]\n",
      "1%|          | 72/11697 [04:15<11:28:16,  3.55s/it]\n",
      "1%|          | 73/11697 [04:19<11:26:26,  3.54s/it]\n",
      "1%|          | 74/11697 [04:22<11:27:25,  3.55s/it]\n",
      "1%|          | 75/11697 [04:26<11:27:29,  3.55s/it]\n",
      "1%|          | 76/11697 [04:30<11:27:12,  3.55s/it]\n",
      "1%|          | 77/11697 [04:33<11:31:20,  3.57s/it]\n",
      "1%|          | 78/11697 [04:37<11:28:56,  3.56s/it]\n",
      "1%|          | 79/11697 [04:40<11:25:58,  3.54s/it]\n",
      "1%|          | 80/11697 [04:44<11:25:07,  3.54s/it]\n",
      "1%|          | 81/11697 [04:47<11:25:35,  3.54s/it]\n",
      "1%|          | 82/11697 [04:51<11:25:04,  3.54s/it]\n",
      "1%|          | 83/11697 [04:54<11:26:22,  3.55s/it]\n",
      "1%|          | 84/11697 [04:58<11:27:10,  3.55s/it]\n",
      "1%|          | 85/11697 [05:01<11:26:51,  3.55s/it]\n",
      "1%|          | 86/11697 [05:05<11:27:00,  3.55s/it]\n",
      "1%|          | 87/11697 [05:09<11:27:24,  3.55s/it]\n",
      "1%|          | 88/11697 [05:12<11:27:39,  3.55s/it]\n",
      "1%|          | 89/11697 [05:16<11:27:11,  3.55s/it]\n",
      "1%|          | 90/11697 [05:19<11:27:13,  3.55s/it]\n",
      "{'loss': 0.278, 'learning_rate': 4.963238437206122e-06, 'epoch': 0.02}\n",
      "1%|          | 90/11697 [05:19<11:27:13,  3.55s/it]\n",
      "1%|          | 91/11697 [05:23<11:29:51,  3.57s/it]\n",
      "1%|          | 92/11697 [05:26<11:32:10,  3.58s/it]\n",
      "1%|          | 93/11697 [05:30<11:32:02,  3.58s/it]\n",
      "1%|          | 94/11697 [05:34<11:31:33,  3.58s/it]\n",
      "1%|          | 95/11697 [05:37<11:31:06,  3.57s/it]\n",
      "1%|          | 96/11697 [05:41<11:30:14,  3.57s/it]\n",
      "1%|          | 97/11697 [05:44<11:30:15,  3.57s/it]\n",
      "1%|          | 98/11697 [05:48<11:29:55,  3.57s/it]\n",
      "1%|          | 99/11697 [05:51<11:30:19,  3.57s/it]\n",
      "1%|          | 100/11697 [05:55<11:32:22,  3.58s/it]\n",
      "1%|          | 101/11697 [05:59<11:35:33,  3.60s/it]\n",
      "1%|          | 102/11697 [06:02<11:31:02,  3.58s/it]\n",
      "1%|          | 103/11697 [06:06<11:32:57,  3.59s/it]\n",
      "1%|          | 104/11697 [06:09<11:32:18,  3.58s/it]\n",
      "1%|          | 105/11697 [06:13<11:32:19,  3.58s/it]\n",
      "1%|          | 106/11697 [06:17<11:34:22,  3.59s/it]\n",
      "1%|          | 107/11697 [06:20<11:30:43,  3.58s/it]\n",
      "1%|          | 108/11697 [06:24<11:41:39,  3.63s/it]\n",
      "1%|          | 109/11697 [06:27<11:38:28,  3.62s/it]\n",
      "1%|          | 110/11697 [06:31<11:32:07,  3.58s/it]\n",
      "1%|          | 111/11697 [06:34<11:26:42,  3.56s/it]\n",
      "1%|          | 112/11697 [06:38<11:23:30,  3.54s/it]\n",
      "1%|          | 113/11697 [06:41<11:20:04,  3.52s/it]\n",
      "1%|          | 114/11697 [06:45<11:17:28,  3.51s/it]\n",
      "1%|          | 115/11697 [06:48<11:16:11,  3.50s/it]\n",
      "1%|          | 116/11697 [06:52<11:15:27,  3.50s/it]\n",
      "1%|          | 117/11697 [06:55<11:15:21,  3.50s/it]\n",
      "1%|          | 118/11697 [06:59<11:14:33,  3.50s/it]\n",
      "1%|          | 119/11697 [07:02<11:17:20,  3.51s/it]\n",
      "1%|          | 120/11697 [07:06<11:16:19,  3.51s/it]\n",
      "{'loss': 0.2816, 'learning_rate': 4.950414636231512e-06, 'epoch': 0.03}\n",
      "1%|          | 120/11697 [07:06<11:16:19,  3.51s/it]\n",
      "1%|          | 121/11697 [07:09<11:15:31,  3.50s/it]\n",
      "1%|          | 122/11697 [07:13<11:15:01,  3.50s/it]\n",
      "1%|          | 123/11697 [07:16<11:14:22,  3.50s/it]\n",
      "1%|          | 124/11697 [07:20<11:14:27,  3.50s/it]\n",
      "1%|          | 125/11697 [07:23<11:20:48,  3.53s/it]\n",
      "1%|          | 126/11697 [07:27<11:22:17,  3.54s/it]\n",
      "1%|          | 127/11697 [07:31<11:31:31,  3.59s/it]\n",
      "1%|          | 128/11697 [07:34<11:29:48,  3.58s/it]\n",
      "1%|          | 129/11697 [07:38<11:28:47,  3.57s/it]\n",
      "1%|          | 130/11697 [07:41<11:27:22,  3.57s/it]\n",
      "1%|          | 131/11697 [07:45<11:26:22,  3.56s/it]\n",
      "1%|          | 132/11697 [07:49<11:25:47,  3.56s/it]\n",
      "1%|          | 133/11697 [07:52<11:25:20,  3.56s/it]\n",
      "1%|          | 134/11697 [07:56<11:25:10,  3.56s/it]\n",
      "1%|          | 135/11697 [07:59<11:24:38,  3.55s/it]\n",
      "1%|          | 136/11697 [08:03<11:24:03,  3.55s/it]\n",
      "1%|          | 137/11697 [08:06<11:23:18,  3.55s/it]\n",
      "1%|          | 138/11697 [08:10<11:22:50,  3.54s/it]\n",
      "1%|          | 139/11697 [08:13<11:22:51,  3.54s/it]\n",
      "1%|          | 140/11697 [08:17<11:22:46,  3.54s/it]\n",
      "1%|          | 141/11697 [08:20<11:23:21,  3.55s/it]\n",
      "1%|          | 142/11697 [08:24<11:24:29,  3.55s/it]\n",
      "1%|          | 143/11697 [08:28<11:24:17,  3.55s/it]\n",
      "1%|          | 144/11697 [08:31<11:24:05,  3.55s/it]\n",
      "1%|          | 145/11697 [08:35<11:23:39,  3.55s/it]\n",
      "1%|          | 146/11697 [08:38<11:24:23,  3.55s/it]\n",
      "1%|▏         | 147/11697 [08:42<11:23:50,  3.55s/it]\n",
      "1%|▏         | 148/11697 [08:45<11:23:47,  3.55s/it]\n",
      "1%|▏         | 149/11697 [08:49<11:23:12,  3.55s/it]\n",
      "1%|▏         | 150/11697 [08:52<11:23:31,  3.55s/it]\n",
      "{'loss': 0.2943, 'learning_rate': 4.937590835256904e-06, 'epoch': 0.04}\n",
      "1%|▏         | 150/11697 [08:52<11:23:31,  3.55s/it]\n",
      "1%|▏         | 151/11697 [08:56<11:23:35,  3.55s/it]\n",
      "1%|▏         | 152/11697 [09:00<11:22:56,  3.55s/it]\n",
      "1%|▏         | 153/11697 [09:03<11:23:28,  3.55s/it]\n",
      "1%|▏         | 154/11697 [09:07<11:23:20,  3.55s/it]\n",
      "1%|▏         | 155/11697 [09:10<11:22:44,  3.55s/it]\n",
      "1%|▏         | 156/11697 [09:14<11:23:00,  3.55s/it]\n",
      "1%|▏         | 157/11697 [09:17<11:22:35,  3.55s/it]\n",
      "1%|▏         | 158/11697 [09:21<11:22:07,  3.55s/it]\n",
      "1%|▏         | 159/11697 [09:24<11:25:08,  3.56s/it]\n",
      "1%|▏         | 160/11697 [09:28<11:27:08,  3.57s/it]\n",
      "1%|▏         | 161/11697 [09:32<11:26:46,  3.57s/it]\n",
      "1%|▏         | 162/11697 [09:35<11:23:59,  3.56s/it]\n",
      "1%|▏         | 163/11697 [09:39<11:22:04,  3.55s/it]\n",
      "1%|▏         | 164/11697 [09:42<11:21:59,  3.55s/it]\n",
      "1%|▏         | 165/11697 [09:46<11:20:16,  3.54s/it]\n",
      "1%|▏         | 166/11697 [09:49<11:19:27,  3.54s/it]\n",
      "1%|▏         | 167/11697 [09:53<11:19:04,  3.53s/it]\n",
      "1%|▏         | 168/11697 [09:56<11:23:08,  3.56s/it]\n",
      "1%|▏         | 169/11697 [10:00<11:21:28,  3.55s/it]\n",
      "1%|▏         | 170/11697 [10:03<11:20:55,  3.54s/it]\n",
      "1%|▏         | 171/11697 [10:07<11:24:00,  3.56s/it]\n",
      "1%|▏         | 172/11697 [10:11<11:22:34,  3.55s/it]\n",
      "1%|▏         | 173/11697 [10:14<11:25:33,  3.57s/it]\n",
      "1%|▏         | 174/11697 [10:18<11:23:11,  3.56s/it]\n",
      "1%|▏         | 175/11697 [10:21<11:31:51,  3.60s/it]\n",
      "2%|▏         | 176/11697 [10:25<11:28:19,  3.58s/it]\n",
      "2%|▏         | 177/11697 [10:28<11:25:41,  3.57s/it]\n",
      "2%|▏         | 178/11697 [10:32<11:32:26,  3.61s/it]\n",
      "2%|▏         | 179/11697 [10:36<11:37:11,  3.63s/it]\n",
      "2%|▏         | 180/11697 [10:40<11:38:06,  3.64s/it]\n",
      "{'loss': 0.2698, 'learning_rate': 4.924767034282295e-06, 'epoch': 0.05}\n",
      "2%|▏         | 180/11697 [10:40<11:38:06,  3.64s/it]\n",
      "2%|▏         | 181/11697 [10:43<11:37:12,  3.63s/it]\n",
      "2%|▏         | 182/11697 [10:47<11:37:12,  3.63s/it]\n",
      "2%|▏         | 183/11697 [10:50<11:40:39,  3.65s/it]\n",
      "2%|▏         | 184/11697 [10:54<11:40:59,  3.65s/it]\n",
      "2%|▏         | 185/11697 [10:58<11:40:41,  3.65s/it]\n",
      "2%|▏         | 186/11697 [11:01<11:43:13,  3.67s/it]\n",
      "2%|▏         | 187/11697 [11:05<11:40:36,  3.65s/it]\n",
      "2%|▏         | 188/11697 [11:09<11:38:52,  3.64s/it]\n",
      "2%|▏         | 189/11697 [11:12<11:32:14,  3.61s/it]\n",
      "2%|▏         | 190/11697 [11:16<11:27:28,  3.58s/it]\n",
      "2%|▏         | 191/11697 [11:19<11:27:28,  3.58s/it]\n",
      "2%|▏         | 192/11697 [11:23<11:24:27,  3.57s/it]\n",
      "2%|▏         | 193/11697 [11:26<11:20:53,  3.55s/it]\n",
      "2%|▏         | 194/11697 [11:30<11:20:15,  3.55s/it]\n",
      "2%|▏         | 195/11697 [11:34<11:22:55,  3.56s/it]\n",
      "2%|▏         | 196/11697 [11:37<11:18:43,  3.54s/it]\n",
      "2%|▏         | 197/11697 [11:41<11:17:06,  3.53s/it]\n",
      "2%|▏         | 198/11697 [11:44<11:16:55,  3.53s/it]\n",
      "2%|▏         | 199/11697 [11:48<11:22:53,  3.56s/it]\n",
      "2%|▏         | 200/11697 [11:51<11:21:09,  3.55s/it]\n",
      "2%|▏         | 201/11697 [11:55<11:26:56,  3.59s/it]\n",
      "2%|▏         | 202/11697 [11:58<11:23:24,  3.57s/it]\n",
      "2%|▏         | 203/11697 [12:02<11:26:29,  3.58s/it]\n",
      "2%|▏         | 204/11697 [12:06<11:30:04,  3.60s/it]\n",
      "2%|▏         | 205/11697 [12:09<11:23:58,  3.57s/it]\n",
      "2%|▏         | 206/11697 [12:13<11:21:50,  3.56s/it]\n",
      "2%|▏         | 207/11697 [12:16<11:18:32,  3.54s/it]\n",
      "2%|▏         | 208/11697 [12:20<11:25:22,  3.58s/it]\n",
      "2%|▏         | 209/11697 [12:23<11:21:59,  3.56s/it]\n",
      "2%|▏         | 210/11697 [12:27<11:19:05,  3.55s/it]\n",
      "{'loss': 0.2717, 'learning_rate': 4.911943233307686e-06, 'epoch': 0.05}\n",
      "2%|▏         | 210/11697 [12:27<11:19:05,  3.55s/it]\n",
      "2%|▏         | 211/11697 [12:30<11:18:30,  3.54s/it]\n",
      "2%|▏         | 212/11697 [12:34<11:17:09,  3.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sagemaker/estimator.py:1504\u001b[0m, in \u001b[0;36mEstimatorBase.logs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1499\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display the logs for Estimator's training job.\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \n\u001b[1;32m   1501\u001b[0m \u001b[39m    If the output is a tty or a Jupyter cell, it will be color-coded based\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[39m    on which instance the log entry is from.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1504\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_training_job\u001b[39m.\u001b[39;49mname, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sagemaker/session.py:5457\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5436\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogs_for_job\u001b[39m(\u001b[39mself\u001b[39m, job_name, wait\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, poll\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, log_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m\"\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   5437\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5438\u001b[0m \n\u001b[1;32m   5439\u001b[0m \u001b[39m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5455\u001b[0m \u001b[39m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5456\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5457\u001b[0m     _logs_for_job(\u001b[39mself\u001b[39;49m, job_name, wait, poll, log_type, timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sagemaker/session.py:7420\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   7417\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m LogState\u001b[39m.\u001b[39mCOMPLETE:\n\u001b[1;32m   7418\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 7420\u001b[0m time\u001b[39m.\u001b[39;49msleep(poll)\n\u001b[1;32m   7422\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m LogState\u001b[39m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   7423\u001b[0m     state \u001b[39m=\u001b[39m LogState\u001b[39m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "huggingface_estimator.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b8e647a79df62bf31906a725b05de775d285962ac600487339d38c51a5c07b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
