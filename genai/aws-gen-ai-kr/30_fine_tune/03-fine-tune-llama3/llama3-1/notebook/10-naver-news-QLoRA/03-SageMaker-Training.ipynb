{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker 에서 Llama 3 파인 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "\n",
    "is_sagemaker_notebook = True\n",
    "#is_sagemaker_notebook = False # use VS Code\n",
    "\n",
    "if is_sagemaker_notebook:\n",
    "    hf_key_name = \"HF_KEY\"\n",
    "    key_val = \"<Type Your HF Key>\"\n",
    "    set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    HF_TOKEN = get_hf_key_env_vars(hf_key_name)\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "    print(\"token: \", HF_TOKEN)\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장된 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder:  ../data/naver-news-summarization-ko\n",
      "train_data_json:  ../data/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_data_json:  ../data/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_data_json:  ../data/naver-news-summarization-ko/test/test_dataset.json\n",
      "full_train_data_json:  ../data/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "full_validation_data_json:  ../data/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "full_test_data_json:  ../data/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%store -r data_folder\n",
    "%store -r train_data_json \n",
    "%store -r validation_data_json \n",
    "%store -r test_data_json \n",
    "%store -r full_train_data_json \n",
    "%store -r full_validation_data_json \n",
    "%store -r full_test_data_json\n",
    "\n",
    "\n",
    "print(\"data_folder: \", data_folder)\n",
    "print(\"train_data_json: \", train_data_json)\n",
    "print(\"validation_data_json: \", validation_data_json)\n",
    "print(\"test_data_json: \", test_data_json)\n",
    "print(\"full_train_data_json: \", full_train_data_json)\n",
    "print(\"full_validation_data_json: \", full_validation_data_json)\n",
    "print(\"full_test_data_json: \", full_test_data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker 기본 변수 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::419974056037:role/service-role/AmazonSageMaker-ExecutionRole-20221206T163436\n",
      "sagemaker bucket: sagemaker-us-west-2-419974056037\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비\n",
    "- 두가지 종류의 데이터 셋을 업로드 합니다.\n",
    "    - Full Dataset: 전체 데이타를 업로드 합니다.\n",
    "    - Sample Dataset: 디버깅 용도의 일부 데이타를 업로드 합니다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 데이터 셋 경로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_path: \n",
      " s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko\n",
      "train_dataset_s3_path: \n",
      " s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_dataset_s3_path: \n",
      " s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_dataset_s3_path: \n",
      " s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/test/test_dataset.json\n",
      "\n",
      "input_path: \n",
      " s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko\n",
      "train_dataset_s3_path: \n",
      " s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "validation_dataset_s3_path: \n",
      " s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "test_dataset_s3_path: \n",
      " s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "def create_s3_path(sess,is_full, data_folder,train_data_json,validation_data_json,test_data_json,verbose=True  ):\n",
    "    dataset_name = data_folder.split('/')[-1]\n",
    "    # save train_dataset to s3 using our SageMaker session\n",
    "    input_path = f's3://{sess.default_bucket()}/datasets/{dataset_name}'\n",
    "    print(\"input_path: \\n\", input_path)\n",
    "\n",
    "    trian_file_name = train_data_json.split('/')[-1]\n",
    "    validation_file_name = validation_data_json.split('/')[-1]\n",
    "    test_file_name = test_data_json.split('/')[-1]\n",
    "\n",
    "    if is_full:\n",
    "        train_dataset_s3_path = f\"{input_path}/full_train/{trian_file_name}\"\n",
    "        validation_dataset_s3_path = f\"{input_path}/full_validation/{validation_file_name}\"\n",
    "        test_dataset_s3_path = f\"{input_path}/full_test/{test_file_name}\"\n",
    "    else:\n",
    "        train_dataset_s3_path = f\"{input_path}/train/{trian_file_name}\"\n",
    "        validation_dataset_s3_path = f\"{input_path}/validation/{validation_file_name}\"\n",
    "        test_dataset_s3_path = f\"{input_path}/test/{test_file_name}\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"train_dataset_s3_path: \\n\", train_dataset_s3_path)\n",
    "        print(\"validation_dataset_s3_path: \\n\", validation_dataset_s3_path)\n",
    "        print(\"test_dataset_s3_path: \\n\", test_dataset_s3_path)\n",
    "\n",
    "    return train_dataset_s3_path, validation_dataset_s3_path, test_dataset_s3_path, input_path\n",
    "\n",
    "train_dataset_s3_path, validation_dataset_s3_path, test_dataset_s3_path, input_path = create_s3_path(\n",
    "                                                                            sess=sess,\n",
    "                                                                            is_full = False,\n",
    "                                                                            data_folder=data_folder,\n",
    "                                                                            train_data_json=train_data_json,\n",
    "                                                                            validation_data_json=validation_data_json,\n",
    "                                                                            test_data_json=test_data_json)    \n",
    "print(\"\")\n",
    "full_train_dataset_s3_path, full_validation_dataset_s3_path, full_test_dataset_s3_path, input_path = create_s3_path(\n",
    "                                                                            sess=sess,\n",
    "                                                                            is_full = True,\n",
    "                                                                            data_folder=data_folder,\n",
    "                                                                            train_data_json=full_train_data_json,\n",
    "                                                                            validation_data_json=full_validation_data_json,\n",
    "                                                                            test_data_json=full_test_data_json)    \n",
    "\n",
    "# full_train_data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이타를 S3 에 업로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_s3_prefix_name(s3_path, verbose=True):\n",
    "    file_name = s3_path.split('/')[-1]\n",
    "    file_name = '/' + file_name\n",
    "    desired_s3_uri = s3_path.split(file_name)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"file_name: \", file_name)\n",
    "        print(\"desired_s3_uri: \", desired_s3_uri)\n",
    "    return desired_s3_uri\n",
    "\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "def upload_data_s3(desired_s3_uri, file_name, verbose=True):\n",
    "    # upload the model yaml file to s3\n",
    "    \n",
    "    file_s3_path = S3Uploader.upload(local_path=file_name, desired_s3_uri=desired_s3_uri)\n",
    "\n",
    "    print(f\"{file_name} is uploaded to:\")\n",
    "    print(file_s3_path)\n",
    "\n",
    "    return file_s3_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug 용 작은 데이터셋 S3 업로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name:  /train_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/train\n",
      "../data/naver-news-summarization-ko/train/train_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/train/train_dataset.json\n",
      "\n",
      "file_name:  /validation_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/validation\n",
      "../data/naver-news-summarization-ko/validation/validation_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "\n",
      "file_name:  /test_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/test\n",
      "../data/naver-news-summarization-ko/test/test_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/test/test_dataset.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/test/test_dataset.json'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "######## Train File\n",
    "# return s3 URI, e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "train_desired_s3_uri = get_s3_prefix_name(train_dataset_s3_path)    \n",
    "# upload local file to e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "upload_data_s3(desired_s3_uri=train_desired_s3_uri, file_name=train_data_json, verbose=True)\n",
    "######## Validation File\n",
    "print(\"\")\n",
    "validation_desired_s3_uri = get_s3_prefix_name(validation_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=validation_desired_s3_uri, file_name=validation_data_json, verbose=True)\n",
    "######## Test File\n",
    "print(\"\")\n",
    "test_desired_s3_uri = get_s3_prefix_name(test_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=test_desired_s3_uri, file_name=test_data_json, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가용 큰 데이터셋 S3 업로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name:  /train_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/full_train\n",
      "../data/naver-news-summarization-ko/full_train/train_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "\n",
      "file_name:  /validation_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/full_validation\n",
      "../data/naver-news-summarization-ko/full_validation/validation_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "\n",
      "file_name:  /test_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/full_test\n",
      "../data/naver-news-summarization-ko/full_test/test_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/full_test/test_dataset.json'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "######## Train File\n",
    "# return s3 URI, e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "full_train_desired_s3_uri = get_s3_prefix_name(full_train_dataset_s3_path)    \n",
    "# upload local file to e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "upload_data_s3(desired_s3_uri=full_train_desired_s3_uri, file_name=full_train_data_json, verbose=True)\n",
    "######## Validation File\n",
    "print(\"\")\n",
    "full_validation_desired_s3_uri = get_s3_prefix_name(full_validation_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=full_validation_desired_s3_uri, file_name=full_validation_data_json, verbose=True)\n",
    "######## Test File\n",
    "print(\"\")\n",
    "full_test_desired_s3_uri = get_s3_prefix_name(full_test_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=full_test_desired_s3_uri, file_name=full_test_data_json, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 업로드 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-28 02:46:15    2.2 KiB datasets/naver-news-summarization-ko/config/sm_llama_3_1_8b_fsdp_qlora.yaml\n",
      "2024-09-28 03:00:06    2.2 KiB datasets/naver-news-summarization-ko/config/sm_llama_3_1_8b_qlora.yaml\n",
      "2024-09-28 03:44:11    2.4 KiB datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml\n",
      "2024-09-28 04:13:14    8.4 MiB datasets/naver-news-summarization-ko/full_test/test_dataset.json\n",
      "2024-09-28 04:13:12   68.0 MiB datasets/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "2024-09-28 04:13:14    7.6 MiB datasets/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "2024-09-28 04:13:12   33.4 KiB datasets/naver-news-summarization-ko/test/test_dataset.json\n",
      "2024-09-28 04:13:12   28.1 KiB datasets/naver-news-summarization-ko/train/train_dataset.json\n",
      "2024-09-28 04:13:12   26.1 KiB datasets/naver-news-summarization-ko/validation/validation_dataset.json\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {input_path}  --recursive --human-readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! aws s3 rm {input_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! aws s3 cp {train_data_json} {train_dataset_s3_path}\n",
    "# ! aws s3 cp {validation_data_json} {validation_dataset_s3_path}\n",
    "# ! aws s3 cp {test_data_json} {test_dataset_s3_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 훈련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "config_folder_name = \"accelerator_config\"\n",
    "os.makedirs(config_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 설정 파일 준비\n",
    "- 목적에 맞게 아래의 두 개의 부분을 주석을 이용하여 사용 하세요.\n",
    "    - For Debug 부분은 일부 샘플 데이타를 통해서 빠르게 디버깅 목적의 파라미터 값 입니다.\n",
    "    - For evaluation: 전체 데이터를 통해서 최적의 파라미터 값 입니다.\n",
    "```\n",
    "###########################             \n",
    "# For Debug\n",
    "###########################             \n",
    "num_train_epochs: 5                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "# For evaluation\n",
    "###########################             \n",
    "# num_train_epochs: 3                    # number of training epochs\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 8          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerator_config/sm_llama_3_1_8b_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerator_config/sm_llama_3_1_8b_qlora.yaml\n",
    "# script parameters\n",
    "#model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "model_id: \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "max_seq_length:  2048              # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\" # path to where SageMaker saves train dataset\n",
    "validation_dataset_path: \"/opt/ml/input/data/validation/\" # path to where SageMaker saves train dataset\n",
    "#test_dataset_path: \"/opt/ml/input/data/test/\"   # path to where SageMaker saves test dataset\n",
    "output_dir: \"/tmp/llama3-1\"            # where the LoRA adapter weight is\n",
    "# training parameters\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "###########################             \n",
    "# For Debug\n",
    "###########################             \n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "# For evaluation\n",
    "###########################             \n",
    "# num_train_epochs: 3                    # number of training epochs\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 8          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정 파일을 S3 에 업로드\n",
    "- 위에 정의한 파일을 업로드 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerator_config/sm_llama_3_1_8b_qlora.yaml is uploaded to:\n",
      "s3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/config/sm_llama_3_1_8b_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_desired_s3_uri = f\"{input_path}/config\"\n",
    "config_model_name = \"accelerator_config/sm_llama_3_1_8b_qlora.yaml\"\n",
    "train_config_s3_path = upload_data_s3(desired_s3_uri=config_desired_s3_uri, file_name=config_model_name, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 입력 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 사이즈 조정 \n",
    "- 디버그 용도이면 run_debug_sample = True, 전데 데이터 이면 False 로 조절 하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/train/train_dataset.json',\n",
       " 'validation': 's3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/validation/validation_dataset.json',\n",
       " 'config': 's3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/config/sm_llama_3_1_8b_qlora.yaml'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_debug_sample = True\n",
    "# run_debug_sample = False\n",
    "if run_debug_sample:\n",
    "  local_data = {\n",
    "    'train': f'file://{train_data_json}',\n",
    "    'validation': f'file://{validation_data_json}',\n",
    "    'config': f'file://{config_model_name}'\n",
    "    }\n",
    "\n",
    "  s3_data = {\n",
    "    'train': train_dataset_s3_path,\n",
    "    'validation': validation_dataset_s3_path,\n",
    "    'config': train_config_s3_path\n",
    "    }  \n",
    "else:\n",
    "  local_data = {\n",
    "    'train': f'file://{train_data_json}',\n",
    "    'validation': f'file://{validation_data_json}',\n",
    "    'config': f'file://{config_model_name}'\n",
    "    }\n",
    "  s3_data = {\n",
    "    'train': full_train_dataset_s3_path,\n",
    "    'validation': full_validation_dataset_s3_path,\n",
    "    'config': train_config_s3_path\n",
    "    }  \n",
    "s3_data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clolud 모드 및 Local 사용\n",
    "- 현재 로컬 모드는 에러 발행. 확인 중 임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Cloud mode is set with ml.g5.12xlarge and 1 of instance_count\n",
      "dataset: \n",
      " {'train': 's3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/train/train_dataset.json', 'validation': 's3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/validation/validation_dataset.json', 'config': 's3://sagemaker-us-west-2-419974056037/datasets/naver-news-summarization-ko/config/sm_llama_3_1_8b_qlora.yaml'}\n"
     ]
    }
   ],
   "source": [
    "#USE_LOCAL_MODE = True\n",
    "USE_LOCAL_MODE = False\n",
    "\n",
    "import torch\n",
    "\n",
    "if USE_LOCAL_MODE:\n",
    "    instance_type = 'local_gpu' if torch.cuda.is_available() else 'local'\n",
    "    instance_count = 1\n",
    "    from sagemaker.local import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    data = local_data \n",
    "    # data = s3_data\n",
    "    metric_definitions = None\n",
    "    nKeepAliveSeconds = None # Warmpool feature\n",
    "    print(\"## Local mode is set\")\n",
    "else:\n",
    "    #instance_type = 'ml.g5.4xlarge'\n",
    "    instance_type = 'ml.g5.12xlarge'\n",
    "    # instance_type = 'ml.g5.48xlarge'\n",
    "    # instance_type = 'ml.p4d.24xlarge'\n",
    "    # Emit: \n",
    "    # {'train_runtime': 37.2985, 'train_samples_per_second': 0.375, 'train_steps_per_second': 0.054, 'train_loss': 2.3541293144226074, 'epoch': 1.0}\n",
    "    # {'eval_loss': 2.50766658782959, 'eval_runtime': 3.4741, 'eval_samples_per_second': 3.454, 'eval_steps_per_second': 0.864, 'epoch': 1.0}\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"train:loss\", \"Regex\": \"'train_loss':(.*?),\"},\n",
    "        {\"Name\": \"validation:loss\", \"Regex\": \"'eval_loss':(.*?),\"}\n",
    "    ]\n",
    "    instance_count = 1\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    data = s3_data\n",
    "    nKeepAliveSeconds = 3600 # Warmpool feature, 1 hour\n",
    "    print(f\"## Cloud mode is set with {instance_type} and {instance_count} of instance_count\")\n",
    "print(\"dataset: \\n\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 Estimator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/finetune_image/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "import time\n",
    "# define Training Job Name \n",
    "job_name = f'llama3-1-8b-naver-news-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "# chkpt_s3_path = f's3://{sess.default_bucket()}/{s3_prefix}/native/checkpoints'\n",
    "\n",
    "# create the Estimator\n",
    "os.environ['USE_SHORT_LIVED_CREDENTIALS']=\"1\" \n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'sm_run_qlora_llama3_1.py',      # train script\n",
    "    source_dir           = '../../scripts',  # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type,  # instances type used for the training job\n",
    "    instance_count       = instance_count,                 # the number of instances used for training\n",
    "    sagemaker_session    = sagemaker_session,\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 256,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',          # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    metric_definitions = metric_definitions,\n",
    "    hyperparameters      =  {\n",
    "        \"config\": \"/opt/ml/input/data/config/sm_llama_3_1_8b_qlora.yaml\" # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost    \n",
    "    keep_alive_period_in_seconds = nKeepAliveSeconds,     # warm pool \n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\n",
    "    environment  = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HF_TOKEN,       # huggingface token to access gated models, e.g. llama 3\n",
    "        #\"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\n",
    "        #\"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\"   # enable CPU RAM efficient loading\n",
    "    }, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 훈련 실행\n",
    "- 로컬 모드시에는 모델 저장을 하지 않습니다. 훈련 스크립트에서 처리 합니다. (현재 모델 저장시에 /tmp 의 용량이 차서 에러가 발생 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_name:test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.experiments.run:The run (training-job-experiment) under experiment (test) already exists. Loading it.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: llama3-1-8b-naver-news-2024-09-28-04-13-2024-09-28-04-13-23-542\n"
     ]
    }
   ],
   "source": [
    "if USE_LOCAL_MODE:\n",
    "    huggingface_estimator.fit(data, wait=False)\n",
    "else:\n",
    "    from sagemaker.experiments.run import Run\n",
    "    from sagemaker.utils import unique_name_from_base\n",
    "    from sagemaker.session import Session\n",
    "\n",
    "    # set new experiment configuration\n",
    "    # naver-news-summarization-ko\n",
    "    experiment_name = data_folder.split('/')[-1] + '-1'\n",
    "    experiment_name = \"test\"\n",
    "    \n",
    "    run_name = f\"training-job-experiment\"\n",
    "    print(f\"experiment_name:{experiment_name}\")    \n",
    "\n",
    "    with Run(experiment_name=experiment_name, run_name=run_name, sagemaker_session=sagemaker_session) as run:\n",
    "        huggingface_estimator.fit(data, wait=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-28 04:13:24 Starting - Starting the training job\n",
      "2024-09-28 04:13:24 Pending - Training job waiting for capacity......\n",
      "2024-09-28 04:14:13 Pending - Preparing the instances for training......\n",
      "2024-09-28 04:15:11 Downloading - Downloading the training image..................\n",
      "2024-09-28 04:18:28 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:06,829 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:06,885 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:06,896 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:06,898 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:06,898 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:08,405 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting datasets==3.0.0 (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.11.1 (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.44.0 (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.12.0 (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.34.2 (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker==2.232.1 (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.232.1-py3-none-any.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.45.1 (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker-mlflow==0.1.0 (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl.metadata (3.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (17.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (2.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (2.32.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.0->-r requirements.txt (line 13)) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (3.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (0.24.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==3.0.0->-r requirements.txt (line 13)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.11.1->-r requirements.txt (line 14)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.11.1->-r requirements.txt (line 14)) (0.8.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0->-r requirements.txt (line 16)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0->-r requirements.txt (line 16)) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting boto3<2.0,>=1.34.142 (from sagemaker==2.232.1->-r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mDownloading boto3-1.35.29-py3-none-any.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (2.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (7.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (6.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (4.21.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (4.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (3.20.3)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker-core<2.0.0,>=1.0.0 (from sagemaker==2.232.1->-r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mDownloading sagemaker_core-1.0.9-py3-none-any.whl.metadata (4.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.232.1->-r requirements.txt (line 18)) (1.26.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.1->-r requirements.txt (line 19)) (2024.7.24)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.21,>=0.20 (from transformers==4.45.1->-r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting mlflow>=2.8 (from sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading mlflow-2.16.2-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.36.0,>=1.35.29 (from boto3<2.0,>=1.34.142->sagemaker==2.232.1->-r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mDownloading botocore-1.35.29-py3-none-any.whl.metadata (5.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker==2.232.1->-r requirements.txt (line 18)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker==2.232.1->-r requirements.txt (line 18)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.0.0->-r requirements.txt (line 13)) (2.3.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.0.0->-r requirements.txt (line 13)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.0.0->-r requirements.txt (line 13)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.0.0->-r requirements.txt (line 13)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.0.0->-r requirements.txt (line 13)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.0.0->-r requirements.txt (line 13)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets==3.0.0->-r requirements.txt (line 13)) (4.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.232.1->-r requirements.txt (line 18)) (3.19.2)\u001b[0m\n",
      "\u001b[34mCollecting mlflow-skinny==2.16.2 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading mlflow_skinny-2.16.2-py3-none-any.whl.metadata (30 kB)\u001b[0m\n",
      "\u001b[34mCollecting Flask<4 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic!=1.10.0,<2 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphene<4 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (3.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (1.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (1.12.0)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy<3,>=1.4.0 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (3.1.4)\u001b[0m\n",
      "\u001b[34mCollecting gunicorn<24 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading databricks_sdk-0.33.0-py3-none-any.whl.metadata (37 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.0.0->-r requirements.txt (line 13)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.0.0->-r requirements.txt (line 13)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.0.0->-r requirements.txt (line 13)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.0.0->-r requirements.txt (line 13)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.0.0->-r requirements.txt (line 13)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.0.0->-r requirements.txt (line 13)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<3.0.0,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker==2.232.1->-r requirements.txt (line 18)) (2.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker==2.232.1->-r requirements.txt (line 18)) (13.7.1)\u001b[0m\n",
      "\u001b[34mCollecting mock<5.0,>4.0 (from sagemaker-core<2.0.0,>=1.0.0->sagemaker==2.232.1->-r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mDownloading mock-4.0.3-py3-none-any.whl.metadata (2.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker==2.232.1->-r requirements.txt (line 18)) (2023.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker==2.232.1->-r requirements.txt (line 18)) (0.33.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker==2.232.1->-r requirements.txt (line 18)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.1->-r requirements.txt (line 14)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.1->-r requirements.txt (line 14)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.11.1->-r requirements.txt (line 14)) (0.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.11.1->-r requirements.txt (line 14)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker==2.232.1->-r requirements.txt (line 18)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker==2.232.1->-r requirements.txt (line 18)) (1.7.6.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker==2.232.1->-r requirements.txt (line 18)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker==2.232.1->-r requirements.txt (line 18)) (21.6.0)\u001b[0m\n",
      "\u001b[34mCollecting Mako (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (3.0.3)\u001b[0m\n",
      "\u001b[34mCollecting itsdangerous>=2.1.2 (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting blinker>=1.6.2 (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading graphql_core-3.2.4-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting aniso8601<10,>=8 (from graphene<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (10.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (3.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker==2.232.1->-r requirements.txt (line 18)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker==2.232.1->-r requirements.txt (line 18)) (2.16.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker==2.232.1->-r requirements.txt (line 18)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker==2.232.1->-r requirements.txt (line 18)) (2.17.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (1.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (3.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (3.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.11.1->-r requirements.txt (line 14)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker==2.232.1->-r requirements.txt (line 18)) (0.1.2)\u001b[0m\n",
      "\u001b[34mCollecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 21)) (0.5.1)\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\u001b[0m\n",
      "\u001b[34mDownloading trl-0.11.1-py3-none-any.whl (318 kB)\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.4/122.4 MB 117.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.12.0-py3-none-any.whl (296 kB)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.232.1-py3-none-any.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 81.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 103.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mDownloading boto3-1.35.29-py3-none-any.whl (139 kB)\u001b[0m\n",
      "\u001b[34mDownloading mlflow-2.16.2-py3-none-any.whl (26.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/26.7 MB 122.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mlflow_skinny-2.16.2-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 111.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sagemaker_core-1.0.9-py3-none-any.whl (384 kB)\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 90.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[34mDownloading botocore-1.35.29-py3-none-any.whl (12.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.6/12.6 MB 117.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading flask-3.0.3-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34mDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34mDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34mDownloading mock-4.0.3-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 113.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\u001b[0m\n",
      "\u001b[34mDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mDownloading databricks_sdk-0.33.0-py3-none-any.whl (562 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 563.0/563.0 kB 16.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34mDownloading graphql_core-3.2.4-py3-none-any.whl (203 kB)\u001b[0m\n",
      "\u001b[34mDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\u001b[0m\n",
      "\u001b[34mDownloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34mDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: aniso8601, wrapt, sqlparse, sqlalchemy, smmap, pyasn1-modules, mock, Mako, itsdangerous, gunicorn, graphql-core, cachetools, blinker, graphql-relay, google-auth, gitdb, Flask, deprecated, botocore, alembic, tokenizers, opentelemetry-api, graphene, gitpython, databricks-sdk, bitsandbytes, accelerate, transformers, opentelemetry-semantic-conventions, datasets, boto3, trl, sagemaker-core, peft, opentelemetry-sdk, sagemaker, mlflow-skinny, mlflow, sagemaker-mlflow\u001b[0m\n",
      "\u001b[34mAttempting uninstall: botocore\u001b[0m\n",
      "\u001b[34mFound existing installation: botocore 1.34.52\u001b[0m\n",
      "\u001b[34mUninstalling botocore-1.34.52:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled botocore-1.34.52\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.15.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.15.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: bitsandbytes\u001b[0m\n",
      "\u001b[34mFound existing installation: bitsandbytes 0.43.3\u001b[0m\n",
      "\u001b[34mUninstalling bitsandbytes-0.43.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled bitsandbytes-0.43.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.25.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.25.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.25.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.18.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.18.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.18.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: boto3\u001b[0m\n",
      "\u001b[34mFound existing installation: boto3 1.34.52\u001b[0m\n",
      "\u001b[34mUninstalling boto3-1.34.52:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled boto3-1.34.52\u001b[0m\n",
      "\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.7.4\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.7.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.7.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.7.1\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.224.4\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.224.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.224.4\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mawscli 1.32.52 requires botocore==1.34.52, but you have botocore 1.35.29 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Flask-3.0.3 Mako-1.3.5 accelerate-0.34.2 alembic-1.13.3 aniso8601-9.0.1 bitsandbytes-0.44.0 blinker-1.8.2 boto3-1.35.29 botocore-1.35.29 cachetools-5.5.0 databricks-sdk-0.33.0 datasets-3.0.0 deprecated-1.2.14 gitdb-4.0.11 gitpython-3.1.43 google-auth-2.35.0 graphene-3.3 graphql-core-3.2.4 graphql-relay-3.2.0 gunicorn-23.0.0 itsdangerous-2.2.0 mlflow-2.16.2 mlflow-skinny-2.16.2 mock-4.0.3 opentelemetry-api-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 peft-0.12.0 pyasn1-modules-0.4.1 sagemaker-2.232.1 sagemaker-core-1.0.9 sagemaker-mlflow-0.1.0 smmap-5.0.1 sqlalchemy-2.0.35 sqlparse-0.5.1 tokenizers-0.20.0 transformers-4.45.1 trl-0.11.1 wrapt-1.16.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:33,103 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:33,103 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:33,181 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:33,251 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:33,264 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:33,323 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-28 04:19:33,335 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"config\": \"/opt/ml/input/data/config\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config\": \"/opt/ml/input/data/config/sm_llama_3_1_8b_qlora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"config\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"llama3-1-8b-naver-news-2024-09-28-04-13-2024-09-28-04-13-23-542\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-419974056037/llama3-1-8b-naver-news-2024-09-28-04-13-2024-09-28-04-13-23-542/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sm_run_qlora_llama3_1\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sm_run_qlora_llama3_1.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"/opt/ml/input/data/config/sm_llama_3_1_8b_qlora.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sm_run_qlora_llama3_1.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"config\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sm_run_qlora_llama3_1\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-419974056037/llama3-1-8b-naver-news-2024-09-28-04-13-2024-09-28-04-13-23-542/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"config\":\"/opt/ml/input/data/config\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/input/data/config/sm_llama_3_1_8b_qlora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama3-1-8b-naver-news-2024-09-28-04-13-2024-09-28-04-13-23-542\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-419974056037/llama3-1-8b-naver-news-2024-09-28-04-13-2024-09-28-04-13-23-542/source/sourcedir.tar.gz\",\"module_name\":\"sm_run_qlora_llama3_1\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sm_run_qlora_llama3_1.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"/opt/ml/input/data/config/sm_llama_3_1_8b_qlora.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CONFIG=/opt/ml/input/data/config\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=/opt/ml/input/data/config/sm_llama_3_1_8b_qlora.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 1 --nproc_per_node 4 sm_run_qlora_llama3_1.py --config /opt/ml/input/data/config/sm_llama_3_1_8b_qlora.yaml\u001b[0m\n",
      "\u001b[34m[2024-09-28 04:19:34,725] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-09-28 04:19:34,725] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-09-28 04:19:34,725] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-09-28 04:19:34,725] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m## SM_CURRENT_INSTANCE_TYPE:  ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34m## script_args: \n",
      " ScriptArguments(train_dataset_path='/opt/ml/input/data/train/', validation_dataset_path='/opt/ml/input/data/validation/', model_id='meta-llama/Meta-Llama-3.1-8B-Instruct', max_seq_length=2048)\u001b[0m\n",
      "\u001b[34m## training_args:\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbatch_eval_metrics=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_do_concat_batches=True,\u001b[0m\n",
      "\u001b[34meval_on_start=False,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34meval_strategy=epoch,\u001b[0m\n",
      "\u001b[34meval_use_gather_object=False,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0002,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/tmp/llama3-1/runs/Sep28_04-19-38_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=constant,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=0.3,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/tmp/llama3-1,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=1,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34mrestore_callback_states_from_checkpoint=False,\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/tmp/llama3-1,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=epoch,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=True,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorch_empty_cache_steps=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_liger_kernel=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.03,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10 examples [00:00, 4289.09 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10 examples [00:00, 7770.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10/10 [00:00<00:00, 985.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10/10 [00:00<00:00, 2573.82 examples/s]\u001b[0m\n",
      "\u001b[34mYou are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m국내 최대규모 나노 전시회이자 세계 3대 나노행사 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀 에서 개최된다. 올해 20회를 맞는 나노코리아는 산업통상자원부 과학기술정보통신부가 공동 주최한다. 나노융합산업연구조합과 나노기술연구협의회가 주관한다. 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다. 강민석 LG이노텍 부사장이 자율주행산업 동향에 따른 나노기술과 인공지능 AI 의 활용 을 주제로 기조강연을 한다. 알베르페르 프랑스 파리 슈드대 교수도 기조 강연을 맡는다. 전시규모는 코로나19 이전 수준으로 회복됐다. 삼성전자 LG 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여한다. 나노 20주년 특별 기념관도 마련된다. 20주년 특별 기념관에는 차세대 반도체 미래차 6세대 6G 이동통신 탄소중립 디지털 바이오 등 6개 분야 혁신 기술이 소개된다. 산업화 세션에서는 지속 가능 성장을 위한 ESG 나노융합기술을 주제로 초청 강연이 열린다. 나노제품거래상담회 전시회테크니컬투어 최신기술발표회 등 다양한 나노 관련 부대 행사도 준비된다.<|eot_id|>\u001b[0m\n",
      "\u001b[34mAssistant: 국내 3대 나노행사 중 하나인 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀에서 개최되는데, 삼성전자 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여할 예정이며 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다.<|eot_id|>\u001b[0m\n",
      "\u001b[34mNCCL version 2.18.5+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:72:150 [2] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:71:149 [1] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:70:147 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:73:148 [3] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34mYou are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m국내 최대규모 나노 전시회이자 세계 3대 나노행사 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀 에서 개최된다. 올해 20회를 맞는 나노코리아는 산업통상자원부 과학기술정보통신부가 공동 주최한다. 나노융합산업연구조합과 나노기술연구협의회가 주관한다. 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다. 강민석 LG이노텍 부사장이 자율주행산업 동향에 따른 나노기술과 인공지능 AI 의 활용 을 주제로 기조강연을 한다. 알베르페르 프랑스 파리 슈드대 교수도 기조 강연을 맡는다. 전시규모는 코로나19 이전 수준으로 회복됐다. 삼성전자 LG 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여한다. 나노 20주년 특별 기념관도 마련된다. 20주년 특별 기념관에는 차세대 반도체 미래차 6세대 6G 이동통신 탄소중립 디지털 바이오 등 6개 분야 혁신 기술이 소개된다. 산업화 세션에서는 지속 가능 성장을 위한 ESG 나노융합기술을 주제로 초청 강연이 열린다. 나노제품거래상담회 전시회테크니컬투어 최신기술발표회 등 다양한 나노 관련 부대 행사도 준비된다.<|eot_id|>\u001b[0m\n",
      "\u001b[34mAssistant: 국내 3대 나노행사 중 하나인 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀에서 개최되는데, 삼성전자 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여할 예정이며 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다.<|eot_id|>You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m국내 최대규모 나노 전시회이자 세계 3대 나노행사 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀 에서 개최된다. 올해 20회를 맞는 나노코리아는 산업통상자원부 과학기술정보통신부가 공동 주최한다. 나노융합산업연구조합과 나노기술연구협의회가 주관한다. 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다. 강민석 LG이노텍 부사장이 자율주행산업 동향에 따른 나노기술과 인공지능 AI 의 활용 을 주제로 기조강연을 한다. 알베르페르 프랑스 파리 슈드대 교수도 기조 강연을 맡는다. 전시규모는 코로나19 이전 수준으로 회복됐다. 삼성전자 LG 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여한다. 나노 20주년 특별 기념관도 마련된다. 20주년 특별 기념관에는 차세대 반도체 미래차 6세대 6G 이동통신 탄소중립 디지털 바이오 등 6개 분야 혁신 기술이 소개된다. 산업화 세션에서는 지속 가능 성장을 위한 ESG 나노융합기술을 주제로 초청 강연이 열린다. 나노제품거래상담회 전시회테크니컬투어 최신기술발표회 등 다양한 나노 관련 부대 행사도 준비된다.<|eot_id|>\u001b[0m\n",
      "\u001b[34mAssistant: 국내 3대 나노행사 중 하나인 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀에서 개최되는데, 삼성전자 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여할 예정이며 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다.<|eot_id|>You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m국내 최대규모 나노 전시회이자 세계 3대 나노행사 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀 에서 개최된다. 올해 20회를 맞는 나노코리아는 산업통상자원부 과학기술정보통신부가 공동 주최한다. 나노융합산업연구조합과 나노기술연구협의회가 주관한다. 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다. 강민석 LG이노텍 부사장이 자율주행산업 동향에 따른 나노기술과 인공지능 AI 의 활용 을 주제로 기조강연을 한다. 알베르페르 프랑스 파리 슈드대 교수도 기조 강연을 맡는다. 전시규모는 코로나19 이전 수준으로 회복됐다. 삼성전자 LG 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여한다. 나노 20주년 특별 기념관도 마련된다. 20주년 특별 기념관에는 차세대 반도체 미래차 6세대 6G 이동통신 탄소중립 디지털 바이오 등 6개 분야 혁신 기술이 소개된다. 산업화 세션에서는 지속 가능 성장을 위한 ESG 나노융합기술을 주제로 초청 강연이 열린다. 나노제품거래상담회 전시회테크니컬투어 최신기술발표회 등 다양한 나노 관련 부대 행사도 준비된다.<|eot_id|>\u001b[0m\n",
      "\u001b[34mAssistant: 국내 3대 나노행사 중 하나인 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀에서 개최되는데, 삼성전자 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여할 예정이며 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다.<|eot_id|>\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:17<00:53, 17.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:17<00:53, 17.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:17<00:53, 17.74s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:17<00:53, 17.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:39<00:39, 19.83s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:39<00:39, 19.85s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:39<00:39, 19.84s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:39<00:39, 19.85s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:55<00:18, 18.51s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:55<00:18, 18.51s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.52s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.52s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [01:00<00:00, 13.06s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [01:00<00:00, 15.17s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [01:00<00:00, 13.05s/it]#015Downloading shards: 100%|██████████| 4/4 [01:00<00:00, 15.17s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [01:00<00:00, 13.06s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [01:00<00:00, 15.17s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [01:00<00:00, 13.06s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [01:00<00:00, 15.17s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.37it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\u001b[0m\n",
      "\u001b[34mDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\u001b[0m\n",
      "\u001b[34mDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\u001b[0m\n",
      "\u001b[34mDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\u001b[0m\n",
      "\u001b[34mDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3 examples [00:00, 239.61 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3 examples [00:00, 389.65 examples/s]\u001b[0m\n",
      "\u001b[34mtrainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:04<00:00,  4.22s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.155303716659546, 'eval_runtime': 1.091, 'eval_samples_per_second': 2.75, 'eval_steps_per_second': 0.917, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:06<00:00,  4.22s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 14.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'train_runtime': 10.5046, 'train_samples_per_second': 0.286, 'train_steps_per_second': 0.095, 'train_loss': 2.07970929145813, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:08<00:00,  4.22s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:08<00:00,  8.81s/it]\u001b[0m\n",
      "\u001b[34mTrying to load a Peft model. It might take a while without feedback\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.58s/it]\u001b[0m\n",
      "\u001b[34mSaving the newly created merged model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2024-09-28 04:22:35,184 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-09-28 04:22:35,185 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-09-28 04:22:35,185 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-09-28 04:22:41 Uploading - Uploading generated training model\n",
      "2024-09-28 04:23:50 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 529\n",
      "Billable seconds: 529\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 경로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_s3_path: \n",
      " {'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-16-15-27-0-2024-07-16-15-27-08-481/output/model/', 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n",
      "Stored 'model_s3_path' (dict)\n"
     ]
    }
   ],
   "source": [
    "model_s3_path = huggingface_estimator.model_data\n",
    "print(\"model_s3_path: \\n\", model_s3_path)\n",
    "\n",
    "%store model_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_image",
   "language": "python",
   "name": "finetune_image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
