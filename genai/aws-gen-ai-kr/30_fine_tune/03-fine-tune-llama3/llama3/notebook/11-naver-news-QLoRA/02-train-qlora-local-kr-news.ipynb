{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4872e4-c90b-434b-bfe5-f88292fba385",
   "metadata": {},
   "source": [
    "# 로컬에서 훈련 하기\n",
    "- 이 노트북은 로컬 (현재 머신) 에서 Hugging Face Accelerator + PyTorch FSDP 로 파인 튜닝 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7aa7-1a2d-4db1-b011-59217c32a83a",
   "metadata": {},
   "source": [
    "## 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268a663-63a3-4d9e-8bd3-8025dec88254",
   "metadata": {},
   "source": [
    "### Hugging Face Token 입력\n",
    "- [중요] HF Key 가 노출이 안되도록 조심하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee8f554-0ac4-499a-bc35-7fbae4b96b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets                      3.0.0\n",
      "sagemaker                     2.232.1\n",
      "sagemaker-core                1.0.4\n",
      "sagemaker_nbi_agent           1.0\n",
      "sagemaker_pyspark             1.4.5\n",
      "torch                         2.4.1\n",
      "transformers                  4.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep -E \"torch|datasets|transformers|sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542f1cbe-5b7e-4fae-bf2d-4a6ac2f3dfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "\n",
    "is_sagemaker_notebook = True\n",
    "if is_sagemaker_notebook:\n",
    "    hf_key_name = \"HF_KEY\"\n",
    "    key_val = \"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\" #\"<Type Your HF Key>\"\n",
    "    set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    HF_TOKEN = get_hf_key_env_vars(hf_key_name)\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc7a0b-4829-4b2c-88b6-fd423732c53a",
   "metadata": {},
   "source": [
    "### 저장된 변수 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bce431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder:  ../data/naver-news-summarization-ko\n",
      "train_data_json:  ../data/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_data_json:  ../data/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_data_json:  ../data/naver-news-summarization-ko/test/test_dataset.json\n",
      "full_train_data_json:  ../data/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "full_validation_data_json:  ../data/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "full_test_data_json:  ../data/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%store -r data_folder\n",
    "%store -r train_data_json \n",
    "%store -r validation_data_json \n",
    "%store -r test_data_json \n",
    "%store -r full_train_data_json \n",
    "%store -r full_validation_data_json \n",
    "%store -r full_test_data_json\n",
    "\n",
    "\n",
    "print(\"data_folder: \", data_folder)\n",
    "print(\"train_data_json: \", train_data_json)\n",
    "print(\"validation_data_json: \", validation_data_json)\n",
    "print(\"test_data_json: \", test_data_json)\n",
    "print(\"full_train_data_json: \", full_train_data_json)\n",
    "print(\"full_validation_data_json: \", full_validation_data_json)\n",
    "print(\"full_test_data_json: \", full_test_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a396bc4f-0b0a-4ffb-8c59-18ed6d0a968d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78f53c-d21d-4fca-b69d-6a85d966353c",
   "metadata": {},
   "source": [
    "## 2. 베이스 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d514e3df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "#model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e8a20c2-ef30-4214-89e2-c58fe7e250b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"llama-3-8b-qlora-naver-news\"\n",
    "output_dir = f\"/home/ec2-user/SageMaker/models/{prefix}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403f6b8",
   "metadata": {},
   "source": [
    "### Config YAML 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72d30120",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerator_config/local_llama_3_8b_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerator_config/local_llama_3_8b_qlora.yaml\n",
    "# script parameters\n",
    "#model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "model_id: \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "#model_id: \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_id: \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "###########################\n",
    "# small samples for Debug\n",
    "###########################\n",
    "train_dataset_path: \"../data/naver-news-summarization-ko/train\"                      # path to dataset\n",
    "validation_dataset_path: \"../data/naver-news-summarization-ko/validation\"                      # path to dataset\n",
    "#test_dataset_path: \"../data/naver-news-summarization-ko/test\"                      # path to dataset\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass\n",
    "###########################\n",
    "# large samples for evaluation\n",
    "###########################\n",
    "# train_dataset_path: \"../data/naver-news-summarization-ko/full_train\"                      # path to dataset\n",
    "# validation_dataset_path: \"../data/naver-news-summarization-ko/full_validation\"                      # path to dataset\n",
    "# test_dataset_path: \"../data/naver-news-summarization-ko/full_test\"                      # path to dataset\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################\n",
    "max_seq_length:  2048              # max sequence length for model and packing of the dataset\n",
    "\n",
    "\n",
    "# training parameters\n",
    "output_dir: \"/home/ec2-user/SageMaker/models/llama-3-8b-fsdp-qlora-naver-news\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "logging_dir: \"/home/ec2-user/SageMaker/logs/llama-3-8b-fsdp-qlora-naver-news\" # log folder for tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true          # use gradient checkpointing to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851f75d",
   "metadata": {},
   "source": [
    "## 3. 훈련 Script 실행\n",
    "\n",
    "아래는 Hugging Face 의 Accelerator 기반 학습 명령어 입니다.\n",
    "- 현재 머신에 4개의 GPU 가 있는 경우 입니다. GPU 가 1개 이면 nproc_per_node=1 로 수정해서 실행 하세요. \n",
    "\n",
    "```\n",
    "!torchrun --nproc_per_node=4 \\\n",
    "../../scripts/local_run_qlora.py \\\n",
    "--config config_folder_name/local_llama_3_8b_qlora.yaml\n",
    "```\n",
    "- 참고\n",
    "    - Launching your 🤗 Accelerate scripts, [Link](https://huggingface.co/docs/accelerate/en/basic_tutorials/launch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "650a43f8-666d-4176-94b5-885f2bcb58cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "config_folder_name = \"accelerator_config\"\n",
    "os.makedirs(config_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9be15-e2bc-4b03-a80b-8849e77d9e19",
   "metadata": {},
   "source": [
    "### Hugging Face  Accelerator 에 제공할 config.yaml 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6de05-6af0-45d0-8ecf-68e796771b96",
   "metadata": {},
   "source": [
    "### IMPORTANT!! Set use reentrant to False when we don't use FSDP\n",
    "```\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\"use_reentrant\":False**}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f81f6fe-4e96-4a6a-a354-d89d2f248351",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0928 02:15:57.183000 140697813051200 torch/distributed/run.py:779] \n",
      "W0928 02:15:57.183000 140697813051200 torch/distributed/run.py:779] *****************************************\n",
      "W0928 02:15:57.183000 140697813051200 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0928 02:15:57.183000 140697813051200 torch/distributed/run.py:779] *****************************************\n",
      "/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "## script_args: \n",
      " ScriptArguments(train_dataset_path='../data/naver-news-summarization-ko/train', validation_dataset_path='../data/naver-news-summarization-ko/validation', model_id='meta-llama/Llama-3.2-1B-Instruct', max_seq_length=2048)\n",
      "## training_args: \n",
      " TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/ec2-user/SageMaker/logs/llama-3-8b-fsdp-qlora-naver-news,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/ec2-user/SageMaker/models/llama-3-8b-fsdp-qlora-naver-news,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/ec2-user/SageMaker/models/llama-3-8b-fsdp-qlora-naver-news,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "[rank0]:     response.raise_for_status()\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank0]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank0]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "[rank0]:     resolved_file = hf_hub_download(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank0]:     return f(*args, **kwargs)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1232, in hf_hub_download\n",
      "[rank0]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1339, in _hf_hub_download_to_cache_dir\n",
      "[rank0]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n",
      "[rank0]:     raise head_call_error\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1746, in _get_metadata_or_catch_error\n",
      "[rank0]:     metadata = get_hf_file_metadata(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1666, in get_hf_file_metadata\n",
      "[rank0]:     r = _request_wrapper(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 364, in _request_wrapper\n",
      "[rank0]:     response = _request_wrapper(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 388, in _request_wrapper\n",
      "[rank0]:     hf_raise_for_status(response)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "[rank0]:     raise _format(GatedRepoError, message, response) from e\n",
      "[rank0]: huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66f766e1-0b0787204233ad86051c9e1a;eba4be2a-a796-4f46-84b1-9eeab2f85f51)\n",
      "\n",
      "[rank0]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank0]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 194, in <module>\n",
      "[rank0]:     training_function(script_args, training_args)\n",
      "[rank0]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 84, in training_function\n",
      "[rank0]:     tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 864, in from_pretrained\n",
      "[rank0]:     config = AutoConfig.from_pretrained(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n",
      "[rank0]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 567, in get_config_dict\n",
      "[rank0]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 626, in _get_config_dict\n",
      "[rank0]:     resolved_config_file = cached_file(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "[rank0]:     raise EnvironmentError(\n",
      "[rank0]: OSError: You are trying to access a gated repo.\n",
      "[rank0]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n",
      "[rank0]: 403 Client Error. (Request ID: Root=1-66f766e1-0b0787204233ad86051c9e1a;eba4be2a-a796-4f46-84b1-9eeab2f85f51)\n",
      "\n",
      "[rank0]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank0]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "[rank3]:     response.raise_for_status()\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank3]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank3]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json\n",
      "\n",
      "[rank3]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "[rank3]:     resolved_file = hf_hub_download(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank3]:     return f(*args, **kwargs)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank3]:     return fn(*args, **kwargs)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1232, in hf_hub_download\n",
      "[rank3]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1339, in _hf_hub_download_to_cache_dir\n",
      "[rank3]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n",
      "[rank3]:     raise head_call_error\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1746, in _get_metadata_or_catch_error\n",
      "[rank3]:     metadata = get_hf_file_metadata(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank3]:     return fn(*args, **kwargs)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1666, in get_hf_file_metadata\n",
      "[rank3]:     r = _request_wrapper(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 364, in _request_wrapper\n",
      "[rank3]:     response = _request_wrapper(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 388, in _request_wrapper\n",
      "[rank3]:     hf_raise_for_status(response)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "[rank3]:     raise _format(GatedRepoError, message, response) from e\n",
      "[rank3]: huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66f766e1-017b80fc70f31d232330d6d2;1179743a-a942-4f43-acd8-f1dbac7421fa)\n",
      "\n",
      "[rank3]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank3]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "\n",
      "[rank3]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 194, in <module>\n",
      "[rank3]:     training_function(script_args, training_args)\n",
      "[rank3]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 84, in training_function\n",
      "[rank3]:     tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 864, in from_pretrained\n",
      "[rank3]:     config = AutoConfig.from_pretrained(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n",
      "[rank3]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 567, in get_config_dict\n",
      "[rank3]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 626, in _get_config_dict\n",
      "[rank3]:     resolved_config_file = cached_file(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "[rank3]:     raise EnvironmentError(\n",
      "[rank3]: OSError: You are trying to access a gated repo.\n",
      "[rank3]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n",
      "[rank3]: 403 Client Error. (Request ID: Root=1-66f766e1-017b80fc70f31d232330d6d2;1179743a-a942-4f43-acd8-f1dbac7421fa)\n",
      "\n",
      "[rank3]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank3]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "[rank2]:     response.raise_for_status()\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank2]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank2]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json\n",
      "\n",
      "[rank2]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "[rank2]:     resolved_file = hf_hub_download(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank2]:     return f(*args, **kwargs)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank2]:     return fn(*args, **kwargs)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1232, in hf_hub_download\n",
      "[rank2]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1339, in _hf_hub_download_to_cache_dir\n",
      "[rank2]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n",
      "[rank2]:     raise head_call_error\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1746, in _get_metadata_or_catch_error\n",
      "[rank2]:     metadata = get_hf_file_metadata(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank2]:     return fn(*args, **kwargs)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1666, in get_hf_file_metadata\n",
      "[rank2]:     r = _request_wrapper(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 364, in _request_wrapper\n",
      "[rank2]:     response = _request_wrapper(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 388, in _request_wrapper\n",
      "[rank2]:     hf_raise_for_status(response)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "[rank2]:     raise _format(GatedRepoError, message, response) from e\n",
      "[rank2]: huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66f766e2-2a6ad4f60016f91d11130497;f6e8ed6b-c5b0-4ee3-a71f-6f9e60f708d8)\n",
      "\n",
      "[rank2]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank2]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "\n",
      "[rank2]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 194, in <module>\n",
      "[rank2]:     training_function(script_args, training_args)\n",
      "[rank2]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 84, in training_function\n",
      "[rank2]:     tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 864, in from_pretrained\n",
      "[rank2]:     config = AutoConfig.from_pretrained(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n",
      "[rank2]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 567, in get_config_dict\n",
      "[rank2]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 626, in _get_config_dict\n",
      "[rank2]:     resolved_config_file = cached_file(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "[rank2]:     raise EnvironmentError(\n",
      "[rank2]: OSError: You are trying to access a gated repo.\n",
      "[rank2]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n",
      "[rank2]: 403 Client Error. (Request ID: Root=1-66f766e2-2a6ad4f60016f91d11130497;f6e8ed6b-c5b0-4ee3-a71f-6f9e60f708d8)\n",
      "\n",
      "[rank2]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank2]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "[rank1]:     response.raise_for_status()\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank1]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank1]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json\n",
      "\n",
      "[rank1]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "[rank1]:     resolved_file = hf_hub_download(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank1]:     return f(*args, **kwargs)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1232, in hf_hub_download\n",
      "[rank1]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1339, in _hf_hub_download_to_cache_dir\n",
      "[rank1]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n",
      "[rank1]:     raise head_call_error\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1746, in _get_metadata_or_catch_error\n",
      "[rank1]:     metadata = get_hf_file_metadata(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1666, in get_hf_file_metadata\n",
      "[rank1]:     r = _request_wrapper(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 364, in _request_wrapper\n",
      "[rank1]:     response = _request_wrapper(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 388, in _request_wrapper\n",
      "[rank1]:     hf_raise_for_status(response)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "[rank1]:     raise _format(GatedRepoError, message, response) from e\n",
      "[rank1]: huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66f766e2-0d23b0f065c7576b40544ffd;0578c8ca-97e0-4219-91e3-596713e2bfb4)\n",
      "\n",
      "[rank1]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank1]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "\n",
      "[rank1]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 194, in <module>\n",
      "[rank1]:     training_function(script_args, training_args)\n",
      "[rank1]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 84, in training_function\n",
      "[rank1]:     tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 864, in from_pretrained\n",
      "[rank1]:     config = AutoConfig.from_pretrained(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n",
      "[rank1]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 567, in get_config_dict\n",
      "[rank1]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 626, in _get_config_dict\n",
      "[rank1]:     resolved_config_file = cached_file(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "[rank1]:     raise EnvironmentError(\n",
      "[rank1]: OSError: You are trying to access a gated repo.\n",
      "[rank1]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n",
      "[rank1]: 403 Client Error. (Request ID: Root=1-66f766e2-0d23b0f065c7576b40544ffd;0578c8ca-97e0-4219-91e3-596713e2bfb4)\n",
      "\n",
      "[rank1]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank1]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "W0928 02:16:02.503000 140697813051200 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 10479 closing signal SIGTERM\n",
      "W0928 02:16:02.503000 140697813051200 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 10480 closing signal SIGTERM\n",
      "W0928 02:16:02.503000 140697813051200 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 10482 closing signal SIGTERM\n",
      "E0928 02:16:02.983000 140697813051200 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 10478) of binary: /home/ec2-user/anaconda3/bin/python3.10\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
      "    run(args)\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 133, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "../../scripts/local_run_qlora.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-09-28_02:16:02\n",
      "  host      : ip-172-16-4-140.us-west-2.compute.internal\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 10478)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 \\\n",
    "../../scripts/local_run_qlora.py \\\n",
    "--config accelerator_config/local_llama_3_8b_qlora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441607e4-f93f-438e-8985-99a76233fe47",
   "metadata": {},
   "source": [
    "## 4. 베이스 모델과 훈련된 모델 머지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebdb212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id: meta-llama/Llama-3.1-8B-Instruct\n",
      "output_dir: /home/ec2-user/SageMaker/models/llama-3-8b-qlora-naver-news\n"
     ]
    }
   ],
   "source": [
    "print (f'model_id: {model_id}')\n",
    "print (f'output_dir: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8acc4",
   "metadata": {},
   "source": [
    "### 모델 머지 및 로컬에 저장\n",
    "- 약 2분 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19543636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa36ec906384b3782d46906c081a7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    torch_dtype=torch.float16, ## why bfloat16이 아니지?\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d0e3827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cpu'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device, merged_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0ea42",
   "metadata": {},
   "source": [
    "### 머지된 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1a6607d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7827def278f4e33b6c3b9a1f7cab31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  pretrained_model_name_or_path = output_dir,\n",
    "  torch_dtype=torch.float16,\n",
    "  quantization_config= {\"load_in_4bit\": True},\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908046b",
   "metadata": {},
   "source": [
    "## 5. 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5388b34",
   "metadata": {},
   "source": [
    "### 테스트 데이터 셋 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eacd377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8a2983e2b54ca48c88ddb6ee424762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_idx:  75\n",
      "messages: \n",
      " [{'content': 'You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.', 'role': 'system'}, {'content': 'Please summarize the goals for journalist in this text:\\n\\n산업현장에서 세종텔레콤의 스마트 안전 플랫폼 솔루션 을 활용하고 있다. 세종텔레콤 제공 세종텔레콤은 태영건설에 중대재해처벌법 대응에 최적화된 스마트 안전 플랫폼 솔루션 납품 계약을 체결했다고 4일 밝혔다. 우선 태영건설 전국 산업현장에 스마트 안전 솔루션 500대 납품 계약을 체결했고 추가 구축을 협의 중이다. 지난 1월 본격 시행된 중대재해처벌법은 산업현장에서 인명사고 발생 시 경영진이나 법인에게 책임을 물을 수 있도록 규정한 법이다. 해당 법은 안전 보건 관련 관리상의 조치 구축을 의무화하고 있으나 기업의 한정적 자원과 부족한 인력 문제로 어려움을 겪고 있다. 세종텔레콤의 스마트 안전 플랫폼 솔루션은 출입관리부터 CCTV 가스탐지 각종 센서 등을 하나로 통합해 현장을 종합 관리할 수 있다. LBS 위치기반 IoT 사물인터넷 등 스마트 기술을 융합했다. 안전 관리 담당자는 각 현장마다 설치된 카메라 및 CCTV 개소별 센서와 통신 인프라를 통해 현장 정보를 실시간으로 확인하고 비상 상황 시에는 전체 현장 또는 해당 구역 상황실 시스템이나 모바일로 근로자에게 안전 조치사항을 지시할 수 있다. 이와 함께 타워크레인에 설치한 360도 카메라를 통해 현장의 불안전 요소를 발견하면 관계자에게 알림을 보낼 수 있다. 지하 작업에서는 이동형 스마트 영상 장비로 안전 사각지대를 살필 수 있고 밀폐된 작업 공간에서는 가스 센서와 신호등형 전광판을 설치해 실시간으로 스마트 상황판에 가스 농도를 전송한다. 유해가스가 허용 농도를 초과하면 현장에서 환기 시스템이 자동으로 작동한다. 현장 내 추락 사고가 발생할 수 있는 개구부에 부착한 센서는 개구부가 비정상적으로 개폐됐을 때 경고음을 보내 위험상황을 알린다. 강효상 세종텔레콤 통신사업본부장은 중대재해처벌법 시행에 따른 건설 현장의 안전사고 예방을 위한 최적의 솔루션이 요구되고 있는 시점에서 스마트 안전 솔루션 예방 플랫폼 구축의 의의가 크다 고 말했다.', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "def get_message_from_dataset(sample_dataset_json_file):\n",
    "    # Load our test dataset\n",
    "    full_test_dataset = load_dataset(\"json\", data_files=sample_dataset_json_file, split=\"train\")\n",
    "\n",
    "    # Test on sample \n",
    "    rand_idx = randint(0, len(full_test_dataset)-1)\n",
    "    rand_idx = 75\n",
    "    print(\"rand_idx: \", rand_idx)\n",
    "    messages = full_test_dataset[rand_idx][\"messages\"][:2]\n",
    "    # messages = test_dataset[rand_idx][\"text\"][:2]\n",
    "    print(\"messages: \\n\", messages)\n",
    "\n",
    "    return messages, full_test_dataset, rand_idx\n",
    "\n",
    "messages, full_test_dataset, rand_idx = get_message_from_dataset(sample_dataset_json_file = full_test_data_json)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b47b",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddaf71a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Query:**\n",
      "Please summarize the goals for journalist in this text:\n",
      "\n",
      "산업현장에서 세종텔레콤의 스마트 안전 플랫폼 솔루션 을 활용하고 있다. 세종텔레콤 제공 세종텔레콤은 태영건설에 중대재해처벌법 대응에 최적화된 스마트 안전 플랫폼 솔루션 납품 계약을 체결했다고 4일 밝혔다. 우선 태영건설 전국 산업현장에 스마트 안전 솔루션 500대 납품 계약을 체결했고 추가 구축을 협의 중이다. 지난 1월 본격 시행된 중대재해처벌법은 산업현장에서 인명사고 발생 시 경영진이나 법인에게 책임을 물을 수 있도록 규정한 법이다. 해당 법은 안전 보건 관련 관리상의 조치 구축을 의무화하고 있으나 기업의 한정적 자원과 부족한 인력 문제로 어려움을 겪고 있다. 세종텔레콤의 스마트 안전 플랫폼 솔루션은 출입관리부터 CCTV 가스탐지 각종 센서 등을 하나로 통합해 현장을 종합 관리할 수 있다. LBS 위치기반 IoT 사물인터넷 등 스마트 기술을 융합했다. 안전 관리 담당자는 각 현장마다 설치된 카메라 및 CCTV 개소별 센서와 통신 인프라를 통해 현장 정보를 실시간으로 확인하고 비상 상황 시에는 전체 현장 또는 해당 구역 상황실 시스템이나 모바일로 근로자에게 안전 조치사항을 지시할 수 있다. 이와 함께 타워크레인에 설치한 360도 카메라를 통해 현장의 불안전 요소를 발견하면 관계자에게 알림을 보낼 수 있다. 지하 작업에서는 이동형 스마트 영상 장비로 안전 사각지대를 살필 수 있고 밀폐된 작업 공간에서는 가스 센서와 신호등형 전광판을 설치해 실시간으로 스마트 상황판에 가스 농도를 전송한다. 유해가스가 허용 농도를 초과하면 현장에서 환기 시스템이 자동으로 작동한다. 현장 내 추락 사고가 발생할 수 있는 개구부에 부착한 센서는 개구부가 비정상적으로 개폐됐을 때 경고음을 보내 위험상황을 알린다. 강효상 세종텔레콤 통신사업본부장은 중대재해처벌법 시행에 따른 건설 현장의 안전사고 예방을 위한 최적의 솔루션이 요구되고 있는 시점에서 스마트 안전 솔루션 예방 플랫폼 구축의 의의가 크다 고 말했다.\n",
      "\n",
      "**Original Answer:**\n",
      "세종텔레콤은 태영건설에 출입관리부터 CCTV 가스탐지 각종 센서 등을 하나로 통합해 현장을 종합 관리할 수 있는 스마트 안전 플랫폼 솔루션 납품 계약을 체결했다고 4일 밝혔으며 태영건설 전국 산업현장에 스마트 안전 솔루션 500대 납품 계약을 체결했고 추가 구축을 협의 중이다.\n",
      "\n",
      "**Generated Answer:**\n",
      " 저는 산업현장에서 안전사고를 예방하고 중대재해처벌법에 부합하는 스마트 안전 플랫폼 솔루션을 구축하는 태영건설과 세종텔레콤의 계약을 소개합니다. \n",
      "\n",
      "태영건설은 세종텔레콤의 스마트 안전 플랫폼 솔루션을 전국 산업현장에 500대 납품 계약을 체결했습니다. 추가 구축을 협의 중입니다. \n",
      "\n",
      "이 계약은 중대재해처벌법에 부합하는 안전 보건 관련 관리상의 조치 구축을 의무화하고 있습니다. 기업의 한정적 자원과 부족한 인력 문제로 어려움을 겪고 있습니다. \n",
      "\n",
      "세종텔레콤의 스마트 안전 플랫폼 솔루션은 출입관리부터 CCTV, 가스탐지 각종 센서 등을 하나로 통합해 현장을 종합 관리할 수 있습니다. LBS 위치기반 IoT 사물인터넷 등 스마트 기술을 융합했다. \n",
      "\n",
      "이 플랫폼은 안전 관리 담당자가 각 현장마다 설치된 카메라 및 CCTV 개소별 센서와 통신 인프라를 통해 현장 정보를 실시간으로 확인할 수 있고, 비상 상황 시에는 전체 현장 또는 해당 구역 상황실 시스템이나 모바일로 근로자에게 안전 조치사항을 지시할 수 있습니다. \n",
      "\n",
      "이 플랫폼은 현장 내 추락 사고가 발생할 수 있는 개구부에 부착한 센서를 통해 개구부가 비정상적으로 개폐됐을 때 경고음을 보내 위험상황을 알립니다. \n",
      "\n",
      "강효상 세종텔레콤 통신사업본부장은 중대재해처벌법 시행에 따른 건설 현장의 안전사고 예방을 위한 최적의 솔루션이 요구되고 있는 시점에서 스마트 안전 솔루션 예방 플랫폼 구축의 의의가 크다 고 말했다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_response(messages, model, tokenizer, full_test_dataset, rand_idx):\n",
    "    input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id= tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "    print(f\"**Query:**\\n{full_test_dataset[rand_idx]['messages'][1]['content']}\\n\")\n",
    "    # print(f\"**Query:**\\n{test_dataset[rand_idx]['text'][1]['content']}\\n\")\n",
    "    # print(f\"**Original Answer:**\\n{test_dataset[rand_idx]['text'][2]['content']}\\n\")\n",
    "    print(f\"**Original Answer:**\\n{full_test_dataset[rand_idx]['messages'][2]['content']}\\n\")\n",
    "    print(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n",
    "\n",
    "generate_response(messages, model, tokenizer, full_test_dataset, rand_idx)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360223a2-02b4-429d-8e47-6ec5ca55247e",
   "metadata": {},
   "source": [
    "### 할당된 CUDA memory를 Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14070303-ea9f-40e5-a56e-cca1bf56f7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a95c1-e28e-463d-8fba-a76a09de2105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-image",
   "language": "python",
   "name": "finetune-image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
