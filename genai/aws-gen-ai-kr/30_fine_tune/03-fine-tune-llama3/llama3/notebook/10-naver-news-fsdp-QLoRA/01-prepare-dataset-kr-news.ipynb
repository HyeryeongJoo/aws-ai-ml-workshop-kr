{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4872e4-c90b-434b-bfe5-f88292fba385",
   "metadata": {},
   "source": [
    "# 데이터셋 준비하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422bb81f",
   "metadata": {},
   "source": [
    "## 0.사전 준비 사항\n",
    "아래 사항을 먼저 준비 하셔야 합니다.\n",
    "\n",
    "- huggingface Acess Key 준비 하기 : [User access tokens](https://huggingface.co/docs/hub/en/security-tokens)\n",
    "- Llama-3-8B 모델 엑세스 권한 얻기: [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7aa7-1a2d-4db1-b011-59217c32a83a",
   "metadata": {},
   "source": [
    "## 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3bbbb",
   "metadata": {},
   "source": [
    "### Hugging Face Token 입력\n",
    "- [중요] HF Key 가 노출이 안되도록 조심하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63b231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "\n",
    "is_sagemaker_notebook = True\n",
    "if is_sagemaker_notebook:\n",
    "    hf_key_name = \"HF_KEY\"\n",
    "    key_val = \"<Type Your HF Key>\"\n",
    "    set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    HF_TOKEN = get_hf_key_env_vars(hf_key_name)\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52049e77",
   "metadata": {},
   "source": [
    "#### 환경 변수 확인 ( HF 의 디폴트 경로 변경 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcebd19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/ec2-user/SageMaker/.cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/home/ec2-user/SageMaker/.cache\"\n",
    "\n",
    "print(\"HF_DATASETS_CACHE: \", os.getenv('HF_DATASETS_CACHE'))\n",
    "print(\"HF_HOME: \", os.getenv('HF_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396bc4f-0b0a-4ffb-8c59-18ed6d0a968d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4b722-f752-4687-8fbe-12855566892c",
   "metadata": {},
   "source": [
    "## 2. 데이터 셋 준비\n",
    "### 데이터 셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3710c3d-71bb-47b1-97eb-a72ef6dcfd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"daekeun-ml/naver-news-summarization-ko\"\n",
    "\n",
    "# dataset = load_dataset(huggingface_dataset_name, \"3.0.0\")\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94062514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96451467-4fc0-49af-bedf-040850c11fbb",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eda452",
   "metadata": {},
   "source": [
    "### Chat Message 형태 템플릿 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1181ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def format_instruction(system_prompt: str, article: str, summary: str):\n",
    "    message = [\n",
    "            {\n",
    "                'content': system_prompt,\n",
    "                'role': 'system'\n",
    "            },\n",
    "            {\n",
    "                'content': f'Please summarize the goals for journalist in this text:\\n\\n{article}',\n",
    "                'role': 'user'\n",
    "            },\n",
    "            {\n",
    "                'content': f'{summary}',\n",
    "                'role': 'assistant'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    return message # json.dumps(message, indent=2) # json.dumps(message, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "# system_prompt = \"You are an AI assistant specialized in news articles. Your role is to provide accurate summaries and insights. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\"\n",
    "# article = \"Within three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert's portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. \\\"We can't ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,\\\" says Rinkert.\"\n",
    "# summary = \"Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.\"\n",
    "\n",
    "# print(format_instruction(system_prompt, article, summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a4269",
   "metadata": {},
   "source": [
    "### Chat Message 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fede492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a43e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_instruction_dataset(data_point):\n",
    "    system_prompt = \"You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": format_instruction(system_prompt, data_point[\"document\"],data_point[\"summary\"])\n",
    "    }\n",
    "\n",
    "def process_dataset(data: Dataset):\n",
    "    return (\n",
    "        data.shuffle(seed=42)\n",
    "        .map(generate_instruction_dataset).remove_columns(columns_to_remove)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390bfdf",
   "metadata": {},
   "source": [
    "#### 전체 데이터 셋에서 일부 데티터 추출 (짧은 실습을 위해서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ddc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_num_debug = 10\n",
    "validation_num_debug = 10\n",
    "test_num_debug = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb7874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_message_dataset(dataset, num_train,num_val, num_test, verbose=False):\n",
    "    ## APPLYING PREPROCESSING ON WHOLE DATASET\n",
    "\n",
    "    train_dataset = process_dataset(dataset[\"train\"].select(range(num_train)))\n",
    "    validation_dataset = process_dataset(dataset[\"validation\"].select(range(num_val)))\n",
    "    test_dataset= process_dataset(dataset[\"test\"].select(range(num_test)))\n",
    "    \n",
    "    if verbose:\n",
    "        print(train_dataset)\n",
    "        print(test_dataset)\n",
    "        print(validation_dataset)\n",
    "\n",
    "    return train_dataset,test_dataset,validation_dataset    \n",
    "\n",
    "train_dataset,test_dataset,validation_dataset = create_message_dataset(dataset=dataset, \n",
    "                                                num_train=train_num_debug,\n",
    "                                                num_val=validation_num_debug, \n",
    "                                                num_test=test_num_debug, verbose=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197aa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b678c9",
   "metadata": {},
   "source": [
    "#### 전체 데이터 셋 저장 (성능 측정을 위해서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec1d03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_train_num = len(dataset[\"train\"])\n",
    "full_validation_num = len(dataset[\"validation\"])\n",
    "full_test_num = len(dataset[\"test\"])\n",
    "\n",
    "# full_train_num = 1000\n",
    "# full_validation_num = 1000\n",
    "# full_test_num = 1000\n",
    "\n",
    "print(\"train_num_samples: \", full_train_num)\n",
    "print(\"validation_num_samples: \", full_validation_num)\n",
    "print(\"test_num_samples: \", full_test_num)\n",
    "\n",
    "full_train_dataset,full_test_dataset,full_validation_dataset = create_message_dataset(dataset=dataset, \n",
    "                                                                num_train=full_train_num,\n",
    "                                                                num_val=full_validation_num, \n",
    "                                                                num_test=full_test_num, verbose=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dda7e7",
   "metadata": {},
   "source": [
    "## 4. 데이터 셋을 JSON 으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af3619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_dataset_json_file(huggingface_dataset_name,train_dataset, validation_dataset, test_dataset, is_full, verbose=True ):\n",
    "    dataset_name = huggingface_dataset_name.split(\"/\")[1]\n",
    "    data_folder = os.path.join(\"../data/\",dataset_name)\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "    if is_full:\n",
    "        train_data_json = os.path.join(data_folder,\"full_train\", \"train_dataset.json\")\n",
    "        validation_data_json = os.path.join(data_folder,\"full_validation\", \"validation_dataset.json\")\n",
    "        test_data_json = os.path.join(data_folder, \"full_test\", \"test_dataset.json\")\n",
    "\n",
    "    else:\n",
    "        train_data_json = os.path.join(data_folder,\"train\", \"train_dataset.json\")\n",
    "        validation_data_json = os.path.join(data_folder,\"validation\", \"validation_dataset.json\")\n",
    "        test_data_json = os.path.join(data_folder, \"test\", \"test_dataset.json\")\n",
    "\n",
    "    # save datasets to disk \n",
    "    train_dataset.to_json(train_data_json, orient=\"records\", force_ascii=False)\n",
    "    validation_dataset.to_json(validation_data_json, orient=\"records\", force_ascii=False)\n",
    "    test_dataset.to_json(test_data_json, orient=\"records\", force_ascii=False)        \n",
    "\n",
    "    if verbose:\n",
    "        print(train_dataset)\n",
    "        print(f\"{train_data_json} is saved\")\n",
    "        print(f\"{validation_data_json} is saved\")\n",
    "        print(f\"{test_data_json} is saved\")                \n",
    "\n",
    "    return data_folder, train_data_json, validation_data_json, test_data_json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2cf0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store debug dataset\n",
    "data_folder, train_data_json, validation_data_json, test_data_json = create_dataset_json_file(huggingface_dataset_name=huggingface_dataset_name,\n",
    "                                                                    train_dataset=train_dataset, \n",
    "                                                                    validation_dataset=validation_dataset, \n",
    "                                                                    test_dataset=test_dataset,\n",
    "                                                                    is_full=False )        \n",
    "\n",
    "# Store full dataset\n",
    "data_folder, full_train_data_json, full_validation_data_json, full_test_data_json = create_dataset_json_file(huggingface_dataset_name=huggingface_dataset_name,\n",
    "                                                                    train_dataset=full_train_dataset, \n",
    "                                                                    validation_dataset=full_validation_dataset, \n",
    "                                                                    test_dataset=full_test_dataset,\n",
    "                                                                    is_full=True )        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c521c6f",
   "metadata": {},
   "source": [
    "### 다음 노트북에서 사용하기 위해 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb035e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store data_folder\n",
    "%store train_data_json \n",
    "%store validation_data_json \n",
    "%store test_data_json \n",
    "%store full_train_data_json \n",
    "%store full_validation_data_json \n",
    "%store full_test_data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5c4cc",
   "metadata": {},
   "source": [
    "## 5. Option: 데이터 셋을 ChatTemplate 형태로 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165e725",
   "metadata": {},
   "source": [
    "### Chat Template 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ba09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfcc19",
   "metadata": {},
   "source": [
    "### Chat Template 으로 변형하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54451f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer        \n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", use_fast=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MLP-KTLim/llama-3-Korean-Bllossom-8B\", use_fast=True)\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "\n",
    "# template dataset\n",
    "def template_dataset(examples):\n",
    "    return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "\n",
    "sample_train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fc688",
   "metadata": {},
   "source": [
    "### 변형된 Chat Message 형태 예시 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042b261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print random sample\n",
    "import random\n",
    "\n",
    "for index in random.sample(range(len(sample_train_dataset)), 1):\n",
    "    print(\"index: \", index)\n",
    "    # index = 5343\n",
    "    print(sample_train_dataset[index][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585868a-cc5c-44ef-b5bd-4160ceca79a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_finetune_fsdp_image",
   "language": "python",
   "name": "conda_finetune_fsdp_image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
