{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [모듈 2.1] 한국어 뉴스 요약 훈련 파이프라인\n",
    "\n",
    "이 노트북은 아래와 같은 목차로 진행 됩니다. 전체를 모두 실행시에 완료 시간은 **약 15분** 소요 됩니다.\n",
    "\n",
    "- 0. 배포를 위한 모델 아티펙트 형태\n",
    "- 1. 환경 설정\n",
    "- 2. 패키징 코드\n",
    "- 3. 세이지 메이커 파이프라인 생성\n",
    "    - 모델 훈련 스텝\n",
    "    - 모델 아티펙트 리패키징 람다 스텝 정의\n",
    "    - 모델 등록 스텝\n",
    "- 4.모델 빌딩 파이프라인 정의 및 실행\n",
    "- 5.Pipeline 캐싱 및 파라미터 이용한 실행\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker                     2.229.0\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1.환경 설정 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/SageMaker/.cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    " \n",
    "is_sagemaker_notebook = True\n",
    "# is_sagemaker_notebook = False # use VS Code\n",
    "\n",
    "if is_sagemaker_notebook:\n",
    "    # HF_TOKEN = getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "    HF_TOKEN = \"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\"\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN') or getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "    print(\"token: \", HF_TOKEN)\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "python path: /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3 is added\n",
      "sys.path:  ['/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/notebook/03-naver-news-lllama3-mlops', '/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python310.zip', '/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10', '/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/lib-dynload', '', '/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages', '/home/ec2-user/SageMaker/huggingface-inferentia2-samples/llama3-70b/llmperf/src', '/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "def add_python_path(module_path):\n",
    "    if os.path.abspath(module_path) not in sys.path:\n",
    "        sys.path.append(os.path.abspath(module_path))\n",
    "        print(f\"python path: {os.path.abspath(module_path)} is added\")\n",
    "    else:\n",
    "        print(f\"python path: {os.path.abspath(module_path)} already exists\")\n",
    "    print(\"sys.path: \", sys.path)\n",
    "\n",
    "module_path = \"../..\"\n",
    "add_python_path(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장된 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %store -r data_folder\n",
    "# %store -r train_data_json \n",
    "# %store -r validation_data_json \n",
    "# %store -r test_data_json \n",
    "# %store -r full_train_data_json \n",
    "# %store -r full_validation_data_json \n",
    "# %store -r full_test_data_json\n",
    "\n",
    "\n",
    "# print(\"data_folder: \", data_folder)\n",
    "# print(\"train_data_json: \", train_data_json)\n",
    "# print(\"validation_data_json: \", validation_data_json)\n",
    "# print(\"test_data_json: \", test_data_json)\n",
    "# print(\"full_train_data_json: \", full_train_data_json)\n",
    "# print(\"full_validation_data_json: \", full_validation_data_json)\n",
    "# print(\"full_test_data_json: \", full_test_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::057716757052:role/gen_ai_gsmoon\n",
      "sagemaker bucket: sagemaker-us-east-1-057716757052\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 훈련 설정 값 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "max_seq_len:  2048              # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\" # path to where SageMaker saves train dataset\n",
    "validation_dataset_path: \"/opt/ml/input/data/validation/\" # path to where SageMaker saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"   # path to where SageMaker saves test dataset\n",
    "output_dir: \"/tmp/llama3\"            # where the LoRA adapter weight is\n",
    "# training parameters\n",
    "# report_to: \"tensorboard\" \n",
    "report_to: \"mlflow\" \n",
    "mlflow_experiment_name: \"llama3-naver-news-fine-tuning\"\n",
    "# report metrics to tensorboard\n",
    "MLFLOW_TRACKING_ARN: \"arn:aws:sagemaker:us-east-1:057716757052:mlflow-tracking-server/my-setup-test3\"\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "###########################             \n",
    "# For Debug\n",
    "###########################             \n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "# For evaluation\n",
    "###########################             \n",
    "# num_train_epochs: 3                    # number of training epochs\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 8          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정 파일 업로드 위치 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_path:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko\n"
     ]
    }
   ],
   "source": [
    "%store -r input_path\n",
    "print(\"input_path: \", input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정 파일을 S3 에 업로드\n",
    "- 위에 정의한 파일을 업로드 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "from scripts.train_util import upload_data_s3\n",
    "\n",
    "config_desired_s3_uri = f\"{input_path}/config\"\n",
    "config_model_name = \"accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml\"\n",
    "train_config_s3_path = upload_data_s3(desired_s3_uri=config_desired_s3_uri, file_name=config_model_name, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련 인스턴스 등 설정\n",
    "- 디버그 용도이면 run_debug_sample = True, 전데 데이터 이면 False 로 조절 하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Cloud mode is set with ml.g5.4xlarge and 1 of instance_count\n"
     ]
    }
   ],
   "source": [
    "# USE_LOCAL_MODE = True\n",
    "USE_LOCAL_MODE = False\n",
    "\n",
    "import torch\n",
    "\n",
    "if USE_LOCAL_MODE:\n",
    "    instance_type = 'local_gpu' if torch.cuda.is_available() else 'local'\n",
    "    instance_count = 1\n",
    "    from sagemaker.local import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    # data = local_data \n",
    "    # data = s3_data\n",
    "    metric_definitions = None\n",
    "    nKeepAliveSeconds = None # Warmpool feature\n",
    "    print(\"## Local mode is set\")\n",
    "else:\n",
    "    instance_type = 'ml.g5.4xlarge'\n",
    "    # instance_type = 'ml.g5.12xlarge'\n",
    "    # instance_type = 'ml.g5.48xlarge'\n",
    "    # instance_type = 'ml.p4d.24xlarge'\n",
    "    # Emit: \n",
    "    # {'train_runtime': 37.2985, 'train_samples_per_second': 0.375, 'train_steps_per_second': 0.054, 'train_loss': 2.3541293144226074, 'epoch': 1.0}\n",
    "    # {'eval_loss': 2.50766658782959, 'eval_runtime': 3.4741, 'eval_samples_per_second': 3.454, 'eval_steps_per_second': 0.864, 'epoch': 1.0}\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"train:loss\", \"Regex\": \"'train_loss':(.*?),\"},\n",
    "        {\"Name\": \"validation:loss\", \"Regex\": \"'eval_loss':(.*?),\"}\n",
    "    ]\n",
    "    instance_count = 1\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    # data = s3_data\n",
    "    nKeepAliveSeconds = 3600 # Warmpool feature, 1 hour\n",
    "    print(f\"## Cloud mode is set with {instance_type} and {instance_count} of instance_count\")\n",
    "# print(\"dataset: \\n\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 데이터 위치 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      " {'train': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train/train_dataset.json', 'validation': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/validation/validation_dataset.json', 'config': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml'}\n"
     ]
    }
   ],
   "source": [
    "%store -r data\n",
    "print(\"data: \\n\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 세이지 메이커 파이프라인 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1. 모델 빌딩 파이프라인 변수 및 세션 생성\n",
    "\n",
    "파이프라인에 인자로 넘길 변수는 아래 크게 3가지 종류가 있습니다.\n",
    "- 모델 레지스트리에 모델 등록시에 모델 승인 상태 값    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "# 입력 데이터\n",
    "# s3_data_loc = ParameterString(\n",
    "#     name=\"InputData\",\n",
    "#     default_value=s3_input_data_uri,\n",
    "# )\n",
    "\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 로컬 모드 설정 \n",
    "- 로컬 모드 사용을 위해서는 Estimator, Pipeline() 오브젝트 생성시에 인자로서 sagemaker_session 에 LocalPipelineSession() 를 할당해야 합니다.\n",
    "- 모델 훈련 스텝은 로컬 모드가 가능합니다. \n",
    "    - 람다 스텝, 모델 등록 스텝은 지원을 하지 않음.\n",
    "- Tip : 노트북 하단에서 Pipeline() 를 선언시에 steps 부분을 주석 처리하면서 사용하세요.\n",
    "``` python\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        s3_data_loc,                \n",
    "        model_approval_status,        \n",
    "    ],\n",
    "    sagemaker_session=pipeline_session,\n",
    "    steps=[step_train],    # 로컬 모드 사용시\n",
    "#   steps=[step_train, step_repackage_lambda, step_model_registration],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### --> Cloud Mode\n"
     ]
    }
   ],
   "source": [
    "# LOCAL_MODE = True # 로컬 모드시 사용\n",
    "LOCAL_MODE = False # 클라우드 모드시 사용\n",
    "if LOCAL_MODE:\n",
    "    from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "    pipeline_session = LocalPipelineSession()\n",
    "    print(\"### --> Local Mode\")\n",
    "else:\n",
    "    from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "    pipeline_session = PipelineSession()\n",
    "    print(\"### --> Cloud Mode\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 캐싱 정의\n",
    "참고: 캐싱 파이프라인 단계: [Caching Pipeline Steps](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/pipelines-caching.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, \n",
    "                           expire_after=\"1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 파이프라인 스텝 단계 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 모델 훈련 스텝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Estimator 생성\n",
    "\n",
    "Estimator 생성시에 인자가 필요 합니다. 주요한 인자만 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "import time\n",
    "# define Training Job Name \n",
    "job_name = f'llama3-8b-naver-news-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "# chkpt_s3_path = f's3://{sess.default_bucket()}/{s3_prefix}/native/checkpoints'\n",
    "\n",
    "# create the Estimator\n",
    "os.environ['USE_SHORT_LIVED_CREDENTIALS']=\"1\" \n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'sm_run_fsdp_qlora_llama3_mlflow.py',      # train script\n",
    "    # source_dir           = '../../scripts',  # directory which includes all the files needed for training\n",
    "    source_dir           = 'src',  # directory which includes all the files needed for training        \n",
    "    instance_type        = instance_type,  # instances type used for the training job\n",
    "    instance_count       = instance_count,                 # the number of instances used for training\n",
    "    sagemaker_session    = sagemaker_session,\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 256,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',          # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    metric_definitions = metric_definitions,\n",
    "    hyperparameters      =  {\n",
    "        \"config\": \"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\" # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost    \n",
    "    keep_alive_period_in_seconds = nKeepAliveSeconds,     # warm pool \n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\n",
    "    environment  = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HF_TOKEN,       # huggingface token to access gated models, e.g. llama 3\n",
    "        \"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\"   # enable CPU RAM efficient loading\n",
    "    }, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 훈련 스탭 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/sagemaker/workflow/steps.py:485: UserWarning: Profiling is enabled on the provided estimator. The default profiler rule includes a timestamp which will change each time the pipeline is upserted, causing cache misses. If profiling is not needed, set disable_profiler to True on the estimator.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name= \"llama3-8b-naver-news-Training\",\n",
    "    estimator=huggingface_estimator,\n",
    "    # estimator=host_estimator,\n",
    "    inputs=data,\n",
    "    cache_config = cache_config, # 캐시 정의     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 모델 등록"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 그룹 생성\n",
    "\n",
    "- 참고\n",
    "    - 모델 그룹 릭스팅 API:  [ListModelPackageGroups](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ListModelPackageGroups.html)\n",
    "    - 모델 지표 등록: [Model Quality Metrics](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/model-monitor-model-quality-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client('sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama3-8b-Naver-News-Summarization exitss\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = f\"Llama3-8b-Naver-News-Summarization\"\n",
    "model_package_group_input_dict = {\n",
    " \"ModelPackageGroupName\" : model_package_group_name,\n",
    " \"ModelPackageGroupDescription\" : \"Sample model package group\"\n",
    "}\n",
    "response = sm_client.list_model_package_groups(NameContains=model_package_group_name)\n",
    "if len(response['ModelPackageGroupSummaryList']) == 0:\n",
    "    print(\"No model group exists\")\n",
    "    print(\"Create model group\")    \n",
    "    \n",
    "    create_model_pacakge_group_response = sm_client.create_model_package_group(**model_package_group_input_dict)\n",
    "    print('ModelPackageGroup Arn : {}'.format(create_model_pacakge_group_response['ModelPackageGroupArn']))    \n",
    "else:\n",
    "    print(f\"{model_package_group_name} exitss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 등록 스텝 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  session=sess,\n",
    "  version=\"2.0.2\",\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml.g5.4xlarge and # of GPU None is set\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "\n",
    "\n",
    "if instance_type == \"ml.p4d.24xlarge\":\n",
    "    num_GPUSs = 8\n",
    "elif instance_type == \"ml.g5.12xlarge\":\n",
    "    num_GPUSs = 4\n",
    "else:\n",
    "    num_GPUSs = None\n",
    "    \n",
    "print(f\"{instance_type} and # of GPU {num_GPUSs} is set\")\n",
    "\n",
    "health_check_timeout = 1200 # 20 minutes\n",
    "\n",
    "# import time\n",
    "# sm_endpoint_name = \"llama3-endpoint-{}\".format(int(time.time()))\n",
    "# print(\"sm_endpoint_name: \\n\", sm_endpoint_name)\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\",       # Path to the model in the container\n",
    "  'SM_NUM_GPUS': f\"{num_GPUSs}\",        # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': \"8000\",           # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': \"8096\",           # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_PREFILL_TOKENS': \"16182\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'MESSAGES_API_ENABLED': \"true\",       # Enable the OpenAI Messages API\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 452 μs, sys: 406 μs, total: 858 μs\n",
      "Wall time: 864 μs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "\n",
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    image_uri=llm_image,\n",
    "    model_data = \"s3://sagemaker-us-east-1-057716757052/pipelines-69ym4x1yzbuc-llama3-8b-naver-news-JSAp9XTtL9/output/model\",\n",
    "    # model_data = step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    env=config\n",
    ")\n",
    "\n",
    "\n",
    "register_model_step_args = model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.g5.4xlarge\", \"ml.g5.12xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    ")\n",
    "\n",
    "step_model_registration = ModelStep(\n",
    "   name=\"RegisterModel\",\n",
    "   step_args=register_model_step_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.모델 빌딩 파이프라인 정의 및 실행\n",
    "위에서 정의한 아래의 4개의 스텝으로 파이프라인 정의를 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "project_prefix = 'llama3-8b-naver-neews-summarization'\n",
    "\n",
    "pipeline_name = project_prefix\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        # s3_data_loc,                \n",
    "        model_approval_status,        \n",
    "    ],\n",
    "    sagemaker_session=pipeline_session,\n",
    "#    steps=[step_train],    \n",
    "    steps=[step_train, step_model_registration],\n",
    "#    steps=[step_repackage_lambda, step_model_registration],    \n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "definition = json.loads(pipeline.definition())\n",
    "# print(\" definition : \\n\", definition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이프라인 운영: 파이프라인 대기 및 실행상태 확인\n",
    "\n",
    "워크플로우의 실행상황을 살펴봅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행이 완료될 때까지 기다립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "WaiterError",
     "evalue": "Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWaiterError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexecution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/sagemaker/workflow/pipeline.py:934\u001b[0m, in \u001b[0;36m_PipelineExecution.wait\u001b[0;34m(self, delay, max_attempts)\u001b[0m\n\u001b[1;32m    905\u001b[0m model \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mwaiter\u001b[38;5;241m.\u001b[39mWaiterModel(\n\u001b[1;32m    906\u001b[0m     {\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    929\u001b[0m     }\n\u001b[1;32m    930\u001b[0m )\n\u001b[1;32m    931\u001b[0m waiter \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mwaiter\u001b[38;5;241m.\u001b[39mcreate_waiter_with_client(\n\u001b[1;32m    932\u001b[0m     waiter_id, model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39msagemaker_client\n\u001b[1;32m    933\u001b[0m )\n\u001b[0;32m--> 934\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPipelineExecutionArn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/botocore/waiter.py:55\u001b[0m, in \u001b[0;36mcreate_waiter_with_client.<locals>.wait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mWaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/botocore/waiter.py:374\u001b[0m, in \u001b[0;36mWaiter.wait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailure\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    373\u001b[0m     reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWaiter encountered a terminal failure state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macceptor\u001b[38;5;241m.\u001b[39mexplanation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WaiterError(\n\u001b[1;32m    375\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    376\u001b[0m         reason\u001b[38;5;241m=\u001b[39mreason,\n\u001b[1;32m    377\u001b[0m         last_response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    378\u001b[0m     )\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_attempts \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_attempts:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_matched_acceptor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mWaiterError\u001b[0m: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\""
     ]
    }
   ],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행된 단계들을 리스트업합니다. 파이프라인의 단계실행 서비스에 의해 시작되거나 완료된 단계를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'llama3-8b-naver-news-Training',\n",
       "  'StartTime': datetime.datetime(2024, 8, 24, 6, 11, 37, 339000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2024, 8, 24, 6, 11, 37, 827000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'CacheHitResult': {'SourcePipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:057716757052:pipeline/llama3-8b-naver-neews-summarization/execution/69ym4x1yzbuc'},\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:057716757052:training-job/pipelines-69ym4x1yzbuc-llama3-8b-naver-news-JSAp9XTtL9'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'RegisterModel-RegisterModel',\n",
       "  'StartTime': datetime.datetime(2024, 8, 24, 6, 11, 37, 339000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2024, 8, 24, 6, 11, 38, 675000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Failed',\n",
       "  'FailureReason': 'ClientError: Failed to invoke sagemaker:CreateModelPackage. Error Details: Cannot find S3 object: pipelines-69ym4x1yzbuc-llama3-8b-naver-news-JSAp9XTtL9/output/model in bucket sagemaker-us-east-1-057716757052. Please check if your S3 object exists and has proper permissions for SageMaker.',\n",
       "  'Metadata': {},\n",
       "  'AttemptCount': 1}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pipeline 캐싱 및 파라미터 이용한 실행\n",
    "캐싱은 2021년 7월 현재 Training, Processing, Transform 의 Step에 적용이 되어 있습니다.\n",
    "\n",
    "상세 사항은 여기를 확인하세요. --> [캐싱 파이프라인 단계](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/pipelines-caching.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # is_cache = True\n",
    "# is_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# from IPython.display import display as dp\n",
    "# import time\n",
    "\n",
    "# if is_cache:\n",
    "#     execution = pipeline.start(\n",
    "#         parameters=dict(\n",
    "#             # model2eval2threshold=0.8,\n",
    "#         )\n",
    "#     )    \n",
    "    \n",
    "#     # execution = pipeline.start()\n",
    "#     execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if is_cache:\n",
    "#     dp(execution.list_steps())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. SageMaker Studio 에서 파이프라인 보기\n",
    "스튜디오를 사용하기 위해서는 아래 링크를 참조하여, 로그인하시기 바랍니다.\n",
    "- [Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. 모델 훈련 파이프라인 실행 내용 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![studio_pipeline_train.png](img/studio_pipeline_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. 모델 훈련 파이프라인 단계 별 실행 결과 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline_graph.png](img/pipeline_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. 모델 레지스트리 \n",
    "- 생성된 \"NCFModel\" 을 확인하세요.\n",
    "    - 나머지 모델 그룹은 저자가 생성한 것이기에 무시하시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model_registry.png](img/model_registry.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. 모델 그룹 버전 정보\n",
    "- 처음 생성한 버전이 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model_group_info.png](img/model_group_info.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. 모델 버전의 등록된 컨테이너 정보\n",
    "- 6.4 그림의 Version - 1 을 클릭하고 Settings 탭을 클릭 후에 스크롤 하면 아래의 컨테이너 정보가 보입니다.\n",
    "- 아래 정보는 모델 등록 스텝에서 우리가 등록한 2가지 중요한 정보 입니다.\n",
    "    - 추론 도커 이미지 주소 \n",
    "    - 리패키징한 모델 아티펙트 주소 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![container_info.png](img/container_info.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "def check_s3_object(bucket, key):\n",
    "    s3 = boto3.client('s3')\n",
    "    s3_full_path = os.path.join(bucket, key)\n",
    "    print(\"## s3_full_path: \", s3_full_path) \n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "        print(f\"Object exists: s3://{bucket}/{key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Object does not exist or is not accessible: s3://{bucket}/{key}\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "def get_training_job_output(job_name):\n",
    "    sm = boto3.client('sagemaker')\n",
    "    try:\n",
    "        response = sm.describe_training_job(TrainingJobName=job_name)\n",
    "        return response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting training job output: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def update_pipeline_model_path(pipeline_name, step_name, new_model_path):\n",
    "    sm = sagemaker.Session()\n",
    "    try:\n",
    "        pipeline = sm.sagemaker_client.describe_pipeline(PipelineName=pipeline_name)\n",
    "        for step in pipeline['PipelineDefinition']['Steps']:\n",
    "            if step['Name'] == step_name:\n",
    "                # This is a placeholder. The actual update logic depends on your pipeline structure\n",
    "                print(f\"Update the model path in step {step_name} to {new_model_path}\")\n",
    "                # You would need to modify the step's Properties here\n",
    "        \n",
    "        # After modifying the pipeline definition, update the pipeline\n",
    "        sm.sagemaker_client.update_pipeline(\n",
    "            PipelineName=pipeline_name,\n",
    "            PipelineDefinition=json.dumps(pipeline['PipelineDefinition'])\n",
    "        )\n",
    "        print(f\"Pipeline {pipeline_name} updated successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating pipeline: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "pipeline_name = \"llama3-8b-naver-neews-summarization\"\n",
    "training_job_name = \"pipelines-69ym4x1yzbuc-llama3-8b-naver-news-JSAp9XTtL9\"\n",
    "register_model_step_name = \"RegisterModel-RegisterModel\"\n",
    "\n",
    "# Check if the model artifact exists\n",
    "model_artifact = get_training_job_output(training_job_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## True\n",
      "## s3_full_path:  sagemaker-us-east-1-057716757052/pipelines-69ym4x1yzbuc-llama3-8b-naver-news-JSAp9XTtL9/output/model\n",
      "Object does not exist or is not accessible: s3://sagemaker-us-east-1-057716757052/pipelines-69ym4x1yzbuc-llama3-8b-naver-news-JSAp9XTtL9/output/model\n",
      "Error: An error occurred (404) when calling the HeadObject operation: Not Found\n",
      "Error updating pipeline: string indices must be integers\n"
     ]
    }
   ],
   "source": [
    "if model_artifact:\n",
    "    print(\"## True\")\n",
    "    bucket, key = model_artifact.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    check_s3_object(bucket, key)\n",
    "\n",
    "    # If the object exists, update the pipeline\n",
    "    update_pipeline_model_path(pipeline_name, register_model_step_name, model_artifact)\n",
    "else:\n",
    "    print(\"## Fasle\")\n",
    "    print(\"Unable to get model artifact path. Check the training job logs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE model/\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls s3://sagemaker-us-east-1-057716757052/pipelines-69ym4x1yzbuc-llama3-8b-naver-news-JSAp9XTtL9/output/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Disable caching for the next run (if needed)\n",
    "sm = sagemaker.Session()\n",
    "sm.sagemaker_client.update_pipeline(\n",
    "    PipelineName=pipeline_name,\n",
    "    PipelineDescription=\"Updated pipeline with caching disabled\",\n",
    "    CacheConfig={\n",
    "        \"Enabled\": False\n",
    "    }\n",
    ")\n",
    "print(\"Pipeline caching disabled for the next run\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_llama3_puy310",
   "language": "python",
   "name": "conda_llama3_puy310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
