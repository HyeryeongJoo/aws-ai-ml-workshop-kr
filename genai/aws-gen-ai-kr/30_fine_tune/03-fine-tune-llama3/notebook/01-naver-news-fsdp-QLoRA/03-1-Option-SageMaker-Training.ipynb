{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Option] SageMaker 에서 Llama 3 파인 튜닝 (Full Dataset 훈련)\n",
    "- 이 노트북은 옵션으로서, 전체 데이터 셋 및 Epoch 수를 늘려서 실행 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter HUGGINGFACE Access Token:  ········\n",
      "Enter HUGGINGFACE Access Token:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:  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\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/SageMaker/.cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# is_sagemaker_notebook = True\n",
    "is_sagemaker_notebook = False # use VS Code\n",
    "\n",
    "if is_sagemaker_notebook:\n",
    "    HF_TOKEN = getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN') or getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "    print(\"token: \", HF_TOKEN)\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장된 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder:  ../data/naver-news-summarization-ko\n",
      "train_data_json:  ../data/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_data_json:  ../data/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_data_json:  ../data/naver-news-summarization-ko/test/test_dataset.json\n",
      "full_train_data_json:  ../data/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "full_validation_data_json:  ../data/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "full_test_data_json:  ../data/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%store -r data_folder\n",
    "%store -r train_data_json \n",
    "%store -r validation_data_json \n",
    "%store -r test_data_json \n",
    "%store -r full_train_data_json \n",
    "%store -r full_validation_data_json \n",
    "%store -r full_test_data_json\n",
    "\n",
    "\n",
    "print(\"data_folder: \", data_folder)\n",
    "print(\"train_data_json: \", train_data_json)\n",
    "print(\"validation_data_json: \", validation_data_json)\n",
    "print(\"test_data_json: \", test_data_json)\n",
    "print(\"full_train_data_json: \", full_train_data_json)\n",
    "print(\"full_validation_data_json: \", full_validation_data_json)\n",
    "print(\"full_test_data_json: \", full_test_data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker 기본 변수 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::057716757052:role/gen_ai_gsmoon\n",
      "sagemaker bucket: sagemaker-us-east-1-057716757052\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비\n",
    "- 두가지 종류의 데이터 셋을 업로드 합니다.\n",
    "    - Full Dataset: 전체 데이타를 업로드 합니다.\n",
    "    - Sample Dataset: 디버깅 용도의 일부 데이타를 업로드 합니다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 데이터 셋 경로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko\n",
      "train_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/test/test_dataset.json\n",
      "\n",
      "input_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko\n",
      "train_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "validation_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "test_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "def create_s3_path(sess,is_full, data_folder,train_data_json,validation_data_json,test_data_json,verbose=True  ):\n",
    "    dataset_name = data_folder.split('/')[-1]\n",
    "    # save train_dataset to s3 using our SageMaker session\n",
    "    input_path = f's3://{sess.default_bucket()}/datasets/{dataset_name}'\n",
    "    print(\"input_path: \\n\", input_path)\n",
    "\n",
    "    trian_file_name = train_data_json.split('/')[-1]\n",
    "    validation_file_name = validation_data_json.split('/')[-1]\n",
    "    test_file_name = test_data_json.split('/')[-1]\n",
    "\n",
    "    if is_full:\n",
    "        train_dataset_s3_path = f\"{input_path}/full_train/{trian_file_name}\"\n",
    "        validation_dataset_s3_path = f\"{input_path}/full_validation/{validation_file_name}\"\n",
    "        test_dataset_s3_path = f\"{input_path}/full_test/{test_file_name}\"\n",
    "    else:\n",
    "        train_dataset_s3_path = f\"{input_path}/train/{trian_file_name}\"\n",
    "        validation_dataset_s3_path = f\"{input_path}/validation/{validation_file_name}\"\n",
    "        test_dataset_s3_path = f\"{input_path}/test/{test_file_name}\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"train_dataset_s3_path: \\n\", train_dataset_s3_path)\n",
    "        print(\"validation_dataset_s3_path: \\n\", validation_dataset_s3_path)\n",
    "        print(\"test_dataset_s3_path: \\n\", test_dataset_s3_path)\n",
    "\n",
    "    return train_dataset_s3_path, validation_dataset_s3_path, test_dataset_s3_path, input_path\n",
    "\n",
    "train_dataset_s3_path, validation_dataset_s3_path, test_dataset_s3_path, input_path = create_s3_path(\n",
    "                                                                            sess=sess,\n",
    "                                                                            is_full = False,\n",
    "                                                                            data_folder=data_folder,\n",
    "                                                                            train_data_json=train_data_json,\n",
    "                                                                            validation_data_json=validation_data_json,\n",
    "                                                                            test_data_json=test_data_json)    \n",
    "print(\"\")\n",
    "full_train_dataset_s3_path, full_validation_dataset_s3_path, full_test_dataset_s3_path, input_path = create_s3_path(\n",
    "                                                                            sess=sess,\n",
    "                                                                            is_full = True,\n",
    "                                                                            data_folder=data_folder,\n",
    "                                                                            train_data_json=full_train_data_json,\n",
    "                                                                            validation_data_json=full_validation_data_json,\n",
    "                                                                            test_data_json=full_test_data_json)    \n",
    "\n",
    "# full_train_data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이타를 S3 에 업로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_s3_prefix_name(s3_path, verbose=True):\n",
    "    file_name = s3_path.split('/')[-1]\n",
    "    file_name = '/' + file_name\n",
    "    desired_s3_uri = s3_path.split(file_name)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"file_name: \", file_name)\n",
    "        print(\"desired_s3_uri: \", desired_s3_uri)\n",
    "    return desired_s3_uri\n",
    "\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "def upload_data_s3(desired_s3_uri, file_name, verbose=True):\n",
    "    # upload the model yaml file to s3\n",
    "    \n",
    "    file_s3_path = S3Uploader.upload(local_path=file_name, desired_s3_uri=desired_s3_uri)\n",
    "\n",
    "    print(f\"{file_name} is uploaded to:\")\n",
    "    print(file_s3_path)\n",
    "\n",
    "    return file_s3_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug 용 작은 데이터셋 S3 업로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name:  /train_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train\n",
      "../data/naver-news-summarization-ko/train/train_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train/train_dataset.json\n",
      "\n",
      "file_name:  /validation_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/validation\n",
      "../data/naver-news-summarization-ko/validation/validation_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "\n",
      "file_name:  /test_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/test\n",
      "../data/naver-news-summarization-ko/test/test_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/test/test_dataset.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/test/test_dataset.json'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "######## Train File\n",
    "# return s3 URI, e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "train_desired_s3_uri = get_s3_prefix_name(train_dataset_s3_path)    \n",
    "# upload local file to e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "upload_data_s3(desired_s3_uri=train_desired_s3_uri, file_name=train_data_json, verbose=True)\n",
    "######## Validation File\n",
    "print(\"\")\n",
    "validation_desired_s3_uri = get_s3_prefix_name(validation_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=validation_desired_s3_uri, file_name=validation_data_json, verbose=True)\n",
    "######## Test File\n",
    "print(\"\")\n",
    "test_desired_s3_uri = get_s3_prefix_name(test_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=test_desired_s3_uri, file_name=test_data_json, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가용 큰 데이터셋 S3 업로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name:  /train_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_train\n",
      "../data/naver-news-summarization-ko/full_train/train_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "\n",
      "file_name:  /validation_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_validation\n",
      "../data/naver-news-summarization-ko/full_validation/validation_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "\n",
      "file_name:  /test_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_test\n",
      "../data/naver-news-summarization-ko/full_test/test_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_test/test_dataset.json'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "######## Train File\n",
    "# return s3 URI, e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "full_train_desired_s3_uri = get_s3_prefix_name(full_train_dataset_s3_path)    \n",
    "# upload local file to e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "upload_data_s3(desired_s3_uri=full_train_desired_s3_uri, file_name=full_train_data_json, verbose=True)\n",
    "######## Validation File\n",
    "print(\"\")\n",
    "full_validation_desired_s3_uri = get_s3_prefix_name(full_validation_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=full_validation_desired_s3_uri, file_name=full_validation_data_json, verbose=True)\n",
    "######## Test File\n",
    "print(\"\")\n",
    "full_test_desired_s3_uri = get_s3_prefix_name(full_test_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=full_test_desired_s3_uri, file_name=full_test_data_json, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 업로드 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-29 07:08:56    2.4 KiB datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml\n",
      "2024-07-29 07:13:09    8.4 MiB datasets/naver-news-summarization-ko/full_test/test_dataset.json\n",
      "2024-07-29 07:13:08   68.0 MiB datasets/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "2024-07-29 07:13:09    7.6 MiB datasets/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "2024-07-29 07:13:08   33.4 KiB datasets/naver-news-summarization-ko/test/test_dataset.json\n",
      "2024-07-29 07:13:08   28.1 KiB datasets/naver-news-summarization-ko/train/train_dataset.json\n",
      "2024-07-29 07:13:08   26.1 KiB datasets/naver-news-summarization-ko/validation/validation_dataset.json\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {input_path}  --recursive --human-readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws s3 rm {input_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws s3 cp {train_data_json} {train_dataset_s3_path}\n",
    "# ! aws s3 cp {validation_data_json} {validation_dataset_s3_path}\n",
    "# ! aws s3 cp {test_data_json} {test_dataset_s3_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 훈련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "config_folder_name = \"accelerator_config\"\n",
    "os.makedirs(config_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 설정 파일 준비\n",
    "- 목적에 맞게 아래의 두 개의 부분을 주석을 이용하여 사용 하세요.\n",
    "    - For Debug 부분은 일부 샘플 데이타를 통해서 빠르게 디버깅 목적의 파라미터 값 입니다.\n",
    "    - For evaluation: 전체 데이터를 통해서 최적의 파라미터 값 입니다.\n",
    "```\n",
    "###########################             \n",
    "# For Debug\n",
    "###########################             \n",
    "num_train_epochs: 5                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "# For evaluation\n",
    "###########################             \n",
    "# num_train_epochs: 3                    # number of training epochs\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 8          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "max_seq_len:  2048              # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\" # path to where SageMaker saves train dataset\n",
    "validation_dataset_path: \"/opt/ml/input/data/validation/\" # path to where SageMaker saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"   # path to where SageMaker saves test dataset\n",
    "output_dir: \"/tmp/llama3\"            # where the LoRA adapter weight is\n",
    "# training parameters\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "###########################             \n",
    "# For Debug\n",
    "###########################             \n",
    "# num_train_epochs: 1                    # number of training epochs\n",
    "# per_device_train_batch_size: 1         # batch size per device during training\n",
    "# per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "# For evaluation\n",
    "###########################             \n",
    "num_train_epochs: 3                    # number of training epochs\n",
    "per_device_train_batch_size: 16         # batch size per device during training\n",
    "per_device_eval_batch_size: 8          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정 파일을 S3 에 업로드\n",
    "- 위에 정의한 파일을 업로드 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_desired_s3_uri = f\"{input_path}/config\"\n",
    "config_model_name = \"accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml\"\n",
    "train_config_s3_path = upload_data_s3(desired_s3_uri=config_desired_s3_uri, file_name=config_model_name, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 입력 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 사이즈 조정 \n",
    "- 디버그 용도이면 run_debug_sample = True, 전데 데이터 이면 False 로 조절 하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_train/train_dataset.json',\n",
       " 'validation': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_validation/validation_dataset.json',\n",
       " 'config': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# run_debug_sample = True\n",
    "run_debug_sample = False\n",
    "if run_debug_sample:\n",
    "  local_data = {\n",
    "    'train': f'file://{train_data_json}',\n",
    "    'validation': f'file://{validation_data_json}',\n",
    "    'config': f'file://{config_model_name}'\n",
    "    }\n",
    "\n",
    "  s3_data = {\n",
    "    'train': train_dataset_s3_path,\n",
    "    'validation': validation_dataset_s3_path,\n",
    "    'config': train_config_s3_path\n",
    "    }  \n",
    "else:\n",
    "  local_data = {\n",
    "    'train': f'file://{train_data_json}',\n",
    "    'validation': f'file://{validation_data_json}',\n",
    "    'config': f'file://{config_model_name}'\n",
    "    }\n",
    "  s3_data = {\n",
    "    'train': full_train_dataset_s3_path,\n",
    "    'validation': full_validation_dataset_s3_path,\n",
    "    'config': train_config_s3_path\n",
    "    }  \n",
    "s3_data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clolud 모드 및 Local 사용\n",
    "- 현재 로컬 모드는 에러 발행. 확인 중 임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Cloud mode is set with ml.g5.48xlarge and 1 of instance_count\n",
      "dataset: \n",
      " {'train': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_train/train_dataset.json', 'validation': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_validation/validation_dataset.json', 'config': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml'}\n"
     ]
    }
   ],
   "source": [
    "# USE_LOCAL_MODE = True\n",
    "USE_LOCAL_MODE = False\n",
    "\n",
    "import torch\n",
    "\n",
    "if USE_LOCAL_MODE:\n",
    "    instance_type = 'local_gpu' if torch.cuda.is_available() else 'local'\n",
    "    instance_count = 1\n",
    "    from sagemaker.local import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    data = local_data \n",
    "    # data = s3_data\n",
    "    metric_definitions = None\n",
    "    nKeepAliveSeconds = None # Warmpool feature\n",
    "    print(\"## Local mode is set\")\n",
    "else:\n",
    "    # instance_type = 'ml.g5.4xlarge'\n",
    "    # instance_type = 'ml.g5.12xlarge'\n",
    "    instance_type = 'ml.g5.48xlarge'\n",
    "    # instance_type = 'ml.p4d.24xlarge'\n",
    "    # Emit: \n",
    "    # {'train_runtime': 37.2985, 'train_samples_per_second': 0.375, 'train_steps_per_second': 0.054, 'train_loss': 2.3541293144226074, 'epoch': 1.0}\n",
    "    # {'eval_loss': 2.50766658782959, 'eval_runtime': 3.4741, 'eval_samples_per_second': 3.454, 'eval_steps_per_second': 0.864, 'epoch': 1.0}\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"train:loss\", \"Regex\": \"'train_loss':(.*?),\"},\n",
    "        {\"Name\": \"validation:loss\", \"Regex\": \"'eval_loss':(.*?),\"}\n",
    "    ]\n",
    "    instance_count = 1\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    data = s3_data\n",
    "    nKeepAliveSeconds = 3600 # Warmpool feature, 1 hour\n",
    "    print(f\"## Cloud mode is set with {instance_type} and {instance_count} of instance_count\")\n",
    "print(\"dataset: \\n\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 Estimator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "import time\n",
    "# define Training Job Name \n",
    "job_name = f'llama3-8b-naver-news-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "# chkpt_s3_path = f's3://{sess.default_bucket()}/{s3_prefix}/native/checkpoints'\n",
    "\n",
    "# create the Estimator\n",
    "os.environ['USE_SHORT_LIVED_CREDENTIALS']=\"1\" \n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'sm_run_fsdp_qlora_llama3.py',      # train script\n",
    "    source_dir           = '../../scripts',  # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type,  # instances type used for the training job\n",
    "    instance_count       = instance_count,                 # the number of instances used for training\n",
    "    sagemaker_session    = sagemaker_session,\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 256,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',          # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    metric_definitions = metric_definitions,\n",
    "    hyperparameters      =  {\n",
    "        \"config\": \"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\" # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost    \n",
    "    keep_alive_period_in_seconds = nKeepAliveSeconds,     # warm pool \n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\n",
    "    environment  = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HF_TOKEN,       # huggingface token to access gated models, e.g. llama 3\n",
    "        \"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\"   # enable CPU RAM efficient loading\n",
    "    }, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 훈련 실행\n",
    "- 로컬 모드시에는 모델 저장을 하지 않습니다. 훈련 스크립트에서 처리 합니다. (현재 모델 저장시에 /tmp 의 용량이 차서 에러가 발생 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_name:naver-news-summarization-ko\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.experiments.run:The run (training-job-experiment) under experiment (naver-news-summarization-ko) already exists. Loading it.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: llama3-8b-naver-news-2024-07-29-07-13-1-2024-07-29-07-13-12-264\n"
     ]
    }
   ],
   "source": [
    "if USE_LOCAL_MODE:\n",
    "    huggingface_estimator.fit(data, wait=False)\n",
    "else:\n",
    "    from sagemaker.experiments.run import Run\n",
    "    from sagemaker.utils import unique_name_from_base\n",
    "    from sagemaker.session import Session\n",
    "\n",
    "    # set new experiment configuration\n",
    "    # naver-news-summarization-ko\n",
    "    experiment_name = data_folder.split('/')[-1]\n",
    "    \n",
    "    run_name = f\"training-job-experiment\"\n",
    "    print(f\"experiment_name:{experiment_name}\")    \n",
    "\n",
    "    with Run(experiment_name=experiment_name, run_name=run_name, sagemaker_session=sagemaker_session) as run:\n",
    "        huggingface_estimator.fit(data,wait=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-29 10:20:14 Starting - Starting the training job\n",
      "2024-07-29 10:20:14 Pending - Preparing the instances for training\n",
      "2024-07-29 10:20:14 Downloading - Downloading the training image\n",
      "2024-07-29 10:20:14 Training - Training image download completed. Training in progress.\n",
      "2024-07-29 10:20:14 Uploading - Uploading generated training model\n",
      "2024-07-29 10:20:14 Completed - Resource released due to keep alive period expiry\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-07-29 07:21:47,464 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-07-29 07:21:47,530 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-29 07:21:47,542 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-07-29 07:21:47,544 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2024-07-29 07:21:47,544 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-07-29 07:21:49,290 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.40.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.6/137.6 kB 8.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.18.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.29.3 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: evaluate==0.4.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.4.1)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.43.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface_hub==0.22.2 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.8.6 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.10.0 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting torch==2.1.1 (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1->-r requirements.txt (line 4)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.22.2->-r requirements.txt (line 6)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.6->-r requirements.txt (line 7)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 9)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 9)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 9)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 9)) (2.1.0)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (0.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (13.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (1.6.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.1->-r requirements.txt (line 9)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.1->-r requirements.txt (line 9)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (0.1.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.0/9.0 MB 94.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 47.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.29.3-py3-none-any.whl (297 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.6/297.6 kB 35.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 20.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 40.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.8.6-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.2/245.2 kB 31.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 28.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 670.2/670.2 MB 2.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 3.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 91.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 64.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 60.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 1.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 19.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 37.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 21.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 13.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.8/209.8 MB 11.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 13.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 98.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 75.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets, bitsandbytes, accelerate, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface_hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.15.1\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.15.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.15.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.1.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.15.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.15.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.15.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: bitsandbytes\u001b[0m\n",
      "\u001b[34mFound existing installation: bitsandbytes 0.42.0\u001b[0m\n",
      "\u001b[34mUninstalling bitsandbytes-0.42.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled bitsandbytes-0.42.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.25.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.25.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.25.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.7.4\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.7.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.7.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.7.1\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.7.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.29.3 bitsandbytes-0.43.1 datasets-2.18.0 huggingface_hub-0.22.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 peft-0.10.0 tokenizers-0.19.1 torch-2.1.1 transformers-4.40.0 trl-0.8.6\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-07-29 07:23:19,918 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-29 07:23:19,918 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-29 07:23:20,003 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-29 07:23:20,080 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-29 07:23:20,092 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2024-07-29 07:23:20,156 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-29 07:23:20,171 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"config\": \"/opt/ml/input/data/config\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config\": \"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"config\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"llama3-8b-naver-news-2024-07-29-07-13-1-2024-07-29-07-13-12-264\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-29-07-13-1-2024-07-29-07-13-12-264/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sm_run_fsdp_qlora_llama3\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sm_run_fsdp_qlora_llama3.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sm_run_fsdp_qlora_llama3.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"config\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.48xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sm_run_fsdp_qlora_llama3\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-29-07-13-1-2024-07-29-07-13-12-264/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"config\":\"/opt/ml/input/data/config\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"llama3-8b-naver-news-2024-07-29-07-13-1-2024-07-29-07-13-12-264\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-29-07-13-1-2024-07-29-07-13-12-264/source/sourcedir.tar.gz\",\"module_name\":\"sm_run_fsdp_qlora_llama3\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sm_run_fsdp_qlora_llama3.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CONFIG=/opt/ml/input/data/config\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 1 --nproc_per_node 8 sm_run_fsdp_qlora_llama3.py --config /opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\u001b[0m\n",
      "\u001b[34m[2024-07-29 07:23:21,608] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-07-29 07:23:21,608] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-07-29 07:23:21,608] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-07-29 07:23:21,608] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m## SM_CURRENT_INSTANCE_TYPE:  ml.g5.48xlarge\u001b[0m\n",
      "\u001b[34m## script_args: \n",
      " ScriptArguments(train_dataset_path='/opt/ml/input/data/train/', validation_dataset_path='/opt/ml/input/data/validation/', model_id='meta-llama/Meta-Llama-3-8B', max_seq_length=512)\u001b[0m\n",
      "\u001b[34m## training_args:\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_do_concat_batches=True,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>, <FSDPOption.OFFLOAD: 'offload'>],\u001b[0m\n",
      "\u001b[34mfsdp_config={'backward_prefetch': 'backward_pre', 'forward_prefetch': 'false', 'use_orig_params': 'false', 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0002,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/tmp/llama3/runs/Jul29_07-23-26_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=constant,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=0.3,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/tmp/llama3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=16,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/tmp/llama3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=epoch,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=True,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.03,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6507 examples [00:00, 34470.38 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13020 examples [00:00, 38620.54 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19607 examples [00:00, 40589.33 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22194 examples [00:00, 39738.19 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2466 examples [00:00, 33720.49 examples/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/22194 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/22194 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/22194 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/22194 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/22194 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/22194 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/22194 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/22194 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 712/22194 [00:00<00:03, 7054.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 759/22194 [00:00<00:02, 7517.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 760/22194 [00:00<00:02, 7527.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 746/22194 [00:00<00:02, 7390.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 765/22194 [00:00<00:02, 7575.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▎         | 778/22194 [00:00<00:02, 7716.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 758/22194 [00:00<00:02, 7507.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 763/22194 [00:00<00:02, 7562.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 1594/22194 [00:00<00:02, 8085.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 1651/22194 [00:00<00:02, 8338.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 1649/22194 [00:00<00:02, 8319.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 1662/22194 [00:00<00:02, 8419.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 1666/22194 [00:00<00:02, 8412.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 1676/22194 [00:00<00:02, 8452.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 1647/22194 [00:00<00:02, 8314.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 1662/22194 [00:00<00:02, 8395.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█▏        | 2516/22194 [00:00<00:02, 8598.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 2605/22194 [00:00<00:02, 8881.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 2602/22194 [00:00<00:02, 8864.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 2595/22194 [00:00<00:02, 8830.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 2637/22194 [00:00<00:02, 8999.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 2641/22194 [00:00<00:02, 8995.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 2602/22194 [00:00<00:02, 8874.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 2626/22194 [00:00<00:02, 8959.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 3504/22194 [00:00<00:02, 8889.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 3563/22194 [00:00<00:02, 9152.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 3552/22194 [00:00<00:02, 9112.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 3532/22194 [00:00<00:02, 9036.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 3605/22194 [00:00<00:02, 9263.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 3597/22194 [00:00<00:02, 9213.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 3555/22194 [00:00<00:02, 9129.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 3589/22194 [00:00<00:02, 9221.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 4503/22194 [00:00<00:01, 8977.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 4519/22194 [00:00<00:01, 9160.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 4522/22194 [00:00<00:01, 9168.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 4512/22194 [00:00<00:01, 9118.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 4536/22194 [00:00<00:01, 9277.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 4525/22194 [00:00<00:01, 9227.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 4532/22194 [00:00<00:01, 9285.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 4517/22194 [00:00<00:01, 9140.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 5507/22194 [00:00<00:01, 9082.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 5524/22194 [00:00<00:01, 9282.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 5525/22194 [00:00<00:01, 9286.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 5516/22194 [00:00<00:01, 9231.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 5528/22194 [00:00<00:01, 9391.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 5528/22194 [00:00<00:01, 9372.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 5538/22194 [00:00<00:01, 9448.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 5522/22194 [00:00<00:01, 9287.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 6498/22194 [00:00<00:01, 9140.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 6517/22194 [00:00<00:01, 9352.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 6515/22194 [00:00<00:01, 9346.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 6517/22194 [00:00<00:01, 9335.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 6518/22194 [00:00<00:01, 9429.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 6523/22194 [00:00<00:01, 9450.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 6530/22194 [00:00<00:01, 9524.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 6520/22194 [00:00<00:01, 9364.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 7521/22194 [00:00<00:01, 9297.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 7516/22194 [00:00<00:01, 9284.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 7507/22194 [00:00<00:01, 9339.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 7523/22194 [00:00<00:01, 9378.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 7522/22194 [00:00<00:01, 9396.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 7527/22194 [00:00<00:01, 9452.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 7896/22194 [00:00<00:01, 9208.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 7518/22194 [00:00<00:01, 9310.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 8518/22194 [00:00<00:01, 9353.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 8519/22194 [00:00<00:01, 9338.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 8515/22194 [00:00<00:01, 9376.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 8530/22194 [00:00<00:01, 9452.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 8520/22194 [00:00<00:01, 9455.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 8529/22194 [00:00<00:01, 9501.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 8520/22194 [00:00<00:01, 9388.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 9212/22194 [00:01<00:01, 9045.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 9521/22194 [00:01<00:01, 9407.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 9516/22194 [00:01<00:01, 9383.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 9509/22194 [00:01<00:01, 9422.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 9525/22194 [00:01<00:01, 9499.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 9524/22194 [00:01<00:01, 9515.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 9532/22194 [00:01<00:01, 9563.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 9520/22194 [00:01<00:01, 9443.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 10119/22194 [00:01<00:01, 9049.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 10513/22194 [00:01<00:01, 9413.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 10517/22194 [00:01<00:01, 9408.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 10513/22194 [00:01<00:01, 9435.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 10524/22194 [00:01<00:01, 9517.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 10518/22194 [00:01<00:01, 9532.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 10533/22194 [00:01<00:01, 9597.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 10519/22194 [00:01<00:01, 9473.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 11031/22194 [00:01<00:01, 9065.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 11515/22194 [00:01<00:01, 9413.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 11520/22194 [00:01<00:01, 9435.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 11519/22194 [00:01<00:01, 9454.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 11519/22194 [00:01<00:01, 9513.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 11526/22194 [00:01<00:01, 9604.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 11524/22194 [00:01<00:01, 9552.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 11522/22194 [00:01<00:01, 9498.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 12000/22194 [00:01<00:01, 9070.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 12518/22194 [00:01<00:01, 9421.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 12529/22194 [00:01<00:01, 9471.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 12519/22194 [00:01<00:01, 9451.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 12521/22194 [00:01<00:01, 9512.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 12528/22194 [00:01<00:01, 9602.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 12527/22194 [00:01<00:01, 9576.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 12522/22194 [00:01<00:01, 9507.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▊    | 13000/22194 [00:01<00:01, 9112.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 13516/22194 [00:01<00:00, 9444.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 13524/22194 [00:01<00:00, 9506.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 13515/22194 [00:01<00:00, 9481.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 13520/22194 [00:01<00:00, 9523.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 13528/22194 [00:01<00:00, 9619.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 13523/22194 [00:01<00:00, 9591.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 13519/22194 [00:01<00:00, 9533.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 14000/22194 [00:01<00:00, 9102.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 14517/22194 [00:01<00:00, 9428.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 14526/22194 [00:01<00:00, 9504.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 14524/22194 [00:01<00:00, 9518.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 14533/22194 [00:01<00:00, 9612.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 14525/22194 [00:01<00:00, 9584.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 14518/22194 [00:01<00:00, 9350.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 14521/22194 [00:01<00:00, 9530.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 15000/22194 [00:01<00:00, 9096.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 15523/22194 [00:01<00:00, 9455.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 15529/22194 [00:01<00:00, 9531.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 15534/22194 [00:01<00:00, 9633.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 15528/22194 [00:01<00:00, 9538.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 15523/22194 [00:01<00:00, 9580.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 15518/22194 [00:01<00:00, 9368.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 15524/22194 [00:01<00:00, 9549.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 16000/22194 [00:01<00:00, 9149.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 16521/22194 [00:01<00:00, 9459.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 16519/22194 [00:01<00:00, 9509.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 16534/22194 [00:01<00:00, 9655.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 16524/22194 [00:01<00:00, 9607.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 16523/22194 [00:01<00:00, 9548.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 16519/22194 [00:01<00:00, 9425.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 16520/22194 [00:01<00:00, 9567.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 17000/22194 [00:01<00:00, 9161.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 17525/22194 [00:01<00:00, 9478.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 17529/22194 [00:01<00:00, 9530.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 17535/22194 [00:01<00:00, 9653.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 17529/22194 [00:01<00:00, 9621.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 17532/22194 [00:01<00:00, 9582.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 17519/22194 [00:01<00:00, 9466.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 17521/22194 [00:01<00:00, 9505.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 18000/22194 [00:01<00:00, 9182.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 18519/22194 [00:01<00:00, 9466.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 18526/22194 [00:01<00:00, 9531.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▎ | 18536/22194 [00:01<00:00, 9671.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 18524/22194 [00:01<00:00, 9628.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 18531/22194 [00:01<00:00, 9610.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 18517/22194 [00:01<00:00, 9506.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 18521/22194 [00:01<00:00, 9521.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 19000/22194 [00:02<00:00, 9215.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 19518/22194 [00:02<00:00, 9457.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 19529/22194 [00:02<00:00, 9531.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 19537/22194 [00:02<00:00, 9692.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 19519/22194 [00:02<00:00, 9560.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 19527/22194 [00:02<00:00, 9542.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 19510/22194 [00:02<00:00, 9495.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 19520/22194 [00:02<00:00, 9527.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 20000/22194 [00:02<00:00, 9246.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 20514/22194 [00:02<00:00, 9399.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 20523/22194 [00:02<00:00, 9461.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 20528/22194 [00:02<00:00, 9604.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 20525/22194 [00:02<00:00, 9502.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 20524/22194 [00:02<00:00, 9506.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 20517/22194 [00:02<00:00, 9455.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 20517/22194 [00:02<00:00, 9468.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▍| 21000/22194 [00:02<00:00, 9236.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 21515/22194 [00:02<00:00, 9396.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 21525/22194 [00:02<00:00, 9486.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 21530/22194 [00:02<00:00, 9605.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 21523/22194 [00:02<00:00, 9509.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 21521/22194 [00:02<00:00, 9504.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 21510/22194 [00:02<00:00, 9448.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 21521/22194 [00:02<00:00, 9463.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 22194/22194 [00:02<00:00, 9306.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 22194/22194 [00:02<00:00, 9343.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 22194/22194 [00:02<00:00, 9405.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 22194/22194 [00:02<00:00, 9419.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 22000/22194 [00:02<00:00, 9269.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 22194/22194 [00:02<00:00, 9313.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 22194/22194 [00:02<00:00, 9335.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 22194/22194 [00:02<00:00, 9354.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 22194/22194 [00:02<00:00, 9071.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2466 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2466 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2466 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2466 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2466 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2466 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2466 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2466 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 1000/2466 [00:00<00:00, 8942.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 1000/2466 [00:00<00:00, 9177.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 1000/2466 [00:00<00:00, 9262.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 1000/2466 [00:00<00:00, 9340.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 1000/2466 [00:00<00:00, 9133.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 1000/2466 [00:00<00:00, 9351.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 1000/2466 [00:00<00:00, 9288.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 1000/2466 [00:00<00:00, 9135.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 2000/2466 [00:00<00:00, 9009.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 2000/2466 [00:00<00:00, 9226.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 2000/2466 [00:00<00:00, 9343.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 2000/2466 [00:00<00:00, 9316.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 2000/2466 [00:00<00:00, 9218.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 2000/2466 [00:00<00:00, 9378.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 2000/2466 [00:00<00:00, 9311.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 2000/2466 [00:00<00:00, 9184.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2466/2466 [00:00<00:00, 9020.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2466/2466 [00:00<00:00, 9217.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2466/2466 [00:00<00:00, 9311.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2466/2466 [00:00<00:00, 9317.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2466/2466 [00:00<00:00, 9180.15 examples/s]\u001b[0m\n",
      "\u001b[34mYou are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m세종 뉴스1 김기남 기자 김동구 환경부 물통합정책관이 5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고 있다.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: 5일동구 환경부 물통합정책관이  5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고  다용도 복합 활용모델을 설명하고   버려지던 유출지하수를 탄소중립 핵심 수자원으로  탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델도 설명하고                                                                                                                                                                                                                                                                                                                                                                                                                                                 <|end_of_text|>\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2466/2466 [00:00<00:00, 9284.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2466/2466 [00:00<00:00, 9243.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2466/2466 [00:00<00:00, 8897.34 examples/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.18.1+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:75:209 [0] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:82:210 [7] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:79:214 [4] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:78:216 [3] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:77:212 [2] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:81:211 [6] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:76:215 [1] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:80:213 [5] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34mYou are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m세종 뉴스1 김기남 기자 김동구 환경부 물통합정책관이 5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고 있다.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: 5일동구 환경부 물통합정책관이  5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고  다용도 복합 활용모델을 설명하고   버려지던 유출지하수를 탄소중립 핵심 수자원으로  탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델도 설명하고                                                                                                                                                                                                                                                                                                                                                                                                                                                 <|end_of_text|>You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m세종 뉴스1 김기남 기자 김동구 환경부 물통합정책관이 5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고 있다.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: 5일동구 환경부 물통합정책관이  5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고  다용도 복합 활용모델을 설명하고   버려지던 유출지하수를 탄소중립 핵심 수자원으로  탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델도 설명하고                                                                                                                                                                                                                                                                                                                                                                                                                                                 <|end_of_text|>\u001b[0m\n",
      "\u001b[34mYou are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m세종 뉴스1 김기남 기자 김동구 환경부 물통합정책관이 5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고 있다.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: 5일동구 환경부 물통합정책관이  5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고  다용도 복합 활용모델을 설명하고   버려지던 유출지하수를 탄소중립 핵심 수자원으로  탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델도 설명하고                                                                                                                                                                                                                                                                                                                                                                                                                                                 <|end_of_text|>You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m세종 뉴스1 김기남 기자 김동구 환경부 물통합정책관이 5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고 있다.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: 5일동구 환경부 물통합정책관이  5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고  다용도 복합 활용모델을 설명하고   버려지던 유출지하수를 탄소중립 핵심 수자원으로  탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델도 설명하고                                                                                                                                                                                                                                                                                                                                                                                                                                                 <|end_of_text|>You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m세종 뉴스1 김기남 기자 김동구 환경부 물통합정책관이 5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고 있다.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: 5일동구 환경부 물통합정책관이  5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고  다용도 복합 활용모델을 설명하고   버려지던 유출지하수를 탄소중립 핵심 수자원으로  탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델도 설명하고                                                                                                                                                                                                                                                                                                                                                                                                                                                 <|end_of_text|>\u001b[0m\n",
      "\u001b[34mYou are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m세종 뉴스1 김기남 기자 김동구 환경부 물통합정책관이 5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고 있다.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: 5일동구 환경부 물통합정책관이  5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고  다용도 복합 활용모델을 설명하고   버려지던 유출지하수를 탄소중립 핵심 수자원으로  탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델도 설명하고                                                                                                                                                                                                                                                                                                                                                                                                                                                 <|end_of_text|>\u001b[0m\n",
      "\u001b[34mYou are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m세종 뉴스1 김기남 기자 김동구 환경부 물통합정책관이 5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고 있다.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: 5일동구 환경부 물통합정책관이  5일 오전 정부세종청사에서 버려지던 유출지하수를 탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델을 설명하고  다용도 복합 활용모델을 설명하고   버려지던 유출지하수를 탄소중립 핵심 수자원으로  탄소중립 핵심 수자원으로 활용하는 다용도 복합 활용모델도 설명하고                                                                                                                                                                                                                                                                                                                                                                                                                                                 <|end_of_text|>\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.20s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.17s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.19s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.21s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:26<00:26, 13.17s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:26<00:26, 13.15s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:26<00:26, 13.15s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:26<00:26, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:26<00:26, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:26<00:26, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:26<00:26, 13.17s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:26<00:26, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.74s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.06s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.69s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.06s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.06s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.59s/it]#015Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.69s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.77s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.81s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.24s/it]#015Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.69s/it]#015Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  2.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  2.36 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1000 examples [00:00, 2294.00 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1973 examples [00:01, 2233.97 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2721 examples [00:01, 3074.31 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3605 examples [00:01, 4048.50 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4599 examples [00:01, 2954.23 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5592 examples [00:01, 3835.69 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6608 examples [00:02, 3000.96 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7591 examples [00:02, 3716.02 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8614 examples [00:03, 2468.36 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9594 examples [00:03, 3147.15 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10614 examples [00:03, 2751.96 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11605 examples [00:03, 3409.31 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12607 examples [00:04, 2844.60 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13610 examples [00:04, 3565.31 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14603 examples [00:04, 2960.67 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15614 examples [00:05, 3637.84 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16601 examples [00:05, 2946.25 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17589 examples [00:05, 3647.66 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18609 examples [00:06, 3024.32 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19608 examples [00:06, 3687.00 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20601 examples [00:06, 2976.08 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21596 examples [00:06, 3688.82 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22495 examples [00:07, 2918.44 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23593 examples [00:07, 2709.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24047 examples [00:08, 2396.92 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 25000 examples [00:08, 3133.03 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 25548 examples [00:08, 2485.62 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 26424 examples [00:08, 3152.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 27503 examples [00:09, 2786.60 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 28075 examples [00:09, 3133.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 29000 examples [00:09, 3942.75 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 30000 examples [00:09, 2981.43 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 31000 examples [00:10, 3789.61 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 31809 examples [00:10, 2974.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 32607 examples [00:10, 3596.09 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 33401 examples [00:10, 2841.73 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 34449 examples [00:11, 3645.54 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 35233 examples [00:11, 4155.02 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 35233 examples [00:11, 3132.68 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  3.05 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1000 examples [00:00, 2701.56 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1974 examples [00:00, 2444.95 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2616 examples [00:01, 3087.14 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3621 examples [00:01, 4307.29 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3963 examples [00:01, 3274.64 examples/s]\u001b[0m\n",
      "\u001b[34mtrainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\u001b[0m\n",
      "\u001b[34m0%|          | 0/414 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/414 [00:28<3:16:51, 28.60s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/414 [00:52<2:56:01, 25.64s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/414 [01:15<2:49:20, 24.72s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/414 [01:39<2:45:53, 24.28s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/414 [02:02<2:43:43, 24.02s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 6/414 [02:26<2:42:17, 23.87s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/414 [02:50<2:41:16, 23.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/414 [03:13<2:40:56, 23.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 9/414 [03:37<2:40:08, 23.73s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10/414 [04:01<2:39:24, 23.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2591, 'grad_norm': 0.2578125, 'learning_rate': 0.0002, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m2%|▏         | 10/414 [04:01<2:39:24, 23.67s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/414 [04:24<2:38:50, 23.65s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/414 [04:48<2:38:20, 23.63s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 13/414 [05:11<2:37:49, 23.61s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 14/414 [05:35<2:37:15, 23.59s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 15/414 [05:58<2:36:47, 23.58s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 16/414 [06:22<2:36:17, 23.56s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 17/414 [06:45<2:35:54, 23.56s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 18/414 [07:09<2:35:30, 23.56s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 19/414 [07:33<2:35:07, 23.56s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 20/414 [07:56<2:35:10, 23.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0202, 'grad_norm': 0.087890625, 'learning_rate': 0.0002, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m5%|▍         | 20/414 [07:56<2:35:10, 23.63s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 21/414 [08:20<2:34:41, 23.62s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 22/414 [08:44<2:34:19, 23.62s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 23/414 [09:07<2:33:57, 23.63s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 24/414 [09:31<2:33:32, 23.62s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 25/414 [09:54<2:33:00, 23.60s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 26/414 [10:18<2:32:32, 23.59s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 27/414 [10:42<2:32:08, 23.59s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 28/414 [11:05<2:31:44, 23.59s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 29/414 [11:29<2:31:19, 23.58s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 30/414 [11:52<2:30:58, 23.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.972, 'grad_norm': 0.10107421875, 'learning_rate': 0.0002, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m7%|▋         | 30/414 [11:52<2:30:58, 23.59s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 31/414 [12:16<2:30:36, 23.59s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 32/414 [12:40<2:30:11, 23.59s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 33/414 [13:03<2:30:15, 23.66s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 34/414 [13:27<2:29:50, 23.66s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 35/414 [13:51<2:29:24, 23.65s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 36/414 [14:14<2:28:52, 23.63s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 37/414 [14:38<2:28:37, 23.65s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 38/414 [15:02<2:28:04, 23.63s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 39/414 [15:25<2:27:33, 23.61s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 40/414 [15:49<2:27:11, 23.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9328, 'grad_norm': 0.1494140625, 'learning_rate': 0.0002, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m10%|▉         | 40/414 [15:49<2:27:11, 23.61s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 41/414 [16:12<2:26:45, 23.61s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 42/414 [16:36<2:26:12, 23.58s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 43/414 [16:59<2:25:48, 23.58s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 44/414 [17:23<2:25:27, 23.59s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 45/414 [17:47<2:25:26, 23.65s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 46/414 [18:10<2:24:54, 23.63s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 47/414 [18:34<2:24:27, 23.62s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 48/414 [18:58<2:23:58, 23.60s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 49/414 [19:21<2:23:42, 23.62s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 50/414 [19:45<2:23:10, 23.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9062, 'grad_norm': 0.1064453125, 'learning_rate': 0.0002, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 50/414 [19:45<2:23:10, 23.60s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 51/414 [20:08<2:22:47, 23.60s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 52/414 [20:32<2:22:22, 23.60s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 53/414 [20:56<2:21:57, 23.59s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 54/414 [21:19<2:21:34, 23.60s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 55/414 [21:43<2:21:08, 23.59s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 56/414 [22:06<2:20:45, 23.59s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 57/414 [22:30<2:20:25, 23.60s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 58/414 [22:54<2:20:31, 23.68s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 59/414 [23:17<2:19:58, 23.66s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 60/414 [23:41<2:19:30, 23.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8836, 'grad_norm': 0.126953125, 'learning_rate': 0.0002, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 60/414 [23:41<2:19:30, 23.65s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 61/414 [24:05<2:19:04, 23.64s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 62/414 [24:28<2:18:40, 23.64s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 63/414 [24:52<2:18:16, 23.64s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 64/414 [25:16<2:17:53, 23.64s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 65/414 [25:39<2:17:33, 23.65s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 66/414 [26:03<2:17:02, 23.63s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 67/414 [26:26<2:16:37, 23.62s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 68/414 [26:50<2:16:10, 23.61s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 69/414 [27:14<2:15:47, 23.62s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 70/414 [27:37<2:15:26, 23.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8539, 'grad_norm': 0.142578125, 'learning_rate': 0.0002, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 70/414 [27:37<2:15:26, 23.62s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 71/414 [28:01<2:15:24, 23.69s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 72/414 [28:25<2:15:00, 23.68s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 73/414 [28:48<2:14:29, 23.67s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 74/414 [29:12<2:14:02, 23.66s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 75/414 [29:36<2:13:37, 23.65s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 76/414 [29:59<2:13:08, 23.63s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 77/414 [30:23<2:12:41, 23.63s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 78/414 [30:46<2:12:16, 23.62s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 79/414 [31:10<2:11:55, 23.63s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 80/414 [31:34<2:11:31, 23.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8507, 'grad_norm': 0.1298828125, 'learning_rate': 0.0002, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 80/414 [31:34<2:11:31, 23.63s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 81/414 [31:57<2:11:13, 23.65s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 82/414 [32:21<2:10:53, 23.66s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 83/414 [32:45<2:10:30, 23.66s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 84/414 [33:09<2:10:31, 23.73s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 85/414 [33:32<2:09:53, 23.69s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 86/414 [33:56<2:09:30, 23.69s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 87/414 [34:20<2:08:58, 23.67s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 88/414 [34:43<2:08:32, 23.66s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 89/414 [35:07<2:08:05, 23.65s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 90/414 [35:30<2:07:34, 23.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8289, 'grad_norm': 0.1435546875, 'learning_rate': 0.0002, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 90/414 [35:30<2:07:34, 23.63s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 91/414 [35:54<2:07:07, 23.62s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 92/414 [36:18<2:06:51, 23.64s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 93/414 [36:41<2:06:31, 23.65s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 94/414 [37:05<2:06:06, 23.64s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 95/414 [37:29<2:05:43, 23.65s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 96/414 [37:52<2:05:19, 23.65s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 97/414 [38:16<2:05:20, 23.72s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 98/414 [38:40<2:04:40, 23.67s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 99/414 [39:03<2:04:10, 23.65s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 100/414 [39:27<2:03:42, 23.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8116, 'grad_norm': 0.14453125, 'learning_rate': 0.0002, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 100/414 [39:27<2:03:42, 23.64s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 101/414 [39:51<2:03:21, 23.65s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 102/414 [40:14<2:02:57, 23.65s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 103/414 [40:38<2:02:32, 23.64s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 104/414 [41:02<2:02:09, 23.64s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 105/414 [41:25<2:01:42, 23.63s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 106/414 [41:49<2:01:18, 23.63s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 107/414 [42:12<2:00:59, 23.65s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 108/414 [42:36<2:00:31, 23.63s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 109/414 [43:00<2:00:11, 23.65s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 110/414 [43:24<2:00:05, 23.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8088, 'grad_norm': 0.158203125, 'learning_rate': 0.0002, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 110/414 [43:24<2:00:05, 23.70s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 111/414 [43:47<1:59:30, 23.66s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 112/414 [44:11<1:59:06, 23.66s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 113/414 [44:34<1:58:40, 23.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 114/414 [44:58<1:58:16, 23.65s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 115/414 [45:22<1:57:50, 23.65s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 116/414 [45:45<1:57:22, 23.63s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 117/414 [46:09<1:56:54, 23.62s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 118/414 [46:33<1:56:35, 23.63s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 119/414 [46:56<1:56:11, 23.63s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 120/414 [47:20<1:55:45, 23.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7988, 'grad_norm': 0.15234375, 'learning_rate': 0.0002, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 120/414 [47:20<1:55:45, 23.62s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 121/414 [47:43<1:55:24, 23.63s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 122/414 [48:07<1:54:56, 23.62s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 123/414 [48:31<1:54:53, 23.69s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 124/414 [48:55<1:54:31, 23.70s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 125/414 [49:18<1:54:05, 23.69s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 126/414 [49:42<1:53:35, 23.66s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 127/414 [50:06<1:53:11, 23.66s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 128/414 [50:29<1:52:45, 23.65s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 129/414 [50:53<1:52:21, 23.66s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 130/414 [51:17<1:51:57, 23.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7814, 'grad_norm': 0.171875, 'learning_rate': 0.0002, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 130/414 [51:17<1:51:57, 23.65s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 131/414 [51:40<1:51:31, 23.65s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 132/414 [52:04<1:51:06, 23.64s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 133/414 [52:27<1:50:39, 23.63s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 134/414 [52:51<1:50:17, 23.63s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 135/414 [53:15<1:49:54, 23.64s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 136/414 [53:39<1:49:52, 23.72s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 137/414 [54:02<1:49:17, 23.67s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 138/414 [54:26<1:48:53, 23.67s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/62 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 2/62 [00:02<01:13,  1.23s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▍         | 3/62 [00:04<01:42,  1.74s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 4/62 [00:07<01:56,  2.01s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 5/62 [00:09<02:03,  2.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 6/62 [00:12<02:06,  2.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█▏        | 7/62 [00:14<02:08,  2.33s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 8/62 [00:17<02:08,  2.37s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 9/62 [00:19<02:07,  2.40s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 10/62 [00:22<02:05,  2.42s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 11/62 [00:24<02:03,  2.43s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 12/62 [00:27<02:02,  2.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 13/62 [00:29<01:59,  2.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 14/62 [00:32<01:57,  2.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 15/62 [00:34<01:55,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 16/62 [00:36<01:53,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 17/62 [00:39<01:50,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 18/62 [00:41<01:48,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 19/62 [00:44<01:45,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 20/62 [00:46<01:43,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 21/62 [00:49<01:40,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 22/62 [00:51<01:38,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 23/62 [00:54<01:36,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▊      | 24/62 [00:56<01:33,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 25/62 [00:59<01:31,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 26/62 [01:01<01:28,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▎     | 27/62 [01:04<01:26,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 28/62 [01:06<01:23,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 29/62 [01:08<01:21,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 30/62 [01:11<01:18,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 31/62 [01:13<01:16,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 32/62 [01:16<01:13,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 33/62 [01:18<01:11,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 34/62 [01:21<01:09,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 35/62 [01:23<01:06,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 36/62 [01:26<01:04,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 37/62 [01:28<01:01,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 38/62 [01:31<00:59,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 39/62 [01:33<00:56,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 40/62 [01:36<00:54,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 41/62 [01:38<00:51,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 42/62 [01:41<00:49,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 43/62 [01:43<00:46,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 44/62 [01:45<00:44,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 45/62 [01:48<00:41,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 46/62 [01:50<00:39,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 47/62 [01:53<00:36,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 48/62 [01:55<00:34,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 49/62 [01:58<00:31,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████  | 50/62 [02:00<00:29,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 51/62 [02:03<00:27,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 52/62 [02:05<00:24,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 53/62 [02:08<00:22,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 54/62 [02:10<00:19,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 55/62 [02:12<00:17,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 56/62 [02:15<00:14,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 57/62 [02:17<00:12,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 58/62 [02:20<00:09,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 59/62 [02:22<00:07,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 60/62 [02:25<00:04,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 61/62 [02:27<00:02,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 62/62 [02:30<00:00,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.7874608039855957, 'eval_runtime': 153.3644, 'eval_samples_per_second': 25.84, 'eval_steps_per_second': 0.404, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 138/414 [56:59<1:48:53, 23.67s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 62/62 [02:30<00:00,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m34%|███▎      | 139/414 [57:40<5:42:37, 74.76s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 140/414 [58:03<4:31:17, 59.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7881, 'grad_norm': 0.18359375, 'learning_rate': 0.0002, 'epoch': 1.01}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 140/414 [58:03<4:31:17, 59.41s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 141/414 [58:27<3:41:21, 48.65s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 142/414 [58:50<3:06:26, 41.13s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 143/414 [59:14<2:42:01, 35.87s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 144/414 [59:38<2:24:53, 32.20s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 145/414 [1:00:01<2:12:45, 29.61s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 146/414 [1:00:25<2:04:13, 27.81s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 147/414 [1:00:49<1:58:18, 26.58s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 148/414 [1:01:12<1:54:12, 25.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 149/414 [1:01:36<1:50:53, 25.11s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 150/414 [1:02:00<1:48:25, 24.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7499, 'grad_norm': 0.19140625, 'learning_rate': 0.0002, 'epoch': 1.09}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 150/414 [1:02:00<1:48:25, 24.64s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 151/414 [1:02:23<1:46:39, 24.33s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 152/414 [1:02:47<1:45:12, 24.09s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 153/414 [1:03:10<1:44:11, 23.95s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 154/414 [1:03:34<1:43:20, 23.85s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 155/414 [1:03:58<1:42:34, 23.76s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 156/414 [1:04:21<1:42:03, 23.73s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 157/414 [1:04:45<1:41:30, 23.70s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 158/414 [1:05:08<1:40:58, 23.67s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 159/414 [1:05:32<1:40:38, 23.68s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 160/414 [1:05:56<1:40:36, 23.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7302, 'grad_norm': 0.1826171875, 'learning_rate': 0.0002, 'epoch': 1.16}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 160/414 [1:05:56<1:40:36, 23.77s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 161/414 [1:06:20<1:39:58, 23.71s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 162/414 [1:06:43<1:39:30, 23.69s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 163/414 [1:07:07<1:38:58, 23.66s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 164/414 [1:07:30<1:38:29, 23.64s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 165/414 [1:07:54<1:38:03, 23.63s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 166/414 [1:08:18<1:37:39, 23.63s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 167/414 [1:08:41<1:37:11, 23.61s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 168/414 [1:09:05<1:36:47, 23.61s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 169/414 [1:09:29<1:36:28, 23.62s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 170/414 [1:09:52<1:35:58, 23.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7405, 'grad_norm': 0.189453125, 'learning_rate': 0.0002, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[34m41%|████      | 170/414 [1:09:52<1:35:58, 23.60s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 171/414 [1:10:16<1:35:33, 23.59s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 172/414 [1:10:39<1:35:17, 23.63s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 173/414 [1:11:03<1:35:11, 23.70s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 174/414 [1:11:27<1:34:43, 23.68s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 175/414 [1:11:50<1:34:13, 23.65s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 176/414 [1:12:14<1:33:48, 23.65s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 177/414 [1:12:38<1:33:19, 23.63s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 178/414 [1:13:01<1:32:59, 23.64s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 179/414 [1:13:25<1:32:34, 23.63s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 180/414 [1:13:49<1:32:08, 23.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7205, 'grad_norm': 0.1806640625, 'learning_rate': 0.0002, 'epoch': 1.3}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 180/414 [1:13:49<1:32:08, 23.63s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 181/414 [1:14:12<1:31:40, 23.61s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 182/414 [1:14:36<1:31:17, 23.61s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 183/414 [1:14:59<1:30:55, 23.62s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 184/414 [1:15:23<1:30:34, 23.63s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 185/414 [1:15:47<1:30:34, 23.73s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 186/414 [1:16:11<1:30:04, 23.70s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 187/414 [1:16:34<1:29:35, 23.68s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 188/414 [1:16:58<1:29:04, 23.65s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 189/414 [1:17:21<1:28:35, 23.63s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 190/414 [1:17:45<1:28:09, 23.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7256, 'grad_norm': 0.1728515625, 'learning_rate': 0.0002, 'epoch': 1.38}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 190/414 [1:17:45<1:28:09, 23.61s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 191/414 [1:18:09<1:27:39, 23.59s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 192/414 [1:18:32<1:27:18, 23.60s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 193/414 [1:18:56<1:26:54, 23.59s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 194/414 [1:19:19<1:26:28, 23.58s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 195/414 [1:19:43<1:26:07, 23.60s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 196/414 [1:20:07<1:25:42, 23.59s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 197/414 [1:20:30<1:25:19, 23.59s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 198/414 [1:20:54<1:25:25, 23.73s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 199/414 [1:21:18<1:24:55, 23.70s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 200/414 [1:21:41<1:24:22, 23.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7229, 'grad_norm': 0.1796875, 'learning_rate': 0.0002, 'epoch': 1.45}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 200/414 [1:21:41<1:24:22, 23.66s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 201/414 [1:22:05<1:23:54, 23.64s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 202/414 [1:22:29<1:23:31, 23.64s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 203/414 [1:22:52<1:23:02, 23.61s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 204/414 [1:23:16<1:22:42, 23.63s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 205/414 [1:23:39<1:22:16, 23.62s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 206/414 [1:24:03<1:21:46, 23.59s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 207/414 [1:24:27<1:21:22, 23.59s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 208/414 [1:24:50<1:20:57, 23.58s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 209/414 [1:25:14<1:20:32, 23.57s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 210/414 [1:25:37<1:20:21, 23.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6896, 'grad_norm': 0.1826171875, 'learning_rate': 0.0002, 'epoch': 1.52}\u001b[0m\n",
      "\u001b[34m51%|█████     | 210/414 [1:25:37<1:20:21, 23.63s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 211/414 [1:26:01<1:20:16, 23.73s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 212/414 [1:26:25<1:19:48, 23.71s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 213/414 [1:26:49<1:19:12, 23.65s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 214/414 [1:27:12<1:18:47, 23.64s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 215/414 [1:27:36<1:18:22, 23.63s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 216/414 [1:27:59<1:17:53, 23.60s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 217/414 [1:28:23<1:17:29, 23.60s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 218/414 [1:28:47<1:17:07, 23.61s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 219/414 [1:29:10<1:16:43, 23.61s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 220/414 [1:29:34<1:16:17, 23.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.686, 'grad_norm': 0.173828125, 'learning_rate': 0.0002, 'epoch': 1.59}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 220/414 [1:29:34<1:16:17, 23.59s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 221/414 [1:29:57<1:15:53, 23.59s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 222/414 [1:30:21<1:15:33, 23.61s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 223/414 [1:30:45<1:15:23, 23.69s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 224/414 [1:31:09<1:15:14, 23.76s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 225/414 [1:31:32<1:14:40, 23.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 226/414 [1:31:56<1:14:10, 23.67s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 227/414 [1:32:19<1:13:40, 23.64s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 228/414 [1:32:43<1:13:13, 23.62s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 229/414 [1:33:07<1:12:48, 23.62s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 230/414 [1:33:30<1:12:25, 23.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6833, 'grad_norm': 0.224609375, 'learning_rate': 0.0002, 'epoch': 1.67}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 230/414 [1:33:30<1:12:25, 23.62s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 231/414 [1:33:54<1:12:00, 23.61s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 232/414 [1:34:17<1:11:37, 23.61s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 233/414 [1:34:41<1:11:08, 23.58s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 234/414 [1:35:05<1:10:44, 23.58s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 235/414 [1:35:28<1:10:22, 23.59s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 236/414 [1:35:52<1:10:08, 23.65s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 237/414 [1:36:16<1:09:54, 23.70s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 238/414 [1:36:39<1:09:24, 23.66s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 239/414 [1:37:03<1:08:56, 23.64s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 240/414 [1:37:26<1:08:27, 23.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6863, 'grad_norm': 0.1962890625, 'learning_rate': 0.0002, 'epoch': 1.74}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 240/414 [1:37:26<1:08:27, 23.61s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 241/414 [1:37:50<1:08:06, 23.62s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 242/414 [1:38:14<1:07:40, 23.61s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 243/414 [1:38:37<1:07:11, 23.58s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 244/414 [1:39:01<1:06:46, 23.57s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 245/414 [1:39:24<1:06:26, 23.59s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 246/414 [1:39:48<1:06:02, 23.58s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 247/414 [1:40:12<1:05:37, 23.58s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 248/414 [1:40:35<1:05:14, 23.58s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 249/414 [1:40:59<1:04:57, 23.62s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 250/414 [1:41:23<1:04:47, 23.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6792, 'grad_norm': 0.2158203125, 'learning_rate': 0.0002, 'epoch': 1.81}\u001b[0m\n",
      "\u001b[34m60%|██████    | 250/414 [1:41:23<1:04:47, 23.71s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 251/414 [1:41:46<1:04:17, 23.67s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 252/414 [1:42:10<1:03:49, 23.64s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 253/414 [1:42:33<1:03:20, 23.60s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 254/414 [1:42:57<1:02:53, 23.58s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 255/414 [1:43:20<1:02:26, 23.56s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 256/414 [1:43:44<1:02:02, 23.56s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 257/414 [1:44:08<1:01:42, 23.58s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 258/414 [1:44:31<1:01:18, 23.58s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 259/414 [1:44:55<1:00:54, 23.58s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 260/414 [1:45:18<1:00:31, 23.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6832, 'grad_norm': 0.2314453125, 'learning_rate': 0.0002, 'epoch': 1.88}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 260/414 [1:45:18<1:00:31, 23.58s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 261/414 [1:45:42<1:00:07, 23.58s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 262/414 [1:46:06<59:51, 23.63s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 263/414 [1:46:30<59:37, 23.69s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 264/414 [1:46:53<59:07, 23.65s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 265/414 [1:47:17<58:40, 23.63s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 266/414 [1:47:40<58:14, 23.61s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 267/414 [1:48:04<57:48, 23.60s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 268/414 [1:48:27<57:26, 23.60s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 269/414 [1:48:51<57:01, 23.60s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 270/414 [1:49:15<56:34, 23.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6554, 'grad_norm': 0.21875, 'learning_rate': 0.0002, 'epoch': 1.96}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 270/414 [1:49:15<56:34, 23.57s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 271/414 [1:49:38<56:11, 23.58s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 272/414 [1:50:02<55:48, 23.58s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 273/414 [1:50:25<55:26, 23.59s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 274/414 [1:50:49<55:03, 23.60s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 275/414 [1:51:13<54:47, 23.65s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 276/414 [1:51:37<54:30, 23.70s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/62 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 2/62 [00:02<01:14,  1.23s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▍         | 3/62 [00:04<01:43,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 4/62 [00:07<01:57,  2.02s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 5/62 [00:09<02:03,  2.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 6/62 [00:12<02:07,  2.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█▏        | 7/62 [00:14<02:08,  2.33s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 8/62 [00:17<02:08,  2.37s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 9/62 [00:19<02:07,  2.40s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 10/62 [00:22<02:05,  2.42s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 11/62 [00:24<02:04,  2.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 12/62 [00:27<02:02,  2.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 13/62 [00:29<02:00,  2.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 14/62 [00:32<01:57,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 15/62 [00:34<01:55,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 16/62 [00:36<01:53,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 17/62 [00:39<01:50,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 18/62 [00:41<01:48,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 19/62 [00:44<01:45,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 20/62 [00:46<01:43,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 21/62 [00:49<01:41,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 22/62 [00:51<01:38,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 23/62 [00:54<01:36,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▊      | 24/62 [00:56<01:33,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 25/62 [00:59<01:31,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 26/62 [01:01<01:28,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▎     | 27/62 [01:04<01:26,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 28/62 [01:06<01:23,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 29/62 [01:09<01:21,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 30/62 [01:11<01:18,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 31/62 [01:13<01:16,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 32/62 [01:16<01:14,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 33/62 [01:18<01:11,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 34/62 [01:21<01:09,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 35/62 [01:23<01:06,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 36/62 [01:26<01:04,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 37/62 [01:28<01:01,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 38/62 [01:31<00:59,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 39/62 [01:33<00:56,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 40/62 [01:36<00:54,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 41/62 [01:38<00:51,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 42/62 [01:41<00:49,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 43/62 [01:43<00:46,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 44/62 [01:46<00:44,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 45/62 [01:48<00:41,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 46/62 [01:51<00:39,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 47/62 [01:53<00:37,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 48/62 [01:55<00:34,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 49/62 [01:58<00:32,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████  | 50/62 [02:00<00:29,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 51/62 [02:03<00:27,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 52/62 [02:05<00:24,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 53/62 [02:08<00:22,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 54/62 [02:10<00:19,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 55/62 [02:13<00:17,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 56/62 [02:15<00:14,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 57/62 [02:18<00:12,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 58/62 [02:20<00:09,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 59/62 [02:23<00:07,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 60/62 [02:25<00:04,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 61/62 [02:28<00:02,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 62/62 [02:30<00:00,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.6925396919250488, 'eval_runtime': 153.6359, 'eval_samples_per_second': 25.795, 'eval_steps_per_second': 0.404, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 276/414 [1:54:10<54:30, 23.70s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 62/62 [02:30<00:00,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 277/414 [1:54:51<2:50:49, 74.81s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 278/414 [1:55:14<2:14:43, 59.43s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 279/414 [1:55:38<1:49:33, 48.69s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 280/414 [1:56:01<1:31:56, 41.17s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6395, 'grad_norm': 0.1943359375, 'learning_rate': 0.0002, 'epoch': 2.03}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 280/414 [1:56:01<1:31:56, 41.17s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 281/414 [1:56:25<1:19:33, 35.89s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 282/414 [1:56:49<1:10:52, 32.22s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 283/414 [1:57:12<1:04:41, 29.63s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 284/414 [1:57:36<1:00:14, 27.80s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 285/414 [1:57:59<57:01, 26.53s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 286/414 [1:58:23<54:49, 25.70s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 287/414 [1:58:47<53:19, 25.19s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 288/414 [1:59:11<51:53, 24.71s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 289/414 [1:59:34<50:46, 24.37s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 290/414 [1:59:58<49:54, 24.15s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.601, 'grad_norm': 0.2080078125, 'learning_rate': 0.0002, 'epoch': 2.1}\u001b[0m\n",
      "\u001b[34m70%|███████   | 290/414 [1:59:58<49:54, 24.15s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 291/414 [2:00:21<49:08, 23.97s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 292/414 [2:00:45<48:31, 23.86s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 293/414 [2:01:09<47:55, 23.76s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 294/414 [2:01:32<47:24, 23.70s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 295/414 [2:01:56<46:55, 23.66s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 296/414 [2:02:19<46:30, 23.65s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 297/414 [2:02:43<46:03, 23.62s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 298/414 [2:03:06<45:38, 23.61s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 299/414 [2:03:30<45:26, 23.71s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 300/414 [2:03:54<45:08, 23.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6004, 'grad_norm': 0.1962890625, 'learning_rate': 0.0002, 'epoch': 2.17}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 300/414 [2:03:54<45:08, 23.75s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 301/414 [2:04:18<44:39, 23.71s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 302/414 [2:04:41<44:10, 23.67s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 303/414 [2:05:05<43:41, 23.62s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 304/414 [2:05:28<43:16, 23.61s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 305/414 [2:05:52<42:53, 23.61s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 306/414 [2:06:16<42:28, 23.60s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 307/414 [2:06:39<42:07, 23.62s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 308/414 [2:07:03<41:42, 23.61s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 309/414 [2:07:26<41:17, 23.60s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 310/414 [2:07:50<40:53, 23.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5941, 'grad_norm': 0.2578125, 'learning_rate': 0.0002, 'epoch': 2.25}\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 310/414 [2:07:50<40:53, 23.60s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 311/414 [2:08:14<40:34, 23.64s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 312/414 [2:08:38<40:26, 23.78s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 313/414 [2:09:02<39:56, 23.73s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 314/414 [2:09:25<39:29, 23.70s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 315/414 [2:09:49<39:03, 23.67s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 316/414 [2:10:12<38:38, 23.66s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 317/414 [2:10:36<38:13, 23.64s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 318/414 [2:11:00<37:48, 23.63s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 319/414 [2:11:23<37:23, 23.62s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 320/414 [2:11:47<36:59, 23.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5923, 'grad_norm': 0.2177734375, 'learning_rate': 0.0002, 'epoch': 2.32}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 320/414 [2:11:47<36:59, 23.61s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 321/414 [2:12:10<36:34, 23.60s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 322/414 [2:12:34<36:10, 23.59s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 323/414 [2:12:58<35:48, 23.61s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 324/414 [2:13:21<35:31, 23.68s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 325/414 [2:13:45<35:11, 23.73s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 326/414 [2:14:09<34:46, 23.71s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 327/414 [2:14:32<34:18, 23.66s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 328/414 [2:14:56<33:52, 23.63s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 329/414 [2:15:20<33:27, 23.61s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 330/414 [2:15:43<33:03, 23.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5957, 'grad_norm': 0.224609375, 'learning_rate': 0.0002, 'epoch': 2.39}\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 330/414 [2:15:43<33:03, 23.61s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 331/414 [2:16:07<32:39, 23.61s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 332/414 [2:16:30<32:15, 23.60s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 333/414 [2:16:54<31:52, 23.62s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 334/414 [2:17:18<31:28, 23.61s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 335/414 [2:17:41<31:04, 23.60s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 336/414 [2:18:05<30:42, 23.62s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 337/414 [2:18:29<30:21, 23.66s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 338/414 [2:18:52<30:02, 23.71s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 339/414 [2:19:16<29:35, 23.67s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 340/414 [2:19:40<29:10, 23.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5914, 'grad_norm': 0.203125, 'learning_rate': 0.0002, 'epoch': 2.46}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 340/414 [2:19:40<29:10, 23.66s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 341/414 [2:20:03<28:45, 23.64s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 342/414 [2:20:27<28:20, 23.62s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 343/414 [2:20:50<27:55, 23.59s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 344/414 [2:21:14<27:31, 23.60s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 345/414 [2:21:38<27:08, 23.61s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 346/414 [2:22:01<26:44, 23.59s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 347/414 [2:22:25<26:21, 23.60s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 348/414 [2:22:48<25:58, 23.61s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 349/414 [2:23:12<25:40, 23.70s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 350/414 [2:23:36<25:13, 23.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5934, 'grad_norm': 0.193359375, 'learning_rate': 0.0002, 'epoch': 2.54}\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 350/414 [2:23:36<25:13, 23.64s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 351/414 [2:24:00<24:54, 23.72s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 352/414 [2:24:23<24:26, 23.66s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 353/414 [2:24:47<24:01, 23.64s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 354/414 [2:25:10<23:36, 23.62s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 355/414 [2:25:34<23:13, 23.61s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 356/414 [2:25:58<22:48, 23.60s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 357/414 [2:26:21<22:25, 23.60s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 358/414 [2:26:45<22:02, 23.61s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 359/414 [2:27:08<21:37, 23.59s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 360/414 [2:27:32<21:13, 23.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5872, 'grad_norm': 0.2275390625, 'learning_rate': 0.0002, 'epoch': 2.61}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 360/414 [2:27:32<21:13, 23.59s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 361/414 [2:27:56<20:49, 23.58s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 362/414 [2:28:19<20:32, 23.70s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 363/414 [2:28:43<20:06, 23.65s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 364/414 [2:29:07<19:46, 23.73s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 365/414 [2:29:31<19:20, 23.69s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 366/414 [2:29:54<18:55, 23.65s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 367/414 [2:30:18<18:29, 23.61s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 368/414 [2:30:41<18:06, 23.61s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 369/414 [2:31:05<17:43, 23.63s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 370/414 [2:31:28<17:18, 23.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5684, 'grad_norm': 0.2177734375, 'learning_rate': 0.0002, 'epoch': 2.68}\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 370/414 [2:31:28<17:18, 23.60s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 371/414 [2:31:52<16:54, 23.59s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 372/414 [2:32:16<16:30, 23.58s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 373/414 [2:32:39<16:07, 23.59s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 374/414 [2:33:03<15:43, 23.58s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 375/414 [2:33:27<15:22, 23.65s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 376/414 [2:33:50<14:57, 23.61s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 377/414 [2:34:14<14:36, 23.69s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 378/414 [2:34:38<14:12, 23.67s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 379/414 [2:35:01<13:47, 23.65s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 380/414 [2:35:25<13:23, 23.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5815, 'grad_norm': 0.203125, 'learning_rate': 0.0002, 'epoch': 2.75}\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 380/414 [2:35:25<13:23, 23.65s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 381/414 [2:35:48<12:59, 23.64s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 382/414 [2:36:12<12:35, 23.60s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 383/414 [2:36:36<12:11, 23.60s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 384/414 [2:36:59<11:47, 23.59s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 385/414 [2:37:23<11:24, 23.61s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 386/414 [2:37:46<11:01, 23.61s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 387/414 [2:38:10<10:37, 23.61s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 388/414 [2:38:34<10:16, 23.69s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 389/414 [2:38:57<09:51, 23.67s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 390/414 [2:39:21<09:29, 23.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5719, 'grad_norm': 0.2158203125, 'learning_rate': 0.0002, 'epoch': 2.83}\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 390/414 [2:39:21<09:29, 23.74s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 391/414 [2:39:45<09:05, 23.70s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 392/414 [2:40:09<08:41, 23.69s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 393/414 [2:40:32<08:17, 23.67s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 394/414 [2:40:56<07:52, 23.65s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 395/414 [2:41:19<07:29, 23.64s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 396/414 [2:41:43<07:05, 23.63s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 397/414 [2:42:07<06:41, 23.63s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 398/414 [2:42:30<06:17, 23.62s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 399/414 [2:42:54<05:54, 23.62s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 400/414 [2:43:18<05:30, 23.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5519, 'grad_norm': 0.298828125, 'learning_rate': 0.0002, 'epoch': 2.9}\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 400/414 [2:43:18<05:30, 23.61s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 401/414 [2:43:41<05:08, 23.70s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 402/414 [2:44:05<04:44, 23.70s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 403/414 [2:44:29<04:21, 23.76s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 404/414 [2:44:53<03:57, 23.71s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 405/414 [2:45:16<03:33, 23.68s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 406/414 [2:45:40<03:09, 23.67s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 407/414 [2:46:04<02:45, 23.66s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 408/414 [2:46:27<02:21, 23.63s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 409/414 [2:46:51<01:58, 23.63s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 410/414 [2:47:14<01:34, 23.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5492, 'grad_norm': 0.2197265625, 'learning_rate': 0.0002, 'epoch': 2.97}\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 410/414 [2:47:14<01:34, 23.63s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 411/414 [2:47:38<01:10, 23.63s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 412/414 [2:48:02<00:47, 23.60s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 413/414 [2:48:25<00:23, 23.62s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 414/414 [2:48:49<00:00, 23.70s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/62 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 2/62 [00:02<01:13,  1.23s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▍         | 3/62 [00:04<01:42,  1.74s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 4/62 [00:07<01:56,  2.01s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 5/62 [00:09<02:03,  2.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 6/62 [00:12<02:07,  2.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█▏        | 7/62 [00:14<02:08,  2.33s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 8/62 [00:17<02:08,  2.37s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 9/62 [00:19<02:07,  2.40s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 10/62 [00:22<02:05,  2.42s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 11/62 [00:24<02:04,  2.43s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 12/62 [00:27<02:02,  2.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 13/62 [00:29<02:00,  2.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 14/62 [00:32<01:57,  2.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 15/62 [00:34<01:55,  2.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 16/62 [00:36<01:52,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 17/62 [00:39<01:50,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 18/62 [00:41<01:48,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 19/62 [00:44<01:45,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 20/62 [00:46<01:43,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 21/62 [00:49<01:40,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 22/62 [00:51<01:38,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 23/62 [00:54<01:36,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▊      | 24/62 [00:56<01:33,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 25/62 [00:59<01:31,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 26/62 [01:01<01:28,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▎     | 27/62 [01:04<01:26,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 28/62 [01:06<01:23,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 29/62 [01:08<01:21,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 30/62 [01:11<01:18,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 31/62 [01:13<01:16,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 32/62 [01:16<01:13,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 33/62 [01:18<01:11,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 34/62 [01:21<01:08,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 35/62 [01:23<01:06,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 36/62 [01:26<01:03,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 37/62 [01:28<01:01,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 38/62 [01:31<00:59,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 39/62 [01:33<00:56,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 40/62 [01:36<00:54,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 41/62 [01:38<00:51,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 42/62 [01:40<00:49,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 43/62 [01:43<00:46,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 44/62 [01:45<00:44,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 45/62 [01:48<00:41,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 46/62 [01:50<00:39,  2.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 47/62 [01:53<00:36,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 48/62 [01:55<00:34,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 49/62 [01:58<00:31,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████  | 50/62 [02:00<00:29,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 51/62 [02:03<00:27,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 52/62 [02:05<00:24,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 53/62 [02:07<00:22,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 54/62 [02:10<00:19,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 55/62 [02:12<00:17,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 56/62 [02:15<00:14,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 57/62 [02:17<00:12,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 58/62 [02:20<00:09,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 59/62 [02:22<00:07,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 60/62 [02:25<00:04,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 61/62 [02:27<00:02,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 62/62 [02:30<00:00,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.62178373336792, 'eval_runtime': 153.3176, 'eval_samples_per_second': 25.848, 'eval_steps_per_second': 0.404, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 414/414 [2:51:22<00:00, 23.70s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 62/62 [02:30<00:00,  2.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'train_runtime': 10300.0227, 'train_samples_per_second': 10.262, 'train_steps_per_second': 0.04, 'train_loss': 1.726944881936778, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 414/414 [2:51:40<00:00, 23.70s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 414/414 [2:51:40<00:00, 24.88s/it]\u001b[0m\n",
      "\u001b[34mTrying to load a Peft model. It might take a while without feedback\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.19s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]\u001b[0m\n",
      "\u001b[34mSaving the newly created merged model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2024-07-29 10:18:36,927 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-29 10:18:36,927 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-29 10:18:36,927 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 10969\n",
      "Billable seconds: 10969\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 경로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized_model_s3_path: \n",
      " {'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-29-07-13-1-2024-07-29-07-13-12-264/output/model/', 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n",
      "Stored 'optimized_model_s3_path' (dict)\n"
     ]
    }
   ],
   "source": [
    "optimized_model_s3_path = huggingface_estimator.model_data\n",
    "print(\"optimized_model_s3_path: \\n\", optimized_model_s3_path)\n",
    "\n",
    "%store optimized_model_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_llama3_puy310",
   "language": "python",
   "name": "conda_llama3_puy310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
