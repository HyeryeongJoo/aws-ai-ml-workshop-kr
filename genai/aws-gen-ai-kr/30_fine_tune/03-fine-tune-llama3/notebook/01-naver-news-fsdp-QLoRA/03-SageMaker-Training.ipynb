{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker 에서 Llama 3 파인 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/SageMaker/.cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "\n",
    "is_sagemaker_notebook = True\n",
    "if is_sagemaker_notebook:\n",
    "    hf_key_name = \"HF_KEY\"\n",
    "    key_val = \"<Type Your HF Key>\"\n",
    "    set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    HF_TOKEN = get_hf_key_env_vars(hf_key_name)\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장된 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder:  ../data/naver-news-summarization-ko\n",
      "train_data_json:  ../data/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_data_json:  ../data/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_data_json:  ../data/naver-news-summarization-ko/test/test_dataset.json\n",
      "full_train_data_json:  ../data/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "full_validation_data_json:  ../data/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "full_test_data_json:  ../data/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%store -r data_folder\n",
    "%store -r train_data_json \n",
    "%store -r validation_data_json \n",
    "%store -r test_data_json \n",
    "%store -r full_train_data_json \n",
    "%store -r full_validation_data_json \n",
    "%store -r full_test_data_json\n",
    "\n",
    "\n",
    "print(\"data_folder: \", data_folder)\n",
    "print(\"train_data_json: \", train_data_json)\n",
    "print(\"validation_data_json: \", validation_data_json)\n",
    "print(\"test_data_json: \", test_data_json)\n",
    "print(\"full_train_data_json: \", full_train_data_json)\n",
    "print(\"full_validation_data_json: \", full_validation_data_json)\n",
    "print(\"full_test_data_json: \", full_test_data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker 기본 변수 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::057716757052:role/gen_ai_gsmoon\n",
      "sagemaker bucket: sagemaker-us-east-1-057716757052\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비\n",
    "- 두가지 종류의 데이터 셋을 업로드 합니다.\n",
    "    - Full Dataset: 전체 데이타를 업로드 합니다.\n",
    "    - Sample Dataset: 디버깅 용도의 일부 데이타를 업로드 합니다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 데이터 셋 경로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko\n",
      "train_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/test/test_dataset.json\n",
      "\n",
      "input_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko\n",
      "train_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "validation_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "test_dataset_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "def create_s3_path(sess,is_full, data_folder,train_data_json,validation_data_json,test_data_json,verbose=True  ):\n",
    "    dataset_name = data_folder.split('/')[-1]\n",
    "    # save train_dataset to s3 using our SageMaker session\n",
    "    input_path = f's3://{sess.default_bucket()}/datasets/{dataset_name}'\n",
    "    print(\"input_path: \\n\", input_path)\n",
    "\n",
    "    trian_file_name = train_data_json.split('/')[-1]\n",
    "    validation_file_name = validation_data_json.split('/')[-1]\n",
    "    test_file_name = test_data_json.split('/')[-1]\n",
    "\n",
    "    if is_full:\n",
    "        train_dataset_s3_path = f\"{input_path}/full_train/{trian_file_name}\"\n",
    "        validation_dataset_s3_path = f\"{input_path}/full_validation/{validation_file_name}\"\n",
    "        test_dataset_s3_path = f\"{input_path}/full_test/{test_file_name}\"\n",
    "    else:\n",
    "        train_dataset_s3_path = f\"{input_path}/train/{trian_file_name}\"\n",
    "        validation_dataset_s3_path = f\"{input_path}/validation/{validation_file_name}\"\n",
    "        test_dataset_s3_path = f\"{input_path}/test/{test_file_name}\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"train_dataset_s3_path: \\n\", train_dataset_s3_path)\n",
    "        print(\"validation_dataset_s3_path: \\n\", validation_dataset_s3_path)\n",
    "        print(\"test_dataset_s3_path: \\n\", test_dataset_s3_path)\n",
    "\n",
    "    return train_dataset_s3_path, validation_dataset_s3_path, test_dataset_s3_path, input_path\n",
    "\n",
    "train_dataset_s3_path, validation_dataset_s3_path, test_dataset_s3_path, input_path = create_s3_path(\n",
    "                                                                            sess=sess,\n",
    "                                                                            is_full = False,\n",
    "                                                                            data_folder=data_folder,\n",
    "                                                                            train_data_json=train_data_json,\n",
    "                                                                            validation_data_json=validation_data_json,\n",
    "                                                                            test_data_json=test_data_json)    \n",
    "print(\"\")\n",
    "full_train_dataset_s3_path, full_validation_dataset_s3_path, full_test_dataset_s3_path, input_path = create_s3_path(\n",
    "                                                                            sess=sess,\n",
    "                                                                            is_full = True,\n",
    "                                                                            data_folder=data_folder,\n",
    "                                                                            train_data_json=full_train_data_json,\n",
    "                                                                            validation_data_json=full_validation_data_json,\n",
    "                                                                            test_data_json=full_test_data_json)    \n",
    "\n",
    "# full_train_data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이타를 S3 에 업로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_s3_prefix_name(s3_path, verbose=True):\n",
    "    file_name = s3_path.split('/')[-1]\n",
    "    file_name = '/' + file_name\n",
    "    desired_s3_uri = s3_path.split(file_name)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"file_name: \", file_name)\n",
    "        print(\"desired_s3_uri: \", desired_s3_uri)\n",
    "    return desired_s3_uri\n",
    "\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "def upload_data_s3(desired_s3_uri, file_name, verbose=True):\n",
    "    # upload the model yaml file to s3\n",
    "    \n",
    "    file_s3_path = S3Uploader.upload(local_path=file_name, desired_s3_uri=desired_s3_uri)\n",
    "\n",
    "    print(f\"{file_name} is uploaded to:\")\n",
    "    print(file_s3_path)\n",
    "\n",
    "    return file_s3_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug 용 작은 데이터셋 S3 업로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name:  /train_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train\n",
      "../data/naver-news-summarization-ko/train/train_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train/train_dataset.json\n",
      "\n",
      "file_name:  /validation_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/validation\n",
      "../data/naver-news-summarization-ko/validation/validation_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "\n",
      "file_name:  /test_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/test\n",
      "../data/naver-news-summarization-ko/test/test_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/test/test_dataset.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/test/test_dataset.json'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "######## Train File\n",
    "# return s3 URI, e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "train_desired_s3_uri = get_s3_prefix_name(train_dataset_s3_path)    \n",
    "# upload local file to e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "upload_data_s3(desired_s3_uri=train_desired_s3_uri, file_name=train_data_json, verbose=True)\n",
    "######## Validation File\n",
    "print(\"\")\n",
    "validation_desired_s3_uri = get_s3_prefix_name(validation_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=validation_desired_s3_uri, file_name=validation_data_json, verbose=True)\n",
    "######## Test File\n",
    "print(\"\")\n",
    "test_desired_s3_uri = get_s3_prefix_name(test_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=test_desired_s3_uri, file_name=test_data_json, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가용 큰 데이터셋 S3 업로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name:  /train_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_train\n",
      "../data/naver-news-summarization-ko/full_train/train_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "\n",
      "file_name:  /validation_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_validation\n",
      "../data/naver-news-summarization-ko/full_validation/validation_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "\n",
      "file_name:  /test_dataset.json\n",
      "desired_s3_uri:  s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_test\n",
      "../data/naver-news-summarization-ko/full_test/test_dataset.json is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/full_test/test_dataset.json'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "######## Train File\n",
    "# return s3 URI, e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "full_train_desired_s3_uri = get_s3_prefix_name(full_train_dataset_s3_path)    \n",
    "# upload local file to e.g: s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train \n",
    "upload_data_s3(desired_s3_uri=full_train_desired_s3_uri, file_name=full_train_data_json, verbose=True)\n",
    "######## Validation File\n",
    "print(\"\")\n",
    "full_validation_desired_s3_uri = get_s3_prefix_name(full_validation_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=full_validation_desired_s3_uri, file_name=full_validation_data_json, verbose=True)\n",
    "######## Test File\n",
    "print(\"\")\n",
    "full_test_desired_s3_uri = get_s3_prefix_name(full_test_dataset_s3_path)    \n",
    "upload_data_s3(desired_s3_uri=full_test_desired_s3_uri, file_name=full_test_data_json, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 업로드 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-14 10:54:57    2.4 KiB datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml\n",
      "2024-07-15 05:52:49    8.4 MiB datasets/naver-news-summarization-ko/full_test/test_dataset.json\n",
      "2024-07-15 05:52:47   68.0 MiB datasets/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "2024-07-15 05:52:48    7.6 MiB datasets/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "2024-07-15 05:52:42   33.4 KiB datasets/naver-news-summarization-ko/test/test_dataset.json\n",
      "2024-07-15 05:52:42   28.1 KiB datasets/naver-news-summarization-ko/train/train_dataset.json\n",
      "2024-07-15 05:52:42   26.1 KiB datasets/naver-news-summarization-ko/validation/validation_dataset.json\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {input_path}  --recursive --human-readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws s3 rm {input_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws s3 cp {train_data_json} {train_dataset_s3_path}\n",
    "# ! aws s3 cp {validation_data_json} {validation_dataset_s3_path}\n",
    "# ! aws s3 cp {test_data_json} {test_dataset_s3_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 훈련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "config_folder_name = \"accelerator_config\"\n",
    "os.makedirs(config_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 설정 파일 준비\n",
    "- 목적에 맞게 아래의 두 개의 부분을 주석을 이용하여 사용 하세요.\n",
    "    - For Debug 부분은 일부 샘플 데이타를 통해서 빠르게 디버깅 목적의 파라미터 값 입니다.\n",
    "    - For evaluation: 전체 데이터를 통해서 최적의 파라미터 값 입니다.\n",
    "```\n",
    "###########################             \n",
    "# For Debug\n",
    "###########################             \n",
    "num_train_epochs: 5                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "# For evaluation\n",
    "###########################             \n",
    "# num_train_epochs: 3                    # number of training epochs\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 8          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "max_seq_len:  2048              # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\" # path to where SageMaker saves train dataset\n",
    "validation_dataset_path: \"/opt/ml/input/data/validation/\" # path to where SageMaker saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"   # path to where SageMaker saves test dataset\n",
    "output_dir: \"/tmp/llama3\"            # where the LoRA adapter weight is\n",
    "# training parameters\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "###########################             \n",
    "# For Debug\n",
    "###########################             \n",
    "num_train_epochs: 5                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "# For evaluation\n",
    "###########################             \n",
    "# num_train_epochs: 3                    # number of training epochs\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 8          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################             \n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정 파일을 S3 에 업로드\n",
    "- 위에 정의한 파일을 업로드 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml is uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_desired_s3_uri = f\"{input_path}/config\"\n",
    "config_model_name = \"accelerator_config/sm_llama_3_8b_fsdp_qlora.yaml\"\n",
    "train_config_s3_path = upload_data_s3(desired_s3_uri=config_desired_s3_uri, file_name=config_model_name, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 입력 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 사이즈 조정 \n",
    "- 디버그 용도이면 run_debug_sample = True, 전데 데이터 이면 False 로 조절 하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train/train_dataset.json',\n",
       " 'validation': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/validation/validation_dataset.json',\n",
       " 'config': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_debug_sample = True\n",
    "# run_debug_sample = False\n",
    "if run_debug_sample:\n",
    "  local_data = {\n",
    "    'train': f'file://{train_data_json}',\n",
    "    'validation': f'file://{validation_data_json}',\n",
    "    'config': f'file://{config_model_name}'\n",
    "    }\n",
    "\n",
    "  s3_data = {\n",
    "    'train': train_dataset_s3_path,\n",
    "    'validation': validation_dataset_s3_path,\n",
    "    'config': train_config_s3_path\n",
    "    }  \n",
    "else:\n",
    "  local_data = {\n",
    "    'train': f'file://{train_data_json}',\n",
    "    'validation': f'file://{validation_data_json}',\n",
    "    'config': f'file://{config_model_name}'\n",
    "    }\n",
    "  s3_data = {\n",
    "    'train': full_train_dataset_s3_path,\n",
    "    'validation': full_validation_dataset_s3_path,\n",
    "    'config': train_config_s3_path\n",
    "    }  \n",
    "s3_data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clolud 모드 및 Local 사용\n",
    "- 현재 로컬 모드는 에러 발행. 확인 중 임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Cloud mode is set with ml.g5.4xlarge and 1 of instance_count\n",
      "dataset: \n",
      " {'train': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/train/train_dataset.json', 'validation': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/validation/validation_dataset.json', 'config': 's3://sagemaker-us-east-1-057716757052/datasets/naver-news-summarization-ko/config/sm_llama_3_8b_fsdp_qlora.yaml'}\n"
     ]
    }
   ],
   "source": [
    "# USE_LOCAL_MODE = True\n",
    "USE_LOCAL_MODE = False\n",
    "\n",
    "import torch\n",
    "\n",
    "if USE_LOCAL_MODE:\n",
    "    instance_type = 'local_gpu' if torch.cuda.is_available() else 'local'\n",
    "    instance_count = 1\n",
    "    from sagemaker.local import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    # data = local_data \n",
    "    data = s3_data\n",
    "    metric_definitions = None\n",
    "    nKeepAliveSeconds = None # Warmpool feature\n",
    "    print(\"## Local mode is set\")\n",
    "else:\n",
    "    instance_type = 'ml.g5.4xlarge'\n",
    "    # instance_type = 'ml.g5.12xlarge'\n",
    "    # instance_type = 'ml.g5.48xlarge'\n",
    "    # instance_type = 'ml.p4d.24xlarge'\n",
    "    # Emit: \n",
    "    # {'train_runtime': 37.2985, 'train_samples_per_second': 0.375, 'train_steps_per_second': 0.054, 'train_loss': 2.3541293144226074, 'epoch': 1.0}\n",
    "    # {'eval_loss': 2.50766658782959, 'eval_runtime': 3.4741, 'eval_samples_per_second': 3.454, 'eval_steps_per_second': 0.864, 'epoch': 1.0}\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"train:loss\", \"Regex\": \"'train_loss':(.*?),\"},\n",
    "        {\"Name\": \"validation:loss\", \"Regex\": \"'eval_loss':(.*?),\"}\n",
    "    ]\n",
    "    instance_count = 1\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    data = s3_data\n",
    "    nKeepAliveSeconds = 3600 # Warmpool feature, 1 hour\n",
    "    print(f\"## Cloud mode is set with {instance_type} and {instance_count} of instance_count\")\n",
    "print(\"dataset: \\n\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 Estimator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "import time\n",
    "# define Training Job Name \n",
    "job_name = f'llama3-8b-naver-news-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "# chkpt_s3_path = f's3://{sess.default_bucket()}/{s3_prefix}/native/checkpoints'\n",
    "\n",
    "# create the Estimator\n",
    "os.environ['USE_SHORT_LIVED_CREDENTIALS']=\"1\" \n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'sm_run_fsdp_qlora_llama3.py',      # train script\n",
    "    source_dir           = '../../scripts',  # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type,  # instances type used for the training job\n",
    "    instance_count       = instance_count,                 # the number of instances used for training\n",
    "    sagemaker_session    = sagemaker_session,\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 256,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',          # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    metric_definitions = metric_definitions,\n",
    "    hyperparameters      =  {\n",
    "        \"config\": \"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\" # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost    \n",
    "    keep_alive_period_in_seconds = nKeepAliveSeconds,     # warm pool \n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\n",
    "    environment  = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HF_TOKEN,       # huggingface token to access gated models, e.g. llama 3\n",
    "        \"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\"   # enable CPU RAM efficient loading\n",
    "    }, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 훈련 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_name:naver-news-summarization-ko\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.experiments.run:The run (training-job-experiment) under experiment (naver-news-summarization-ko) already exists. Loading it.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: llama3-8b-naver-news-2024-07-15-05-54-5-2024-07-15-05-55-02-818\n"
     ]
    }
   ],
   "source": [
    "if USE_LOCAL_MODE:\n",
    "    huggingface_estimator.fit(data, wait=False)\n",
    "else:\n",
    "    from sagemaker.experiments.run import Run\n",
    "    from sagemaker.utils import unique_name_from_base\n",
    "    from sagemaker.session import Session\n",
    "\n",
    "    # set new experiment configuration\n",
    "    # naver-news-summarization-ko\n",
    "    experiment_name = data_folder.split('/')[-1]\n",
    "    \n",
    "    run_name = f\"training-job-experiment\"\n",
    "    print(f\"experiment_name:{experiment_name}\")    \n",
    "\n",
    "    with Run(experiment_name=experiment_name, run_name=run_name, sagemaker_session=sagemaker_session) as run:\n",
    "        huggingface_estimator.fit(data,wait=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-15 05:55:03 Starting - Starting the training job\n",
      "2024-07-15 05:55:03 Pending - Training job waiting for capacity.........................................................................................................\n",
      "2024-07-15 06:12:33 Pending - Preparing the instances for training...\n",
      "2024-07-15 06:13:05 Downloading - Downloading input data...\n",
      "2024-07-15 06:13:20 Downloading - Downloading the training image..................\n",
      "2024-07-15 06:16:42 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-07-15 06:17:14,880 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-07-15 06:17:14,897 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-15 06:17:14,907 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-07-15 06:17:14,909 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2024-07-15 06:17:14,909 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-07-15 06:17:16,395 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.40.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.6/137.6 kB 9.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.18.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.29.3 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: evaluate==0.4.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.4.1)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.43.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface_hub==0.22.2 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.8.6 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.10.0 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting torch==2.1.1 (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1->-r requirements.txt (line 4)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.22.2->-r requirements.txt (line 6)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.6->-r requirements.txt (line 7)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 9)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 9)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 9)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 9)) (2.1.0)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (0.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (13.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (1.6.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.1->-r requirements.txt (line 9)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.1->-r requirements.txt (line 9)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (0.1.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.0/9.0 MB 88.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 42.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.29.3-py3-none-any.whl (297 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.6/297.6 kB 31.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 17.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 41.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.8.6-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.2/245.2 kB 32.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 29.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 670.2/670.2 MB 2.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 3.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 75.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 58.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 41.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 1.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 32.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 16.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 10.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.8/209.8 MB 10.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 15.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 89.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 65.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets, bitsandbytes, accelerate, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface_hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.15.1\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.15.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.15.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.1.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.15.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.15.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.15.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: bitsandbytes\u001b[0m\n",
      "\u001b[34mFound existing installation: bitsandbytes 0.42.0\u001b[0m\n",
      "\u001b[34mUninstalling bitsandbytes-0.42.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled bitsandbytes-0.42.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.25.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.25.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.25.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.7.4\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.7.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.7.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.7.1\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.7.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.29.3 bitsandbytes-0.43.1 datasets-2.18.0 huggingface_hub-0.22.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 peft-0.10.0 tokenizers-0.19.1 torch-2.1.1 transformers-4.40.0 trl-0.8.6\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-07-15 06:18:42,153 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-15 06:18:42,153 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-15 06:18:42,194 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-15 06:18:42,224 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-15 06:18:42,236 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2024-07-15 06:18:42,254 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-15 06:18:42,267 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"config\": \"/opt/ml/input/data/config\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config\": \"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"config\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"llama3-8b-naver-news-2024-07-15-05-54-5-2024-07-15-05-55-02-818\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-15-05-54-5-2024-07-15-05-55-02-818/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sm_run_fsdp_qlora_llama3\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sm_run_fsdp_qlora_llama3.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sm_run_fsdp_qlora_llama3.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.4xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"config\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sm_run_fsdp_qlora_llama3\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-15-05-54-5-2024-07-15-05-55-02-818/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.4xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"config\":\"/opt/ml/input/data/config\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"llama3-8b-naver-news-2024-07-15-05-54-5-2024-07-15-05-55-02-818\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-15-05-54-5-2024-07-15-05-55-02-818/source/sourcedir.tar.gz\",\"module_name\":\"sm_run_fsdp_qlora_llama3\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sm_run_fsdp_qlora_llama3.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CONFIG=/opt/ml/input/data/config\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=/opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 1 --nproc_per_node 1 sm_run_fsdp_qlora_llama3.py --config /opt/ml/input/data/config/sm_llama_3_8b_fsdp_qlora.yaml\u001b[0m\n",
      "\u001b[34m## script_args: \n",
      " ScriptArguments(train_dataset_path='/opt/ml/input/data/train/', validation_dataset_path='/opt/ml/input/data/validation/', model_id='meta-llama/Meta-Llama-3-8B', max_seq_length=512)\u001b[0m\n",
      "\u001b[34m## training_args:\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_do_concat_batches=True,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>, <FSDPOption.OFFLOAD: 'offload'>],\u001b[0m\n",
      "\u001b[34mfsdp_config={'backward_prefetch': 'backward_pre', 'forward_prefetch': 'false', 'use_orig_params': 'false', 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0002,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/tmp/llama3/runs/Jul15_06-18-47_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=constant,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=0.3,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=5,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/tmp/llama3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=1,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/tmp/llama3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=epoch,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=True,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.03,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10 examples [00:00, 4067.80 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10 examples [00:00, 6387.91 examples/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10/10 [00:00<00:00, 349.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10/10 [00:00<00:00, 2595.16 examples/s]\u001b[0m\n",
      "\u001b[34mYou are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\u001b[0m\n",
      "\u001b[34mHuman: Please summarize the goals for journalist in this text:\u001b[0m\n",
      "\u001b[34m국내 최대규모 나노 전시회이자 세계 3대 나노행사 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀 에서 개최된다. 올해 20회를 맞는 나노코리아는 산업통상자원부 과학기술정보통신부가 공동 주최한다. 나노융합산업연구조합과 나노기술연구협의회가 주관한다. 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다. 강민석 LG이노텍 부사장이 자율주행산업 동향에 따른 나노기술과 인공지능 AI 의 활용 을 주제로 기조강연을 한다. 알베르페르 프랑스 파리 슈드대 교수도 기조 강연을 맡는다. 전시규모는 코로나19 이전 수준으로 회복됐다. 삼성전자 LG 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여한다. 나노 20주년 특별 기념관도 마련된다. 20주년 특별 기념관에는 차세대 반도체 미래차 6세대 6G 이동통신 탄소중립 디지털 바이오 등 6개 분야 혁신 기술이 소개된다. 산업화 세션에서는 지속 가능 성장을 위한 ESG 나노융합기술을 주제로 초청 강연이 열린다. 나노제품거래상담회 전시회테크니컬투어 최신기술발표회 등 다양한 나노 관련 부대 행사도 준비된다.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: 국내 3대 나노행사 중 하나인 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀에서 개최되는데, 삼성전자 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여할 예정이며 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:11<00:35, 11.93s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:25<00:25, 12.66s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:37<00:12, 12.45s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:40<00:00,  8.72s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:40<00:00, 10.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14 examples [00:00, 1035.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12 examples [00:00, 1644.93 examples/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.18.1+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:76:127 [0] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:76:127 [0] nccl_net_ofi_init:1237 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34mtrainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\u001b[0m\n",
      "\u001b[34m0%|          | 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 1/35 [00:14<08:04, 14.26s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 2/35 [00:17<04:19,  7.86s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 3/35 [00:21<03:07,  5.85s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 4/35 [00:24<02:31,  4.89s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 5/35 [00:27<02:10,  4.36s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 6/35 [00:31<01:57,  4.04s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 7/35 [00:34<01:48,  3.88s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/12 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 2/12 [00:00<00:02,  4.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 3/12 [00:00<00:02,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 4/12 [00:01<00:02,  2.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 5/12 [00:01<00:02,  2.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 6/12 [00:02<00:02,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 7/12 [00:02<00:02,  2.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 8/12 [00:03<00:01,  2.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 9/12 [00:03<00:01,  2.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 10/12 [00:04<00:00,  2.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 11/12 [00:04<00:00,  2.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:04<00:00,  2.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.3964643478393555, 'eval_runtime': 5.3499, 'eval_samples_per_second': 2.243, 'eval_steps_per_second': 2.243, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:04<00:00,  2.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 7/35 [00:40<01:48,  3.88s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 8/35 [00:46<02:49,  6.27s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 9/35 [00:49<02:19,  5.38s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 10/35 [00:53<01:59,  4.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3156, 'grad_norm': 0.408203125, 'learning_rate': 0.0002, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 10/35 [00:53<01:59,  4.77s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 11/35 [00:56<01:44,  4.36s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 12/35 [00:59<01:33,  4.07s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 13/35 [01:03<01:25,  3.87s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 14/35 [01:06<01:18,  3.74s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/12 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 2/12 [00:00<00:02,  4.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 3/12 [00:00<00:02,  3.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 4/12 [00:01<00:02,  2.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 5/12 [00:01<00:02,  2.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 6/12 [00:02<00:02,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 7/12 [00:02<00:02,  2.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 8/12 [00:03<00:01,  2.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 9/12 [00:03<00:01,  2.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 10/12 [00:04<00:00,  2.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 11/12 [00:04<00:00,  2.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:04<00:00,  2.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.2944509983062744, 'eval_runtime': 5.3651, 'eval_samples_per_second': 2.237, 'eval_steps_per_second': 2.237, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:04<00:00,  2.26it/s]#033[A#015 40%|████      | 14/35 [01:12<01:18,  3.74s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 15/35 [01:18<02:00,  6.03s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 16/35 [01:21<01:39,  5.25s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 17/35 [01:24<01:24,  4.70s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 18/35 [01:28<01:13,  4.31s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 19/35 [01:31<01:04,  4.04s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 20/35 [01:35<00:57,  3.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9061, 'grad_norm': 0.64453125, 'learning_rate': 0.0002, 'epoch': 2.86}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 20/35 [01:35<00:57,  3.85s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 21/35 [01:38<00:52,  3.75s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/12 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 2/12 [00:00<00:02,  4.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 3/12 [00:00<00:02,  3.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 4/12 [00:01<00:02,  2.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 5/12 [00:01<00:02,  2.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 6/12 [00:02<00:02,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 7/12 [00:02<00:02,  2.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 8/12 [00:03<00:01,  2.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 9/12 [00:03<00:01,  2.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 10/12 [00:03<00:00,  2.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 11/12 [00:04<00:00,  2.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:04<00:00,  2.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.352625846862793, 'eval_runtime': 5.3539, 'eval_samples_per_second': 2.241, 'eval_steps_per_second': 2.241, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:04<00:00,  2.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 21/35 [01:44<00:52,  3.75s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 22/35 [01:50<01:18,  6.02s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 23/35 [01:53<01:02,  5.23s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 24/35 [01:56<00:51,  4.68s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 25/35 [02:00<00:42,  4.29s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 26/35 [02:03<00:36,  4.03s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 27/35 [02:07<00:30,  3.84s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 28/35 [02:10<00:25,  3.71s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/12 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 2/12 [00:00<00:02,  4.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 3/12 [00:00<00:02,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 4/12 [00:01<00:02,  2.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 5/12 [00:01<00:02,  2.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 6/12 [00:02<00:02,  2.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 7/12 [00:02<00:02,  2.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 8/12 [00:03<00:01,  2.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 9/12 [00:03<00:01,  2.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 10/12 [00:03<00:00,  2.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 11/12 [00:04<00:00,  2.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:04<00:00,  2.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.4386303424835205, 'eval_runtime': 5.3343, 'eval_samples_per_second': 2.25, 'eval_steps_per_second': 2.25, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:04<00:00,  2.27it/s]#033[A#015 80%|████████  | 28/35 [02:15<00:25,  3.71s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 29/35 [02:21<00:36,  6.01s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 30/35 [02:25<00:26,  5.23s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5373, 'grad_norm': 1.2109375, 'learning_rate': 0.0002, 'epoch': 4.29}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 30/35 [02:25<00:26,  5.23s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 31/35 [02:28<00:18,  4.69s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 32/35 [02:32<00:12,  4.31s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 33/35 [02:35<00:08,  4.04s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 34/35 [02:38<00:03,  3.86s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 35/35 [02:42<00:00,  3.72s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/12 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 2/12 [00:00<00:02,  4.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 3/12 [00:00<00:02,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 4/12 [00:01<00:02,  2.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 5/12 [00:01<00:02,  2.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 6/12 [00:02<00:02,  2.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 7/12 [00:02<00:02,  2.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 8/12 [00:03<00:01,  2.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 9/12 [00:03<00:01,  2.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 10/12 [00:03<00:00,  2.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 11/12 [00:04<00:00,  2.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:04<00:00,  2.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.5469655990600586, 'eval_runtime': 5.3359, 'eval_samples_per_second': 2.249, 'eval_steps_per_second': 2.249, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 12/12 [00:04<00:00,  2.28it/s]#033[A#015100%|██████████| 35/35 [02:47<00:00,  3.72s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'train_runtime': 170.3965, 'train_samples_per_second': 0.411, 'train_steps_per_second': 0.205, 'train_loss': 1.7866914749145508, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 35/35 [02:50<00:00,  3.72s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 35/35 [02:50<00:00,  4.87s/it]\u001b[0m\n",
      "\u001b[34mTrying to load a Peft model. It might take a while without feedback\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[W PyInterpreter.cpp:221] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.92s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.28s/it]\u001b[0m\n",
      "\u001b[34mSaving the newly created merged model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2024-07-15 06:24:14,236 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-15 06:24:14,236 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-15 06:24:14,236 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-07-15 06:24:24 Uploading - Uploading generated training model\n",
      "2024-07-15 06:25:42 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 758\n",
      "Billable seconds: 758\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 경로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_s3_path: \n",
      " {'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-15-05-54-5-2024-07-15-05-55-02-818/output/model/', 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n",
      "Stored 'model_s3_path' (dict)\n"
     ]
    }
   ],
   "source": [
    "model_s3_path = huggingface_estimator.model_data\n",
    "print(\"model_s3_path: \\n\", model_s3_path)\n",
    "\n",
    "%store model_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_llama3_puy310",
   "language": "python",
   "name": "conda_llama3_puy310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
