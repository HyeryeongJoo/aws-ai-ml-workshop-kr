{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4872e4-c90b-434b-bfe5-f88292fba385",
   "metadata": {},
   "source": [
    "# 데이터셋 준비하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422bb81f",
   "metadata": {},
   "source": [
    "## 0.사전 준비 사항\n",
    "아래 사항을 먼저 준비 하셔야 합니다.\n",
    "\n",
    "- huggingface Acess Key 준비 하기 : [User access tokens](https://huggingface.co/docs/hub/en/security-tokens)\n",
    "- Llama-3-8B 모델 엑세스 권한 얻기: [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7aa7-1a2d-4db1-b011-59217c32a83a",
   "metadata": {},
   "source": [
    "## 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3bbbb",
   "metadata": {},
   "source": [
    "### Hugging Face Token 입력\n",
    "- [중요] HF Key 가 노출이 안되도록 조심하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8869029e-5c58-4f80-8f33-3db4a9fe67e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets                  3.0.0\n",
      "sagemaker                 2.232.1\n",
      "sagemaker-core            1.0.9\n",
      "torch                     2.4.1\n",
      "transformers              4.40.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep -E \"torch|datasets|transformers|sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c63b231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
      "/bin/bash: -c: line 0: `huggingface-cli login --token <Type Your HF Key>'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "\n",
    "is_sagemaker_notebook = True\n",
    "if is_sagemaker_notebook:\n",
    "    hf_key_name = \"HF_KEY\"\n",
    "    key_val = \"<Type Your HF Key>\"\n",
    "    set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    HF_TOKEN = get_hf_key_env_vars(hf_key_name)\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52049e77",
   "metadata": {},
   "source": [
    "#### 환경 변수 확인 ( HF 의 디폴트 경로 변경 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdcebd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_DATASETS_CACHE:  /home/ec2-user/SageMaker/.cache\n",
      "HF_HOME:  /home/ec2-user/SageMaker/.cache\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "#os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/ec2-user/SageMaker/.cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/ec2-user/SageMaker/.cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/home/ec2-user/SageMaker/.cache\"\n",
    "\n",
    "#print(\"TRANSFORMERS_CACHE: \", os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(\"HF_DATASETS_CACHE: \", os.getenv('HF_DATASETS_CACHE'))\n",
    "print(\"HF_HOME: \", os.getenv('HF_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a396bc4f-0b0a-4ffb-8c59-18ed6d0a968d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4b722-f752-4687-8fbe-12855566892c",
   "metadata": {},
   "source": [
    "## 2. 데이터 셋 준비\n",
    "### 데이터 셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3710c3d-71bb-47b1-97eb-a72ef6dcfd53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 22194\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2466\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2740\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"daekeun-ml/naver-news-summarization-ko\"\n",
    "\n",
    "# dataset = load_dataset(huggingface_dataset_name, \"3.0.0\")\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94062514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2022-07-03 17:14:37',\n",
       " 'category': 'economy',\n",
       " 'press': 'YTN ',\n",
       " 'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
       " 'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " 'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
       " 'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96451467-4fc0-49af-bedf-040850c11fbb",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eda452",
   "metadata": {},
   "source": [
    "### Chat Message 형태 템플릿 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df1181ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def format_instruction(system_prompt: str, article: str, summary: str):\n",
    "    message = [\n",
    "            {\n",
    "                'content': system_prompt,\n",
    "                'role': 'system'\n",
    "            },\n",
    "            {\n",
    "                'content': f'Please summarize the goals for journalist in this text:\\n\\n{article}',\n",
    "                'role': 'user'\n",
    "            },\n",
    "            {\n",
    "                'content': f'{summary}',\n",
    "                'role': 'assistant'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    return message # json.dumps(message, indent=2) # json.dumps(message, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "# system_prompt = \"You are an AI assistant specialized in news articles. Your role is to provide accurate summaries and insights. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\"\n",
    "# article = \"Within three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert's portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. \\\"We can't ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,\\\" says Rinkert.\"\n",
    "# summary = \"Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.\"\n",
    "\n",
    "# print(format_instruction(system_prompt, article, summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a4269",
   "metadata": {},
   "source": [
    "### Chat Message 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fede492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'category', 'press', 'title', 'document', 'link', 'summary']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b5a43e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_instruction_dataset(data_point):\n",
    "    system_prompt = \"You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": format_instruction(system_prompt, data_point[\"document\"],data_point[\"summary\"])\n",
    "    }\n",
    "\n",
    "def process_dataset(data: Dataset):\n",
    "    return (\n",
    "        data.shuffle(seed=42)\n",
    "        .map(generate_instruction_dataset).remove_columns(columns_to_remove)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390bfdf",
   "metadata": {},
   "source": [
    "#### 전체 데이터 셋에서 일부 데티터 추출 (짧은 실습을 위해서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c75ddc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_num_debug = 10\n",
    "validation_num_debug = 10\n",
    "test_num_debug = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0edb7874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def create_message_dataset(dataset, num_train,num_val, num_test, verbose=False):\n",
    "    ## APPLYING PREPROCESSING ON WHOLE DATASET\n",
    "\n",
    "    train_dataset = process_dataset(dataset[\"train\"].select(range(num_train)))\n",
    "    validation_dataset = process_dataset(dataset[\"validation\"].select(range(num_val)))\n",
    "    test_dataset= process_dataset(dataset[\"test\"].select(range(num_test)))\n",
    "    \n",
    "    if verbose:\n",
    "        print(train_dataset)\n",
    "        print(test_dataset)\n",
    "        print(validation_dataset)\n",
    "\n",
    "    return train_dataset,test_dataset,validation_dataset    \n",
    "\n",
    "train_dataset,test_dataset,validation_dataset = create_message_dataset(dataset=dataset, \n",
    "                                                num_train=train_num_debug,\n",
    "                                                num_val=validation_num_debug, \n",
    "                                                num_test=test_num_debug, verbose=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7197aa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b678c9",
   "metadata": {},
   "source": [
    "#### 전체 데이터 셋 저장 (성능 측정을 위해서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dec1d03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num_samples:  22194\n",
      "validation_num_samples:  2466\n",
      "test_num_samples:  2740\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 22194\n",
      "})\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 2740\n",
      "})\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 2466\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "full_train_num = len(dataset[\"train\"])\n",
    "full_validation_num = len(dataset[\"validation\"])\n",
    "full_test_num = len(dataset[\"test\"])\n",
    "\n",
    "# full_train_num = 1000\n",
    "# full_validation_num = 1000\n",
    "# full_test_num = 1000\n",
    "\n",
    "print(\"train_num_samples: \", full_train_num)\n",
    "print(\"validation_num_samples: \", full_validation_num)\n",
    "print(\"test_num_samples: \", full_test_num)\n",
    "\n",
    "full_train_dataset,full_test_dataset,full_validation_dataset = create_message_dataset(dataset=dataset, \n",
    "                                                                num_train=full_train_num,\n",
    "                                                                num_val=full_validation_num, \n",
    "                                                                num_test=full_test_num, verbose=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dda7e7",
   "metadata": {},
   "source": [
    "## 4. 데이터 셋을 JSON 으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8af3619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_dataset_json_file(huggingface_dataset_name,train_dataset, validation_dataset, test_dataset, is_full, verbose=True ):\n",
    "    dataset_name = huggingface_dataset_name.split(\"/\")[1]\n",
    "    data_folder = os.path.join(\"../data/\",dataset_name)\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "    if is_full:\n",
    "        train_data_json = os.path.join(data_folder,\"full_train\", \"train_dataset.json\")\n",
    "        validation_data_json = os.path.join(data_folder,\"full_validation\", \"validation_dataset.json\")\n",
    "        test_data_json = os.path.join(data_folder, \"full_test\", \"test_dataset.json\")\n",
    "\n",
    "    else:\n",
    "        train_data_json = os.path.join(data_folder,\"train\", \"train_dataset.json\")\n",
    "        validation_data_json = os.path.join(data_folder,\"validation\", \"validation_dataset.json\")\n",
    "        test_data_json = os.path.join(data_folder, \"test\", \"test_dataset.json\")\n",
    "\n",
    "    # save datasets to disk \n",
    "    train_dataset.to_json(train_data_json, orient=\"records\", force_ascii=False)\n",
    "    validation_dataset.to_json(validation_data_json, orient=\"records\", force_ascii=False)\n",
    "    test_dataset.to_json(test_data_json, orient=\"records\", force_ascii=False)        \n",
    "\n",
    "    if verbose:\n",
    "        print(train_dataset)\n",
    "        print(f\"{train_data_json} is saved\")\n",
    "        print(f\"{validation_data_json} is saved\")\n",
    "        print(f\"{test_data_json} is saved\")                \n",
    "\n",
    "    return data_folder, train_data_json, validation_data_json, test_data_json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6c2cf0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 380.19ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 945.73ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 868.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 10\n",
      "})\n",
      "../data/naver-news-summarization-ko/train/train_dataset.json is saved\n",
      "../data/naver-news-summarization-ko/validation/validation_dataset.json is saved\n",
      "../data/naver-news-summarization-ko/test/test_dataset.json is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 23/23 [00:01<00:00, 15.21ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 16.46ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 15.67ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 22194\n",
      "})\n",
      "../data/naver-news-summarization-ko/full_train/train_dataset.json is saved\n",
      "../data/naver-news-summarization-ko/full_validation/validation_dataset.json is saved\n",
      "../data/naver-news-summarization-ko/full_test/test_dataset.json is saved\n"
     ]
    }
   ],
   "source": [
    "# Store debug dataset\n",
    "data_folder, train_data_json, validation_data_json, test_data_json = create_dataset_json_file(huggingface_dataset_name=huggingface_dataset_name,\n",
    "                                                                    train_dataset=train_dataset, \n",
    "                                                                    validation_dataset=validation_dataset, \n",
    "                                                                    test_dataset=test_dataset,\n",
    "                                                                    is_full=False )        \n",
    "\n",
    "# Store full dataset\n",
    "data_folder, full_train_data_json, full_validation_data_json, full_test_data_json = create_dataset_json_file(huggingface_dataset_name=huggingface_dataset_name,\n",
    "                                                                    train_dataset=full_train_dataset, \n",
    "                                                                    validation_dataset=full_validation_dataset, \n",
    "                                                                    test_dataset=full_test_dataset,\n",
    "                                                                    is_full=True )        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c521c6f",
   "metadata": {},
   "source": [
    "### 다음 노트북에서 사용하기 위해 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70fb035e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'data_folder' (str)\n",
      "Stored 'train_data_json' (str)\n",
      "Stored 'validation_data_json' (str)\n",
      "Stored 'test_data_json' (str)\n",
      "Stored 'full_train_data_json' (str)\n",
      "Stored 'full_validation_data_json' (str)\n",
      "Stored 'full_test_data_json' (str)\n"
     ]
    }
   ],
   "source": [
    "%store data_folder\n",
    "%store train_data_json \n",
    "%store validation_data_json \n",
    "%store test_data_json \n",
    "%store full_train_data_json \n",
    "%store full_validation_data_json \n",
    "%store full_test_data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5c4cc",
   "metadata": {},
   "source": [
    "## 5. Option: 데이터 셋을 ChatTemplate 형태로 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165e725",
   "metadata": {},
   "source": [
    "### Chat Template 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea4ba09c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'] }}{% elif message['role'] == 'user' %}{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}{% elif message['role'] == 'assistant' %}{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\\n\\nAssistant: ' }}{% endif %}\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfcc19",
   "metadata": {},
   "source": [
    "### Chat Template 으로 변형하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54451f86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer        \n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", use_fast=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MLP-KTLim/llama-3-Korean-Bllossom-8B\", use_fast=True)\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "\n",
    "# template dataset\n",
    "def template_dataset(examples):\n",
    "    return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "\n",
    "sample_train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "# sample_test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "# sample_validation_dataset = validation_dataset.map(template_dataset, remove_columns=[\"messages\"])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fc688",
   "metadata": {},
   "source": [
    "### 변형된 Chat Message 형태 예시 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9042b261",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  9\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "코트라 BMW 오픈이노베이션 IR 로드쇼 추진 스타트업 6개사 선발…현지서 기술 소개하고 상담 추진 기술실증 PoC 프로젝트까지 지원 이데일리 함정선 기자 한국의 스타트업들이 글로벌 기업인 BMW와 에어버스 Airbus 에 혁신 기술을 소개하고 협력 기회를 모색하는 자리가 마련됐다. KOTRA 코트라 는 한국벤처투자 BMW와 스타트업 개러지 코리아 등과 지난 6월 29일부터 7월1일까지 독일 뮌헨에서 ‘BMW 오픈이노베이션 IR 로드쇼’ 사업을 추진했다고 3일 밝혔다. 이번 BMW 오픈이노베이션 IR 로드쇼에서는 지난 4월 서울 컨벤션센터 SETEC 에서 개최된 ‘BMW 오픈이노베이션 피칭데이’를 통해 선발한 스타트업 6개사 15명이 참가했다. 참가 스타트업 6개사는 4박 5일간 독일 뮌헨을 방문해 BMW와 에어버스를 비롯한 독일의 글로벌 제조사에 자신들의 혁신 기술을 소개하고 협업을 제안했다. 6월 29일에는 BMW 그룹의 연구혁신센터에서 참가 스타트업 6개사의 피칭과 전시가 이뤄졌다. 피칭 행사에는 BMW 임직원 총 150명이 한국 스타트업의 피칭을 보기 위해 온·오프라인으로 모였고 하루 동안 총 350명이 전시 부스에 방문하는 등 BMW의 임직원들은 한국의 혁신 기술 스타트업과 협업하는 데에 큰 관심을 보였다. 이는 현장에서 일대일 상담으로 이어졌으며 코트라에 따르면 현재 참가 스타트업과 BMW그룹의 사업 부서 간 기술실증 PoC 프로젝트 추진 가능성이 논의되고 있다. 드라가나 코스틱 Dragana Kostic BMW 테크놀로지 오피스 Technology Office 아시아태평양본부 총괄은 “코로나 이후 한국의 스타트업이 본사를 직접 방문해 기술을 소개하는 것은 이번 행사가 처음”이라며 “참가한 스타트업들이 모두 우수해 앞으로 많은 협업 프로젝트가 이뤄질 것으로 기대한다”고 말했다. 6월 30일에는 뮌헨 외곽지역에 소재한 에어버스 그룹의 연구 혁신센터를 방문해 참가 스타트업 6개사의 피칭 행사를 진행했다. 에어버스 블루스카이 Blue Sky 오픈이노베이션 전담 조직 를 비롯한 그룹 내 핵심 인력을 대상으로 이루어진 이번 행사에서는 피칭 이후 질의응답까지 진행해 현장에서 기술 검증을 위한 여러 가지 논의가 이어졌다. 장 도미닉 코스트 Jean Dominique Coste 에어버스 블루스카이 그룹장은 “에어버스는 한국 스타트업과의 기술협력을 위해 모든 가능성을 열어두고 지속적으로 논의할 것”이라고 말했다. 이외에도 참가 스타트업 중 2개사는 이번 출장 기간 중 100년의 역사를 자랑하는 글로벌 자동차 부품기업인 베바스토 Webasto 자율주행·정밀엔지니어링 기업인 아큐론 Accuron 도 방문해 일대일 상담을 진행하고 협업 가능성을 논의했다. 참가 스타트업 6개사는 △AI 가상인간 D사 △인공신경망처리장치 NPU 기반 AI 반도체 D사 △친환경 가죽 시트 및 가죽실 제조사 A사 △클라우드 기반 VR 및 메타버스 솔루션 I사 △개발자용 AI 비서 L사 △디지털 ID 인증 및 보안솔루션 S사 등이며 한국에 돌아온 이후 BMW 및 에어버스와의 협업 프로젝트를 본격적으로 추진할 예정이다. 유정열 코트라 사장은 “글로벌기업과의 협업은 우리 스타트업이 해외 시장에서 스케일업할 수 있는 매우 중요한 기회”라며 “코트라의 128개 해외무역관이 우리 스타트업의 손과 발이 돼 현지의 글로벌기업과 협업할 기회를 지속적으로 만들어 나갈 것”이라고 밝혔다. BMW FIZ Center 연구혁신센터 에서 참가 스타트업이 BMW 직원을 대상으로 피칭을 진행하고 있다.<|eot_id|>\n",
      "\n",
      "Assistant: KOTRA 코트라 는 한국벤처투자 BMW와 스타트업 개러지 코리아 등과 지난 6월 29일부터 7월1일까지 독일 뮌헨에서 ‘BMW 오픈이노베이션 IR 로드쇼’ 사업을 추진했다고 3일 밝혔는데, 이번 BMW 오픈이노베이션 IR 로드쇼에서는 지난 4월 ‘BMW 오픈이노베이션 피칭데이’를 통해 선발한 스타트업 6개사 15명이 참가했으며 장 도미닉 코스트 Jean Dominique Coste 에어버스 블루스카이 그룹장은 “에어버스는 한국 스타트업과의 기술협력을 위해 모든 가능성을 열어두고 지속적으로 논의할 것”이라고 말했다.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# print random sample\n",
    "import random\n",
    "\n",
    "for index in random.sample(range(len(sample_train_dataset)), 1):\n",
    "    print(\"index: \", index)\n",
    "    # index = 5343\n",
    "    print(sample_train_dataset[index][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585868a-cc5c-44ef-b5bd-4160ceca79a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_finetune-image",
   "language": "python",
   "name": "conda_finetune-image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
