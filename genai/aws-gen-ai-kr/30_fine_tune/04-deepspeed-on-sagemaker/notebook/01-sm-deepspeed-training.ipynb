{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1941c3c-ea62-41a3-b9d5-8e78330dddc7",
   "metadata": {},
   "source": [
    "# DeepSpeed 가속기를 Amazon SageMaker 에서 사용하기\n",
    "\n",
    "이 노트북은 DeepSpeed 를 SageMaker 에서 모델 학습을 하기 위해서 버트 모델을 네이버 영화 리뷰로 학습을 다루고 있습니다.\n",
    "<br>\n",
    "아래와 같이 3가지로 테스트를 할 수 있습니다.\n",
    "- 현재의 노트북 인스턴스에서 모델 학습하기\n",
    "    - 다음 섹션에서 진행 됩니다. --> ## 3. 로컬 (SageMaker Notebook Instance) 에서 실행 \n",
    "- SageMaker 의 로컬 모드로 학습하기 ( 3. SageMaker Training 준비에서 USE_LOCAL_MODE = True 로 수정)\n",
    "- SageMaker 의 클라우드 모드로 학습하기 ( 3. SageMaker Training 준비에서 USE_LOCAL_MODE = False 로 수정)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 사전 필수 사항\n",
    "\n",
    "### 실습환경\n",
    "- 이 노트북은 [SageMaker Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) 에서 테스트 완료 되었습니다.\n",
    "    - 환경: ml.p3.2xlarge 및 생성시에 Additional configuration 클릭하고 200 GB EBS 추가 하였습니다. --> 참고: [Create a Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html)\n",
    "### 분산 훈련\n",
    "- 훈련(Training) job 수행 시 최소 ml.p3.2xlarge 이상의 훈련 인스턴스가 필요하며, 분산 훈련 핸즈온은 `ml.p3.16xlarge` 인스턴스를 권장합니다. 만약 인스턴스 사용에 제한이 걸려 있다면 [Request a service quota increase for SageMaker resources](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.html#service-limit-increase-request-procedure)를 참조하여 인스턴스 제한을 해제해 주세요.\n",
    "- instance_count 도 현재 1 로 세팅되어 있습니다. 2개의 ml.p3.2xlarge 및 ml.p4d.24xlarge 에서도 테스트 되었습니다.\n",
    "### 환경 세팅\n",
    "- 이 노트북을 실행하기 전에 [README.md](..setup/README.md) 를 참조하시고, 먼저 실행 해주세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4fe66",
   "metadata": {},
   "source": [
    "### [중요] 설치된 패키지 확인\n",
    "- 아래를 실행한면 `deepspeed 0.14.2` 등이 설치가 된 것을 확인할 수 있습니다.\n",
    "- 만일 설치 패키지 결과가 보이지 않으면, 노트북 오른쪽 상단에 쥬피터 커널이 사전에 설치하신 커널로 선택 (예: deepspeed-py310) 이 되어 있는지 확인 해주세요.\n",
    "- 그래도 문제가 해결이 되지 않으면, Web Brower 를 재로딩 해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a62e76c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## installed packages \n",
      "boto3                     1.34.101\n",
      "datasets                  2.14.6\n",
      "deepspeed                 0.14.2\n",
      "evaluate                  0.4.0\n",
      "fsspec                    2023.10.0\n",
      "s3fs                      0.4.2\n",
      "sagemaker                 2.219.0\n",
      "scikit-learn              1.4.1.post1\n",
      "torch                     2.1.0\n",
      "transformers              4.30.2\n"
     ]
    }
   ],
   "source": [
    "print(\"## installed packages \")\n",
    "! pip list | grep -E \"datasets|transformers|fsspec|evaluate|deepspeed|s3fs|boto3|sagemaker|scikit-learn|torch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce1190-1d1d-45bf-aae6-98f9ab009032",
   "metadata": {},
   "source": [
    "## 2. 데이터 셋 준비\n",
    "---\n",
    "\n",
    "SageMaker 훈련을 위해 전처리된 데이터셋을 S3에 업로드합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af744f78",
   "metadata": {},
   "source": [
    "### SageMaker 사용 위한 역할, 리젼 등의 정보 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c06ed4-05af-43e6-ad7e-d24307f297e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "[{2147392357.py:28} INFO - sagemaker role arn: arn:aws:iam::057716757052:role/workshop-sagemaker-kfp-role2\n",
      "[{2147392357.py:29} INFO - sagemaker bucket: sagemaker-us-east-1-057716757052\n",
      "[{2147392357.py:30} INFO - sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import time\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(f\"sagemaker role arn: {role}\")\n",
    "logging.info(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "logging.info(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ad4a6",
   "metadata": {},
   "source": [
    "### 모델 선택 및 데이터 셋 정의\n",
    "- 모델은 [bert-base-multilingual-cased](https://huggingface.co/google-bert/bert-base-multilingual-cased) 를 사용합니다. model_id 를 hyperparmeter 로 제공하여, 훈련 스크립트 (scripts/train_deepspeed.py) 에서 BertForSequenceClassification 를 통하여 모델을 로딩 합니다.\n",
    "- 데이터 세트는 네이버 영화 리뷰 데이터 세트를 로딩 합니다.  [e9t/nsmc](https://huggingface.co/datasets/e9t/nsmc)\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5da250f4-93a8-4ff1-9516-78801fd0b40a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define the model repo\n",
    "model_id = 'bert-base-multilingual-cased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'nsmc'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'datasets/nsmc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ab657",
   "metadata": {},
   "source": [
    "### 데이터 셋 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ba528e-a30e-4f17-a93e-f74f188a78ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 3.18k/3.18k [00:00<00:00, 14.6MB/s]\n",
      "Downloading readme: 100%|██████████| 3.74k/3.74k [00:00<00:00, 16.1MB/s]\n",
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/6.33M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  76%|███████▋  | 4.83M/6.33M [00:00<00:00, 48.3MB/s]\u001b[A\n",
      "Downloading data: 14.6MB [00:00, 49.6MB/s]                            \u001b[A\n",
      "Downloading data files:  50%|█████     | 1/2 [00:02<00:02,  2.16s/it]\n",
      "Downloading data:   0%|          | 0.00/2.12M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 4.89MB [00:00, 47.3MB/s]                   \u001b[A\n",
      "Downloading data files: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1381.07it/s]\n",
      "Generating train split: 100%|██████████| 150000/150000 [00:04<00:00, 34836.39 examples/s]\n",
      "Generating test split: 100%|██████████| 50000/50000 [00:01<00:00, 35297.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{3407595512.py:8} INFO -  loaded train_dataset length is: 2000\n",
      "[{3407595512.py:9} INFO -  loaded eval_dataset length is: 2000\n",
      "[{3407595512.py:10} INFO - {'id': '10020916', 'document': 'For Carl.칼 세이건으로 시작해서 칼 세이건으로 끝난다.', 'label': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "train_dataset, eval_dataset = load_dataset(dataset_name, split=['train', 'test'])\n",
    "\n",
    "num_samples_for_debug = 2000\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(num_samples_for_debug))\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(num_samples_for_debug))\n",
    "\n",
    "logging.info(f\" loaded train_dataset length is: {len(train_dataset)}\")\n",
    "logging.info(f\" loaded eval_dataset length is: {len(eval_dataset)}\")\n",
    "logging.info(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4221e27f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'document', 'label'],\n",
       "     num_rows: 2000\n",
       " }),\n",
       " {'id': '10020916',\n",
       "  'document': 'For Carl.칼 세이건으로 시작해서 칼 세이건으로 끝난다.',\n",
       "  'label': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c6787",
   "metadata": {},
   "source": [
    "### 데이터 셋의 인코딩\n",
    "- 데이터 셋의 document 의 텍스트를 토큰나이저를 통하여 input_ids, attention_mask 로 변환 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9464d37-021e-4980-8b8a-313d90022fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 9043.35 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 9253.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['document'], padding='max_length', max_length=128, truncation=True)\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=['id', 'document'])\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True, remove_columns=['id', 'document'])\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "eval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "eval_dataset = eval_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b421cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 2000\n",
       " }),\n",
       " {'labels': tensor(1),\n",
       "  'input_ids': tensor([  101, 11399, 12225,   119,  9788,  9435, 10739, 71439, 11467,  9485,\n",
       "          38709, 70146,  9788,  9435, 10739, 71439, 11467,  8977, 33305, 11903,\n",
       "            119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0])})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2ba5b",
   "metadata": {},
   "source": [
    "### 데이터 셋을 Local 에 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f01498c2-ddc8-425d-932b-42348f401941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2000/2000 [00:00<00:00, 203943.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2000/2000 [00:00<00:00, 282978.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'train'\n",
    "eval_dir = 'eval'\n",
    "!rm -rf {train_dir} {eval_dir}\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(eval_dir, exist_ok=True) \n",
    "\n",
    "if not os.listdir(train_dir):\n",
    "    train_dataset.save_to_disk(train_dir)\n",
    "if not os.listdir(eval_dir):\n",
    "    eval_dataset.save_to_disk(eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e4cd58-30bb-4dc6-9bc4-3e4d609232d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/fsspec/registry.py:272: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2000/2000 [00:00<00:00, 10399.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_input_path:\n",
      " s3://sagemaker-us-east-1-057716757052/datasets/nsmc/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2000/2000 [00:00<00:00, 17113.08 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_input_path:\n",
      " s3://sagemaker-us-east-1-057716757052/datasets/nsmc/eval\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "train_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/{train_dir}'\n",
    "train_dataset.save_to_disk(train_input_path)\n",
    "\n",
    "print(\"\")\n",
    "print(\"train_input_path:\\n\", train_input_path)\n",
    "# save eval_dataset to s3\n",
    "eval_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/{eval_dir}'\n",
    "eval_dataset.save_to_disk(eval_input_path)\n",
    "print(\"eval_input_path:\\n\", eval_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e377d3-51a9-4503-8d0a-88a9b8373cdc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. 로컬 (SageMaker Notebook Instance) 에서 실행 \n",
    "- 개발 단계에서 코드의 생성, 간단한 실험 및 디버깅시에 주로 사용합니다.\n",
    "---\n",
    "\n",
    "- SageMaker에서 훈련을 수행하기 전에 먼저 로컬 개발 환경에서 모델 훈련 코드를 개발하고 디버깅해야 합니다. SageMaker 노트북 인스턴스에서 작업하는 경우 GPU가 탑재된 인스턴스(p-family)를 사용하셔야 합니다.\n",
    "- [중요] 참고로 아래의 실행은 \"로컬 환경 세팅\" 이 정확하지 않으면, 에러가 자주 발생합니다. \n",
    "- 이후에 다른 버전 (예: Python, PyTorch ) 으로 업그레이드시에, 현재의 설정읠 참고 하시면서 업그레이드 하세요.\n",
    "\n",
    "### DeepSpeed 로컬 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711b43bd-6b96-4270-af1f-372261c16982",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: \n",
      "cd scripts && deepspeed train_deepspeed.py \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2024-06-10 02:17:43,891] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "df: ‘/home/ec2-user/.triton/autotune’: No such file or directory\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "[2024-06-10 02:17:45,463] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-06-10 02:17:45,464] [INFO] [runner.py:568:main] cmd = /home/ec2-user/anaconda3/envs/deepspeed-py310/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train_deepspeed.py\n",
      "[2024-06-10 02:17:48,673] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "[2024-06-10 02:17:49,448] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2024-06-10 02:17:49,448] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2024-06-10 02:17:49,448] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2024-06-10 02:17:49,448] [INFO] [launch.py:164:main] dist_world_size=1\n",
      "[2024-06-10 02:17:49,448] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "[2024-06-10 02:17:49,449] [INFO] [launch.py:256:main] process 24408 spawned with command: ['/home/ec2-user/anaconda3/envs/deepspeed-py310/bin/python3.10', '-u', 'train_deepspeed.py', '--local_rank=0']\n",
      "[2024-06-10 02:17:54,181] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "## os.environ has WORLD_SIZE\n",
      "[2024-06-10 02:17:54,899] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "06/10/2024 02:17:54 - INFO - root -   Initialized the distributed environment. world_size=1, rank=0, local_rank=0\n",
      "06/10/2024 02:17:54 - INFO - root -    loaded train_dataset length is: 2000\n",
      "06/10/2024 02:17:54 - INFO - root -    loaded test_dataset length is: 2000\n",
      "/home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "model.safetensors: 100%|██████████████████████| 714M/714M [00:01<00:00, 428MB/s]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2024-06-10 02:17:58,460] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-06-10 02:18:00,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /home/ec2-user/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Creating extension directory /home/ec2-user/.cache/torch_extensions/py310_cu121/cpu_adam...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/4] /usr/local/cuda-12.1/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda-12.1/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include/TH -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.1/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda-12.1/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include/TH -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.1/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda-12.1/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -c /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda-12.1/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include/TH -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.1/include -isystem /home/ec2-user/anaconda3/envs/deepspeed-py310/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda-12.1/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -c /home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o \n",
      "[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.1/lib64 -lcudart -o cpu_adam.so\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 42.6085159778595 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-06-10 02:18:44,260] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2024-06-10 02:18:44,260] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-06-10 02:18:44,269] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-06-10 02:18:44,269] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-06-10 02:18:44,269] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-06-10 02:18:44,269] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000\n",
      "[2024-06-10 02:18:44,269] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000\n",
      "[2024-06-10 02:18:44,269] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-06-10 02:18:44,269] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "[2024-06-10 02:18:45,899] [INFO] [utils.py:779:see_memory_usage] Before initializing optimizer states\n",
      "[2024-06-10 02:18:45,900] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 0.51 GB         Max_CA 1 GB \n",
      "[2024-06-10 02:18:45,901] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 6.2 GB, percent = 10.4%\n",
      "[2024-06-10 02:18:46,704] [INFO] [utils.py:779:see_memory_usage] After initializing optimizer states\n",
      "[2024-06-10 02:18:46,705] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 0.51 GB         Max_CA 1 GB \n",
      "[2024-06-10 02:18:46,705] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.25 GB, percent = 12.1%\n",
      "[2024-06-10 02:18:46,705] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized\n",
      "[2024-06-10 02:18:46,824] [INFO] [utils.py:779:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-06-10 02:18:46,825] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 0.51 GB         Max_CA 1 GB \n",
      "[2024-06-10 02:18:46,825] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.22 GB, percent = 12.1%\n",
      "[2024-06-10 02:18:46,831] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
      "[2024-06-10 02:18:46,831] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-06-10 02:18:46,832] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-06-10 02:18:46,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:18:46,832] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f18e8203250>\n",
      "[2024-06-10 02:18:46,833] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-06-10 02:18:46,834] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-06-10 02:18:46,835] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   optimizer_name ............... adamw\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08}\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   steps_per_print .............. 10\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False\n",
      "[2024-06-10 02:18:46,836] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=True, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-06-10 02:18:46,837] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-06-10 02:18:46,837] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-06-10 02:18:46,837] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-06-10 02:18:46,837] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"gradient_accumulation\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true, \n",
      "            \"fast_init\": true\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08\n",
      "        }\n",
      "    }\n",
      "}\n",
      "06/10/2024 02:18:46 - INFO - root -   num_training_steps: 126\n",
      "06/10/2024 02:18:46 - INFO - root -   ==== Epoch 1 start ====\n",
      "06/10/2024 02:18:48 - INFO - root -   Train loss: 0.6953125                     1.50s/it]\u001b[0m\n",
      "Training epoch 1:  14%|\u001b[34m███▋                      \u001b[0m| 9/63 [00:06<00:31,  1.70it/s]\u001b[0m[2024-06-10 02:18:53,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:18:53,574] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=55.46832900896623, CurrSamplesPerSec=58.14996115905033, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:18:54 - INFO - root -   Train loss: 0.671875                      1.75it/s]\u001b[0m\n",
      "Training epoch 1:  30%|\u001b[34m███████▌                 \u001b[0m| 19/63 [00:11<00:24,  1.80it/s]\u001b[0m[2024-06-10 02:18:59,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:18:59,115] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=56.8499521865539, CurrSamplesPerSec=57.99668572411311, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:18:59 - INFO - root -   Train loss: 0.6796875                     1.80it/s]\u001b[0m\n",
      "Training epoch 1:  46%|\u001b[34m███████████▌             \u001b[0m| 29/63 [00:17<00:18,  1.80it/s]\u001b[0m[2024-06-10 02:19:04,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:19:04,661] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=57.24033748329861, CurrSamplesPerSec=58.11878829238636, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:19:05 - INFO - root -   Train loss: 0.65625                       1.80it/s]\u001b[0m\n",
      "Training epoch 1:  62%|\u001b[34m███████████████▍         \u001b[0m| 39/63 [00:22<00:13,  1.76it/s]\u001b[0m[2024-06-10 02:19:10,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:19:10,276] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=57.24099091139277, CurrSamplesPerSec=56.6796880410675, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:19:10 - INFO - root -   Train loss: 0.71875                       1.77it/s]\u001b[0m\n",
      "Training epoch 1:  78%|\u001b[34m███████████████████▍     \u001b[0m| 49/63 [00:28<00:07,  1.80it/s]\u001b[0m[2024-06-10 02:19:15,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:19:15,831] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=57.368631325504104, CurrSamplesPerSec=57.85219466631839, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:19:16 - INFO - root -   Train loss: 0.65625                       1.80it/s]\u001b[0m\n",
      "Training epoch 1:  94%|\u001b[34m███████████████████████▍ \u001b[0m| 59/63 [00:33<00:02,  1.80it/s]\u001b[0m[2024-06-10 02:19:21,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:19:21,388] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=57.44783062794385, CurrSamplesPerSec=57.86780892348618, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:19:21 - INFO - root -   Train loss: 0.6953125                     1.80it/s]\u001b[0m\n",
      "Training epoch 1: 100%|\u001b[34m█████████████████████████\u001b[0m| 63/63 [00:36<00:00,  1.74it/s]\u001b[0m\n",
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 17.8MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.77k/6.77k [00:00<00:00, 26.7MB/s]\n",
      "Downloading builder script: 100%|██████████| 7.55k/7.55k [00:00<00:00, 23.6MB/s]\n",
      "Downloading builder script: 100%|██████████| 7.36k/7.36k [00:00<00:00, 20.6MB/s]\n",
      "06/10/2024 02:19:27 - INFO - root -   Eval. loss: 0.59375\n",
      "06/10/2024 02:19:27 - INFO - root -   {'accuracy': 0.535,\n",
      " 'f1': 0.10576923076923077,\n",
      " 'precision': 0.7534246575342466,\n",
      " 'recall': 0.05687693898655636}\n",
      "06/10/2024 02:19:27 - INFO - root -   ==== Epoch 2 start ====\n",
      "06/10/2024 02:19:28 - INFO - root -   Train loss: 0.67578125                    1.80it/s]\u001b[0m\n",
      "Training epoch 2:  10%|\u001b[34m██▍                       \u001b[0m| 6/63 [00:03<00:31,  1.80it/s]\u001b[0m[2024-06-10 02:19:31,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:19:31,808] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=57.67705518479653, CurrSamplesPerSec=58.131097328574896, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:19:34 - INFO - root -   Train loss: 0.66796875                    1.81it/s]\u001b[0m\n",
      "Training epoch 2:  25%|\u001b[34m██████▎                  \u001b[0m| 16/63 [00:08<00:25,  1.81it/s]\u001b[0m[2024-06-10 02:19:37,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:19:37,340] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=57.731297047307024, CurrSamplesPerSec=58.09642386261621, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:19:39 - INFO - root -   Train loss: 0.66015625                    1.81it/s]\u001b[0m\n",
      "Training epoch 2:  41%|\u001b[34m██████████▎              \u001b[0m| 26/63 [00:14<00:20,  1.80it/s]\u001b[0m[2024-06-10 02:19:42,885] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:19:42,885] [INFO] [timer.py:260:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=57.756548284250336, CurrSamplesPerSec=57.95876834452844, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:19:45 - INFO - root -   Train loss: 0.58984375                    1.80it/s]\u001b[0m\n",
      "Training epoch 2:  57%|\u001b[34m██████████████▎          \u001b[0m| 36/63 [00:19<00:14,  1.81it/s]\u001b[0m[2024-06-10 02:19:48,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:19:48,423] [INFO] [timer.py:260:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=57.78571421510871, CurrSamplesPerSec=58.000094205294076, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:19:50 - INFO - root -   Train loss: 0.59765625                    1.81it/s]\u001b[0m\n",
      "Training epoch 2:  73%|\u001b[34m██████████████████▎      \u001b[0m| 46/63 [00:25<00:09,  1.80it/s]\u001b[0m[2024-06-10 02:19:53,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:19:53,973] [INFO] [timer.py:260:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=57.79729695207654, CurrSamplesPerSec=57.800996181848404, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:19:56 - INFO - root -   Train loss: 0.7890625                     1.81it/s]\u001b[0m\n",
      "Training epoch 2:  89%|\u001b[34m██████████████████████▏  \u001b[0m| 56/63 [00:31<00:03,  1.80it/s]\u001b[0m[2024-06-10 02:19:59,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2024-06-10 02:19:59,533] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=57.79822738971507, CurrSamplesPerSec=57.916676591462604, MemAllocated=0.52GB, MaxMemAllocated=4.0GB\n",
      "06/10/2024 02:20:01 - INFO - root -   Train loss: 0.51171875                    1.80it/s]\u001b[0m\n",
      "Training epoch 2: 100%|\u001b[34m█████████████████████████\u001b[0m| 63/63 [00:34<00:00,  1.81it/s]\u001b[0m\n",
      "06/10/2024 02:20:07 - INFO - root -   Eval. loss: 0.50390625\n",
      "06/10/2024 02:20:07 - INFO - root -   {'accuracy': 0.709,\n",
      " 'f1': 0.7246925260170294,\n",
      " 'precision': 0.6678291194420227,\n",
      " 'recall': 0.7921406411582212}\n",
      "06/10/2024 02:20:07 - INFO - root -   ==== Save Model ====\n",
      "06/10/2024 02:20:11 - INFO - root -   Elapsed time: 0:02:16.677554\n",
      "[2024-06-10 02:20:13,594] [INFO] [launch.py:351:main] Process 24408 exits successfully.\n",
      "CPU times: user 2.61 s, sys: 705 ms, total: 3.32 s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TRAIN_DEEPSPEED_CMD = f\"\"\"cd scripts && deepspeed train_deepspeed.py \\\n",
    "\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{TRAIN_DEEPSPEED_CMD}')\n",
    "! {TRAIN_DEEPSPEED_CMD}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29c869-2154-4673-9ec2-8894b1a3cbe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3. SageMaker Training 준비\n",
    "---\n",
    "SageMaker에 대한 대표적인 오해가 여전히 많은 분들이 SageMaker 훈련을 위해 소스 코드를 전면적으로 수정해야 한다고 생각합니다. 하지만, 실제로는 별도의 소스 코드 수정 없이 기존 여러분이 사용했던 파이썬 스크립트에 SageMaker 훈련에 필요한 SageMaker 전용 환경 변수들만 추가하면 됩니다.\n",
    "\n",
    "SageMaker 훈련은 훈련 작업을 호출할 때, \n",
    "- 1) 훈련 EC2 인스턴스 프로비저닝\n",
    "- 2) 컨테이너 구동을 위한 도커 이미지 및 훈련 데이터 다운로드\n",
    "- 3) 컨테이너 구동\n",
    "- 4) 컨테이너 환경에서 훈련 수행\n",
    "- 5) 컨테이너 환경에서 S3의 특정 버킷에 저장\n",
    "- 6) 훈련 인스턴스 종료로 구성됩니다. 따라서, 훈련 수행 로직은 아래 예시와 같이 기존 개발 환경과 동일합니다.\n",
    "    - 아래와 같이 구동된 컨테이너 안에서 아래의 경로에서 , 아래와 같은 형식으로 훈련 스크립트가 실행 됩니다.\n",
    "\n",
    "        `/opt/conda/bin/python train_hf.py --num_epochs 5 --train_batch_size 32 ...`\n",
    "\n",
    "이 과정에서 컨테이너 환경에 필요한 환경 변수(예: 모델 경로, 훈련 데이터 경로) 들은 사전에 지정되어 있으며, 이 환경 변수들이 설정되어 있어야 훈련에 필요한 파일들의 경로를 인식할 수 있습니다. 대표적인 환경 변수들에 대한 자세한 내용은 https://github.com/aws/sagemaker-containers#important-environment-variables 을 참조하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d7337",
   "metadata": {},
   "source": [
    "### 로컬 모드 혹은 클라우드 모드 설정\n",
    "- 아래를 주석으로 조절 하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02e9eb15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Cloud mode is set with ml.g5.2xlarge and 1 of instance_count\n"
     ]
    }
   ],
   "source": [
    "# USE_LOCAL_MODE = True\n",
    "USE_LOCAL_MODE = False\n",
    "\n",
    "if USE_LOCAL_MODE:\n",
    "    instance_type = 'local_gpu'\n",
    "    instance_count = 1\n",
    "    batch_size = 32\n",
    "    from sagemaker.local import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    print(\"## Local mode is set\")\n",
    "\n",
    "else:\n",
    "    instance_type = 'ml.g5.2xlarge'\n",
    "    # instance_type = 'ml.p3.8xlarge'\n",
    "    # instance_type = 'ml.p3.16xlarge'\n",
    "    instance_count = 1\n",
    "    batch_size = 32\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    print(f\"## Cloud mode is set with {instance_type} and {instance_count} of instance_count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad850a30",
   "metadata": {},
   "source": [
    "## Hyperparameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef61fd36-be73-4a69-bf1d-6b2c549873f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    'num_epochs': 3,                    # number of training epochs\n",
    "    'seed': 42,                         # seed\n",
    "    'train_batch_size': batch_size,     # batch size for training\n",
    "    'eval_batch_size': batch_size*2,    # batch size for evaluation\n",
    "    'warmup_steps': 0,                  # warmup steps\n",
    "    'learning_rate': 3e-5,              # learning rate used during training\n",
    "    'model_id': model_id                # pre-trained model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de3bf28",
   "metadata": {},
   "source": [
    "### mpi_options 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d27fc875-0d23-401a-b636-80d08ace6bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enabled': True, 'processes_per_host': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if instance_type == 'local_gpu':\n",
    "    import torch\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_gpus}\")    \n",
    "\n",
    "    mpi_options = {\n",
    "    \"enabled\" : True,            # Required\n",
    "    \"processes_per_host\" : num_gpus,    # Required\n",
    "    # \"custom_mpi_options\" : \"--mca btl_vader_single_copy_mechanism none\"\n",
    "}\n",
    "elif instance_type == 'ml.p3.16xlarge' :\n",
    "    mpi_options = {\n",
    "        \"enabled\" : True,            # Required\n",
    "        \"processes_per_host\" : 8,    # Required\n",
    "        # \"custom_mpi_options\" : \"--mca btl_vader_single_copy_mechanism none\"\n",
    "    }    \n",
    "elif instance_type == 'ml.p3.8xlarge' :\n",
    "    mpi_options = {\n",
    "        \"enabled\" : True,            # Required\n",
    "        \"processes_per_host\" : 4,    # Required\n",
    "        # \"custom_mpi_options\" : \"--mca btl_vader_single_copy_mechanism none\"\n",
    "    }    \n",
    "    \n",
    "else:\n",
    "    mpi_options = {\n",
    "        \"enabled\" : True,            # Required\n",
    "        \"processes_per_host\" : 1,    # Required\n",
    "        # \"custom_mpi_options\" : \"--mca btl_vader_single_copy_mechanism none\"\n",
    "    }\n",
    "\n",
    "mpi_options\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9912904",
   "metadata": {},
   "source": [
    "### 훈련 도커 이미지 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90e60aab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_uri: \n",
      " 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "#image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\"\n",
    "image_uri = '763104351884.dkr.ecr.{}.amazonaws.com/pytorch-training:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker'.format(region)\n",
    "print(\"image_uri: \\n\", image_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c0fd6",
   "metadata": {},
   "source": [
    "### Estimator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41af34c0-a0f5-48ba-b763-2e61b546691f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# define Training Job Name \n",
    "job_name = f'deepspeed-nsmc-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "chkpt_s3_path = f's3://{sess.default_bucket()}/{s3_prefix}/native/checkpoints'\n",
    "\n",
    "# create the Estimator\n",
    "sm_estimator = PyTorch(\n",
    "    entry_point           = 'train_deepspeed.py',  # fine-tuning script used in training jon\n",
    "    source_dir            = './scripts',        # directory where fine-tuning script is stored\n",
    "      image_uri = image_uri,\n",
    "    instance_type         = instance_type,      # instances type used for the training job\n",
    "    instance_count        = instance_count,     # the number of instances used for training\n",
    "    base_job_name         = job_name,           # the name of the training job\n",
    "    role                  = role,               # IAM role used in training job to access AWS ressources, e.g. S3\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    py_version            = 'py38',             # the python version used in the training job\n",
    "    hyperparameters       = hyperparameters,    # the hyperparameter used for running the training job\n",
    "    distribution          = {\"mpi\": mpi_options},\n",
    "    disable_profiler     = True,\n",
    "    debugger_hook_config  = False,\n",
    "    #keep_alive_period_in_seconds = 20*60     # warm pool    \n",
    "    #checkpoint_s3_uri     = chkpt_s3_path,\n",
    "    #checkpoint_local_path ='/opt/ml/checkpoints',  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9fc18",
   "metadata": {},
   "source": [
    "### 훈련에 사용할 데이터 설정\n",
    "- 위에서 업로드한 S3 의 경로를 기술 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6e150c9-dbaa-49b1-95b0-19801315e67e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': train_input_path,\n",
    "    'eval': eval_input_path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e04575",
   "metadata": {},
   "source": [
    "## 4. 훈련 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8f27dd0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{session.py:1002} INFO - Creating training-job with name: deepspeed-nsmc-2024-06-10-03-41-59-2024-06-10-03-42-01-988\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "sm_estimator.fit(data, wait=False)\n",
    "train_job_name = sm_estimator.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e9a283",
   "metadata": {},
   "source": [
    "### 훈련 잡과 클라우드 워치 로그 경로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78f7d720-0c55-44ae-8fa8-0b553eff29d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b> [PyTorch DeepSpeed Training] Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/deepspeed-nsmc-2024-06-10-03-41-59-2024-06-10-03-42-01-988\">Training Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b> [PyTorch DeepSpeed Training] Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=deepspeed-nsmc-2024-06-10-03-41-59-2024-06-10-03-42-01-988;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def make_console_link(region, train_job_name, train_task='[Training]'):\n",
    "    train_job_link = f'<b> {train_task} Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/jobs/{train_job_name}\">Training Job</a></b>'   \n",
    "    cloudwatch_link = f'<b> {train_task} Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={region}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={train_job_name};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>'\n",
    "    return train_job_link, cloudwatch_link  \n",
    "        \n",
    "train_job_link, cloudwatch_link = make_console_link(region, train_job_name, '[PyTorch DeepSpeed Training]')\n",
    "\n",
    "display(HTML(train_job_link))\n",
    "display(HTML(cloudwatch_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c4d21",
   "metadata": {},
   "source": [
    "### 훈련 로그 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a018a419",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-10 03:42:02 Starting - Starting the training job\n",
      "2024-06-10 03:42:02 Pending - Training job waiting for capacity......\n",
      "2024-06-10 03:42:48 Pending - Preparing the instances for training...\n",
      "2024-06-10 03:43:34 Downloading - Downloading the training image.....................\n",
      "2024-06-10 03:46:50 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:16,307 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:16,327 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:16,338 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:16,340 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:16,562 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.14.6\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.6-py3-none-any.whl (493 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.7/493.7 kB 19.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.30.2\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 79.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.0\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 19.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.14.2\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.2.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 68.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting xxhash\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.6/194.6 kB 35.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (11.0.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 72.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (0.70.14)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0.0,>=0.14.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 54.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 84.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 77.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.5.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 776.2/776.2 kB 63.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.14.0-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.8/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 4)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.8/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 4)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 4)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.8/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 4)) (9.0.0)\u001b[0m\n",
      "\u001b[34mCollecting pydantic\u001b[0m\n",
      "\u001b[34mDownloading pydantic-2.7.3-py3-none-any.whl (409 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 409.6/409.6 kB 53.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pynvml\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 14.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 4)) (1.12.1+cu113)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.3/129.3 kB 19.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (308 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.8/308.8 kB 42.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.9/240.9 kB 35.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (22.2.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0.0,>=0.14.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 55.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.1-py3-none-any.whl (401 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.3/401.3 kB 49.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.2/401.2 kB 51.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 50.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.1-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.6/388.6 kB 47.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.0-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.5/388.5 kB 55.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 42.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.2/346.2 kB 54.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.21.2-py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.2/346.2 kB 48.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.21.1-py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.1/346.1 kB 51.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.21.0-py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.1/346.1 kB 49.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 330.1/330.1 kB 49.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 330.3/330.3 kB 49.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 330.1/330.1 kB 45.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.20.0-py3-none-any.whl (329 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 329.1/329.1 kB 47.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.7/311.7 kB 45.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.19.3-py3-none-any.whl (311 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.2/311.2 kB 45.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.19.2-py3-none-any.whl (311 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.2/311.2 kB 44.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.1/311.1 kB 40.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.2/311.2 kB 44.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 46.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 47.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6->-r requirements.txt (line 1)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 1)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 1)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.14.6->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.14.6->-r requirements.txt (line 1)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mCollecting annotated-types>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=3.7.4.3\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mCollecting pydantic-core==2.18.4\u001b[0m\n",
      "\u001b[34mDownloading pydantic_core-2.18.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 75.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.6->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.2-py3-none-any.whl size=1432250 sha256=b771817d75fdea5e14a8500f8bc9dd8a9fb420e8e17e11497cc35601eb992eff\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5b/c9/23/10523cd399fff612d183aca4bd456ab8a651c581f49adcf0fc\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, xxhash, typing-extensions, safetensors, regex, pynvml, multidict, frozenlist, filelock, async-timeout, yarl, responses, pydantic-core, huggingface-hub, annotated-types, aiosignal, transformers, pydantic, aiohttp, deepspeed, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.4.0\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.4.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.4.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+b3d766e\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+b3d766e:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+b3d766e\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 datasets-2.14.6 deepspeed-0.14.2 evaluate-0.4.0 filelock-3.14.0 frozenlist-1.4.1 huggingface-hub-0.17.3 multidict-6.0.5 pydantic-2.7.3 pydantic-core-2.18.4 pynvml-11.5.0 regex-2024.5.15 responses-0.18.0 safetensors-0.4.3 tokenizers-0.13.3 transformers-4.30.2 typing-extensions-4.12.2 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,428 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,428 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,468 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,502 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,515 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,515 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,517 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,518 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1'] process_per_hosts: 1 num_processes: 1\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,519 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,540 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,574 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,586 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 1\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"eval\": \"/opt/ml/input/data/eval\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eval_batch_size\": 64,\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"model_id\": \"bert-base-multilingual-cased\",\n",
      "        \"num_epochs\": 3,\n",
      "        \"seed\": 42,\n",
      "        \"train_batch_size\": 32,\n",
      "        \"warmup_steps\": 0\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"eval\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"deepspeed-nsmc-2024-06-10-03-41-59-2024-06-10-03-42-01-988\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/deepspeed-nsmc-2024-06-10-03-41-59-2024-06-10-03-42-01-988/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deepspeed\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deepspeed.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"eval_batch_size\":64,\"learning_rate\":3e-05,\"model_id\":\"bert-base-multilingual-cased\",\"num_epochs\":3,\"seed\":42,\"train_batch_size\":32,\"warmup_steps\":0}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deepspeed.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":1}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"eval\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deepspeed\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/deepspeed-nsmc-2024-06-10-03-41-59-2024-06-10-03-42-01-988/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":1},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"eval_batch_size\":64,\"learning_rate\":3e-05,\"model_id\":\"bert-base-multilingual-cased\",\"num_epochs\":3,\"seed\":42,\"train_batch_size\":32,\"warmup_steps\":0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"deepspeed-nsmc-2024-06-10-03-41-59-2024-06-10-03-42-01-988\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/deepspeed-nsmc-2024-06-10-03-41-59-2024-06-10-03-42-01-988/source/sourcedir.tar.gz\",\"module_name\":\"train_deepspeed\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deepspeed.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--eval_batch_size\",\"64\",\"--learning_rate\",\"3e-05\",\"--model_id\",\"bert-base-multilingual-cased\",\"--num_epochs\",\"3\",\"--seed\",\"42\",\"--train_batch_size\",\"32\",\"--warmup_steps\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_EVAL=/opt/ml/input/data/eval\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=bert-base-multilingual-cased\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=42\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 1 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_EVAL -x SM_CHANNEL_TRAIN -x SM_HP_EVAL_BATCH_SIZE -x SM_HP_LEARNING_RATE -x SM_HP_MODEL_ID -x SM_HP_NUM_EPOCHS -x SM_HP_SEED -x SM_HP_TRAIN_BATCH_SIZE -x SM_HP_WARMUP_STEPS -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py train_deepspeed.py --eval_batch_size 64 --learning_rate 3e-05 --model_id bert-base-multilingual-cased --num_epochs 3 --seed 42 --train_batch_size 32 --warmup_steps 0\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:39,608 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m[2024-06-10 03:47:41.913: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2024-06-10 03:47:42,010] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m please install triton==1.0.0 if you want to use sparse attention\u001b[0m\n",
      "\u001b[34m2024-06-10 03:47:44,893 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mData for JOB [41001,1] offset 0 Total slots allocated 1\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [41001,1] App: 0 Process rank: 0 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:47:47,166] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[93m [WARNING] #033[0m please install triton==1.0.0 if you want to use sparse attention\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:## os.environ has OMPI_COMM_WORLD_SIZE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:47:49,753] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:47:49 - INFO - root -   Initialized the distributed environment. world_size=1, rank=0, local_rank=0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:47:49 - INFO - root -    loaded train_dataset length is: 2000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:47:49 - INFO - root -    loaded test_dataset length is: 2000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading config.json: 100%|██████████| 625/625 [00:00<00:00, 128kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:   3%|▎         | 21.0M/714M [00:00<00:04, 152MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:   6%|▌         | 41.9M/714M [00:00<00:03, 181MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  10%|█         | 73.4M/714M [00:00<00:02, 226MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  16%|█▌        | 115M/714M [00:00<00:02, 277MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  22%|██▏       | 157M/714M [00:00<00:01, 313MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  28%|██▊       | 199M/714M [00:00<00:01, 327MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  34%|███▍      | 241M/714M [00:00<00:01, 341MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  40%|███▉      | 283M/714M [00:00<00:01, 353MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  46%|████▌     | 325M/714M [00:01<00:01, 343MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  51%|█████▏    | 367M/714M [00:01<00:01, 337MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  57%|█████▋    | 409M/714M [00:01<00:00, 345MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  63%|██████▎   | 451M/714M [00:01<00:00, 360MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  69%|██████▉   | 493M/714M [00:01<00:00, 362MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  75%|███████▍  | 535M/714M [00:01<00:00, 366MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  81%|████████  | 577M/714M [00:01<00:00, 373MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  87%|████████▋ | 619M/714M [00:01<00:00, 367MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  92%|█████████▏| 661M/714M [00:01<00:00, 367MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors:  98%|█████████▊| 703M/714M [00:02<00:00, 372MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading model.safetensors: 100%|██████████| 714M/714M [00:02<00:00, 338MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:47:53,447] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:47:56 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:47:56 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:211 [0] NCCL INFO Bootstrap : Using eth0:10.0.139.210<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:211 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:211 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:211 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:211 [0] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:211 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:211 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:211 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.139.210<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:211 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:211:256 [0] NCCL INFO comm 0x7ff5800030d0 rank 0 nranks 1 cudaDev 0 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:47:56,105] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Using /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Creating extension directory /root/.cache/torch_extensions/py38_cu113/cpu_adam...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Detected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Emitting ninja build file /root/.cache/torch_extensions/py38_cu113/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Building extension module cpu_adam...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[1/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Loading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Time to load cpu_adam op: 26.262118816375732 seconds\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Adam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:25,322] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:25,323] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:25,331] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:25,331] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:25,331] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:25,332] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:25,332] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:25,332] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:25,332] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:27,497] [INFO] [utils.py:779:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:27,498] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 0.51 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:27,498] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 6.62 GB, percent = 21.4%\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,509] [INFO] [utils.py:779:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,509] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 0.51 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,509] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.64 GB, percent = 24.7%\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,510] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,591] [INFO] [utils.py:779:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,591] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 0.51 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,591] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.64 GB, percent = 24.7%\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,595] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,595] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,595] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,596] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"partition_activations\": false, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"contiguous_memory_optimization\": false, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"cpu_checkpointing\": false, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"number_checkpoints\": null, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"synchronize_checkpoint_boundary\": false, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"profile\": false\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   autotuning_config ............ {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"enabled\": false, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"start_step\": null, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"end_step\": null, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"metric_path\": null, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"arg_mappings\": null, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"metric\": \"throughput\", \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"model_info\": null, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"results_dir\": \"autotuning_results\", \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"exps_dir\": \"autotuning_exps\", \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"overwrite\": true, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"fast\": true, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"start_profile_step\": 3, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"end_profile_step\": 5, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"tuner_type\": \"gridsearch\", \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"tuner_early_stopping\": 5, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"tuner_num_trials\": 50, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"model_info_path\": null, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"mp_size\": 1, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"max_train_batch_size\": null, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"min_train_batch_size\": 1, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"min_train_micro_batch_size_per_gpu\": 1, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff61524ca90>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,597] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,598] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"enabled\": false, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"recompute_fwd_factor\": 0.0, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"profile_step\": 1, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"module_depth\": -1, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"top_modules\": 1, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"detailed\": true, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"output_file\": null\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   graph_harvesting ............. False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,599] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   nebula_config ................ {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"enabled\": false, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"persistent_storage_path\": null, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"persistent_time_interval\": 100, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"num_of_version_in_retention\": 2, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"enable_nebula_load\": true, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"load_path\": null\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   steps_per_print .............. 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   train_batch_size ............. 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,600] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:1000:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:1000:print]   world_size ................... 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=True, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:1000:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:28,601] [INFO] [config.py:986:print_user_config]   json = {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"train_micro_batch_size_per_gpu\": 32, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"gradient_accumulation\": 1, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"steps_per_print\": 10, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"bf16\": {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:        \"enabled\": true\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    }, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"zero_optimization\": {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:        \"stage\": 2, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:        \"contiguous_gradients\": true, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:        \"overlap_comm\": true, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:        \"offload_optimizer\": {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:            \"device\": \"cpu\", \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:            \"pin_memory\": true, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:            \"fast_init\": true\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:        }\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    }, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"optimizer\": {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:        \"type\": \"AdamW\", \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:        \"params\": {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:            \"lr\": 3e-05, \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:            \"betas\": [0.9, 0.999], \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:            \"eps\": 1e-08\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:        }\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    }\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:48:28 - INFO - root -   num_training_steps: 189\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:48:28 - INFO - root -   ==== Epoch 1 start ====\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:   0%|#033[34m          #033[0m| 0/63 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:29.935: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:29.980 algo-1:211 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:30.118 algo-1:211 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:   2%|#033[34m▏         #033[0m| 1/63 [00:02<02:51,  2.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:48:31 - INFO - root -   Train loss: 0.69921875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:   2%|#033[34m▏         #033[0m| 1/63 [00:02<02:51,  2.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:   3%|#033[34m▎         #033[0m| 2/63 [00:03<01:25,  1.41s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:   5%|#033[34m▍         #033[0m| 3/63 [00:03<00:58,  1.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:   6%|#033[34m▋         #033[0m| 4/63 [00:04<00:45,  1.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:   8%|#033[34m▊         #033[0m| 5/63 [00:04<00:37,  1.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  10%|#033[34m▉         #033[0m| 6/63 [00:05<00:33,  1.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  11%|#033[34m█         #033[0m| 7/63 [00:05<00:30,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  13%|#033[34m█▎        #033[0m| 8/63 [00:05<00:28,  1.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  14%|#033[34m█▍        #033[0m| 9/63 [00:06<00:26,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:35,451] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:35,452] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=71.58342297301702, CurrSamplesPerSec=71.93108818647735, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  16%|#033[34m█▌        #033[0m| 10/63 [00:06<00:25,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  17%|#033[34m█▋        #033[0m| 11/63 [00:07<00:24,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015                                                                 #015[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  17%|#033[34m█▋        #033[0m| 11/63 [00:07<00:24,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:48:35 - INFO - root -   Train loss: 0.63671875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  19%|#033[34m█▉        #033[0m| 12/63 [00:07<00:23,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  21%|#033[34m██        #033[0m| 13/63 [00:08<00:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  22%|#033[34m██▏       #033[0m| 14/63 [00:08<00:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  24%|#033[34m██▍       #033[0m| 15/63 [00:09<00:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  25%|#033[34m██▌       #033[0m| 16/63 [00:09<00:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  27%|#033[34m██▋       #033[0m| 17/63 [00:10<00:21,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  29%|#033[34m██▊       #033[0m| 18/63 [00:10<00:20,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  30%|#033[34m███       #033[0m| 19/63 [00:10<00:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:40,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:40,014] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=71.36407165984357, CurrSamplesPerSec=71.52674944270329, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  32%|#033[34m███▏      #033[0m| 20/63 [00:11<00:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  33%|#033[34m███▎      #033[0m| 21/63 [00:11<00:19,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:48:40 - INFO - root -   Train loss: 0.6953125\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  33%|#033[34m███▎      #033[0m| 21/63 [00:11<00:19,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  35%|#033[34m███▍      #033[0m| 22/63 [00:12<00:18,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  37%|#033[34m███▋      #033[0m| 23/63 [00:12<00:18,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  38%|#033[34m███▊      #033[0m| 24/63 [00:13<00:17,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  40%|#033[34m███▉      #033[0m| 25/63 [00:13<00:17,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  41%|#033[34m████▏     #033[0m| 26/63 [00:14<00:16,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  43%|#033[34m████▎     #033[0m| 27/63 [00:14<00:16,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  44%|#033[34m████▍     #033[0m| 28/63 [00:15<00:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  46%|#033[34m████▌     #033[0m| 29/63 [00:15<00:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:44,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:44,546] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=71.44987109218631, CurrSamplesPerSec=72.1074613843556, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  48%|#033[34m████▊     #033[0m| 30/63 [00:15<00:14,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  49%|#033[34m████▉     #033[0m| 31/63 [00:16<00:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:48:44 - INFO - root -   Train loss: 0.6796875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  49%|#033[34m████▉     #033[0m| 31/63 [00:16<00:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  51%|#033[34m█████     #033[0m| 32/63 [00:16<00:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  52%|#033[34m█████▏    #033[0m| 33/63 [00:17<00:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  54%|#033[34m█████▍    #033[0m| 34/63 [00:17<00:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  56%|#033[34m█████▌    #033[0m| 35/63 [00:18<00:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  57%|#033[34m█████▋    #033[0m| 36/63 [00:18<00:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  59%|#033[34m█████▊    #033[0m| 37/63 [00:19<00:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  60%|#033[34m██████    #033[0m| 38/63 [00:19<00:11,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  62%|#033[34m██████▏   #033[0m| 39/63 [00:20<00:10,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:49,081] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:49,082] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=71.48384659672035, CurrSamplesPerSec=72.0325402392431, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  63%|#033[34m██████▎   #033[0m| 40/63 [00:20<00:10,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  65%|#033[34m██████▌   #033[0m| 41/63 [00:20<00:09,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:48:49 - INFO - root -   Train loss: 0.6328125\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  65%|#033[34m██████▌   #033[0m| 41/63 [00:20<00:09,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  67%|#033[34m██████▋   #033[0m| 42/63 [00:21<00:09,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  68%|#033[34m██████▊   #033[0m| 43/63 [00:21<00:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  70%|#033[34m██████▉   #033[0m| 44/63 [00:22<00:08,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  71%|#033[34m███████▏  #033[0m| 45/63 [00:22<00:08,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  73%|#033[34m███████▎  #033[0m| 46/63 [00:23<00:07,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  75%|#033[34m███████▍  #033[0m| 47/63 [00:23<00:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  76%|#033[34m███████▌  #033[0m| 48/63 [00:24<00:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  78%|#033[34m███████▊  #033[0m| 49/63 [00:24<00:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:53,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:53,600] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=71.55984141085425, CurrSamplesPerSec=71.68310638461506, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  79%|#033[34m███████▉  #033[0m| 50/63 [00:24<00:05,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  81%|#033[34m████████  #033[0m| 51/63 [00:25<00:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015                                                                 #015[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  81%|#033[34m████████  #033[0m| 51/63 [00:25<00:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:48:54 - INFO - root -   Train loss: 0.6171875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  83%|#033[34m████████▎ #033[0m| 52/63 [00:25<00:04,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  84%|#033[34m████████▍ #033[0m| 53/63 [00:26<00:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  86%|#033[34m████████▌ #033[0m| 54/63 [00:26<00:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  87%|#033[34m████████▋ #033[0m| 55/63 [00:27<00:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  89%|#033[34m████████▉ #033[0m| 56/63 [00:27<00:03,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  90%|#033[34m█████████ #033[0m| 57/63 [00:28<00:02,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  92%|#033[34m█████████▏#033[0m| 58/63 [00:28<00:02,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  94%|#033[34m█████████▎#033[0m| 59/63 [00:29<00:01,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:58,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:48:58,117] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=71.61536584964213, CurrSamplesPerSec=71.99915457764556, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  95%|#033[34m█████████▌#033[0m| 60/63 [00:29<00:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  97%|#033[34m█████████▋#033[0m| 61/63 [00:29<00:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:48:58 - INFO - root -   Train loss: 0.72265625\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  97%|#033[34m█████████▋#033[0m| 61/63 [00:29<00:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1:  98%|#033[34m█████████▊#033[0m| 62/63 [00:30<00:00,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1: 100%|#033[34m██████████#033[0m| 63/63 [00:30<00:00,  2.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 1: 100%|#033[34m██████████#033[0m| 63/63 [00:30<00:00,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 7.17MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 6.77k/6.77k [00:00<00:00, 10.8MB/s][1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 7.55k/7.55k [00:00<00:00, 11.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 7.36k/7.36k [00:00<00:00, 8.02MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:06 - INFO - root -   Eval. loss: 0.5546875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:06 - INFO - root -   {'accuracy': 0.5955,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: 'f1': 0.3218776194467728,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: 'precision': 0.8495575221238938,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: 'recall': 0.19855222337125128}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:06 - INFO - root -   ==== Epoch 2 start ====\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:   0%|#033[34m          #033[0m| 0/63 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:   2%|#033[34m▏         #033[0m| 1/63 [00:00<00:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:06 - INFO - root -   Train loss: 0.63671875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:   2%|#033[34m▏         #033[0m| 1/63 [00:00<00:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:   3%|#033[34m▎         #033[0m| 2/63 [00:00<00:27,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:   5%|#033[34m▍         #033[0m| 3/63 [00:01<00:26,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:   6%|#033[34m▋         #033[0m| 4/63 [00:01<00:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:   8%|#033[34m▊         #033[0m| 5/63 [00:02<00:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  10%|#033[34m▉         #033[0m| 6/63 [00:02<00:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:09,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:09,317] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=71.73984958536124, CurrSamplesPerSec=71.73041903309264, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  11%|#033[34m█         #033[0m| 7/63 [00:03<00:25,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  13%|#033[34m█▎        #033[0m| 8/63 [00:03<00:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  14%|#033[34m█▍        #033[0m| 9/63 [00:04<00:24,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  16%|#033[34m█▌        #033[0m| 10/63 [00:04<00:23,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  17%|#033[34m█▋        #033[0m| 11/63 [00:04<00:23,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:11 - INFO - root -   Train loss: 0.5859375\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  17%|#033[34m█▋        #033[0m| 11/63 [00:04<00:23,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  19%|#033[34m█▉        #033[0m| 12/63 [00:05<00:23,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  21%|#033[34m██        #033[0m| 13/63 [00:05<00:22,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  22%|#033[34m██▏       #033[0m| 14/63 [00:06<00:22,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  24%|#033[34m██▍       #033[0m| 15/63 [00:06<00:21,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  25%|#033[34m██▌       #033[0m| 16/63 [00:07<00:21,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:13,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:13,841] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=71.74065154694144, CurrSamplesPerSec=72.12183648882905, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  27%|#033[34m██▋       #033[0m| 17/63 [00:07<00:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  29%|#033[34m██▊       #033[0m| 18/63 [00:08<00:20,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  30%|#033[34m███       #033[0m| 19/63 [00:08<00:20,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  32%|#033[34m███▏      #033[0m| 20/63 [00:09<00:19,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  33%|#033[34m███▎      #033[0m| 21/63 [00:09<00:19,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:15 - INFO - root -   Train loss: 0.578125\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  33%|#033[34m███▎      #033[0m| 21/63 [00:09<00:19,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  35%|#033[34m███▍      #033[0m| 22/63 [00:09<00:18,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  37%|#033[34m███▋      #033[0m| 23/63 [00:10<00:18,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  38%|#033[34m███▊      #033[0m| 24/63 [00:10<00:17,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  40%|#033[34m███▉      #033[0m| 25/63 [00:11<00:17,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  41%|#033[34m████▏     #033[0m| 26/63 [00:11<00:16,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:18,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:18,388] [INFO] [timer.py:260:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=71.70071726091942, CurrSamplesPerSec=71.85303231139197, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  43%|#033[34m████▎     #033[0m| 27/63 [00:12<00:16,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  44%|#033[34m████▍     #033[0m| 28/63 [00:12<00:15,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  46%|#033[34m████▌     #033[0m| 29/63 [00:13<00:15,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  48%|#033[34m████▊     #033[0m| 30/63 [00:13<00:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  49%|#033[34m████▉     #033[0m| 31/63 [00:14<00:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:20 - INFO - root -   Train loss: 0.458984375\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  49%|#033[34m████▉     #033[0m| 31/63 [00:14<00:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  51%|#033[34m█████     #033[0m| 32/63 [00:14<00:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  52%|#033[34m█████▏    #033[0m| 33/63 [00:14<00:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  54%|#033[34m█████▍    #033[0m| 34/63 [00:15<00:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  56%|#033[34m█████▌    #033[0m| 35/63 [00:15<00:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  57%|#033[34m█████▋    #033[0m| 36/63 [00:16<00:12,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:22,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:22,901] [INFO] [timer.py:260:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=71.72279311045891, CurrSamplesPerSec=71.89148069925852, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  59%|#033[34m█████▊    #033[0m| 37/63 [00:16<00:11,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  60%|#033[34m██████    #033[0m| 38/63 [00:17<00:11,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  62%|#033[34m██████▏   #033[0m| 39/63 [00:17<00:10,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  63%|#033[34m██████▎   #033[0m| 40/63 [00:18<00:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  65%|#033[34m██████▌   #033[0m| 41/63 [00:18<00:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:24 - INFO - root -   Train loss: 0.54296875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  65%|#033[34m██████▌   #033[0m| 41/63 [00:18<00:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  67%|#033[34m██████▋   #033[0m| 42/63 [00:18<00:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  68%|#033[34m██████▊   #033[0m| 43/63 [00:19<00:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  70%|#033[34m██████▉   #033[0m| 44/63 [00:19<00:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  71%|#033[34m███████▏  #033[0m| 45/63 [00:20<00:08,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  73%|#033[34m███████▎  #033[0m| 46/63 [00:20<00:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:27,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:27,442] [INFO] [timer.py:260:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=71.70470857938318, CurrSamplesPerSec=70.46966818000679, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  75%|#033[34m███████▍  #033[0m| 47/63 [00:21<00:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  76%|#033[34m███████▌  #033[0m| 48/63 [00:21<00:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  78%|#033[34m███████▊  #033[0m| 49/63 [00:22<00:06,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  79%|#033[34m███████▉  #033[0m| 50/63 [00:22<00:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  81%|#033[34m████████  #033[0m| 51/63 [00:23<00:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:29 - INFO - root -   Train loss: 0.53125\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  81%|#033[34m████████  #033[0m| 51/63 [00:23<00:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  83%|#033[34m████████▎ #033[0m| 52/63 [00:23<00:04,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  84%|#033[34m████████▍ #033[0m| 53/63 [00:23<00:04,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  86%|#033[34m████████▌ #033[0m| 54/63 [00:24<00:04,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  87%|#033[34m████████▋ #033[0m| 55/63 [00:24<00:03,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  89%|#033[34m████████▉ #033[0m| 56/63 [00:25<00:03,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:31,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:31,979] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=71.69443067208793, CurrSamplesPerSec=70.0598185888726, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  90%|#033[34m█████████ #033[0m| 57/63 [00:25<00:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  92%|#033[34m█████████▏#033[0m| 58/63 [00:26<00:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  94%|#033[34m█████████▎#033[0m| 59/63 [00:26<00:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  95%|#033[34m█████████▌#033[0m| 60/63 [00:27<00:01,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  97%|#033[34m█████████▋#033[0m| 61/63 [00:27<00:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:33 - INFO - root -   Train loss: 0.43359375\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  97%|#033[34m█████████▋#033[0m| 61/63 [00:27<00:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2:  98%|#033[34m█████████▊#033[0m| 62/63 [00:28<00:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2: 100%|#033[34m██████████#033[0m| 63/63 [00:28<00:00,  2.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 2: 100%|#033[34m██████████#033[0m| 63/63 [00:28<00:00,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:37 - INFO - root -   Eval. loss: 0.54296875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:37 - INFO - root -   {'accuracy': 0.752,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: 'f1': 0.7367303609341824,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: 'precision': 0.7568157033805889,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: 'recall': 0.717683557394002}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:37 - INFO - root -   ==== Epoch 3 start ====\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:   0%|#033[34m          #033[0m| 0/63 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:   2%|#033[34m▏         #033[0m| 1/63 [00:00<00:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:37 - INFO - root -   Train loss: 0.40234375\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:   2%|#033[34m▏         #033[0m| 1/63 [00:00<00:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:   3%|#033[34m▎         #033[0m| 2/63 [00:00<00:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:   5%|#033[34m▍         #033[0m| 3/63 [00:01<00:27,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:39,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:39,117] [INFO] [timer.py:260:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=71.67777445874982, CurrSamplesPerSec=70.54077869466342, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:   6%|#033[34m▋         #033[0m| 4/63 [00:01<00:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:   8%|#033[34m▊         #033[0m| 5/63 [00:02<00:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  10%|#033[34m▉         #033[0m| 6/63 [00:02<00:25,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  11%|#033[34m█         #033[0m| 7/63 [00:03<00:25,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  13%|#033[34m█▎        #033[0m| 8/63 [00:03<00:24,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  14%|#033[34m█▍        #033[0m| 9/63 [00:04<00:24,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  16%|#033[34m█▌        #033[0m| 10/63 [00:04<00:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  17%|#033[34m█▋        #033[0m| 11/63 [00:05<00:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:42 - INFO - root -   Train loss: 0.275390625\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  17%|#033[34m█▋        #033[0m| 11/63 [00:05<00:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  19%|#033[34m█▉        #033[0m| 12/63 [00:05<00:23,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  21%|#033[34m██        #033[0m| 13/63 [00:05<00:22,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:43,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:43,644] [INFO] [timer.py:260:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=71.68161026601926, CurrSamplesPerSec=72.40144589569852, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  22%|#033[34m██▏       #033[0m| 14/63 [00:06<00:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  24%|#033[34m██▍       #033[0m| 15/63 [00:06<00:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  25%|#033[34m██▌       #033[0m| 16/63 [00:07<00:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  27%|#033[34m██▋       #033[0m| 17/63 [00:07<00:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  29%|#033[34m██▊       #033[0m| 18/63 [00:08<00:20,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  30%|#033[34m███       #033[0m| 19/63 [00:08<00:19,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  32%|#033[34m███▏      #033[0m| 20/63 [00:09<00:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  33%|#033[34m███▎      #033[0m| 21/63 [00:09<00:18,  2.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:46 - INFO - root -   Train loss: 0.236328125\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  33%|#033[34m███▎      #033[0m| 21/63 [00:09<00:18,  2.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  35%|#033[34m███▍      #033[0m| 22/63 [00:09<00:18,  2.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  37%|#033[34m███▋      #033[0m| 23/63 [00:10<00:17,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:48,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:48,135] [INFO] [timer.py:260:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=71.72514108956989, CurrSamplesPerSec=70.48654394245024, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  38%|#033[34m███▊      #033[0m| 24/63 [00:10<00:17,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  40%|#033[34m███▉      #033[0m| 25/63 [00:11<00:17,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  41%|#033[34m████▏     #033[0m| 26/63 [00:11<00:16,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  43%|#033[34m████▎     #033[0m| 27/63 [00:12<00:16,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  44%|#033[34m████▍     #033[0m| 28/63 [00:12<00:16,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  46%|#033[34m████▌     #033[0m| 29/63 [00:13<00:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  48%|#033[34m████▊     #033[0m| 30/63 [00:13<00:15,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  49%|#033[34m████▉     #033[0m| 31/63 [00:14<00:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:51 - INFO - root -   Train loss: 0.3203125\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  49%|#033[34m████▉     #033[0m| 31/63 [00:14<00:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  51%|#033[34m█████     #033[0m| 32/63 [00:14<00:14,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  52%|#033[34m█████▏    #033[0m| 33/63 [00:14<00:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:52,686] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:52,687] [INFO] [timer.py:260:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=71.70257113301705, CurrSamplesPerSec=71.8290758017758, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  54%|#033[34m█████▍    #033[0m| 34/63 [00:15<00:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  56%|#033[34m█████▌    #033[0m| 35/63 [00:15<00:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  57%|#033[34m█████▋    #033[0m| 36/63 [00:16<00:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  59%|#033[34m█████▊    #033[0m| 37/63 [00:16<00:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  60%|#033[34m██████    #033[0m| 38/63 [00:17<00:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  62%|#033[34m██████▏   #033[0m| 39/63 [00:17<00:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  63%|#033[34m██████▎   #033[0m| 40/63 [00:18<00:10,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  65%|#033[34m██████▌   #033[0m| 41/63 [00:18<00:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:49:55 - INFO - root -   Train loss: 0.3046875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  65%|#033[34m██████▌   #033[0m| 41/63 [00:18<00:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  67%|#033[34m██████▋   #033[0m| 42/63 [00:18<00:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  68%|#033[34m██████▊   #033[0m| 43/63 [00:19<00:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:57,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:49:57,185] [INFO] [timer.py:260:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=71.73192828210185, CurrSamplesPerSec=71.58675894556285, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  70%|#033[34m██████▉   #033[0m| 44/63 [00:19<00:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  71%|#033[34m███████▏  #033[0m| 45/63 [00:20<00:08,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  73%|#033[34m███████▎  #033[0m| 46/63 [00:20<00:07,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  75%|#033[34m███████▍  #033[0m| 47/63 [00:21<00:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  76%|#033[34m███████▌  #033[0m| 48/63 [00:21<00:06,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  78%|#033[34m███████▊  #033[0m| 49/63 [00:22<00:06,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  79%|#033[34m███████▉  #033[0m| 50/63 [00:22<00:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  81%|#033[34m████████  #033[0m| 51/63 [00:23<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015                                                                 #015[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  81%|#033[34m████████  #033[0m| 51/63 [00:23<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:50:00 - INFO - root -   Train loss: 0.6328125\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  83%|#033[34m████████▎ #033[0m| 52/63 [00:23<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  84%|#033[34m████████▍ #033[0m| 53/63 [00:24<00:04,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:50:01,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-06-10 03:50:01,766] [INFO] [timer.py:260:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=71.68667799656433, CurrSamplesPerSec=69.39207132273185, MemAllocated=0.5GB, MaxMemAllocated=3.98GB\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  86%|#033[34m████████▌ #033[0m| 54/63 [00:24<00:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  87%|#033[34m████████▋ #033[0m| 55/63 [00:24<00:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  89%|#033[34m████████▉ #033[0m| 56/63 [00:25<00:03,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  90%|#033[34m█████████ #033[0m| 57/63 [00:25<00:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  92%|#033[34m█████████▏#033[0m| 58/63 [00:26<00:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  94%|#033[34m█████████▎#033[0m| 59/63 [00:26<00:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  95%|#033[34m█████████▌#033[0m| 60/63 [00:27<00:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  97%|#033[34m█████████▋#033[0m| 61/63 [00:27<00:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:50:04 - INFO - root -   Train loss: 0.435546875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  97%|#033[34m█████████▋#033[0m| 61/63 [00:27<00:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3:  98%|#033[34m█████████▊#033[0m| 62/63 [00:28<00:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3: 100%|#033[34m██████████#033[0m| 63/63 [00:28<00:00,  2.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Training epoch 3: 100%|#033[34m██████████#033[0m| 63/63 [00:28<00:00,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:50:08 - INFO - root -   Eval. loss: 0.51953125\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:50:08 - INFO - root -   {'accuracy': 0.7735,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: 'f1': 0.7586574320724561,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: 'precision': 0.7824175824175824,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: 'recall': 0.7362978283350569}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:50:08 - INFO - root -   ==== Save Model ====\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/10/2024 03:50:09 - INFO - root -   Elapsed time: 0:02:19.327803\u001b[0m\n",
      "\u001b[34m2024-06-10 03:50:11,144 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-10 03:50:11,144 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-10 03:50:11,144 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[34m2024-06-10 03:50:41,175 sagemaker-training-toolkit INFO     Finished writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[34m2024-06-10 03:50:41,175 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-06-10 03:51:04 Uploading - Uploading generated training model\n",
      "2024-06-10 03:51:04 Completed - Training job completed\n",
      "Training seconds: 464\n",
      "Billable seconds: 464\n"
     ]
    }
   ],
   "source": [
    "sm_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2264b3ea-4f2f-4183-a8cf-270f211606ff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. (Optional) Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e32290-3431-4c99-b1f5-41f157e2e2b0",
   "metadata": {},
   "source": [
    "### Copy S3 model artifact to local directory\n",
    "- S3에 저장된 모델 아티팩트를 로컬 경로로 복사하여 압축을 해제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "239d818b-30e2-4883-a7a4-08e1430a0dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "download: s3://sagemaker-us-east-1-057716757052/deepspeed-nsmc-2024-06-10-03-41-59-2024-06-10-03-42-01-988/output/model.tar.gz to model_from_sagemaker/model.tar.gz\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "local_model_dir = 'model_from_sagemaker'\n",
    "\n",
    "if not os.path.exists(local_model_dir):\n",
    "    os.makedirs(local_model_dir)\n",
    "\n",
    "!aws s3 cp {sm_estimator.model_data} {local_model_dir}/model.tar.gz\n",
    "!tar -xzf {local_model_dir}/model.tar.gz -C {local_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e765d7",
   "metadata": {},
   "source": [
    "### Base 모델 및 토큰나이저 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67024925-3bb5-4baa-95fb-8cce14411a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/deepspeed-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "        \n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fff953b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aba09c-c3af-4480-a338-0ba5f2738c91",
   "metadata": {},
   "source": [
    "### Load DDP model to a non-DDP model\n",
    "- 데이터 병렬화를 적용하여 모델을 훈련하면 모델의 weight의 key값에 `module`이 붙게 되어 모델 로딩 시 오류가 발생합니다. 따라서, 이를 제거해 주는 후처리 과정이 필요합니다. - 후처리가 번거롭다면, DDP로 훈련 후 저장할 때 명시적으로 `module`를 제외하고 저장하는 방법도 있습니다.\n",
    "    - 참조: https://discuss.pytorch.org/t/how-to-switch-model-trained-on-2-gpus-to-1-gpu/20039\n",
    "\n",
    "    ```python\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    ...\n",
    "    model_to_save.state_dict()\n",
    "    torch.save({'model': model_to_save.state_dict())\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c373035",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_filename:  model_from_sagemaker/model.pt\n",
      "module.bert.embeddings.position_ids\n",
      "module.bert.embeddings.word_embeddings.weight\n",
      "module.bert.embeddings.position_embeddings.weight\n",
      "module.bert.embeddings.token_type_embeddings.weight\n",
      "module.bert.embeddings.LayerNorm.weight\n",
      "module.bert.embeddings.LayerNorm.bias\n",
      "module.bert.encoder.layer.0.attention.self.query.weight\n",
      "module.bert.encoder.layer.0.attention.self.query.bias\n",
      "module.bert.encoder.layer.0.attention.self.key.weight\n",
      "module.bert.encoder.layer.0.attention.self.key.bias\n",
      "module.bert.encoder.layer.0.attention.self.value.weight\n",
      "module.bert.encoder.layer.0.attention.self.value.bias\n",
      "module.bert.encoder.layer.0.attention.output.dense.weight\n",
      "module.bert.encoder.layer.0.attention.output.dense.bias\n",
      "module.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "module.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "module.bert.encoder.layer.0.intermediate.dense.weight\n",
      "module.bert.encoder.layer.0.intermediate.dense.bias\n",
      "module.bert.encoder.layer.0.output.dense.weight\n",
      "module.bert.encoder.layer.0.output.dense.bias\n",
      "module.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "module.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "module.bert.encoder.layer.1.attention.self.query.weight\n",
      "module.bert.encoder.layer.1.attention.self.query.bias\n",
      "module.bert.encoder.layer.1.attention.self.key.weight\n",
      "module.bert.encoder.layer.1.attention.self.key.bias\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "model_filename = glob.glob(f'{local_model_dir}/*.pt')[0]\n",
    "print(\"model_filename: \", model_filename)\n",
    "state_dict = torch.load(model_filename)\n",
    "for idx, key in enumerate(state_dict):\n",
    "    print(key)\n",
    "    if idx == 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506bdfb",
   "metadata": {},
   "source": [
    "### 훈련된 모델 파라미터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c97f735f-6fd8-4bde-bd96-d13060902d70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict = {}\n",
    "for key in state_dict:\n",
    "    new_key = key.replace('module.','')\n",
    "    new_state_dict[new_key] = state_dict[key]\n",
    "\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae5016",
   "metadata": {},
   "source": [
    "### 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4de79bf8-2ee3-449d-bc28-f935deab8aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_model(model, tokenizer, text):\n",
    "    encode_plus_token = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    output = model(**encode_plus_token)\n",
    "    softmax_fn = torch.nn.Softmax(dim=1)\n",
    "    softmax_output = softmax_fn(output[0])\n",
    "    _, prediction = torch.max(softmax_output, dim=1)\n",
    "\n",
    "    predicted_class_idx = prediction.item()\n",
    "    score = softmax_output[0][predicted_class_idx]\n",
    "    print(f\"predicted_class: {predicted_class_idx}, score={score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c8c312d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_class: 1, score=0.9877290725708008\n"
     ]
    }
   ],
   "source": [
    "text = \"이 영화 너무 재미있어요\"\n",
    "inference_model(model, tokenizer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "769b7c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_class: 1, score=0.9625192284584045\n"
     ]
    }
   ],
   "source": [
    "text = \"'기생충'은 많은 사람들로부터 호평을 받았고, 매우 사실적으로 묘사 했습니다.\"\n",
    "inference_model(model, tokenizer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1179fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_deepspeed-py310",
   "language": "python",
   "name": "conda_deepspeed-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
