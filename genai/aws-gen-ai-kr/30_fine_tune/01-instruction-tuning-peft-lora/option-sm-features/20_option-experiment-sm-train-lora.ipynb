{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korean LLM (Large Language Model) fine-tuning on SageMaker\n",
    "---\n",
    "\n",
    "- 허깅페이스 인증 정보 설정: `huggingface-cli login`\n",
    "    - https://huggingface.co/join\n",
    "    - https://huggingface.co/settings/tokens\n",
    "    \n",
    "\n",
    "## Overview \n",
    "바로 이전 모듈까지는 기존에 온프레미스에서 개발했던 환경과 동일한 환경으로 모델을 빌드하고 훈련했습니다. 하지만 아래와 같은 상황들에서도 기존 환경을 사용하는 것이 바람직할까요?\n",
    "\n",
    "- 온프레미스의 GPU가 총 1장으로 훈련 시간이 너무 오래 소요됨\n",
    "- 가용 서버 대수가 2대인데 10개의 딥러닝 모델을 동시에 훈련해야 함\n",
    "- 필요한 상황에만 GPU를 활용\n",
    "\n",
    "Amazon SageMaker는 데이터 과학자들 및 머신 러닝 엔지니어들을 위한 완전 관리형 머신 러닝 서비스로 훈련 및 추론 수행 시 인프라 설정에 대한 추가 작업이 필요하지 있기에, 단일 GPU 기반의 딥러닝 훈련을 포함한 멀티 GPU 및 멀티 인스턴스 분산 훈련을 보다 쉽고 빠르게 수행할 수 있습니다. SageMaker는 다양한 유즈케이스들에 적합한 예제들을 지속적으로 업데이트하고 있으며, 한국어 세션 및 자료들도 제공되고 있습니다.\n",
    "\n",
    "### Note\n",
    "- 이미 기본적인 Hugging Face 용법 및 자연어 처리에 익숙하신 분들은 앞 모듈을 생략하고 이 모듈부터 핸즈온을 시작하셔도 됩니다.\n",
    "- 이 노트북은 SageMaker 기본 API를 참조하므로, SageMaker Studio, SageMaker 노트북 인스턴스 또는 AWS CLI가 설정된 로컬 시스템에서 실행해야 합니다. SageMaker Studio 또는 SageMaker 노트북 인스턴스를 사용하는 경우 PyTorch 기반 커널을 선택하세요.\n",
    "- 훈련(Training) job 수행 시 최소 `ml.g5.2xlarge` 훈련 인스턴스를 권장하며, 분산 훈련 수행 시에는 `ml.g5.12xlarge` 훈련 인스턴스를 권장합니다. 만약 인스턴스 사용에 제한이 걸려 있다면 [Request a service quota increase for SageMaker resources](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.html#service-limit-increase-request-procedure)를 참조하여 인스턴스 제한을 해제해 주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r bucket_prefix dataset_prefix_50_samples s3_data_path s3_data_path_50_samples s3_data_path_10000_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ERROR] 1번 모듈 노트북을 다시 실행해 주세요.\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bucket_prefix\n",
    "    dataset_prefix\n",
    "    s3_data_path\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] 1번 모듈 노트북을 다시 실행해 주세요.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Download LLM from Hugging Face hub\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p /home/ec2-user/SageMaker/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\"]\n",
    "\n",
    "# create model dir\n",
    "model_name = HF_MODEL_ID.split(\"/\")[-1].replace('.', '-')\n",
    "model_tar_dir = Path(f\"/home/ec2-user/SageMaker/models/{model_name}\")\n",
    "if not os.path.isdir(model_tar_dir):\n",
    "    os.makedirs(model_tar_dir, exist_ok=True)\n",
    "    # Download model from Hugging Face into model_dir\n",
    "    snapshot_download(\n",
    "        HF_MODEL_ID, \n",
    "        local_dir=str(model_tar_dir), \n",
    "        local_dir_use_symlinks=False,\n",
    "        allow_patterns=allow_patterns,\n",
    "        cache_dir=\"/home/ec2-user/SageMaker/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Save LLM to S3\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role arn: arn:aws:iam::057716757052:role/dt2gsmoon\n",
      "SageMaker bucket: sagemaker-us-east-1-057716757052\n",
      "SageMaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "bucket = None\n",
    "if bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "print(f\"SageMaker role arn: {role}\")\n",
    "print(f\"SageMaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"SageMaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_pretrained_model_path = f\"s3://{bucket}/{bucket_prefix}/huggingface-models/{model_name}/\"\n",
    "s3_chkpt_path= f\"s3://{bucket}/{bucket_prefix}/huggingface-models/{model_name}/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws configure set default.s3.max_concurrent_requests 100\n",
    "aws configure set default.s3.max_queue_size 10000\n",
    "aws configure set default.s3.multipart_threshold 1GB\n",
    "aws configure set default.s3.multipart_chunksize 64MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync {model_tar_dir} {s3_pretrained_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. SageMaker Training\n",
    "---\n",
    "\n",
    "SageMaker에 대한 대표적인 오해가 여전히 많은 분들이 SageMaker 훈련을 위해 소스 코드를 전면적으로 수정해야 한다고 생각합니다. 하지만, 실제로는 별도의 소스 코드 수정 없이 기존 여러분이 사용했던 파이썬 스크립트에 SageMaker 훈련에 필요한 SageMaker 전용 환경 변수들만 추가하면 됩니다.\n",
    "\n",
    "SageMaker 훈련은 훈련 작업을 호출할 때, 1) 훈련 EC2 인스턴스 프로비저닝 - 2) 컨테이너 구동을 위한 도커 이미지 및 훈련 데이터 다운로드 - 3) 컨테이너 구동 - 4) 컨테이너 환경에서 훈련 수행 - 5) 컨테이너 환경에서 S3의 특정 버킷에 저장 - 6) 훈련 인스턴스 종료로 구성됩니다. 따라서, 훈련 수행 로직은 아래 예시와 같이 기존 개발 환경과 동일합니다.\n",
    "\n",
    "`/opt/conda/bin/python train_hf.py --num_epochs 5 --train_batch_size 32 ...`\n",
    "\n",
    "이 과정에서 컨테이너 환경에 필요한 환경 변수(예: 모델 경로, 훈련 데이터 경로) 들은 사전에 지정되어 있으며, 이 환경 변수들이 설정되어 있어야 훈련에 필요한 파일들의 경로를 인식할 수 있습니다. 대표적인 환경 변수들에 대한 자세한 내용은 https://github.com/aws/sagemaker-containers#important-environment-variables 을 참조하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup SageMaker Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_WANDB = False\n",
    "LOCAL_MODE = False\n",
    "# LOCAL_MODE = True\n",
    "\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_WANDB:\n",
    "    wandb.sagemaker_auth(path=\"src\")\n",
    "    entry_point = \"run-wandb.sh\"\n",
    "else:\n",
    "    entry_point = \"run.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml.g5.48xlarge\n"
     ]
    }
   ],
   "source": [
    "instance_type = 'local_gpu' if LOCAL_MODE else 'ml.g5.48xlarge'\n",
    "print(instance_type)\n",
    "\n",
    "if instance_type in ['local', 'local_gpu']:\n",
    "    from sagemaker.local import LocalSession\n",
    "    sm_session = LocalSession()\n",
    "    sm_session.config = {'local': {'local_code': True}}\n",
    "else:\n",
    "    sm_session = sagemaker.session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### SageMaker Training\n",
    "- Base Container image link : https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "- CloudWatch 에 사용자 정의 Metrics 를 추가 참조 (예: Training Loss)\n",
    "    - [Monitor and Analyze Training Jobs Using Amazon CloudWatch Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Define Training Job Name \n",
    "job_name = name_from_base(f\"{model_name}-lora-peft\")\n",
    "\n",
    "# See https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.1.0-gpu-py310-cu121-ubuntu20.04-sagemaker'\n",
    "hparams = {}\n",
    "\n",
    "# max_run = 6*60*60 # 6 hours\n",
    "max_run = 72*60*60 # 3 days\n",
    "\n",
    "use_spot_instances = False\n",
    "if use_spot_instances:\n",
    "    max_wait = 12*60*60 # 12 hours: spot instance waiting + max runtime\n",
    "else:\n",
    "    max_wait = None\n",
    "\n",
    "# Emit: {'loss': 1.0877, 'learning_rate': 6.860813704496788e-05, 'epoch': 0.78} captured by CloudWatch\n",
    "\n",
    "metric_definitions=[\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"'loss':(.*?),\"}\n",
    "]\n",
    "\n",
    "# Create the Estimator\n",
    "estimator = PyTorch(\n",
    "    image_uri=image_uri,\n",
    "    entry_point=entry_point,        # train script\n",
    "    source_dir='../src',               # directory which includes all the files needed for training\n",
    "    instance_type=instance_type,    # instances type used for the training job\n",
    "    instance_count=1,               # the number of instances used for training\n",
    "    base_job_name=job_name,         # the name of the training job\n",
    "    role=role,                      # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    sagemaker_session=sm_session,   # sagemaker session\n",
    "    volume_size=300,                # the size of the EBS volume in GB\n",
    "    hyperparameters=hparams,\n",
    "    debugger_hook_config=False,\n",
    "    disable_profile=True,\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait if use_spot_instances else None,\n",
    "    checkpoint_s3_uri=s3_chkpt_path if instance_type not in ['local', 'local_gpu'] else None,\n",
    "    checkpoint_local_path='/opt/ml/checkpoints' if instance_type not in ['local', 'local_gpu'] else None,\n",
    "    metric_definitions = metric_definitions\n",
    "    #environment={\"TRANSFORMERS_OFFLINE\": \"1\", \"HF_DATASETS_OFFLINE\":\"1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training job\n",
    "S3에서 훈련 인스턴스로 복사될 데이터를 지정한 후 SageMaker 훈련 job을 시작합니다. 모델 크기, 데이터 세트 크기에 따라서 몇십 분에서 몇 시간까지 소요될 수 있습니다.\n",
    "- [SageMaker Experiment Doc](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-experiments/sagemaker_job_tracking/pytorch_script_mode_training_job.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_name:kullm-polyglot-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.experiments.run:The run (training-job-experiment) under experiment (kullm-polyglot-12) already exists. Loading it.\n",
      "INFO:sagemaker:Creating training-job with name: kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-11-16-41-377\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> [Fine-tuning] Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-11-16-41-377\">Training Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b> [Fine-tuning] Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-11-16-41-377;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if LOCAL_MODE:\n",
    "    estimator.fit(\n",
    "        {\n",
    "            \"pretrained\": f'file://../../../../../../SageMaker/models/{model_name}', # VS Code on SageMaker Notebook Instance\n",
    "            # \"pretrained\": f'file://../../models/{model_name}', # Jupyter Notebook on SageMaker Notebook Instance\n",
    "            \"training\": f'file://../{dataset_prefix}'\n",
    "        },\n",
    "        wait=False\n",
    "    )\n",
    "else:\n",
    "    from sagemaker.experiments.run import Run\n",
    "    from sagemaker.utils import unique_name_from_base\n",
    "    from sagemaker.session import Session\n",
    "\n",
    "    # set new experiment configuration\n",
    "    experiment_name = f\"{HF_MODEL_ID.split('/')[1].split('.')[0]}\"\n",
    "    # experiment_name = unique_name_from_base(experiment_name)\n",
    "    \n",
    "    run_name = f\"training-job-experiment\"\n",
    "    print(f\"experiment_name:{experiment_name}\")    \n",
    "\n",
    "    with Run(experiment_name=experiment_name, run_name=run_name, sagemaker_session=Session()) as run:\n",
    "        fast_file = lambda x: TrainingInput(x, input_mode=\"FastFile\")\n",
    "        estimator.fit(\n",
    "            {\n",
    "                \"pretrained\": fast_file(s3_pretrained_model_path),\n",
    "                #\"training\": fast_file(s3_data_path_50_samples),\n",
    "                # \"training\": fast_file(s3_data_path_10000_samples),                                \n",
    "                \"training\": fast_file(s3_data_path),\n",
    "            },\n",
    "            wait=False\n",
    "        )\n",
    "\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    def make_console_link(region, train_job_name, train_task='[Training]'):\n",
    "        train_job_link = f'<b> {train_task} Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/jobs/{train_job_name}\">Training Job</a></b>'   \n",
    "        cloudwatch_link = f'<b> {train_task} Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={region}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={train_job_name};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>'\n",
    "        return train_job_link, cloudwatch_link  \n",
    "\n",
    "    train_job_name = estimator.latest_training_job.job_name\n",
    "    train_job_link, cloudwatch_link = make_console_link(region, train_job_name, '[Fine-tuning]')\n",
    "\n",
    "    display(HTML(train_job_link))\n",
    "    display(HTML(cloudwatch_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### View Logs\n",
    "훈련 로그는 CloudWatch Logs를 통해서 확인할 수 있습니다. 만약 다른 코드 셀을 실행하고 싶다면 이 코드 셀의 실행을 중단하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-07 10:07:08 Starting - Starting the training job\n",
      "2024-04-07 10:07:08 Pending - Training job waiting for capacity......\n",
      "2024-04-07 10:08:02 Pending - Preparing the instances for training......\n",
      "2024-04-07 10:08:57 Downloading - Downloading input data...\n",
      "2024-04-07 10:09:33 Downloading - Downloading the training image..................\n",
      "2024-04-07 10:12:30 Training - Training image download completed. Training in progress.bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2024-04-07 10:12:31,376 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-04-07 10:12:31,430 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-07 10:12:31,440 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-04-07 10:12:31,441 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-04-07 10:12:32,937 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-07 10:12:34,205 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-07 10:12:34,279 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-07 10:12:34,290 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"pretrained\": \"/opt/ml/input/data/pretrained\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"pretrained\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-10-07-08-390\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-10-07-08-390/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run.sh\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={}\n",
      "SM_USER_ENTRY_POINT=run.sh\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"pretrained\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"pretrained\",\"training\"]\n",
      "SM_CURRENT_HOST=algo-2\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run.sh\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=48\n",
      "SM_NUM_GPUS=4\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-10-07-08-390/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"pretrained\":\"/opt/ml/input/data/pretrained\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"pretrained\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-10-07-08-390\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-10-07-08-390/source/sourcedir.tar.gz\",\"module_name\":\"run.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run.sh\"}\n",
      "SM_USER_ARGS=[]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_PRETRAINED=/opt/ml/input/data/pretrained\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "/bin/sh -c \"./run.sh \"\n",
      "2024-04-07 10:12:34,290 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "2024-04-07 10:12:34,290 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Collecting peft==0.7.1 (from -r requirements.txt (line 1))\n",
      "Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting accelerate==0.25.0 (from -r requirements.txt (line 2))\n",
      "Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers==4.36.2 (from -r requirements.txt (line 3))\n",
      "Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.8/126.8 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting bitsandbytes==0.41.3 (from -r requirements.txt (line 4))\n",
      "Downloading bitsandbytes-0.41.3-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting datasets==2.16.0 (from -r requirements.txt (line 5))\n",
      "Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting wandb (from -r requirements.txt (line 6))\n",
      "Downloading wandb-0.16.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (4.66.1)\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2024-04-07 10:12:31,412 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-04-07 10:12:31,464 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-07 10:12:31,474 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-04-07 10:12:31,476 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-04-07 10:12:32,969 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-07 10:12:34,190 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-07 10:12:34,264 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-07 10:12:34,275 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"pretrained\": \"/opt/ml/input/data/pretrained\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"pretrained\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-10-07-08-390\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-10-07-08-390/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run.sh\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={}\n",
      "SM_USER_ENTRY_POINT=run.sh\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"pretrained\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"pretrained\",\"training\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run.sh\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=48\n",
      "SM_NUM_GPUS=4\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-10-07-08-390/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"pretrained\":\"/opt/ml/input/data/pretrained\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"pretrained\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-10-07-08-390\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/kullm-polyglot-12-8b-v2-lora-peft-2024--2024-04-07-10-07-08-390/source/sourcedir.tar.gz\",\"module_name\":\"run.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run.sh\"}\n",
      "SM_USER_ARGS=[]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_PRETRAINED=/opt/ml/input/data/pretrained\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "/bin/sh -c \"./run.sh \"\n",
      "2024-04-07 10:12:34,275 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "2024-04-07 10:12:34,275 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Collecting peft==0.7.1 (from -r requirements.txt (line 1))\n",
      "Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting accelerate==0.25.0 (from -r requirements.txt (line 2))\n",
      "Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers==4.36.2 (from -r requirements.txt (line 3))\n",
      "Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.8/126.8 kB 12.0 MB/s eta 0:00:00\n",
      "Collecting bitsandbytes==0.41.3 (from -r requirements.txt (line 4))\n",
      "Downloading bitsandbytes-0.41.3-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting datasets==2.16.0 (from -r requirements.txt (line 5))\n",
      "Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting wandb (from -r requirements.txt (line 6))\n",
      "Downloading wandb-0.16.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1->-r requirements.txt (line 1)) (4.66.1)\n",
      "Collecting safetensors (from peft==0.7.1->-r requirements.txt (line 1))\n",
      "Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting huggingface-hub>=0.17.0 (from peft==0.7.1->-r requirements.txt (line 1))\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2->-r requirements.txt (line 3)) (3.13.1)\n",
      "Collecting safetensors (from peft==0.7.1->-r requirements.txt (line 1))\n",
      "Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting huggingface-hub>=0.17.0 (from peft==0.7.1->-r requirements.txt (line 1))\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2->-r requirements.txt (line 3)) (3.13.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.36.2->-r requirements.txt (line 3))\n",
      "Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2->-r requirements.txt (line 3)) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.2->-r requirements.txt (line 3))\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r requirements.txt (line 5)) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r requirements.txt (line 5)) (2.2.1)\n",
      "Collecting xxhash (from datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r requirements.txt (line 5)) (0.70.16)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.36.2->-r requirements.txt (line 3))\n",
      "Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 7.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2->-r requirements.txt (line 3)) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.2->-r requirements.txt (line 3))\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r requirements.txt (line 5)) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r requirements.txt (line 5)) (2.2.1)\n",
      "Collecting xxhash (from datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r requirements.txt (line 5)) (0.70.16)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 6)) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 6))\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 6))\n",
      "Downloading sentry_sdk-1.44.1-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 6))\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting setproctitle (from wandb->-r requirements.txt (line 6))\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 6)) (68.1.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb->-r requirements.txt (line 6))\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 6)) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 6)) (1.16.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5)) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 6)) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 6))\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 6))\n",
      "Downloading sentry_sdk-1.44.1-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 6))\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting setproctitle (from wandb->-r requirements.txt (line 6))\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 6)) (68.1.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb->-r requirements.txt (line 6))\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 6)) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 6)) (1.16.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5)) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6))\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6))\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.1->-r requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2->-r requirements.txt (line 3)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2->-r requirements.txt (line 3)) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r requirements.txt (line 5)) (2024.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6))\n",
      "Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.7.1->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.7.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.3/168.3 kB 25.3 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 265.7/265.7 kB 39.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.2/8.2 MB 105.4 MB/s eta 0:00:00\n",
      "Downloading bitsandbytes-0.41.3-py3-none-any.whl (92.6 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.1->-r requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2->-r requirements.txt (line 3)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2->-r requirements.txt (line 3)) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.16.0->-r requirements.txt (line 5))\n",
      "Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r requirements.txt (line 5)) (2024.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6))\n",
      "Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.7.1->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.7.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.3/168.3 kB 27.3 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 265.7/265.7 kB 30.5 MB/s eta 0:00:00\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.2/8.2 MB 115.6 MB/s eta 0:00:00\n",
      "Downloading bitsandbytes-0.41.3-py3-none-any.whl (92.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 25.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 507.1/507.1 kB 45.5 MB/s eta 0:00:00\n",
      "Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 97.3 MB/s eta 0:00:00\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 14.6 MB/s eta 0:00:00\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 26.7 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 88.0 MB/s eta 0:00:00\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 28.6 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 14.6 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 774.0/774.0 kB 69.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 77.6 MB/s eta 0:00:00\n",
      "Downloading sentry_sdk-1.44.1-py2.py3-none-any.whl (266 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.1/266.1 kB 33.5 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 102.3 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 17.4 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 25.9 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 30.4 MB/s eta 0:00:00\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 11.1 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 20.5 MB/s eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 28.8 MB/s eta 0:00:00\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 28.4 MB/s eta 0:00:00\n",
      "Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 507.1/507.1 kB 50.4 MB/s eta 0:00:00\n",
      "Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 92.1 MB/s eta 0:00:00\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 17.8 MB/s eta 0:00:00\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 28.5 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 82.6 MB/s eta 0:00:00\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 36.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 11.8 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 774.0/774.0 kB 76.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 89.6 MB/s eta 0:00:00\n",
      "Downloading sentry_sdk-1.44.1-py2.py3-none-any.whl (266 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.1/266.1 kB 34.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 47.9 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 24.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 21.6 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 37.1 MB/s eta 0:00:00\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 10.3 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 15.7 MB/s eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 34.5 MB/s eta 0:00:00\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: bitsandbytes, appdirs, xxhash, smmap, setproctitle, sentry-sdk, safetensors, regex, pyarrow-hotfix, multidict, fsspec, frozenlist, docker-pycreds, dill, async-timeout, yarl, multiprocess, huggingface-hub, gitdb, aiosignal, tokenizers, GitPython, aiohttp, accelerate, wandb, transformers, peft, datasets\n",
      "Installing collected packages: bitsandbytes, appdirs, xxhash, smmap, setproctitle, sentry-sdk, safetensors, regex, pyarrow-hotfix, multidict, fsspec, frozenlist, docker-pycreds, dill, async-timeout, yarl, multiprocess, huggingface-hub, gitdb, aiosignal, tokenizers, GitPython, aiohttp, accelerate, wandb, transformers, peft, datasets\n",
      "Attempting uninstall: fsspec\n",
      "Found existing installation: fsspec 2024.2.0\n",
      "Uninstalling fsspec-2024.2.0:\n",
      "Successfully uninstalled fsspec-2024.2.0\n",
      "Attempting uninstall: fsspec\n",
      "Found existing installation: fsspec 2024.2.0\n",
      "Uninstalling fsspec-2024.2.0:\n",
      "Successfully uninstalled fsspec-2024.2.0\n",
      "Attempting uninstall: dill\n",
      "Found existing installation: dill 0.3.8\n",
      "Uninstalling dill-0.3.8:\n",
      "Successfully uninstalled dill-0.3.8\n",
      "Attempting uninstall: dill\n",
      "Found existing installation: dill 0.3.8\n",
      "Uninstalling dill-0.3.8:\n",
      "Successfully uninstalled dill-0.3.8\n",
      "Attempting uninstall: multiprocess\n",
      "Found existing installation: multiprocess 0.70.16\n",
      "Uninstalling multiprocess-0.70.16:\n",
      "Successfully uninstalled multiprocess-0.70.16\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.22.0\n",
      "Uninstalling accelerate-0.22.0:\n",
      "Successfully uninstalled accelerate-0.22.0\n",
      "Attempting uninstall: multiprocess\n",
      "Found existing installation: multiprocess 0.70.16\n",
      "Uninstalling multiprocess-0.70.16:\n",
      "Successfully uninstalled multiprocess-0.70.16\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.22.0\n",
      "Uninstalling accelerate-0.22.0:\n",
      "Successfully uninstalled accelerate-0.22.0\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\n",
      "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\n",
      "Successfully installed GitPython-3.1.43 accelerate-0.25.0 aiohttp-3.9.3 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.3 bitsandbytes-0.41.3 datasets-2.16.0 dill-0.3.7 docker-pycreds-0.4.0 frozenlist-1.4.1 fsspec-2023.10.0 gitdb-4.0.11 huggingface-hub-0.22.2 multidict-6.0.5 multiprocess-0.70.15 peft-0.7.1 pyarrow-hotfix-0.6 regex-2023.12.25 safetensors-0.4.2 sentry-sdk-1.44.1 setproctitle-1.3.3 smmap-5.0.1 tokenizers-0.15.2 transformers-4.36.2 wandb-0.16.6 xxhash-3.4.1 yarl-1.9.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "torchrun --nnodes 1 --nproc_per_node 4 train.py --base_model nlpai-lab/kullm-polyglot-12.8b-v2 --pretrained_model_path /opt/ml/input/data/pretrained/ --cache_dir /tmp/huggingface-cache --data_path /opt/ml/input/data/training/ --output_dir /opt/ml/checkpoints --save_path /opt/ml/model/ --batch_size 2 --gradient_accumulation_steps 2 --num_epochs 1 --learning_rate 3e-4 --lora_r 8 --lora_alpha 32 --lora_dropout 0.05 --lora_target_modules [query_key_value, xxx] --logging_steps 1 --save_steps 40 --eval_steps 40 --weight_decay 0. --warmup_steps 50 --warmup_ratio 0.03 --lr_scheduler_type linear\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\n",
      "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\n",
      "Successfully installed GitPython-3.1.43 accelerate-0.25.0 aiohttp-3.9.3 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.3 bitsandbytes-0.41.3 datasets-2.16.0 dill-0.3.7 docker-pycreds-0.4.0 frozenlist-1.4.1 fsspec-2023.10.0 gitdb-4.0.11 huggingface-hub-0.22.2 multidict-6.0.5 multiprocess-0.70.15 peft-0.7.1 pyarrow-hotfix-0.6 regex-2023.12.25 safetensors-0.4.2 sentry-sdk-1.44.1 setproctitle-1.3.3 smmap-5.0.1 tokenizers-0.15.2 transformers-4.36.2 wandb-0.16.6 xxhash-3.4.1 yarl-1.9.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "torchrun --nnodes 1 --nproc_per_node 4 train.py --base_model nlpai-lab/kullm-polyglot-12.8b-v2 --pretrained_model_path /opt/ml/input/data/pretrained/ --cache_dir /tmp/huggingface-cache --data_path /opt/ml/input/data/training/ --output_dir /opt/ml/checkpoints --save_path /opt/ml/model/ --batch_size 2 --gradient_accumulation_steps 2 --num_epochs 1 --learning_rate 3e-4 --lora_r 8 --lora_alpha 32 --lora_dropout 0.05 --lora_target_modules [query_key_value, xxx] --logging_steps 1 --save_steps 40 --eval_steps 40 --weight_decay 0. --warmup_steps 50 --warmup_ratio 0.03 --lr_scheduler_type linear\n",
      "[2024-04-07 10:12:52,061] torch.distributed.run: [WARNING] \n",
      "[2024-04-07 10:12:52,061] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-04-07 10:12:52,061] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-04-07 10:12:52,061] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-04-07 10:12:52,069] torch.distributed.run: [WARNING] \n",
      "[2024-04-07 10:12:52,069] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-04-07 10:12:52,069] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-04-07 10:12:52,069] torch.distributed.run: [WARNING] *****************************************\n",
      "Namespace(base_model='nlpai-lab/kullm-polyglot-12.8b-v2', cache_dir='/tmp/huggingface-cache', pretrained_model_path='/opt/ml/input/data/pretrained/', data_path='/opt/ml/input/data/training/', output_dir='/opt/ml/checkpoints', save_path='/opt/ml/model/', save_merged_model=False, batch_size=2, num_epochs=1, learning_rate=0.0003, gradient_accumulation_steps=2, lr_scheduler_type='linear', quant_8bit=False, quant_4bit=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, group_by_length=False, wandb_project='', wandb_run_name='', wandb_watch='', wandb_log_model='', save_steps=40, eval_steps=40, bf16=True)\n",
      "world_size: 4\n",
      "Activated Distributed Data Parallel.\n",
      "Namespace(base_model='nlpai-lab/kullm-polyglot-12.8b-v2', cache_dir='/tmp/huggingface-cache', pretrained_model_path='/opt/ml/input/data/pretrained/', data_path='/opt/ml/input/data/training/', output_dir='/opt/ml/checkpoints', save_path='/opt/ml/model/', save_merged_model=False, batch_size=2, num_epochs=1, learning_rate=0.0003, gradient_accumulation_steps=2, lr_scheduler_type='linear', quant_8bit=False, quant_4bit=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, group_by_length=False, wandb_project='', wandb_run_name='', wandb_watch='', wandb_log_model='', save_steps=40, eval_steps=40, bf16=True)\n",
      "world_size: 4\n",
      "Activated Distributed Data Parallel.\n",
      "Namespace(base_model='nlpai-lab/kullm-polyglot-12.8b-v2', cache_dir='/tmp/huggingface-cache', pretrained_model_path='/opt/ml/input/data/pretrained/', data_path='/opt/ml/input/data/training/', output_dir='/opt/ml/checkpoints', save_path='/opt/ml/model/', save_merged_model=False, batch_size=2, num_epochs=1, learning_rate=0.0003, gradient_accumulation_steps=2, lr_scheduler_type='linear', quant_8bit=False, quant_4bit=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, group_by_length=False, wandb_project='', wandb_run_name='', wandb_watch='', wandb_log_model='', save_steps=40, eval_steps=40, bf16=True)\n",
      "Training Alpaca-LoRA model with params:\n",
      "base_model: nlpai-lab/kullm-polyglot-12.8b-v2\n",
      "cache_dir: /tmp/huggingface-cache\n",
      "pretrained_model_path: /opt/ml/input/data/pretrained/\n",
      "data_path: /opt/ml/input/data/training/\n",
      "output_dir: /opt/ml/checkpoints\n",
      "save_path: /opt/ml/model/\n",
      "save_merged_model: False\n",
      "batch_size: 2\n",
      "num_epochs: 1\n",
      "learning_rate: 0.0003\n",
      "gradient_accumulation_steps: 2\n",
      "lr_scheduler_type: linear\n",
      "quant_8bit: False\n",
      "quant_4bit: True\n",
      "lora_r: 8\n",
      "lora_alpha: 32\n",
      "lora_dropout: 0.05\n",
      "group_by_length: False\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "save_steps: 40\n",
      "eval_steps: 40\n",
      "Namespace(base_model='nlpai-lab/kullm-polyglot-12.8b-v2', cache_dir='/tmp/huggingface-cache', pretrained_model_path='/opt/ml/input/data/pretrained/', data_path='/opt/ml/input/data/training/', output_dir='/opt/ml/checkpoints', save_path='/opt/ml/model/', save_merged_model=False, batch_size=2, num_epochs=1, learning_rate=0.0003, gradient_accumulation_steps=2, lr_scheduler_type='linear', quant_8bit=False, quant_4bit=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, group_by_length=False, wandb_project='', wandb_run_name='', wandb_watch='', wandb_log_model='', save_steps=40, eval_steps=40, bf16=True)\n",
      "world_size: 4\n",
      "Activated Distributed Data Parallel.\n",
      "world_size: 4\n",
      "Activated Distributed Data Parallel.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Namespace(base_model='nlpai-lab/kullm-polyglot-12.8b-v2', cache_dir='/tmp/huggingface-cache', pretrained_model_path='/opt/ml/input/data/pretrained/', data_path='/opt/ml/input/data/training/', output_dir='/opt/ml/checkpoints', save_path='/opt/ml/model/', save_merged_model=False, batch_size=2, num_epochs=1, learning_rate=0.0003, gradient_accumulation_steps=2, lr_scheduler_type='linear', quant_8bit=False, quant_4bit=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, group_by_length=False, wandb_project='', wandb_run_name='', wandb_watch='', wandb_log_model='', save_steps=40, eval_steps=40, bf16=True)\n",
      "Training Alpaca-LoRA model with params:\n",
      "base_model: nlpai-lab/kullm-polyglot-12.8b-v2\n",
      "cache_dir: /tmp/huggingface-cache\n",
      "pretrained_model_path: /opt/ml/input/data/pretrained/\n",
      "data_path: /opt/ml/input/data/training/\n",
      "output_dir: /opt/ml/checkpoints\n",
      "save_path: /opt/ml/model/\n",
      "save_merged_model: False\n",
      "batch_size: 2\n",
      "num_epochs: 1\n",
      "learning_rate: 0.0003\n",
      "gradient_accumulation_steps: 2\n",
      "lr_scheduler_type: linear\n",
      "quant_8bit: False\n",
      "quant_4bit: True\n",
      "lora_r: 8\n",
      "lora_alpha: 32\n",
      "lora_dropout: 0.05\n",
      "group_by_length: False\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "save_steps: 40\n",
      "eval_steps: 40\n",
      "world_size: 4\n",
      "Activated Distributed Data Parallel.\n",
      "Namespace(base_model='nlpai-lab/kullm-polyglot-12.8b-v2', cache_dir='/tmp/huggingface-cache', pretrained_model_path='/opt/ml/input/data/pretrained/', data_path='/opt/ml/input/data/training/', output_dir='/opt/ml/checkpoints', save_path='/opt/ml/model/', save_merged_model=False, batch_size=2, num_epochs=1, learning_rate=0.0003, gradient_accumulation_steps=2, lr_scheduler_type='linear', quant_8bit=False, quant_4bit=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, group_by_length=False, wandb_project='', wandb_run_name='', wandb_watch='', wandb_log_model='', save_steps=40, eval_steps=40, bf16=True)\n",
      "world_size: 4\n",
      "Activated Distributed Data Parallel.\n",
      "Namespace(base_model='nlpai-lab/kullm-polyglot-12.8b-v2', cache_dir='/tmp/huggingface-cache', pretrained_model_path='/opt/ml/input/data/pretrained/', data_path='/opt/ml/input/data/training/', output_dir='/opt/ml/checkpoints', save_path='/opt/ml/model/', save_merged_model=False, batch_size=2, num_epochs=1, learning_rate=0.0003, gradient_accumulation_steps=2, lr_scheduler_type='linear', quant_8bit=False, quant_4bit=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, group_by_length=False, wandb_project='', wandb_run_name='', wandb_watch='', wandb_log_model='', save_steps=40, eval_steps=40, bf16=True)\n",
      "world_size: 4\n",
      "Activated Distributed Data Parallel.\n",
      "Namespace(base_model='nlpai-lab/kullm-polyglot-12.8b-v2', cache_dir='/tmp/huggingface-cache', pretrained_model_path='/opt/ml/input/data/pretrained/', data_path='/opt/ml/input/data/training/', output_dir='/opt/ml/checkpoints', save_path='/opt/ml/model/', save_merged_model=False, batch_size=2, num_epochs=1, learning_rate=0.0003, gradient_accumulation_steps=2, lr_scheduler_type='linear', quant_8bit=False, quant_4bit=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, group_by_length=False, wandb_project='', wandb_run_name='', wandb_watch='', wandb_log_model='', save_steps=40, eval_steps=40, bf16=True)\n",
      "world_size: 4\n",
      "Activated Distributed Data Parallel.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:29<00:58, 29.33s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:29<00:58, 29.35s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:29<00:59, 29.55s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:29<00:59, 29.59s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:30<01:00, 30.30s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:30<01:00, 30.33s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:30<01:00, 30.48s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:30<01:01, 30.52s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:57<00:28, 28.86s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:58<00:28, 28.92s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:58<00:29, 29.04s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:58<00:29, 29.05s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:58<00:28, 28.75s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:58<00:28, 28.78s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:58<00:28, 28.86s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:58<00:28, 28.87s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 23.55s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 25.06s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 23.59s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 25.09s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 23.61s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 25.10s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 23.61s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 25.11s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 23.37s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 25.00s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 23.38s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 25.01s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 23.43s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 25.04s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 23.44s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:15<00:00, 25.05s/it]\n",
      "Checkpoint /opt/ml/checkpoints/checkpoint-9520/adapter_model.bin not found\n",
      "Checkpoint /opt/ml/checkpoints/checkpoint-9520/adapter_model.bin not foundCheckpoint /opt/ml/checkpoints/checkpoint-9520/adapter_model.bin not foundCheckpoint /opt/ml/checkpoints/checkpoint-9520/adapter_model.bin not found\n",
      "trainable params: 6,553,600 || all params: 12,900,157,440 || trainable%: 0.05080248074863806\n",
      "trainable params: 6,553,600 || all params: 12,900,157,440 || trainable%: 0.05080248074863806\n",
      "trainable params: 6,553,600 || all params: 12,900,157,440 || trainable%: 0.05080248074863806\n",
      "trainable params: 6,553,600 || all params: 12,900,157,440 || trainable%: 0.05080248074863806\n",
      "NCCL version 2.18.5+cuda12.1\n",
      "Checkpoint /opt/ml/checkpoints/checkpoint-9520/adapter_model.bin not found\n",
      "Checkpoint /opt/ml/checkpoints/checkpoint-9520/adapter_model.bin not found\n",
      "Checkpoint /opt/ml/checkpoints/checkpoint-9520/adapter_model.bin not found\n",
      "Checkpoint /opt/ml/checkpoints/checkpoint-9520/adapter_model.bin not found\n",
      "trainable params: 6,553,600 || all params: 12,900,157,440 || trainable%: 0.05080248074863806\n",
      "trainable params: 6,553,600 || all params: 12,900,157,440 || trainable%: 0.05080248074863806\n",
      "trainable params: 6,553,600 || all params: 12,900,157,440 || trainable%: 0.05080248074863806\n",
      "trainable params: 6,553,600 || all params: 12,900,157,440 || trainable%: 0.05080248074863806\n",
      "NCCL version 2.18.5+cuda12.1\n",
      "algo-2:82:408 [3] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-2:80:409 [1] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-2:79:407 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-2:81:410 [2] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-1:79:407 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-1:80:409 [1] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-1:82:408 [3] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-1:81:410 [2] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "0%|          | 0/625 [00:00<?, ?it/s]\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "0%|          | 0/625 [00:00<?, ?it/s]\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "0%|          | 1/625 [00:15<2:38:33, 15.25s/it]\n",
      "{'loss': 1.0105, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.0}\n",
      "0%|          | 1/625 [00:15<2:38:33, 15.25s/it]\n",
      "0%|          | 1/625 [00:15<2:38:51, 15.28s/it]\n",
      "{'loss': 1.0105, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.0}\n",
      "0%|          | 1/625 [00:15<2:38:51, 15.28s/it]\n",
      "0%|          | 2/625 [00:25<2:10:10, 12.54s/it]\n",
      "{'loss': 1.0015, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.0}\n",
      "0%|          | 2/625 [00:25<2:10:10, 12.54s/it]\n",
      "0%|          | 2/625 [00:25<2:10:16, 12.55s/it]\n",
      "{'loss': 1.0015, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.0}\n",
      "0%|          | 2/625 [00:25<2:10:16, 12.55s/it]\n",
      "0%|          | 3/625 [00:33<1:48:35, 10.48s/it]\n",
      "{'loss': 1.323, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.0}\n",
      "0%|          | 3/625 [00:33<1:48:35, 10.48s/it]\n",
      "0%|          | 3/625 [00:33<1:48:38, 10.48s/it]\n",
      "{'loss': 1.323, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.0}\n",
      "0%|          | 3/625 [00:33<1:48:38, 10.48s/it]\n",
      "1%|          | 4/625 [00:42<1:40:49,  9.74s/it]\n",
      "{'loss': 1.1257, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.01}\n",
      "1%|          | 4/625 [00:42<1:40:49,  9.74s/it]\n",
      "1%|          | 4/625 [00:42<1:40:51,  9.74s/it]\n",
      "{'loss': 1.1252, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.01}\n",
      "1%|          | 4/625 [00:42<1:40:51,  9.74s/it]\n",
      "1%|          | 5/625 [00:54<1:50:05, 10.65s/it]\n",
      "{'loss': 1.0481, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.01}\n",
      "1%|          | 5/625 [00:54<1:50:05, 10.65s/it]\n",
      "1%|          | 5/625 [00:54<1:50:06, 10.65s/it]\n",
      "{'loss': 1.048, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.01}\n",
      "1%|          | 5/625 [00:54<1:50:06, 10.65s/it]\n",
      "1%|          | 6/625 [01:10<2:07:59, 12.41s/it]\n",
      "{'loss': 0.9181, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.01}\n",
      "1%|          | 6/625 [01:10<2:07:59, 12.41s/it]\n",
      "1%|          | 6/625 [01:10<2:07:59, 12.41s/it]\n",
      "{'loss': 0.9177, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.01}\n",
      "1%|          | 6/625 [01:10<2:07:59, 12.41s/it]\n",
      "1%|          | 7/625 [01:25<2:14:46, 13.08s/it]\n",
      "{'loss': 0.8823, 'learning_rate': 2.1e-05, 'epoch': 0.01}\n",
      "1%|          | 7/625 [01:25<2:14:46, 13.08s/it]\n",
      "1%|          | 7/625 [01:25<2:14:46, 13.08s/it]\n",
      "{'loss': 0.883, 'learning_rate': 2.1e-05, 'epoch': 0.01}\n",
      "1%|          | 7/625 [01:25<2:14:46, 13.08s/it]\n",
      "1%|▏         | 8/625 [01:32<1:55:48, 11.26s/it]\n",
      "{'loss': 1.1633, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.01}\n",
      "1%|▏         | 8/625 [01:32<1:55:48, 11.26s/it]\n",
      "1%|▏         | 8/625 [01:32<1:55:47, 11.26s/it]\n",
      "{'loss': 1.1633, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.01}\n",
      "1%|▏         | 8/625 [01:32<1:55:47, 11.26s/it]\n",
      "1%|▏         | 9/625 [01:48<2:12:35, 12.91s/it]\n",
      "{'loss': 1.1492, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.01}\n",
      "1%|▏         | 9/625 [01:48<2:12:35, 12.91s/it]\n",
      "1%|▏         | 9/625 [01:49<2:12:35, 12.91s/it]\n",
      "{'loss': 1.1494, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.01}\n",
      "1%|▏         | 9/625 [01:49<2:12:35, 12.91s/it]\n",
      "2%|▏         | 10/625 [02:02<2:13:45, 13.05s/it]\n",
      "{'loss': 0.8961, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.02}\n",
      "2%|▏         | 10/625 [02:02<2:13:45, 13.05s/it]\n",
      "2%|▏         | 10/625 [02:02<2:13:45, 13.05s/it]\n",
      "{'loss': 0.8968, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.02}\n",
      "2%|▏         | 10/625 [02:02<2:13:45, 13.05s/it]\n",
      "2%|▏         | 11/625 [02:14<2:10:42, 12.77s/it]\n",
      "{'loss': 1.0537, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.02}\n",
      "2%|▏         | 11/625 [02:14<2:10:42, 12.77s/it]\n",
      "2%|▏         | 11/625 [02:14<2:10:43, 12.77s/it]\n",
      "{'loss': 1.0539, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.02}\n",
      "2%|▏         | 11/625 [02:14<2:10:43, 12.77s/it]\n",
      "2%|▏         | 12/625 [02:25<2:04:19, 12.17s/it]\n",
      "{'loss': 1.2436, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.02}\n",
      "2%|▏         | 12/625 [02:25<2:04:19, 12.17s/it]\n",
      "2%|▏         | 12/625 [02:25<2:04:19, 12.17s/it]\n",
      "{'loss': 1.2434, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.02}\n",
      "2%|▏         | 12/625 [02:25<2:04:19, 12.17s/it]\n",
      "2%|▏         | 13/625 [02:40<2:12:28, 12.99s/it]\n",
      "{'loss': 1.1102, 'learning_rate': 3.9e-05, 'epoch': 0.02}\n",
      "2%|▏         | 13/625 [02:40<2:12:28, 12.99s/it]\n",
      "2%|▏         | 13/625 [02:40<2:12:27, 12.99s/it]\n",
      "{'loss': 1.1101, 'learning_rate': 3.9e-05, 'epoch': 0.02}\n",
      "2%|▏         | 13/625 [02:40<2:12:27, 12.99s/it]\n",
      "2%|▏         | 14/625 [02:47<1:55:17, 11.32s/it]\n",
      "{'loss': 1.0257, 'learning_rate': 4.2e-05, 'epoch': 0.02}\n",
      "2%|▏         | 14/625 [02:47<1:55:17, 11.32s/it]\n",
      "2%|▏         | 14/625 [02:47<1:55:15, 11.32s/it]\n",
      "{'loss': 1.0256, 'learning_rate': 4.2e-05, 'epoch': 0.02}\n",
      "2%|▏         | 14/625 [02:47<1:55:15, 11.32s/it]\n",
      "2%|▏         | 15/625 [03:02<2:07:23, 12.53s/it]\n",
      "{'loss': 1.0211, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.02}\n",
      "2%|▏         | 15/625 [03:02<2:07:23, 12.53s/it]\n",
      "2%|▏         | 15/625 [03:02<2:07:23, 12.53s/it]\n",
      "{'loss': 1.0209, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.02}\n",
      "2%|▏         | 15/625 [03:02<2:07:23, 12.53s/it]\n",
      "3%|▎         | 16/625 [03:12<1:56:38, 11.49s/it]\n",
      "{'loss': 1.016, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.03}\n",
      "3%|▎         | 16/625 [03:12<1:56:38, 11.49s/it]\n",
      "3%|▎         | 16/625 [03:12<1:56:38, 11.49s/it]\n",
      "{'loss': 1.0161, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.03}\n",
      "3%|▎         | 16/625 [03:12<1:56:38, 11.49s/it]\n",
      "3%|▎         | 17/625 [03:25<2:02:36, 12.10s/it]\n",
      "{'loss': 1.0847, 'learning_rate': 5.1e-05, 'epoch': 0.03}\n",
      "3%|▎         | 17/625 [03:25<2:02:36, 12.10s/it]\n",
      "3%|▎         | 17/625 [03:25<2:02:37, 12.10s/it]\n",
      "{'loss': 1.0844, 'learning_rate': 5.1e-05, 'epoch': 0.03}\n",
      "3%|▎         | 17/625 [03:25<2:02:37, 12.10s/it]\n",
      "3%|▎         | 18/625 [03:39<2:08:17, 12.68s/it]\n",
      "{'loss': 1.1216, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.03}\n",
      "3%|▎         | 18/625 [03:39<2:08:17, 12.68s/it]\n",
      "3%|▎         | 18/625 [03:39<2:08:20, 12.69s/it]\n",
      "{'loss': 1.1212, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.03}\n",
      "3%|▎         | 18/625 [03:39<2:08:20, 12.69s/it]\n",
      "3%|▎         | 19/625 [03:56<2:20:48, 13.94s/it]\n",
      "{'loss': 1.1986, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.03}\n",
      "3%|▎         | 19/625 [03:56<2:20:48, 13.94s/it]\n",
      "3%|▎         | 19/625 [03:56<2:20:50, 13.95s/it]\n",
      "{'loss': 1.1991, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.03}\n",
      "3%|▎         | 19/625 [03:56<2:20:50, 13.95s/it]\n",
      "3%|▎         | 20/625 [04:10<2:19:53, 13.87s/it]\n",
      "{'loss': 0.9636, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.03}\n",
      "3%|▎         | 20/625 [04:10<2:19:53, 13.87s/it]\n",
      "3%|▎         | 20/625 [04:10<2:19:56, 13.88s/it]\n",
      "{'loss': 0.9638, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.03}\n",
      "3%|▎         | 20/625 [04:10<2:19:56, 13.88s/it]\n",
      "3%|▎         | 21/625 [04:22<2:14:24, 13.35s/it]\n",
      "{'loss': 1.0185, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.03}\n",
      "3%|▎         | 21/625 [04:22<2:14:24, 13.35s/it]\n",
      "3%|▎         | 21/625 [04:22<2:14:26, 13.36s/it]\n",
      "{'loss': 1.0194, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.03}\n",
      "3%|▎         | 21/625 [04:22<2:14:26, 13.36s/it]\n",
      "4%|▎         | 22/625 [04:30<1:59:42, 11.91s/it]\n",
      "{'loss': 1.1887, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.04}\n",
      "4%|▎         | 22/625 [04:30<1:59:42, 11.91s/it]\n",
      "4%|▎         | 22/625 [04:30<1:59:43, 11.91s/it]\n",
      "{'loss': 1.1896, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.04}\n",
      "4%|▎         | 22/625 [04:30<1:59:43, 11.91s/it]\n",
      "4%|▎         | 23/625 [04:43<2:01:06, 12.07s/it]\n",
      "{'loss': 1.0246, 'learning_rate': 6.9e-05, 'epoch': 0.04}\n",
      "4%|▎         | 23/625 [04:43<2:01:06, 12.07s/it]\n",
      "4%|▎         | 23/625 [04:43<2:01:07, 12.07s/it]\n",
      "{'loss': 1.0243, 'learning_rate': 6.9e-05, 'epoch': 0.04}\n",
      "4%|▎         | 23/625 [04:43<2:01:07, 12.07s/it]\n",
      "4%|▍         | 24/625 [04:55<2:02:38, 12.24s/it]\n",
      "{'loss': 1.1153, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.04}\n",
      "4%|▍         | 24/625 [04:55<2:02:38, 12.24s/it]\n",
      "4%|▍         | 24/625 [04:56<2:02:40, 12.25s/it]\n",
      "{'loss': 1.1156, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.04}\n",
      "4%|▍         | 24/625 [04:56<2:02:40, 12.25s/it]\n",
      "4%|▍         | 25/625 [05:12<2:15:57, 13.60s/it]\n",
      "{'loss': 0.8986, 'learning_rate': 7.5e-05, 'epoch': 0.04}\n",
      "4%|▍         | 25/625 [05:12<2:15:57, 13.60s/it]\n",
      "4%|▍         | 25/625 [05:12<2:15:59, 13.60s/it]\n",
      "{'loss': 0.8987, 'learning_rate': 7.5e-05, 'epoch': 0.04}\n",
      "4%|▍         | 25/625 [05:12<2:15:59, 13.60s/it]\n",
      "4%|▍         | 26/625 [05:22<2:03:22, 12.36s/it]\n",
      "{'loss': 1.0991, 'learning_rate': 7.8e-05, 'epoch': 0.04}\n",
      "4%|▍         | 26/625 [05:22<2:03:22, 12.36s/it]\n",
      "4%|▍         | 26/625 [05:22<2:03:23, 12.36s/it]\n",
      "{'loss': 1.0993, 'learning_rate': 7.8e-05, 'epoch': 0.04}\n",
      "4%|▍         | 26/625 [05:22<2:03:23, 12.36s/it]\n",
      "4%|▍         | 27/625 [05:36<2:07:47, 12.82s/it]\n",
      "{'loss': 0.9642, 'learning_rate': 8.1e-05, 'epoch': 0.04}\n",
      "4%|▍         | 27/625 [05:36<2:07:47, 12.82s/it]\n",
      "4%|▍         | 27/625 [05:36<2:07:49, 12.82s/it]\n",
      "{'loss': 0.9644, 'learning_rate': 8.1e-05, 'epoch': 0.04}\n",
      "4%|▍         | 27/625 [05:36<2:07:49, 12.82s/it]\n",
      "4%|▍         | 28/625 [05:50<2:12:53, 13.36s/it]\n",
      "{'loss': 1.1043, 'learning_rate': 8.4e-05, 'epoch': 0.04}\n",
      "4%|▍         | 28/625 [05:50<2:12:53, 13.36s/it]\n",
      "4%|▍         | 28/625 [05:50<2:12:54, 13.36s/it]\n",
      "{'loss': 1.1053, 'learning_rate': 8.4e-05, 'epoch': 0.04}\n",
      "4%|▍         | 28/625 [05:50<2:12:54, 13.36s/it]\n",
      "5%|▍         | 29/625 [06:00<2:03:32, 12.44s/it]\n",
      "{'loss': 1.296, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.05}\n",
      "5%|▍         | 29/625 [06:00<2:03:32, 12.44s/it]\n",
      "5%|▍         | 29/625 [06:01<2:03:33, 12.44s/it]\n",
      "{'loss': 1.2966, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.05}\n",
      "5%|▍         | 29/625 [06:01<2:03:33, 12.44s/it]\n",
      "5%|▍         | 30/625 [06:08<1:48:00, 10.89s/it]\n",
      "{'loss': 1.0045, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.05}\n",
      "5%|▍         | 30/625 [06:08<1:48:00, 10.89s/it]\n",
      "5%|▍         | 30/625 [06:08<1:48:00, 10.89s/it]\n",
      "{'loss': 1.0041, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.05}\n",
      "5%|▍         | 30/625 [06:08<1:48:00, 10.89s/it]\n",
      "5%|▍         | 31/625 [06:20<1:50:40, 11.18s/it]\n",
      "{'loss': 0.9906, 'learning_rate': 9.3e-05, 'epoch': 0.05}\n",
      "5%|▍         | 31/625 [06:20<1:50:40, 11.18s/it]\n",
      "5%|▌         | 32/625 [06:31<1:50:28, 11.18s/it]\n",
      "{'loss': 1.1734, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.05}\n",
      "5%|▌         | 32/625 [06:31<1:50:28, 11.18s/it]\n",
      "5%|▌         | 32/625 [06:31<1:50:28, 11.18s/it]\n",
      "{'loss': 1.173, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.05}\n",
      "5%|▌         | 32/625 [06:31<1:50:28, 11.18s/it]\n",
      "5%|▌         | 33/625 [06:44<1:56:01, 11.76s/it]\n",
      "{'loss': 1.0368, 'learning_rate': 9.9e-05, 'epoch': 0.05}\n",
      "5%|▌         | 33/625 [06:44<1:56:01, 11.76s/it]\n",
      "5%|▌         | 33/625 [06:44<1:56:03, 11.76s/it]\n",
      "{'loss': 1.0372, 'learning_rate': 9.9e-05, 'epoch': 0.05}\n",
      "5%|▌         | 33/625 [06:44<1:56:03, 11.76s/it]\n",
      "5%|▌         | 34/625 [06:55<1:53:32, 11.53s/it]\n",
      "{'loss': 1.1857, 'learning_rate': 0.000102, 'epoch': 0.05}\n",
      "5%|▌         | 34/625 [06:55<1:53:32, 11.53s/it]\n",
      "6%|▌         | 35/625 [07:06<1:51:32, 11.34s/it]\n",
      "{'loss': 1.1094, 'learning_rate': 0.00010499999999999999, 'epoch': 0.06}\n",
      "6%|▌         | 35/625 [07:06<1:51:32, 11.34s/it]\n",
      "6%|▌         | 35/625 [07:06<1:51:33, 11.34s/it]\n",
      "{'loss': 1.1096, 'learning_rate': 0.00010499999999999999, 'epoch': 0.06}\n",
      "6%|▌         | 35/625 [07:06<1:51:33, 11.34s/it]\n",
      "6%|▌         | 36/625 [07:14<1:42:56, 10.49s/it]\n",
      "{'loss': 1.0674, 'learning_rate': 0.00010799999999999998, 'epoch': 0.06}\n",
      "6%|▌         | 36/625 [07:14<1:42:56, 10.49s/it]\n",
      "6%|▌         | 36/625 [07:14<1:42:56, 10.49s/it]\n",
      "{'loss': 1.0678, 'learning_rate': 0.00010799999999999998, 'epoch': 0.06}\n",
      "6%|▌         | 36/625 [07:14<1:42:56, 10.49s/it]\n",
      "6%|▌         | 37/625 [07:28<1:52:02, 11.43s/it]\n",
      "{'loss': 1.0631, 'learning_rate': 0.00011099999999999999, 'epoch': 0.06}\n",
      "6%|▌         | 37/625 [07:28<1:52:02, 11.43s/it]\n",
      "6%|▌         | 37/625 [07:28<1:52:01, 11.43s/it]\n",
      "{'loss': 1.0629, 'learning_rate': 0.00011099999999999999, 'epoch': 0.06}\n",
      "6%|▌         | 37/625 [07:28<1:52:01, 11.43s/it]\n",
      "6%|▌         | 38/625 [07:40<1:54:54, 11.75s/it]\n",
      "{'loss': 0.9871, 'learning_rate': 0.00011399999999999999, 'epoch': 0.06}\n",
      "6%|▌         | 38/625 [07:40<1:54:54, 11.75s/it]\n",
      "6%|▌         | 38/625 [07:40<1:54:53, 11.74s/it]\n",
      "{'loss': 0.9872, 'learning_rate': 0.00011399999999999999, 'epoch': 0.06}\n",
      "6%|▌         | 38/625 [07:40<1:54:53, 11.74s/it]\n",
      "6%|▌         | 39/625 [07:51<1:50:41, 11.33s/it]\n",
      "{'loss': 0.9897, 'learning_rate': 0.000117, 'epoch': 0.06}\n",
      "6%|▌         | 39/625 [07:51<1:50:41, 11.33s/it]\n",
      "6%|▌         | 39/625 [07:51<1:50:40, 11.33s/it]\n",
      "{'loss': 0.9897, 'learning_rate': 0.000117, 'epoch': 0.06}\n",
      "6%|▌         | 39/625 [07:51<1:50:40, 11.33s/it]\n",
      "6%|▋         | 40/625 [08:02<1:51:04, 11.39s/it]\n",
      "{'loss': 0.9853, 'learning_rate': 0.00011999999999999999, 'epoch': 0.06}\n",
      "6%|▋         | 40/625 [08:02<1:51:04, 11.39s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "6%|▋         | 40/625 [08:02<1:51:03, 11.39s/it]\n",
      "{'loss': 0.9856, 'learning_rate': 0.00011999999999999999, 'epoch': 0.06}\n",
      "6%|▋         | 40/625 [08:02<1:51:03, 11.39s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "7%|▋         | 41/625 [08:17<2:00:39, 12.40s/it]\n",
      "{'loss': 1.0759, 'learning_rate': 0.00012299999999999998, 'epoch': 0.07}\n",
      "7%|▋         | 41/625 [08:17<2:00:39, 12.40s/it]\n",
      "7%|▋         | 41/625 [08:17<2:00:40, 12.40s/it]\n",
      "{'loss': 1.076, 'learning_rate': 0.00012299999999999998, 'epoch': 0.07}\n",
      "7%|▋         | 41/625 [08:17<2:00:40, 12.40s/it]\n",
      "7%|▋         | 42/625 [08:26<1:51:42, 11.50s/it]\n",
      "{'loss': 1.0303, 'learning_rate': 0.00012599999999999997, 'epoch': 0.07}\n",
      "7%|▋         | 42/625 [08:26<1:51:42, 11.50s/it]\n",
      "7%|▋         | 42/625 [08:26<1:51:41, 11.50s/it]\n",
      "{'loss': 1.0299, 'learning_rate': 0.00012599999999999997, 'epoch': 0.07}\n",
      "7%|▋         | 42/625 [08:26<1:51:41, 11.50s/it]\n",
      "7%|▋         | 43/625 [08:39<1:53:42, 11.72s/it]\n",
      "{'loss': 1.0247, 'learning_rate': 0.000129, 'epoch': 0.07}\n",
      "7%|▋         | 43/625 [08:39<1:53:42, 11.72s/it]\n",
      "7%|▋         | 43/625 [08:39<1:53:42, 11.72s/it]\n",
      "{'loss': 1.0251, 'learning_rate': 0.000129, 'epoch': 0.07}\n",
      "7%|▋         | 43/625 [08:39<1:53:42, 11.72s/it]\n",
      "7%|▋         | 44/625 [08:55<2:05:32, 12.97s/it]\n",
      "{'loss': 1.0896, 'learning_rate': 0.00013199999999999998, 'epoch': 0.07}\n",
      "7%|▋         | 44/625 [08:55<2:05:32, 12.97s/it]\n",
      "7%|▋         | 44/625 [08:55<2:05:33, 12.97s/it]\n",
      "{'loss': 1.0898, 'learning_rate': 0.00013199999999999998, 'epoch': 0.07}\n",
      "7%|▋         | 44/625 [08:55<2:05:33, 12.97s/it]\n",
      "7%|▋         | 45/625 [09:09<2:09:08, 13.36s/it]\n",
      "{'loss': 1.1697, 'learning_rate': 0.000135, 'epoch': 0.07}\n",
      "7%|▋         | 45/625 [09:09<2:09:08, 13.36s/it]\n",
      "7%|▋         | 45/625 [09:09<2:09:12, 13.37s/it]\n",
      "{'loss': 1.1698, 'learning_rate': 0.000135, 'epoch': 0.07}\n",
      "7%|▋         | 45/625 [09:09<2:09:12, 13.37s/it]\n",
      "7%|▋         | 46/625 [09:21<2:06:34, 13.12s/it]\n",
      "{'loss': 1.1681, 'learning_rate': 0.000138, 'epoch': 0.07}\n",
      "7%|▋         | 46/625 [09:21<2:06:34, 13.12s/it]\n",
      "7%|▋         | 46/625 [09:21<2:06:37, 13.12s/it]\n",
      "{'loss': 1.1689, 'learning_rate': 0.000138, 'epoch': 0.07}\n",
      "7%|▋         | 46/625 [09:21<2:06:37, 13.12s/it]\n",
      "8%|▊         | 47/625 [09:32<1:58:38, 12.32s/it]\n",
      "{'loss': 0.9355, 'learning_rate': 0.00014099999999999998, 'epoch': 0.08}\n",
      "8%|▊         | 47/625 [09:32<1:58:38, 12.32s/it]\n",
      "8%|▊         | 47/625 [09:32<1:58:39, 12.32s/it]\n",
      "{'loss': 0.9353, 'learning_rate': 0.00014099999999999998, 'epoch': 0.08}\n",
      "8%|▊         | 47/625 [09:32<1:58:39, 12.32s/it]\n",
      "8%|▊         | 48/625 [09:41<1:50:12, 11.46s/it]\n",
      "{'loss': 1.0804, 'learning_rate': 0.00014399999999999998, 'epoch': 0.08}\n",
      "8%|▊         | 48/625 [09:41<1:50:12, 11.46s/it]\n",
      "8%|▊         | 48/625 [09:41<1:50:13, 11.46s/it]\n",
      "{'loss': 1.0797, 'learning_rate': 0.00014399999999999998, 'epoch': 0.08}\n",
      "8%|▊         | 48/625 [09:41<1:50:13, 11.46s/it]\n",
      "8%|▊         | 49/625 [09:49<1:39:39, 10.38s/it]\n",
      "{'loss': 1.2285, 'learning_rate': 0.000147, 'epoch': 0.08}\n",
      "8%|▊         | 49/625 [09:49<1:39:39, 10.38s/it]\n",
      "8%|▊         | 49/625 [09:49<1:39:40, 10.38s/it]\n",
      "{'loss': 1.2277, 'learning_rate': 0.000147, 'epoch': 0.08}\n",
      "8%|▊         | 49/625 [09:49<1:39:40, 10.38s/it]\n",
      "8%|▊         | 50/625 [10:00<1:40:10, 10.45s/it]\n",
      "{'loss': 1.0835, 'learning_rate': 0.00015, 'epoch': 0.08}\n",
      "8%|▊         | 50/625 [10:00<1:40:10, 10.45s/it]\n",
      "8%|▊         | 50/625 [10:00<1:40:10, 10.45s/it]\n",
      "{'loss': 1.0826, 'learning_rate': 0.00015, 'epoch': 0.08}\n",
      "8%|▊         | 50/625 [10:00<1:40:10, 10.45s/it]\n",
      "8%|▊         | 51/625 [10:14<1:52:09, 11.72s/it]\n",
      "{'loss': 1.0627, 'learning_rate': 0.00015299999999999998, 'epoch': 0.08}\n",
      "8%|▊         | 51/625 [10:14<1:52:09, 11.72s/it]\n",
      "8%|▊         | 51/625 [10:15<1:52:10, 11.73s/it]\n",
      "{'loss': 1.0636, 'learning_rate': 0.00015299999999999998, 'epoch': 0.08}\n",
      "8%|▊         | 51/625 [10:15<1:52:10, 11.73s/it]\n",
      "8%|▊         | 52/625 [10:28<1:57:11, 12.27s/it]\n",
      "{'loss': 0.8125, 'learning_rate': 0.000156, 'epoch': 0.08}\n",
      "8%|▊         | 52/625 [10:28<1:57:11, 12.27s/it]\n",
      "8%|▊         | 52/625 [10:28<1:57:11, 12.27s/it]\n",
      "{'loss': 0.8122, 'learning_rate': 0.000156, 'epoch': 0.08}\n",
      "8%|▊         | 52/625 [10:28<1:57:11, 12.27s/it]\n",
      "8%|▊         | 53/625 [10:47<2:17:10, 14.39s/it]\n",
      "{'loss': 1.0968, 'learning_rate': 0.000159, 'epoch': 0.08}\n",
      "8%|▊         | 53/625 [10:47<2:17:10, 14.39s/it]\n",
      "8%|▊         | 53/625 [10:47<2:17:11, 14.39s/it]\n",
      "{'loss': 1.0963, 'learning_rate': 0.000159, 'epoch': 0.08}\n",
      "8%|▊         | 53/625 [10:47<2:17:11, 14.39s/it]\n",
      "9%|▊         | 54/625 [10:54<1:54:05, 11.99s/it]\n",
      "{'loss': 0.9962, 'learning_rate': 0.000162, 'epoch': 0.09}\n",
      "9%|▊         | 54/625 [10:54<1:54:05, 11.99s/it]\n",
      "9%|▊         | 54/625 [10:54<1:54:06, 11.99s/it]\n",
      "{'loss': 0.9962, 'learning_rate': 0.000162, 'epoch': 0.09}\n",
      "9%|▊         | 54/625 [10:54<1:54:06, 11.99s/it]\n",
      "9%|▉         | 55/625 [11:01<1:39:44, 10.50s/it]\n",
      "{'loss': 1.0521, 'learning_rate': 0.000165, 'epoch': 0.09}\n",
      "9%|▉         | 55/625 [11:01<1:39:44, 10.50s/it]\n",
      "9%|▉         | 55/625 [11:01<1:39:44, 10.50s/it]\n",
      "{'loss': 1.0525, 'learning_rate': 0.000165, 'epoch': 0.09}\n",
      "9%|▉         | 55/625 [11:01<1:39:44, 10.50s/it]\n",
      "9%|▉         | 56/625 [11:14<1:47:53, 11.38s/it]\n",
      "{'loss': 1.1465, 'learning_rate': 0.000168, 'epoch': 0.09}\n",
      "9%|▉         | 56/625 [11:14<1:47:53, 11.38s/it]\n",
      "9%|▉         | 56/625 [11:14<1:47:53, 11.38s/it]\n",
      "{'loss': 1.1468, 'learning_rate': 0.000168, 'epoch': 0.09}\n",
      "9%|▉         | 56/625 [11:14<1:47:53, 11.38s/it]\n",
      "9%|▉         | 57/625 [11:29<1:58:19, 12.50s/it]\n",
      "{'loss': 1.0284, 'learning_rate': 0.00017099999999999998, 'epoch': 0.09}\n",
      "9%|▉         | 57/625 [11:29<1:58:19, 12.50s/it]\n",
      "9%|▉         | 57/625 [11:29<1:58:18, 12.50s/it]\n",
      "{'loss': 1.0293, 'learning_rate': 0.00017099999999999998, 'epoch': 0.09}\n",
      "9%|▉         | 57/625 [11:29<1:58:18, 12.50s/it]\n",
      "9%|▉         | 58/625 [11:37<1:44:40, 11.08s/it]\n",
      "{'loss': 1.0939, 'learning_rate': 0.00017399999999999997, 'epoch': 0.09}\n",
      "9%|▉         | 58/625 [11:37<1:44:40, 11.08s/it]\n",
      "9%|▉         | 58/625 [11:37<1:44:40, 11.08s/it]\n",
      "{'loss': 1.0939, 'learning_rate': 0.00017399999999999997, 'epoch': 0.09}\n",
      "9%|▉         | 58/625 [11:37<1:44:40, 11.08s/it]\n",
      "9%|▉         | 59/625 [11:50<1:48:43, 11.52s/it]\n",
      "{'loss': 1.1469, 'learning_rate': 0.00017699999999999997, 'epoch': 0.09}\n",
      "9%|▉         | 59/625 [11:50<1:48:43, 11.52s/it]\n",
      "9%|▉         | 59/625 [11:50<1:48:44, 11.53s/it]\n",
      "{'loss': 1.1468, 'learning_rate': 0.00017699999999999997, 'epoch': 0.09}\n",
      "9%|▉         | 59/625 [11:50<1:48:44, 11.53s/it]\n",
      "10%|▉         | 60/625 [11:58<1:38:34, 10.47s/it]\n",
      "{'loss': 0.8615, 'learning_rate': 0.00017999999999999998, 'epoch': 0.1}\n",
      "10%|▉         | 60/625 [11:58<1:38:34, 10.47s/it]\n",
      "10%|▉         | 60/625 [11:58<1:38:35, 10.47s/it]\n",
      "{'loss': 0.8611, 'learning_rate': 0.00017999999999999998, 'epoch': 0.1}\n",
      "10%|▉         | 60/625 [11:58<1:38:35, 10.47s/it]\n",
      "10%|▉         | 61/625 [12:20<2:10:32, 13.89s/it]\n",
      "{'loss': 1.1781, 'learning_rate': 0.00018299999999999998, 'epoch': 0.1}\n",
      "10%|▉         | 61/625 [12:20<2:10:32, 13.89s/it]\n",
      "10%|▉         | 61/625 [12:20<2:10:32, 13.89s/it]\n",
      "{'loss': 1.1786, 'learning_rate': 0.00018299999999999998, 'epoch': 0.1}\n",
      "10%|▉         | 61/625 [12:20<2:10:32, 13.89s/it]\n",
      "10%|▉         | 62/625 [12:35<2:15:01, 14.39s/it]\n",
      "{'loss': 1.1808, 'learning_rate': 0.000186, 'epoch': 0.1}\n",
      "10%|▉         | 62/625 [12:35<2:15:01, 14.39s/it]\n",
      "10%|▉         | 62/625 [12:35<2:15:02, 14.39s/it]\n",
      "{'loss': 1.1811, 'learning_rate': 0.000186, 'epoch': 0.1}\n",
      "10%|▉         | 62/625 [12:35<2:15:02, 14.39s/it]\n",
      "10%|█         | 63/625 [12:51<2:20:06, 14.96s/it]\n",
      "{'loss': 1.1796, 'learning_rate': 0.00018899999999999999, 'epoch': 0.1}\n",
      "10%|█         | 63/625 [12:51<2:20:06, 14.96s/it]\n",
      "10%|█         | 63/625 [12:51<2:20:09, 14.96s/it]\n",
      "{'loss': 1.1796, 'learning_rate': 0.00018899999999999999, 'epoch': 0.1}\n",
      "10%|█         | 63/625 [12:51<2:20:09, 14.96s/it]\n",
      "10%|█         | 64/625 [13:04<2:13:36, 14.29s/it]\n",
      "{'loss': 1.0795, 'learning_rate': 0.00019199999999999998, 'epoch': 0.1}\n",
      "10%|█         | 64/625 [13:04<2:13:36, 14.29s/it]\n",
      "10%|█         | 64/625 [13:04<2:13:38, 14.29s/it]\n",
      "{'loss': 1.0797, 'learning_rate': 0.00019199999999999998, 'epoch': 0.1}\n",
      "10%|█         | 64/625 [13:04<2:13:38, 14.29s/it]\n",
      "10%|█         | 65/625 [13:14<1:59:44, 12.83s/it]\n",
      "{'loss': 1.119, 'learning_rate': 0.000195, 'epoch': 0.1}\n",
      "10%|█         | 65/625 [13:14<1:59:44, 12.83s/it]\n",
      "10%|█         | 65/625 [13:14<1:59:45, 12.83s/it]\n",
      "{'loss': 1.118, 'learning_rate': 0.000195, 'epoch': 0.1}\n",
      "10%|█         | 65/625 [13:14<1:59:45, 12.83s/it]\n",
      "11%|█         | 66/625 [13:28<2:04:53, 13.41s/it]\n",
      "{'loss': 0.9516, 'learning_rate': 0.000198, 'epoch': 0.11}\n",
      "11%|█         | 66/625 [13:28<2:04:53, 13.41s/it]\n",
      "11%|█         | 66/625 [13:28<2:04:56, 13.41s/it]\n",
      "{'loss': 0.9518, 'learning_rate': 0.000198, 'epoch': 0.11}\n",
      "11%|█         | 66/625 [13:28<2:04:56, 13.41s/it]\n",
      "11%|█         | 67/625 [13:47<2:18:21, 14.88s/it]\n",
      "{'loss': 0.9679, 'learning_rate': 0.000201, 'epoch': 0.11}\n",
      "11%|█         | 67/625 [13:47<2:18:21, 14.88s/it]\n",
      "11%|█         | 67/625 [13:47<2:18:23, 14.88s/it]\n",
      "{'loss': 0.9674, 'learning_rate': 0.000201, 'epoch': 0.11}\n",
      "11%|█         | 67/625 [13:47<2:18:23, 14.88s/it]\n",
      "11%|█         | 68/625 [14:03<2:22:29, 15.35s/it]\n",
      "{'loss': 0.8438, 'learning_rate': 0.000204, 'epoch': 0.11}\n",
      "11%|█         | 68/625 [14:03<2:22:29, 15.35s/it]\n",
      "11%|█         | 68/625 [14:03<2:22:30, 15.35s/it]\n",
      "{'loss': 0.8431, 'learning_rate': 0.000204, 'epoch': 0.11}\n",
      "11%|█         | 68/625 [14:03<2:22:30, 15.35s/it]\n",
      "11%|█         | 69/625 [14:15<2:12:25, 14.29s/it]\n",
      "{'loss': 0.9644, 'learning_rate': 0.00020699999999999996, 'epoch': 0.11}\n",
      "11%|█         | 69/625 [14:15<2:12:25, 14.29s/it]\n",
      "11%|█         | 69/625 [14:15<2:12:26, 14.29s/it]\n",
      "{'loss': 0.965, 'learning_rate': 0.00020699999999999996, 'epoch': 0.11}\n",
      "11%|█         | 69/625 [14:15<2:12:26, 14.29s/it]\n",
      "11%|█         | 70/625 [14:23<1:54:26, 12.37s/it]\n",
      "{'loss': 1.2026, 'learning_rate': 0.00020999999999999998, 'epoch': 0.11}\n",
      "11%|█         | 70/625 [14:23<1:54:26, 12.37s/it]\n",
      "11%|█         | 70/625 [14:23<1:54:27, 12.37s/it]\n",
      "{'loss': 1.2019, 'learning_rate': 0.00020999999999999998, 'epoch': 0.11}\n",
      "11%|█         | 70/625 [14:23<1:54:27, 12.37s/it]\n",
      "11%|█▏        | 71/625 [14:44<2:19:25, 15.10s/it]\n",
      "{'loss': 0.8631, 'learning_rate': 0.00021299999999999997, 'epoch': 0.11}\n",
      "11%|█▏        | 71/625 [14:44<2:19:25, 15.10s/it]\n",
      "11%|█▏        | 71/625 [14:44<2:19:25, 15.10s/it]\n",
      "{'loss': 0.8633, 'learning_rate': 0.00021299999999999997, 'epoch': 0.11}\n",
      "11%|█▏        | 71/625 [14:44<2:19:25, 15.10s/it]\n",
      "12%|█▏        | 72/625 [14:52<1:59:31, 12.97s/it]\n",
      "{'loss': 1.0567, 'learning_rate': 0.00021599999999999996, 'epoch': 0.12}\n",
      "12%|█▏        | 72/625 [14:52<1:59:31, 12.97s/it]\n",
      "12%|█▏        | 72/625 [14:52<1:59:31, 12.97s/it]\n",
      "{'loss': 1.0574, 'learning_rate': 0.00021599999999999996, 'epoch': 0.12}\n",
      "12%|█▏        | 72/625 [14:52<1:59:31, 12.97s/it]\n",
      "12%|█▏        | 73/625 [15:05<1:58:22, 12.87s/it]\n",
      "{'loss': 0.9235, 'learning_rate': 0.00021899999999999998, 'epoch': 0.12}\n",
      "12%|█▏        | 73/625 [15:05<1:58:22, 12.87s/it]\n",
      "12%|█▏        | 73/625 [15:05<1:58:22, 12.87s/it]\n",
      "{'loss': 0.9228, 'learning_rate': 0.00021899999999999998, 'epoch': 0.12}\n",
      "12%|█▏        | 73/625 [15:05<1:58:22, 12.87s/it]\n",
      "12%|█▏        | 74/625 [15:22<2:11:04, 14.27s/it]\n",
      "{'loss': 1.1001, 'learning_rate': 0.00022199999999999998, 'epoch': 0.12}\n",
      "12%|█▏        | 74/625 [15:22<2:11:04, 14.27s/it]\n",
      "12%|█▏        | 74/625 [15:22<2:11:04, 14.27s/it]\n",
      "{'loss': 1.0994, 'learning_rate': 0.00022199999999999998, 'epoch': 0.12}\n",
      "12%|█▏        | 74/625 [15:22<2:11:04, 14.27s/it]\n",
      "12%|█▏        | 75/625 [15:31<1:54:23, 12.48s/it]\n",
      "{'loss': 1.0965, 'learning_rate': 0.000225, 'epoch': 0.12}\n",
      "12%|█▏        | 75/625 [15:31<1:54:23, 12.48s/it]\n",
      "12%|█▏        | 75/625 [15:31<1:54:23, 12.48s/it]\n",
      "{'loss': 1.0971, 'learning_rate': 0.000225, 'epoch': 0.12}\n",
      "12%|█▏        | 75/625 [15:31<1:54:23, 12.48s/it]\n",
      "12%|█▏        | 76/625 [15:37<1:37:56, 10.70s/it]\n",
      "{'loss': 1.0431, 'learning_rate': 0.00022799999999999999, 'epoch': 0.12}\n",
      "12%|█▏        | 76/625 [15:37<1:37:56, 10.70s/it]\n",
      "12%|█▏        | 76/625 [15:37<1:37:56, 10.70s/it]\n",
      "{'loss': 1.0436, 'learning_rate': 0.00022799999999999999, 'epoch': 0.12}\n",
      "12%|█▏        | 76/625 [15:37<1:37:56, 10.70s/it]\n",
      "12%|█▏        | 77/625 [15:52<1:49:35, 12.00s/it]\n",
      "{'loss': 1.1191, 'learning_rate': 0.00023099999999999998, 'epoch': 0.12}\n",
      "12%|█▏        | 77/625 [15:52<1:49:35, 12.00s/it]\n",
      "12%|█▏        | 77/625 [15:52<1:49:34, 12.00s/it]\n",
      "{'loss': 1.119, 'learning_rate': 0.00023099999999999998, 'epoch': 0.12}\n",
      "12%|█▏        | 77/625 [15:52<1:49:34, 12.00s/it]\n",
      "12%|█▏        | 78/625 [16:08<1:58:51, 13.04s/it]\n",
      "{'loss': 0.8236, 'learning_rate': 0.000234, 'epoch': 0.12}\n",
      "12%|█▏        | 78/625 [16:08<1:58:51, 13.04s/it]\n",
      "12%|█▏        | 78/625 [16:08<1:58:51, 13.04s/it]\n",
      "{'loss': 0.8236, 'learning_rate': 0.000234, 'epoch': 0.12}\n",
      "12%|█▏        | 78/625 [16:08<1:58:51, 13.04s/it]\n",
      "13%|█▎        | 79/625 [16:27<2:16:25, 14.99s/it]\n",
      "{'loss': 0.9978, 'learning_rate': 0.000237, 'epoch': 0.13}\n",
      "13%|█▎        | 79/625 [16:27<2:16:25, 14.99s/it]\n",
      "13%|█▎        | 79/625 [16:27<2:16:26, 14.99s/it]\n",
      "{'loss': 0.9974, 'learning_rate': 0.000237, 'epoch': 0.13}\n",
      "13%|█▎        | 79/625 [16:27<2:16:26, 14.99s/it]\n",
      "13%|█▎        | 80/625 [16:36<1:58:52, 13.09s/it]\n",
      "{'loss': 1.0565, 'learning_rate': 0.00023999999999999998, 'epoch': 0.13}\n",
      "13%|█▎        | 80/625 [16:36<1:58:52, 13.09s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "13%|█▎        | 80/625 [16:36<1:58:53, 13.09s/it]\n",
      "{'loss': 1.0577, 'learning_rate': 0.00023999999999999998, 'epoch': 0.13}\n",
      "13%|█▎        | 80/625 [16:36<1:58:53, 13.09s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "13%|█▎        | 81/625 [16:46<1:49:37, 12.09s/it]\n",
      "{'loss': 1.3303, 'learning_rate': 0.000243, 'epoch': 0.13}\n",
      "13%|█▎        | 81/625 [16:46<1:49:37, 12.09s/it]\n",
      "13%|█▎        | 81/625 [16:46<1:49:38, 12.09s/it]\n",
      "{'loss': 1.331, 'learning_rate': 0.000243, 'epoch': 0.13}\n",
      "13%|█▎        | 81/625 [16:46<1:49:38, 12.09s/it]\n",
      "13%|█▎        | 82/625 [17:00<1:54:10, 12.62s/it]\n",
      "{'loss': 1.1233, 'learning_rate': 0.00024599999999999996, 'epoch': 0.13}\n",
      "13%|█▎        | 82/625 [17:00<1:54:10, 12.62s/it]\n",
      "13%|█▎        | 82/625 [17:00<1:54:12, 12.62s/it]\n",
      "{'loss': 1.1243, 'learning_rate': 0.00024599999999999996, 'epoch': 0.13}\n",
      "13%|█▎        | 82/625 [17:00<1:54:12, 12.62s/it]\n",
      "13%|█▎        | 83/625 [17:08<1:42:23, 11.34s/it]\n",
      "{'loss': 1.1087, 'learning_rate': 0.000249, 'epoch': 0.13}\n",
      "13%|█▎        | 83/625 [17:08<1:42:23, 11.34s/it]\n",
      "13%|█▎        | 83/625 [17:08<1:42:25, 11.34s/it]\n",
      "{'loss': 1.1081, 'learning_rate': 0.000249, 'epoch': 0.13}\n",
      "13%|█▎        | 83/625 [17:08<1:42:25, 11.34s/it]\n",
      "13%|█▎        | 84/625 [17:16<1:33:32, 10.37s/it]\n",
      "{'loss': 1.1904, 'learning_rate': 0.00025199999999999995, 'epoch': 0.13}\n",
      "13%|█▎        | 84/625 [17:16<1:33:32, 10.37s/it]\n",
      "13%|█▎        | 84/625 [17:16<1:33:33, 10.38s/it]\n",
      "{'loss': 1.1912, 'learning_rate': 0.00025199999999999995, 'epoch': 0.13}\n",
      "13%|█▎        | 84/625 [17:16<1:33:33, 10.38s/it]\n",
      "14%|█▎        | 85/625 [17:29<1:41:28, 11.27s/it]\n",
      "{'loss': 1.1189, 'learning_rate': 0.00025499999999999996, 'epoch': 0.14}\n",
      "14%|█▎        | 85/625 [17:29<1:41:28, 11.27s/it]\n",
      "14%|█▎        | 85/625 [17:29<1:41:29, 11.28s/it]\n",
      "{'loss': 1.1192, 'learning_rate': 0.00025499999999999996, 'epoch': 0.14}\n",
      "14%|█▎        | 85/625 [17:29<1:41:29, 11.28s/it]\n",
      "14%|█▍        | 86/625 [17:42<1:45:36, 11.76s/it]\n",
      "{'loss': 0.8112, 'learning_rate': 0.000258, 'epoch': 0.14}\n",
      "14%|█▍        | 86/625 [17:42<1:45:36, 11.76s/it]\n",
      "14%|█▍        | 86/625 [17:42<1:45:37, 11.76s/it]\n",
      "{'loss': 0.8113, 'learning_rate': 0.000258, 'epoch': 0.14}\n",
      "14%|█▍        | 86/625 [17:42<1:45:37, 11.76s/it]\n",
      "14%|█▍        | 87/625 [17:47<1:27:22,  9.75s/it]\n",
      "{'loss': 0.9942, 'learning_rate': 0.000261, 'epoch': 0.14}\n",
      "14%|█▍        | 87/625 [17:47<1:27:22,  9.75s/it]\n",
      "14%|█▍        | 87/625 [17:47<1:27:23,  9.75s/it]\n",
      "{'loss': 0.9941, 'learning_rate': 0.000261, 'epoch': 0.14}\n",
      "14%|█▍        | 87/625 [17:47<1:27:23,  9.75s/it]\n",
      "14%|█▍        | 88/625 [18:02<1:41:37, 11.35s/it]\n",
      "{'loss': 0.918, 'learning_rate': 0.00026399999999999997, 'epoch': 0.14}\n",
      "14%|█▍        | 88/625 [18:02<1:41:37, 11.35s/it]\n",
      "14%|█▍        | 88/625 [18:03<1:41:38, 11.36s/it]\n",
      "{'loss': 0.9179, 'learning_rate': 0.00026399999999999997, 'epoch': 0.14}\n",
      "14%|█▍        | 88/625 [18:03<1:41:38, 11.36s/it]\n",
      "14%|█▍        | 89/625 [18:18<1:53:21, 12.69s/it]\n",
      "{'loss': 0.9494, 'learning_rate': 0.000267, 'epoch': 0.14}\n",
      "14%|█▍        | 89/625 [18:18<1:53:21, 12.69s/it]\n",
      "14%|█▍        | 89/625 [18:18<1:53:22, 12.69s/it]\n",
      "{'loss': 0.9498, 'learning_rate': 0.000267, 'epoch': 0.14}\n",
      "14%|█▍        | 89/625 [18:18<1:53:22, 12.69s/it]\n",
      "14%|█▍        | 90/625 [18:30<1:51:01, 12.45s/it]\n",
      "{'loss': 1.1927, 'learning_rate': 0.00027, 'epoch': 0.14}\n",
      "14%|█▍        | 90/625 [18:30<1:51:01, 12.45s/it]\n",
      "14%|█▍        | 90/625 [18:30<1:51:03, 12.46s/it]\n",
      "{'loss': 1.1938, 'learning_rate': 0.00027, 'epoch': 0.14}\n",
      "14%|█▍        | 90/625 [18:30<1:51:03, 12.46s/it]\n",
      "15%|█▍        | 91/625 [18:38<1:37:48, 10.99s/it]\n",
      "{'loss': 1.2214, 'learning_rate': 0.00027299999999999997, 'epoch': 0.15}\n",
      "15%|█▍        | 91/625 [18:38<1:37:48, 10.99s/it]\n",
      "15%|█▍        | 91/625 [18:38<1:37:49, 10.99s/it]\n",
      "{'loss': 1.222, 'learning_rate': 0.00027299999999999997, 'epoch': 0.15}\n",
      "15%|█▍        | 91/625 [18:38<1:37:49, 10.99s/it]\n",
      "15%|█▍        | 92/625 [18:50<1:41:18, 11.40s/it]\n",
      "{'loss': 1.1582, 'learning_rate': 0.000276, 'epoch': 0.15}\n",
      "15%|█▍        | 92/625 [18:50<1:41:18, 11.40s/it]\n",
      "15%|█▍        | 92/625 [18:50<1:41:19, 11.41s/it]\n",
      "{'loss': 1.1577, 'learning_rate': 0.000276, 'epoch': 0.15}\n",
      "15%|█▍        | 92/625 [18:50<1:41:19, 11.41s/it]\n",
      "15%|█▍        | 93/625 [19:06<1:52:22, 12.67s/it]\n",
      "{'loss': 0.9781, 'learning_rate': 0.000279, 'epoch': 0.15}\n",
      "15%|█▍        | 93/625 [19:06<1:52:22, 12.67s/it]\n",
      "15%|█▍        | 93/625 [19:06<1:52:23, 12.68s/it]\n",
      "{'loss': 0.978, 'learning_rate': 0.000279, 'epoch': 0.15}\n",
      "15%|█▍        | 93/625 [19:06<1:52:23, 12.68s/it]\n",
      "15%|█▌        | 94/625 [19:20<1:57:42, 13.30s/it]\n",
      "{'loss': 1.2711, 'learning_rate': 0.00028199999999999997, 'epoch': 0.15}\n",
      "15%|█▌        | 94/625 [19:20<1:57:42, 13.30s/it]\n",
      "15%|█▌        | 94/625 [19:21<1:57:43, 13.30s/it]\n",
      "{'loss': 1.2702, 'learning_rate': 0.00028199999999999997, 'epoch': 0.15}\n",
      "15%|█▌        | 94/625 [19:21<1:57:43, 13.30s/it]\n",
      "15%|█▌        | 95/625 [19:32<1:52:51, 12.78s/it]\n",
      "{'loss': 1.1797, 'learning_rate': 0.000285, 'epoch': 0.15}\n",
      "15%|█▌        | 95/625 [19:32<1:52:51, 12.78s/it]\n",
      "15%|█▌        | 95/625 [19:32<1:52:53, 12.78s/it]\n",
      "{'loss': 1.1793, 'learning_rate': 0.000285, 'epoch': 0.15}\n",
      "15%|█▌        | 95/625 [19:32<1:52:53, 12.78s/it]\n",
      "15%|█▌        | 96/625 [19:47<1:57:48, 13.36s/it]\n",
      "{'loss': 0.8347, 'learning_rate': 0.00028799999999999995, 'epoch': 0.15}\n",
      "15%|█▌        | 96/625 [19:47<1:57:48, 13.36s/it]\n",
      "15%|█▌        | 96/625 [19:47<1:57:50, 13.36s/it]\n",
      "{'loss': 0.8363, 'learning_rate': 0.00028799999999999995, 'epoch': 0.15}\n",
      "15%|█▌        | 96/625 [19:47<1:57:50, 13.36s/it]\n",
      "16%|█▌        | 97/625 [19:59<1:54:17, 12.99s/it]\n",
      "{'loss': 0.8944, 'learning_rate': 0.00029099999999999997, 'epoch': 0.16}\n",
      "16%|█▌        | 97/625 [19:59<1:54:17, 12.99s/it]\n",
      "16%|█▌        | 97/625 [19:59<1:54:20, 12.99s/it]\n",
      "{'loss': 0.8938, 'learning_rate': 0.00029099999999999997, 'epoch': 0.16}\n",
      "16%|█▌        | 97/625 [19:59<1:54:20, 12.99s/it]\n",
      "16%|█▌        | 98/625 [20:17<2:06:57, 14.45s/it]\n",
      "{'loss': 0.9269, 'learning_rate': 0.000294, 'epoch': 0.16}\n",
      "16%|█▌        | 98/625 [20:17<2:06:57, 14.45s/it]\n",
      "16%|█▌        | 98/625 [20:17<2:06:58, 14.46s/it]\n",
      "{'loss': 0.9266, 'learning_rate': 0.000294, 'epoch': 0.16}\n",
      "16%|█▌        | 98/625 [20:17<2:06:58, 14.46s/it]\n",
      "16%|█▌        | 99/625 [20:30<2:04:15, 14.17s/it]\n",
      "{'loss': 0.9399, 'learning_rate': 0.00029699999999999996, 'epoch': 0.16}\n",
      "16%|█▌        | 99/625 [20:30<2:04:15, 14.17s/it]\n",
      "16%|█▌        | 99/625 [20:30<2:04:15, 14.17s/it]\n",
      "{'loss': 0.9389, 'learning_rate': 0.00029699999999999996, 'epoch': 0.16}\n",
      "16%|█▌        | 99/625 [20:30<2:04:15, 14.17s/it]\n",
      "16%|█▌        | 100/625 [20:39<1:49:45, 12.54s/it]\n",
      "{'loss': 1.1462, 'learning_rate': 0.0003, 'epoch': 0.16}\n",
      "16%|█▌        | 100/625 [20:39<1:49:45, 12.54s/it]\n",
      "16%|█▌        | 100/625 [20:39<1:49:44, 12.54s/it]\n",
      "{'loss': 1.1464, 'learning_rate': 0.0003, 'epoch': 0.16}\n",
      "16%|█▌        | 100/625 [20:39<1:49:44, 12.54s/it]\n",
      "16%|█▌        | 101/625 [20:49<1:42:52, 11.78s/it]\n",
      "{'loss': 1.023, 'learning_rate': 0.0002994285714285714, 'epoch': 0.16}\n",
      "16%|█▌        | 101/625 [20:49<1:42:52, 11.78s/it]\n",
      "16%|█▌        | 101/625 [20:49<1:42:52, 11.78s/it]\n",
      "{'loss': 1.0227, 'learning_rate': 0.0002994285714285714, 'epoch': 0.16}\n",
      "16%|█▌        | 101/625 [20:49<1:42:52, 11.78s/it]\n",
      "16%|█▋        | 102/625 [21:00<1:41:49, 11.68s/it]\n",
      "{'loss': 0.9167, 'learning_rate': 0.0002988571428571428, 'epoch': 0.16}\n",
      "16%|█▋        | 102/625 [21:00<1:41:49, 11.68s/it]\n",
      "16%|█▋        | 102/625 [21:01<1:41:49, 11.68s/it]\n",
      "{'loss': 0.9173, 'learning_rate': 0.0002988571428571428, 'epoch': 0.16}\n",
      "16%|█▋        | 102/625 [21:01<1:41:49, 11.68s/it]\n",
      "16%|█▋        | 103/625 [21:10<1:37:00, 11.15s/it]\n",
      "{'loss': 0.9783, 'learning_rate': 0.00029828571428571426, 'epoch': 0.16}\n",
      "16%|█▋        | 103/625 [21:10<1:37:00, 11.15s/it]\n",
      "16%|█▋        | 103/625 [21:10<1:37:00, 11.15s/it]\n",
      "{'loss': 0.9779, 'learning_rate': 0.00029828571428571426, 'epoch': 0.16}\n",
      "16%|█▋        | 103/625 [21:10<1:37:00, 11.15s/it]\n",
      "17%|█▋        | 104/625 [21:26<1:47:34, 12.39s/it]\n",
      "{'loss': 1.0348, 'learning_rate': 0.0002977142857142857, 'epoch': 0.17}\n",
      "17%|█▋        | 104/625 [21:26<1:47:34, 12.39s/it]\n",
      "17%|█▋        | 104/625 [21:26<1:47:35, 12.39s/it]\n",
      "{'loss': 1.0347, 'learning_rate': 0.0002977142857142857, 'epoch': 0.17}\n",
      "17%|█▋        | 104/625 [21:26<1:47:35, 12.39s/it]\n",
      "17%|█▋        | 105/625 [21:34<1:38:06, 11.32s/it]\n",
      "{'loss': 1.2997, 'learning_rate': 0.00029714285714285715, 'epoch': 0.17}\n",
      "17%|█▋        | 105/625 [21:34<1:38:06, 11.32s/it]\n",
      "17%|█▋        | 105/625 [21:35<1:38:06, 11.32s/it]\n",
      "{'loss': 1.2996, 'learning_rate': 0.00029714285714285715, 'epoch': 0.17}\n",
      "17%|█▋        | 105/625 [21:35<1:38:06, 11.32s/it]\n",
      "17%|█▋        | 106/625 [21:46<1:37:19, 11.25s/it]\n",
      "{'loss': 1.0822, 'learning_rate': 0.00029657142857142854, 'epoch': 0.17}\n",
      "17%|█▋        | 106/625 [21:46<1:37:19, 11.25s/it]\n",
      "17%|█▋        | 106/625 [21:46<1:37:19, 11.25s/it]\n",
      "{'loss': 1.0833, 'learning_rate': 0.00029657142857142854, 'epoch': 0.17}\n",
      "17%|█▋        | 106/625 [21:46<1:37:19, 11.25s/it]\n",
      "17%|█▋        | 107/625 [21:59<1:42:22, 11.86s/it]\n",
      "{'loss': 1.0357, 'learning_rate': 0.000296, 'epoch': 0.17}\n",
      "17%|█▋        | 107/625 [21:59<1:42:22, 11.86s/it]\n",
      "17%|█▋        | 107/625 [21:59<1:42:21, 11.86s/it]\n",
      "{'loss': 1.0354, 'learning_rate': 0.000296, 'epoch': 0.17}\n",
      "17%|█▋        | 107/625 [21:59<1:42:21, 11.86s/it]\n",
      "17%|█▋        | 108/625 [22:13<1:48:44, 12.62s/it]\n",
      "{'loss': 0.9899, 'learning_rate': 0.0002954285714285714, 'epoch': 0.17}\n",
      "17%|█▋        | 108/625 [22:13<1:48:44, 12.62s/it]\n",
      "17%|█▋        | 108/625 [22:13<1:48:45, 12.62s/it]\n",
      "{'loss': 0.9893, 'learning_rate': 0.0002954285714285714, 'epoch': 0.17}\n",
      "17%|█▋        | 108/625 [22:13<1:48:45, 12.62s/it]\n",
      "17%|█▋        | 109/625 [22:21<1:36:01, 11.17s/it]\n",
      "{'loss': 1.1595, 'learning_rate': 0.0002948571428571428, 'epoch': 0.17}\n",
      "17%|█▋        | 109/625 [22:21<1:36:01, 11.17s/it]\n",
      "17%|█▋        | 109/625 [22:21<1:36:01, 11.17s/it]\n",
      "{'loss': 1.1611, 'learning_rate': 0.0002948571428571428, 'epoch': 0.17}\n",
      "17%|█▋        | 109/625 [22:21<1:36:01, 11.17s/it]\n",
      "18%|█▊        | 110/625 [22:34<1:41:50, 11.86s/it]\n",
      "{'loss': 1.0778, 'learning_rate': 0.00029428571428571427, 'epoch': 0.18}\n",
      "18%|█▊        | 110/625 [22:34<1:41:50, 11.86s/it]\n",
      "18%|█▊        | 110/625 [22:35<1:41:50, 11.86s/it]\n",
      "{'loss': 1.0763, 'learning_rate': 0.00029428571428571427, 'epoch': 0.18}\n",
      "18%|█▊        | 110/625 [22:35<1:41:50, 11.86s/it]\n",
      "18%|█▊        | 111/625 [22:52<1:54:59, 13.42s/it]\n",
      "{'loss': 0.9666, 'learning_rate': 0.0002937142857142857, 'epoch': 0.18}\n",
      "18%|█▊        | 111/625 [22:52<1:54:59, 13.42s/it]\n",
      "18%|█▊        | 111/625 [22:52<1:54:59, 13.42s/it]\n",
      "{'loss': 0.9661, 'learning_rate': 0.0002937142857142857, 'epoch': 0.18}\n",
      "18%|█▊        | 111/625 [22:52<1:54:59, 13.42s/it]\n",
      "18%|█▊        | 112/625 [23:03<1:49:38, 12.82s/it]\n",
      "{'loss': 1.0771, 'learning_rate': 0.0002931428571428571, 'epoch': 0.18}\n",
      "18%|█▊        | 112/625 [23:03<1:49:38, 12.82s/it]\n",
      "18%|█▊        | 112/625 [23:03<1:49:38, 12.82s/it]\n",
      "{'loss': 1.0766, 'learning_rate': 0.0002931428571428571, 'epoch': 0.18}\n",
      "18%|█▊        | 112/625 [23:03<1:49:38, 12.82s/it]\n",
      "18%|█▊        | 113/625 [23:16<1:50:15, 12.92s/it]\n",
      "{'loss': 0.9215, 'learning_rate': 0.00029257142857142855, 'epoch': 0.18}\n",
      "18%|█▊        | 113/625 [23:16<1:50:15, 12.92s/it]\n",
      "18%|█▊        | 113/625 [23:16<1:50:16, 12.92s/it]\n",
      "{'loss': 0.9228, 'learning_rate': 0.00029257142857142855, 'epoch': 0.18}\n",
      "18%|█▊        | 113/625 [23:16<1:50:16, 12.92s/it]\n",
      "18%|█▊        | 114/625 [23:27<1:44:14, 12.24s/it]\n",
      "{'loss': 1.1118, 'learning_rate': 0.000292, 'epoch': 0.18}\n",
      "18%|█▊        | 114/625 [23:27<1:44:14, 12.24s/it]\n",
      "18%|█▊        | 114/625 [23:27<1:44:15, 12.24s/it]\n",
      "{'loss': 1.1104, 'learning_rate': 0.000292, 'epoch': 0.18}\n",
      "18%|█▊        | 114/625 [23:27<1:44:15, 12.24s/it]\n",
      "18%|█▊        | 115/625 [23:45<1:59:29, 14.06s/it]\n",
      "{'loss': 0.9209, 'learning_rate': 0.0002914285714285714, 'epoch': 0.18}\n",
      "18%|█▊        | 115/625 [23:45<1:59:29, 14.06s/it]\n",
      "18%|█▊        | 115/625 [23:45<1:59:29, 14.06s/it]\n",
      "{'loss': 0.9198, 'learning_rate': 0.0002914285714285714, 'epoch': 0.18}\n",
      "18%|█▊        | 115/625 [23:45<1:59:29, 14.06s/it]\n",
      "19%|█▊        | 116/625 [23:59<1:59:56, 14.14s/it]\n",
      "{'loss': 0.9785, 'learning_rate': 0.00029085714285714283, 'epoch': 0.19}\n",
      "19%|█▊        | 116/625 [23:59<1:59:56, 14.14s/it]\n",
      "19%|█▊        | 116/625 [24:00<1:59:56, 14.14s/it]\n",
      "{'loss': 0.9776, 'learning_rate': 0.00029085714285714283, 'epoch': 0.19}\n",
      "19%|█▊        | 116/625 [24:00<1:59:56, 14.14s/it]\n",
      "19%|█▊        | 117/625 [24:12<1:54:43, 13.55s/it]\n",
      "{'loss': 0.8888, 'learning_rate': 0.0002902857142857143, 'epoch': 0.19}\n",
      "19%|█▊        | 117/625 [24:12<1:54:43, 13.55s/it]\n",
      "19%|█▊        | 117/625 [24:12<1:54:42, 13.55s/it]\n",
      "{'loss': 0.8888, 'learning_rate': 0.0002902857142857143, 'epoch': 0.19}\n",
      "19%|█▊        | 117/625 [24:12<1:54:42, 13.55s/it]\n",
      "19%|█▉        | 118/625 [24:20<1:42:22, 12.12s/it]\n",
      "{'loss': 1.0369, 'learning_rate': 0.0002897142857142857, 'epoch': 0.19}\n",
      "19%|█▉        | 118/625 [24:20<1:42:22, 12.12s/it]\n",
      "19%|█▉        | 118/625 [24:20<1:42:22, 12.11s/it]\n",
      "{'loss': 1.0383, 'learning_rate': 0.0002897142857142857, 'epoch': 0.19}\n",
      "19%|█▉        | 118/625 [24:20<1:42:22, 12.11s/it]\n",
      "19%|█▉        | 119/625 [24:34<1:46:28, 12.62s/it]\n",
      "{'loss': 1.0504, 'learning_rate': 0.0002891428571428571, 'epoch': 0.19}\n",
      "19%|█▉        | 119/625 [24:34<1:46:28, 12.62s/it]\n",
      "19%|█▉        | 119/625 [24:34<1:46:27, 12.62s/it]\n",
      "{'loss': 1.0501, 'learning_rate': 0.0002891428571428571, 'epoch': 0.19}\n",
      "19%|█▉        | 119/625 [24:34<1:46:27, 12.62s/it]\n",
      "19%|█▉        | 120/625 [24:45<1:41:22, 12.04s/it]\n",
      "{'loss': 1.0796, 'learning_rate': 0.00028857142857142856, 'epoch': 0.19}\n",
      "19%|█▉        | 120/625 [24:45<1:41:22, 12.04s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "19%|█▉        | 120/625 [24:45<1:41:21, 12.04s/it]\n",
      "{'loss': 1.0795, 'learning_rate': 0.00028857142857142856, 'epoch': 0.19}\n",
      "19%|█▉        | 120/625 [24:45<1:41:21, 12.04s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "19%|█▉        | 121/625 [25:00<1:47:51, 12.84s/it]\n",
      "{'loss': 0.9423, 'learning_rate': 0.00028799999999999995, 'epoch': 0.19}\n",
      "19%|█▉        | 121/625 [25:00<1:47:51, 12.84s/it]\n",
      "19%|█▉        | 121/625 [25:00<1:47:51, 12.84s/it]\n",
      "{'loss': 0.942, 'learning_rate': 0.00028799999999999995, 'epoch': 0.19}\n",
      "19%|█▉        | 121/625 [25:00<1:47:51, 12.84s/it]\n",
      "20%|█▉        | 122/625 [25:09<1:39:36, 11.88s/it]\n",
      "{'loss': 1.1506, 'learning_rate': 0.0002874285714285714, 'epoch': 0.2}\n",
      "20%|█▉        | 122/625 [25:09<1:39:36, 11.88s/it]\n",
      "20%|█▉        | 122/625 [25:09<1:39:36, 11.88s/it]\n",
      "{'loss': 1.1498, 'learning_rate': 0.0002874285714285714, 'epoch': 0.2}\n",
      "20%|█▉        | 122/625 [25:09<1:39:36, 11.88s/it]\n",
      "20%|█▉        | 123/625 [25:21<1:38:10, 11.73s/it]\n",
      "{'loss': 1.0725, 'learning_rate': 0.00028685714285714284, 'epoch': 0.2}\n",
      "20%|█▉        | 123/625 [25:21<1:38:10, 11.73s/it]\n",
      "20%|█▉        | 123/625 [25:21<1:38:09, 11.73s/it]\n",
      "{'loss': 1.0719, 'learning_rate': 0.00028685714285714284, 'epoch': 0.2}\n",
      "20%|█▉        | 123/625 [25:21<1:38:09, 11.73s/it]\n",
      "20%|█▉        | 124/625 [25:29<1:29:37, 10.73s/it]\n",
      "{'loss': 1.0172, 'learning_rate': 0.00028628571428571424, 'epoch': 0.2}\n",
      "20%|█▉        | 124/625 [25:29<1:29:37, 10.73s/it]\n",
      "20%|█▉        | 124/625 [25:29<1:29:36, 10.73s/it]\n",
      "{'loss': 1.0182, 'learning_rate': 0.00028628571428571424, 'epoch': 0.2}\n",
      "20%|█▉        | 124/625 [25:29<1:29:36, 10.73s/it]\n",
      "20%|██        | 125/625 [25:44<1:39:13, 11.91s/it]\n",
      "{'loss': 0.96, 'learning_rate': 0.0002857142857142857, 'epoch': 0.2}\n",
      "20%|██        | 125/625 [25:44<1:39:13, 11.91s/it]\n",
      "20%|██        | 125/625 [25:44<1:39:14, 11.91s/it]\n",
      "{'loss': 0.9618, 'learning_rate': 0.0002857142857142857, 'epoch': 0.2}\n",
      "20%|██        | 125/625 [25:44<1:39:14, 11.91s/it]\n",
      "20%|██        | 126/625 [25:57<1:42:07, 12.28s/it]\n",
      "{'loss': 1.1005, 'learning_rate': 0.00028514285714285713, 'epoch': 0.2}\n",
      "20%|██        | 126/625 [25:57<1:42:07, 12.28s/it]\n",
      "20%|██        | 126/625 [25:57<1:42:08, 12.28s/it]\n",
      "{'loss': 1.1009, 'learning_rate': 0.00028514285714285713, 'epoch': 0.2}\n",
      "20%|██        | 126/625 [25:57<1:42:08, 12.28s/it]\n",
      "20%|██        | 127/625 [26:02<1:24:37, 10.20s/it]\n",
      "{'loss': 1.1341, 'learning_rate': 0.00028457142857142857, 'epoch': 0.2}\n",
      "20%|██        | 127/625 [26:02<1:24:37, 10.20s/it]\n",
      "20%|██        | 127/625 [26:02<1:24:37, 10.20s/it]\n",
      "{'loss': 1.1339, 'learning_rate': 0.00028457142857142857, 'epoch': 0.2}\n",
      "20%|██        | 127/625 [26:02<1:24:37, 10.20s/it]\n",
      "20%|██        | 128/625 [26:09<1:15:48,  9.15s/it]\n",
      "{'loss': 1.0577, 'learning_rate': 0.00028399999999999996, 'epoch': 0.2}\n",
      "20%|██        | 128/625 [26:09<1:15:48,  9.15s/it]\n",
      "20%|██        | 128/625 [26:09<1:15:48,  9.15s/it]\n",
      "{'loss': 1.0593, 'learning_rate': 0.00028399999999999996, 'epoch': 0.2}\n",
      "20%|██        | 128/625 [26:09<1:15:48,  9.15s/it]\n",
      "21%|██        | 129/625 [26:28<1:41:01, 12.22s/it]\n",
      "{'loss': 1.2332, 'learning_rate': 0.0002834285714285714, 'epoch': 0.21}\n",
      "21%|██        | 129/625 [26:28<1:41:01, 12.22s/it]\n",
      "21%|██        | 129/625 [26:28<1:41:01, 12.22s/it]\n",
      "{'loss': 1.2334, 'learning_rate': 0.0002834285714285714, 'epoch': 0.21}\n",
      "21%|██        | 129/625 [26:28<1:41:01, 12.22s/it]\n",
      "21%|██        | 130/625 [26:38<1:35:30, 11.58s/it]\n",
      "{'loss': 0.965, 'learning_rate': 0.0002828571428571428, 'epoch': 0.21}\n",
      "21%|██        | 130/625 [26:38<1:35:30, 11.58s/it]\n",
      "21%|██        | 130/625 [26:38<1:35:30, 11.58s/it]\n",
      "{'loss': 0.967, 'learning_rate': 0.0002828571428571428, 'epoch': 0.21}\n",
      "21%|██        | 130/625 [26:38<1:35:30, 11.58s/it]\n",
      "21%|██        | 131/625 [26:48<1:29:33, 10.88s/it]\n",
      "{'loss': 1.1786, 'learning_rate': 0.00028228571428571425, 'epoch': 0.21}\n",
      "21%|██        | 131/625 [26:48<1:29:33, 10.88s/it]\n",
      "21%|██        | 131/625 [26:48<1:29:33, 10.88s/it]\n",
      "{'loss': 1.1768, 'learning_rate': 0.00028228571428571425, 'epoch': 0.21}\n",
      "21%|██        | 131/625 [26:48<1:29:33, 10.88s/it]\n",
      "21%|██        | 132/625 [26:58<1:29:15, 10.86s/it]\n",
      "{'loss': 1.1982, 'learning_rate': 0.0002817142857142857, 'epoch': 0.21}\n",
      "21%|██        | 132/625 [26:58<1:29:15, 10.86s/it]\n",
      "21%|██        | 132/625 [26:58<1:29:14, 10.86s/it]\n",
      "{'loss': 1.1985, 'learning_rate': 0.0002817142857142857, 'epoch': 0.21}\n",
      "21%|██        | 132/625 [26:58<1:29:14, 10.86s/it]\n",
      "21%|██▏       | 133/625 [27:12<1:36:19, 11.75s/it]\n",
      "{'loss': 1.2906, 'learning_rate': 0.00028114285714285714, 'epoch': 0.21}\n",
      "21%|██▏       | 133/625 [27:12<1:36:19, 11.75s/it]\n",
      "21%|██▏       | 133/625 [27:12<1:36:17, 11.74s/it]\n",
      "{'loss': 1.2915, 'learning_rate': 0.00028114285714285714, 'epoch': 0.21}\n",
      "21%|██▏       | 133/625 [27:12<1:36:17, 11.74s/it]\n",
      "21%|██▏       | 134/625 [27:20<1:26:22, 10.56s/it]\n",
      "{'loss': 1.021, 'learning_rate': 0.00028057142857142853, 'epoch': 0.21}\n",
      "21%|██▏       | 134/625 [27:20<1:26:22, 10.56s/it]\n",
      "21%|██▏       | 134/625 [27:20<1:26:21, 10.55s/it]\n",
      "{'loss': 1.0212, 'learning_rate': 0.00028057142857142853, 'epoch': 0.21}\n",
      "21%|██▏       | 134/625 [27:20<1:26:21, 10.55s/it]\n",
      "22%|██▏       | 135/625 [27:27<1:17:16,  9.46s/it]\n",
      "{'loss': 1.1961, 'learning_rate': 0.00028, 'epoch': 0.22}\n",
      "22%|██▏       | 135/625 [27:27<1:17:16,  9.46s/it]\n",
      "22%|██▏       | 135/625 [27:27<1:17:16,  9.46s/it]\n",
      "{'loss': 1.1948, 'learning_rate': 0.00028, 'epoch': 0.22}\n",
      "22%|██▏       | 135/625 [27:27<1:17:16,  9.46s/it]\n",
      "22%|██▏       | 136/625 [27:33<1:09:47,  8.56s/it]\n",
      "{'loss': 0.8549, 'learning_rate': 0.00027942857142857137, 'epoch': 0.22}\n",
      "22%|██▏       | 136/625 [27:33<1:09:47,  8.56s/it]\n",
      "22%|██▏       | 136/625 [27:33<1:09:46,  8.56s/it]\n",
      "{'loss': 0.8524, 'learning_rate': 0.00027942857142857137, 'epoch': 0.22}\n",
      "22%|██▏       | 136/625 [27:33<1:09:46,  8.56s/it]\n",
      "22%|██▏       | 137/625 [27:50<1:28:21, 10.86s/it]\n",
      "{'loss': 1.2222, 'learning_rate': 0.0002788571428571428, 'epoch': 0.22}\n",
      "22%|██▏       | 137/625 [27:50<1:28:21, 10.86s/it]\n",
      "22%|██▏       | 137/625 [27:50<1:28:20, 10.86s/it]\n",
      "{'loss': 1.2218, 'learning_rate': 0.0002788571428571428, 'epoch': 0.22}\n",
      "22%|██▏       | 137/625 [27:50<1:28:20, 10.86s/it]\n",
      "22%|██▏       | 138/625 [28:00<1:26:57, 10.71s/it]\n",
      "{'loss': 1.0745, 'learning_rate': 0.00027828571428571426, 'epoch': 0.22}\n",
      "22%|██▏       | 138/625 [28:00<1:26:57, 10.71s/it]\n",
      "22%|██▏       | 138/625 [28:00<1:26:57, 10.71s/it]\n",
      "{'loss': 1.0757, 'learning_rate': 0.00027828571428571426, 'epoch': 0.22}\n",
      "22%|██▏       | 138/625 [28:00<1:26:57, 10.71s/it]\n",
      "22%|██▏       | 139/625 [28:14<1:34:55, 11.72s/it]\n",
      "{'loss': 1.1338, 'learning_rate': 0.0002777142857142857, 'epoch': 0.22}\n",
      "22%|██▏       | 139/625 [28:14<1:34:55, 11.72s/it]\n",
      "22%|██▏       | 139/625 [28:14<1:34:54, 11.72s/it]\n",
      "{'loss': 1.1341, 'learning_rate': 0.0002777142857142857, 'epoch': 0.22}\n",
      "22%|██▏       | 139/625 [28:14<1:34:54, 11.72s/it]\n",
      "22%|██▏       | 140/625 [28:21<1:22:41, 10.23s/it]\n",
      "{'loss': 1.2079, 'learning_rate': 0.00027714285714285715, 'epoch': 0.22}\n",
      "22%|██▏       | 140/625 [28:21<1:22:41, 10.23s/it]\n",
      "22%|██▏       | 140/625 [28:21<1:22:41, 10.23s/it]\n",
      "{'loss': 1.2072, 'learning_rate': 0.00027714285714285715, 'epoch': 0.22}\n",
      "22%|██▏       | 140/625 [28:21<1:22:41, 10.23s/it]\n",
      "23%|██▎       | 141/625 [28:37<1:36:17, 11.94s/it]\n",
      "{'loss': 1.1159, 'learning_rate': 0.00027657142857142854, 'epoch': 0.23}\n",
      "23%|██▎       | 141/625 [28:37<1:36:17, 11.94s/it]\n",
      "23%|██▎       | 141/625 [28:37<1:36:17, 11.94s/it]\n",
      "{'loss': 1.1162, 'learning_rate': 0.00027657142857142854, 'epoch': 0.23}\n",
      "23%|██▎       | 141/625 [28:37<1:36:17, 11.94s/it]\n",
      "23%|██▎       | 142/625 [28:58<1:59:15, 14.81s/it]\n",
      "{'loss': 1.0032, 'learning_rate': 0.000276, 'epoch': 0.23}\n",
      "23%|██▎       | 142/625 [28:58<1:59:15, 14.81s/it]\n",
      "23%|██▎       | 142/625 [28:58<1:59:18, 14.82s/it]\n",
      "{'loss': 1.0048, 'learning_rate': 0.000276, 'epoch': 0.23}\n",
      "23%|██▎       | 142/625 [28:58<1:59:18, 14.82s/it]\n",
      "23%|██▎       | 143/625 [29:13<1:58:26, 14.74s/it]\n",
      "{'loss': 1.0726, 'learning_rate': 0.0002754285714285714, 'epoch': 0.23}\n",
      "23%|██▎       | 143/625 [29:13<1:58:26, 14.74s/it]\n",
      "23%|██▎       | 143/625 [29:13<1:58:30, 14.75s/it]\n",
      "{'loss': 1.0722, 'learning_rate': 0.0002754285714285714, 'epoch': 0.23}\n",
      "23%|██▎       | 143/625 [29:13<1:58:30, 14.75s/it]\n",
      "23%|██▎       | 144/625 [29:27<1:57:41, 14.68s/it]\n",
      "{'loss': 1.1672, 'learning_rate': 0.0002748571428571428, 'epoch': 0.23}\n",
      "23%|██▎       | 144/625 [29:27<1:57:41, 14.68s/it]\n",
      "23%|██▎       | 144/625 [29:27<1:57:43, 14.69s/it]\n",
      "{'loss': 1.1669, 'learning_rate': 0.0002748571428571428, 'epoch': 0.23}\n",
      "23%|██▎       | 144/625 [29:27<1:57:43, 14.69s/it]\n",
      "23%|██▎       | 145/625 [29:39<1:50:50, 13.86s/it]\n",
      "{'loss': 1.1645, 'learning_rate': 0.00027428571428571427, 'epoch': 0.23}\n",
      "23%|██▎       | 145/625 [29:39<1:50:50, 13.86s/it]\n",
      "23%|██▎       | 145/625 [29:39<1:50:51, 13.86s/it]\n",
      "{'loss': 1.1643, 'learning_rate': 0.00027428571428571427, 'epoch': 0.23}\n",
      "23%|██▎       | 145/625 [29:39<1:50:51, 13.86s/it]\n",
      "23%|██▎       | 146/625 [29:54<1:52:40, 14.11s/it]\n",
      "{'loss': 1.0038, 'learning_rate': 0.0002737142857142857, 'epoch': 0.23}\n",
      "23%|██▎       | 146/625 [29:54<1:52:40, 14.11s/it]\n",
      "23%|██▎       | 146/625 [29:54<1:52:41, 14.12s/it]\n",
      "{'loss': 1.0036, 'learning_rate': 0.0002737142857142857, 'epoch': 0.23}\n",
      "23%|██▎       | 146/625 [29:54<1:52:41, 14.12s/it]\n",
      "24%|██▎       | 147/625 [30:01<1:34:47, 11.90s/it]\n",
      "{'loss': 1.0905, 'learning_rate': 0.0002731428571428571, 'epoch': 0.24}\n",
      "24%|██▎       | 147/625 [30:01<1:34:47, 11.90s/it]\n",
      "24%|██▎       | 147/625 [30:01<1:34:48, 11.90s/it]\n",
      "{'loss': 1.0909, 'learning_rate': 0.0002731428571428571, 'epoch': 0.24}\n",
      "24%|██▎       | 147/625 [30:01<1:34:48, 11.90s/it]\n",
      "24%|██▎       | 148/625 [30:12<1:32:59, 11.70s/it]\n",
      "{'loss': 1.104, 'learning_rate': 0.00027257142857142855, 'epoch': 0.24}\n",
      "24%|██▎       | 148/625 [30:12<1:32:59, 11.70s/it]\n",
      "24%|██▎       | 148/625 [30:12<1:32:59, 11.70s/it]\n",
      "{'loss': 1.1026, 'learning_rate': 0.00027257142857142855, 'epoch': 0.24}\n",
      "24%|██▎       | 148/625 [30:12<1:32:59, 11.70s/it]\n",
      "24%|██▍       | 149/625 [30:24<1:34:14, 11.88s/it]\n",
      "{'loss': 1.0908, 'learning_rate': 0.00027199999999999994, 'epoch': 0.24}\n",
      "24%|██▍       | 149/625 [30:24<1:34:14, 11.88s/it]\n",
      "24%|██▍       | 149/625 [30:24<1:34:14, 11.88s/it]\n",
      "{'loss': 1.091, 'learning_rate': 0.00027199999999999994, 'epoch': 0.24}\n",
      "24%|██▍       | 149/625 [30:24<1:34:14, 11.88s/it]\n",
      "24%|██▍       | 150/625 [30:35<1:32:05, 11.63s/it]\n",
      "{'loss': 1.1007, 'learning_rate': 0.0002714285714285714, 'epoch': 0.24}\n",
      "24%|██▍       | 150/625 [30:35<1:32:05, 11.63s/it]\n",
      "24%|██▍       | 150/625 [30:35<1:32:05, 11.63s/it]\n",
      "{'loss': 1.0994, 'learning_rate': 0.0002714285714285714, 'epoch': 0.24}\n",
      "24%|██▍       | 150/625 [30:35<1:32:05, 11.63s/it]\n",
      "24%|██▍       | 151/625 [30:52<1:44:20, 13.21s/it]\n",
      "{'loss': 1.0412, 'learning_rate': 0.00027085714285714283, 'epoch': 0.24}\n",
      "24%|██▍       | 151/625 [30:52<1:44:20, 13.21s/it]\n",
      "24%|██▍       | 151/625 [30:52<1:44:19, 13.21s/it]\n",
      "{'loss': 1.0402, 'learning_rate': 0.00027085714285714283, 'epoch': 0.24}\n",
      "24%|██▍       | 151/625 [30:52<1:44:19, 13.21s/it]\n",
      "24%|██▍       | 152/625 [31:05<1:43:11, 13.09s/it]\n",
      "{'loss': 0.9914, 'learning_rate': 0.0002702857142857143, 'epoch': 0.24}\n",
      "24%|██▍       | 152/625 [31:05<1:43:11, 13.09s/it]\n",
      "24%|██▍       | 152/625 [31:05<1:43:11, 13.09s/it]\n",
      "{'loss': 0.9918, 'learning_rate': 0.0002702857142857143, 'epoch': 0.24}\n",
      "24%|██▍       | 152/625 [31:05<1:43:11, 13.09s/it]\n",
      "24%|██▍       | 153/625 [31:18<1:42:48, 13.07s/it]\n",
      "{'loss': 0.9904, 'learning_rate': 0.0002697142857142857, 'epoch': 0.24}\n",
      "24%|██▍       | 153/625 [31:18<1:42:48, 13.07s/it]\n",
      "24%|██▍       | 153/625 [31:18<1:42:48, 13.07s/it]\n",
      "{'loss': 0.9893, 'learning_rate': 0.0002697142857142857, 'epoch': 0.24}\n",
      "24%|██▍       | 153/625 [31:18<1:42:48, 13.07s/it]\n",
      "25%|██▍       | 154/625 [31:31<1:42:34, 13.07s/it]\n",
      "{'loss': 1.0151, 'learning_rate': 0.0002691428571428571, 'epoch': 0.25}\n",
      "25%|██▍       | 154/625 [31:31<1:42:34, 13.07s/it]\n",
      "25%|██▍       | 154/625 [31:31<1:42:34, 13.07s/it]\n",
      "{'loss': 1.0126, 'learning_rate': 0.0002691428571428571, 'epoch': 0.25}\n",
      "25%|██▍       | 154/625 [31:31<1:42:34, 13.07s/it]\n",
      "25%|██▍       | 155/625 [31:44<1:43:13, 13.18s/it]\n",
      "{'loss': 1.1897, 'learning_rate': 0.00026857142857142856, 'epoch': 0.25}\n",
      "25%|██▍       | 155/625 [31:44<1:43:13, 13.18s/it]\n",
      "25%|██▍       | 155/625 [31:45<1:43:16, 13.18s/it]\n",
      "{'loss': 1.1905, 'learning_rate': 0.00026857142857142856, 'epoch': 0.25}\n",
      "25%|██▍       | 155/625 [31:45<1:43:16, 13.18s/it]\n",
      "25%|██▍       | 156/625 [31:59<1:46:40, 13.65s/it]\n",
      "{'loss': 1.1338, 'learning_rate': 0.00026799999999999995, 'epoch': 0.25}\n",
      "25%|██▍       | 156/625 [31:59<1:46:40, 13.65s/it]\n",
      "25%|██▍       | 156/625 [31:59<1:46:43, 13.65s/it]\n",
      "{'loss': 1.1339, 'learning_rate': 0.00026799999999999995, 'epoch': 0.25}\n",
      "25%|██▍       | 156/625 [31:59<1:46:43, 13.65s/it]\n",
      "25%|██▌       | 157/625 [32:12<1:44:46, 13.43s/it]\n",
      "{'loss': 0.9964, 'learning_rate': 0.0002674285714285714, 'epoch': 0.25}\n",
      "25%|██▌       | 157/625 [32:12<1:44:46, 13.43s/it]\n",
      "25%|██▌       | 157/625 [32:12<1:44:49, 13.44s/it]\n",
      "{'loss': 0.9954, 'learning_rate': 0.0002674285714285714, 'epoch': 0.25}\n",
      "25%|██▌       | 157/625 [32:12<1:44:49, 13.44s/it]\n",
      "25%|██▌       | 158/625 [32:22<1:35:50, 12.31s/it]\n",
      "{'loss': 1.0599, 'learning_rate': 0.00026685714285714285, 'epoch': 0.25}\n",
      "25%|██▌       | 158/625 [32:22<1:35:50, 12.31s/it]\n",
      "25%|██▌       | 158/625 [32:22<1:35:52, 12.32s/it]\n",
      "{'loss': 1.0626, 'learning_rate': 0.00026685714285714285, 'epoch': 0.25}\n",
      "25%|██▌       | 158/625 [32:22<1:35:52, 12.32s/it]\n",
      "25%|██▌       | 159/625 [32:37<1:42:43, 13.23s/it]\n",
      "{'loss': 0.8822, 'learning_rate': 0.0002662857142857143, 'epoch': 0.25}\n",
      "25%|██▌       | 159/625 [32:37<1:42:43, 13.23s/it]\n",
      "25%|██▌       | 159/625 [32:37<1:42:44, 13.23s/it]\n",
      "{'loss': 0.8823, 'learning_rate': 0.0002662857142857143, 'epoch': 0.25}\n",
      "25%|██▌       | 159/625 [32:37<1:42:44, 13.23s/it]\n",
      "26%|██▌       | 160/625 [32:53<1:47:30, 13.87s/it]\n",
      "{'loss': 1.1833, 'learning_rate': 0.0002657142857142857, 'epoch': 0.26}\n",
      "26%|██▌       | 160/625 [32:53<1:47:30, 13.87s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "26%|██▌       | 160/625 [32:53<1:47:31, 13.88s/it]\n",
      "{'loss': 1.1839, 'learning_rate': 0.0002657142857142857, 'epoch': 0.26}\n",
      "26%|██▌       | 160/625 [32:53<1:47:31, 13.88s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "26%|██▌       | 161/625 [33:07<1:47:32, 13.91s/it]\n",
      "{'loss': 0.9936, 'learning_rate': 0.00026514285714285713, 'epoch': 0.26}\n",
      "26%|██▌       | 161/625 [33:07<1:47:32, 13.91s/it]\n",
      "26%|██▌       | 161/625 [33:07<1:47:33, 13.91s/it]\n",
      "{'loss': 0.993, 'learning_rate': 0.00026514285714285713, 'epoch': 0.26}\n",
      "26%|██▌       | 161/625 [33:07<1:47:33, 13.91s/it]\n",
      "26%|██▌       | 162/625 [33:16<1:37:54, 12.69s/it]\n",
      "{'loss': 1.1137, 'learning_rate': 0.0002645714285714285, 'epoch': 0.26}\n",
      "26%|██▌       | 162/625 [33:16<1:37:54, 12.69s/it]\n",
      "26%|██▌       | 162/625 [33:17<1:37:55, 12.69s/it]\n",
      "{'loss': 1.1142, 'learning_rate': 0.0002645714285714285, 'epoch': 0.26}\n",
      "26%|██▌       | 162/625 [33:17<1:37:55, 12.69s/it]\n",
      "26%|██▌       | 163/625 [33:26<1:30:20, 11.73s/it]\n",
      "{'loss': 1.1281, 'learning_rate': 0.00026399999999999997, 'epoch': 0.26}\n",
      "26%|██▌       | 163/625 [33:26<1:30:20, 11.73s/it]\n",
      "26%|██▌       | 163/625 [33:26<1:30:21, 11.73s/it]\n",
      "{'loss': 1.1259, 'learning_rate': 0.00026399999999999997, 'epoch': 0.26}\n",
      "26%|██▌       | 163/625 [33:26<1:30:21, 11.73s/it]\n",
      "26%|██▌       | 164/625 [33:43<1:43:23, 13.46s/it]\n",
      "{'loss': 1.2433, 'learning_rate': 0.0002634285714285714, 'epoch': 0.26}\n",
      "26%|██▌       | 164/625 [33:43<1:43:23, 13.46s/it]\n",
      "26%|██▌       | 164/625 [33:44<1:43:24, 13.46s/it]\n",
      "{'loss': 1.2437, 'learning_rate': 0.0002634285714285714, 'epoch': 0.26}\n",
      "26%|██▌       | 164/625 [33:44<1:43:24, 13.46s/it]\n",
      "26%|██▋       | 165/625 [33:54<1:36:45, 12.62s/it]\n",
      "{'loss': 0.9444, 'learning_rate': 0.00026285714285714286, 'epoch': 0.26}\n",
      "26%|██▋       | 165/625 [33:54<1:36:45, 12.62s/it]\n",
      "26%|██▋       | 165/625 [33:54<1:36:46, 12.62s/it]\n",
      "{'loss': 0.9464, 'learning_rate': 0.00026285714285714286, 'epoch': 0.26}\n",
      "26%|██▋       | 165/625 [33:54<1:36:46, 12.62s/it]\n",
      "27%|██▋       | 166/625 [34:09<1:41:59, 13.33s/it]\n",
      "{'loss': 1.0237, 'learning_rate': 0.0002622857142857143, 'epoch': 0.27}\n",
      "27%|██▋       | 166/625 [34:09<1:41:59, 13.33s/it]\n",
      "27%|██▋       | 166/625 [34:09<1:42:00, 13.34s/it]\n",
      "{'loss': 1.0225, 'learning_rate': 0.0002622857142857143, 'epoch': 0.27}\n",
      "27%|██▋       | 166/625 [34:09<1:42:00, 13.34s/it]\n",
      "27%|██▋       | 167/625 [34:15<1:24:44, 11.10s/it]\n",
      "{'loss': 0.9074, 'learning_rate': 0.0002617142857142857, 'epoch': 0.27}\n",
      "27%|██▋       | 167/625 [34:15<1:24:44, 11.10s/it]\n",
      "27%|██▋       | 167/625 [34:15<1:24:45, 11.10s/it]\n",
      "{'loss': 0.9097, 'learning_rate': 0.0002617142857142857, 'epoch': 0.27}\n",
      "27%|██▋       | 167/625 [34:15<1:24:45, 11.10s/it]\n",
      "27%|██▋       | 168/625 [34:30<1:33:02, 12.22s/it]\n",
      "{'loss': 0.9145, 'learning_rate': 0.00026114285714285714, 'epoch': 0.27}\n",
      "27%|██▋       | 168/625 [34:30<1:33:02, 12.22s/it]\n",
      "27%|██▋       | 168/625 [34:30<1:33:03, 12.22s/it]\n",
      "{'loss': 0.916, 'learning_rate': 0.00026114285714285714, 'epoch': 0.27}\n",
      "27%|██▋       | 168/625 [34:30<1:33:03, 12.22s/it]\n",
      "27%|██▋       | 169/625 [34:51<1:52:34, 14.81s/it]\n",
      "{'loss': 1.0508, 'learning_rate': 0.00026057142857142853, 'epoch': 0.27}\n",
      "27%|██▋       | 169/625 [34:51<1:52:34, 14.81s/it]\n",
      "27%|██▋       | 169/625 [34:51<1:52:36, 14.82s/it]\n",
      "{'loss': 1.051, 'learning_rate': 0.00026057142857142853, 'epoch': 0.27}\n",
      "27%|██▋       | 169/625 [34:51<1:52:36, 14.82s/it]\n",
      "27%|██▋       | 170/625 [35:08<1:57:39, 15.52s/it]\n",
      "{'loss': 1.2054, 'learning_rate': 0.00026, 'epoch': 0.27}\n",
      "27%|██▋       | 170/625 [35:08<1:57:39, 15.52s/it]\n",
      "27%|██▋       | 170/625 [35:08<1:57:41, 15.52s/it]\n",
      "{'loss': 1.2057, 'learning_rate': 0.00026, 'epoch': 0.27}\n",
      "27%|██▋       | 170/625 [35:08<1:57:41, 15.52s/it]\n",
      "27%|██▋       | 171/625 [35:16<1:40:13, 13.24s/it]\n",
      "{'loss': 1.1048, 'learning_rate': 0.0002594285714285714, 'epoch': 0.27}\n",
      "27%|██▋       | 171/625 [35:16<1:40:13, 13.24s/it]\n",
      "27%|██▋       | 171/625 [35:16<1:40:13, 13.25s/it]\n",
      "{'loss': 1.1034, 'learning_rate': 0.0002594285714285714, 'epoch': 0.27}\n",
      "27%|██▋       | 171/625 [35:16<1:40:13, 13.25s/it]\n",
      "28%|██▊       | 172/625 [35:23<1:26:26, 11.45s/it]\n",
      "{'loss': 1.0595, 'learning_rate': 0.0002588571428571428, 'epoch': 0.28}\n",
      "28%|██▊       | 172/625 [35:23<1:26:26, 11.45s/it]\n",
      "28%|██▊       | 172/625 [35:23<1:26:27, 11.45s/it]\n",
      "{'loss': 1.0594, 'learning_rate': 0.0002588571428571428, 'epoch': 0.28}\n",
      "28%|██▊       | 172/625 [35:23<1:26:27, 11.45s/it]\n",
      "28%|██▊       | 173/625 [35:28<1:12:32,  9.63s/it]\n",
      "{'loss': 1.1858, 'learning_rate': 0.00025828571428571426, 'epoch': 0.28}\n",
      "28%|██▊       | 173/625 [35:28<1:12:32,  9.63s/it]\n",
      "28%|██▊       | 173/625 [35:29<1:12:32,  9.63s/it]\n",
      "{'loss': 1.1858, 'learning_rate': 0.00025828571428571426, 'epoch': 0.28}\n",
      "28%|██▊       | 173/625 [35:29<1:12:32,  9.63s/it]\n",
      "28%|██▊       | 174/625 [35:48<1:34:04, 12.52s/it]\n",
      "{'loss': 0.9495, 'learning_rate': 0.0002577142857142857, 'epoch': 0.28}\n",
      "28%|██▊       | 174/625 [35:48<1:34:04, 12.52s/it]\n",
      "28%|██▊       | 174/625 [35:48<1:34:05, 12.52s/it]\n",
      "{'loss': 0.948, 'learning_rate': 0.0002577142857142857, 'epoch': 0.28}\n",
      "28%|██▊       | 174/625 [35:48<1:34:05, 12.52s/it]\n",
      "28%|██▊       | 175/625 [36:00<1:32:24, 12.32s/it]\n",
      "{'loss': 0.9602, 'learning_rate': 0.0002571428571428571, 'epoch': 0.28}\n",
      "28%|██▊       | 175/625 [36:00<1:32:24, 12.32s/it]\n",
      "28%|██▊       | 175/625 [36:00<1:32:25, 12.32s/it]\n",
      "{'loss': 0.9607, 'learning_rate': 0.0002571428571428571, 'epoch': 0.28}\n",
      "28%|██▊       | 175/625 [36:00<1:32:25, 12.32s/it]\n",
      "28%|██▊       | 176/625 [36:05<1:17:53, 10.41s/it]\n",
      "{'loss': 1.0397, 'learning_rate': 0.00025657142857142854, 'epoch': 0.28}\n",
      "28%|██▊       | 176/625 [36:05<1:17:53, 10.41s/it]\n",
      "28%|██▊       | 176/625 [36:06<1:17:54, 10.41s/it]\n",
      "{'loss': 1.0409, 'learning_rate': 0.00025657142857142854, 'epoch': 0.28}\n",
      "28%|██▊       | 176/625 [36:06<1:17:54, 10.41s/it]\n",
      "28%|██▊       | 177/625 [36:15<1:15:57, 10.17s/it]\n",
      "{'loss': 1.0464, 'learning_rate': 0.000256, 'epoch': 0.28}\n",
      "28%|██▊       | 177/625 [36:15<1:15:57, 10.17s/it]\n",
      "28%|██▊       | 177/625 [36:15<1:15:57, 10.17s/it]\n",
      "{'loss': 1.0478, 'learning_rate': 0.000256, 'epoch': 0.28}\n",
      "28%|██▊       | 177/625 [36:15<1:15:57, 10.17s/it]\n",
      "28%|██▊       | 178/625 [36:27<1:20:37, 10.82s/it]\n",
      "{'loss': 1.0155, 'learning_rate': 0.0002554285714285714, 'epoch': 0.28}\n",
      "28%|██▊       | 178/625 [36:27<1:20:37, 10.82s/it]\n",
      "28%|██▊       | 178/625 [36:28<1:20:39, 10.83s/it]\n",
      "{'loss': 1.0169, 'learning_rate': 0.0002554285714285714, 'epoch': 0.28}\n",
      "28%|██▊       | 178/625 [36:28<1:20:39, 10.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m estimator\u001b[39m.\u001b[39;49mlogs()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1523\u001b[0m, in \u001b[0;36mEstimatorBase.logs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1518\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display the logs for Estimator's training job.\u001b[39;00m\n\u001b[1;32m   1519\u001b[0m \n\u001b[1;32m   1520\u001b[0m \u001b[39m    If the output is a tty or a Jupyter cell, it will be color-coded based\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[39m    on which instance the log entry is from.\u001b[39;00m\n\u001b[1;32m   1522\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1523\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_training_job\u001b[39m.\u001b[39;49mname, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:5766\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5745\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogs_for_job\u001b[39m(\u001b[39mself\u001b[39m, job_name, wait\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, poll\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, log_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m\"\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   5746\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5747\u001b[0m \n\u001b[1;32m   5748\u001b[0m \u001b[39m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5764\u001b[0m \u001b[39m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5765\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5766\u001b[0m     _logs_for_job(\u001b[39mself\u001b[39;49m, job_name, wait, poll, log_type, timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:7945\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   7942\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m LogState\u001b[39m.\u001b[39mCOMPLETE:\n\u001b[1;32m   7943\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 7945\u001b[0m time\u001b[39m.\u001b[39;49msleep(poll)\n\u001b[1;32m   7947\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m LogState\u001b[39m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   7948\u001b[0m     state \u001b[39m=\u001b[39m LogState\u001b[39m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-job-experiment-1712469870-497e\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (Optional) Copy S3 model artifact to local directory\n",
    "S3에 저장된 모델 아티팩트를 로컬 경로로 복사하여 압축을 해제합니다. 필요 시 로컬 환경에서 모델을 로드하여 추론을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "local_model_dir = 'model_from_sagemaker'\n",
    "\n",
    "if not os.path.exists(local_model_dir):\n",
    "    os.makedirs(local_model_dir)\n",
    "\n",
    "!aws s3 cp {estimator.model_data} {local_model_dir}/model.tar.gz\n",
    "!tar -xzf {local_model_dir}/model.tar.gz -C {local_model_dir}\n",
    "!rm {local_model_dir}/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store train_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                     2.1.0\n",
      "torch-model-archiver      0.7.1b20230208\n",
      "torch-workflow-archiver   0.2.11b20231012\n",
      "torchaudio                2.1.0\n",
      "torchdata                 0.7.0\n",
      "torchserve                0.8.2b20230828\n",
      "torchtext                 0.16.0\n",
      "torchvision               0.16.0\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
