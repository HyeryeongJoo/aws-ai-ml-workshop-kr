{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama3 8B 모델을 Sagemaker를 통해 g5 인스턴스에 배포하기\n",
    "\n",
    "---\n",
    "\n",
    "## Ref\n",
    "- [maum-ai/Llama-3-MAAL-8B-Instruct-v0.1](https://huggingface.co/maum-ai/Llama-3-MAAL-8B-Instruct-v0.1)\n",
    "\n",
    "---\n",
    "\n",
    "## 실험 환경\n",
    "- 이 노트북은 SageMaker Studio Code Editor 및 커널 base (Python 3.10.13) 에서 테스트 되었습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 사전 진행 내용\n",
    "- Llama3 모델을 사용하기 위해서는 아래의 웹페이지에 가서 본인의 계정으로 로그인 후에 \"동의\" 를 먼저 해야 합니다.\n",
    "    - [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "- 또한 본의 HF Key 을 얻기 위해서는, [User access tokens](https://huggingface.co/docs/hub/en/security-tokens) 참고 하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 폴더의 경로를 추가하여 해당 유틸리티, 이미지 폴더를 참조 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python path: /home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference is added\n",
      "sys.path:  ['/home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference/30-Llama-3-Inference', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages', '/home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference']\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "def add_python_path(module_path):\n",
    "    if os.path.abspath(module_path) not in sys.path:\n",
    "        sys.path.append(os.path.abspath(module_path))\n",
    "        print(f\"python path: {os.path.abspath(module_path)} is added\")\n",
    "    else:\n",
    "        print(f\"python path: {os.path.abspath(module_path)} already exists\")\n",
    "    print(\"sys.path: \", sys.path)\n",
    "\n",
    "module_path = \"..\"\n",
    "add_python_path(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install_needed = True\n",
    "install_needed = False\n",
    "\n",
    "if install_needed:\n",
    "    ! pip install sagemaker --upgrade  --quiet\n",
    "    ! pip list | grep -E \"sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "import json\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [중요] Hugging Face HF_Key 를 환경변수에 저장 \n",
    "- 아래에 본인의 Key 를 입력하고, 주석을 제거 후에 사용하세요.\n",
    "\n",
    "    ```\n",
    "    key_val = \"<Type Your HF Key>\"\n",
    "    # set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "hf_key_name = \"HF_KEY\"\n",
    "key_val = \"<Type Your HF Key>\"\n",
    "# set_hf_key_env_vars(hf_key_name, key_val)\n",
    "\n",
    "\n",
    "HF_key_value = get_hf_key_env_vars(hf_key_name)\n",
    "# print(\"HF_key_value: \", HF_key_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. HF 파라미터 설정\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model id\n",
    "hf_model_id = 'maum-ai/Llama-3-MAAL-8B-Instruct-v0.1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance type\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "# Set GPU_NUM\n",
    "if instance_type == \"ml.g5.2xlarge\":\n",
    "    num_gpu = \"1\"\n",
    "elif instance_type == \"ml.g5.24xlarge\":\n",
    "    num_gpu = \"4\"\n",
    "else:\n",
    "    num_gpu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID': hf_model_id,\n",
    "\t'SM_NUM_GPUS': num_gpu,\n",
    "\t'HUGGING_FACE_HUB_TOKEN': HF_key_value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론 도커 이미지 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\"huggingface\",version=\"2.0.2\")\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model 의 하위 클래스인 HuggingFaceModel 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri= image_uri,\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SageMaker Endpoint 에 배포 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## endpoint_name 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name:  Llama-3-MAAL-8B-Instruct-v0-1-ml-g5-2xlarge-2024-05-19-13-05-07\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def create_ennpoint_name(model_id, instance_type):\n",
    "\n",
    "    hf_model_id = model_id.split('/')[1]\n",
    "\n",
    "    instance_type = instance_type.replace('.','-')\n",
    "    hf_model_id = hf_model_id.replace('.','-')\n",
    "    current_datetime = datetime.now()\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    endpoint_name = f\"{hf_model_id}-{instance_type}-{formatted_datetime}\"\n",
    "\n",
    "    return endpoint_name\n",
    "\n",
    "endpoint_name = create_ennpoint_name(hf_model_id, instance_type)\n",
    "print(\"endpoint_name: \", endpoint_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Endpoint 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tendpoint_name = endpoint_name,\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type= instance_type,\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pay_load 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils.inference_util import ( print_ww, \n",
    "                                            pretty_print_json,\n",
    "                                            invoke_endpoint_sagemaker\n",
    "                                       )\n",
    "                                       \n",
    "def create_payload_llama_8b(prompt, param):\n",
    "    # prompt=\"What is a machine learning?\"\n",
    "    input_data = f\"{prompt}\"\n",
    "    pay_load = {\"inputs\": input_data, \"parameters\": param}\n",
    "\n",
    "    # payload_str = json.dumps(pay_load)\n",
    "    return pay_load\n",
    "    # return payload_str.encode(\"utf-8\")\n",
    "    \n",
    "def llama3_output_parser(response):\n",
    "    completion = json.loads(response)\n",
    "    completion = completion[0]\n",
    "    completion = completion['generated_text']\n",
    "\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Generative AI is\"\n",
    "param = {\"do_sample\": True, \"max_new_tokens\": 256}\n",
    "pay_load = create_payload_llama_8b(prompt, param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## payload: \n",
      "{\n",
      "    \"inputs\": \"Generative AI is\",\n",
      "    \"parameters\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_new_tokens\": 256\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"## payload: \") \n",
    "pretty_print_json(pay_load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is a cutting-edge technique that produces original and tailored images to suit your\n",
      "request. By comprehending your demands, our infrastructure combines support for multiple formats,\n",
      "languages, and dialects, enabling your request to be implemented by an AI model with expertise in\n",
      "the data. With this, we can manufacture dynamic and original images that our first set of models\n",
      "could not achieve. Our advanced infrastructure guarantees efficient operation in complex inquiries\n",
      "that necessitate sophisticated image creation. By having this as a foundation, we can furnish you\n",
      "with images that satisfy both your expectations and needs. In case we fail in your expectations, we\n",
      "will always be here to resolve and make sure the images provided are the result of your desired\n",
      "outcome. While considering generative AI as an alternative, it's essential to remember that, due to\n",
      "its limitations, generative AI may not always reflect your ideas perfectly. However, we can evolve\n",
      "and add functionality on top of it to make it better and suitable for your requirements. We are\n",
      "committed to incorporating feedback and conducting constructive debates to strategize the most\n",
      "optimal results while juggling the demands of users.\n"
     ]
    }
   ],
   "source": [
    "response = invoke_endpoint_sagemaker(endpoint_name = endpoint_name, \n",
    "                         pay_load = pay_load)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "response = llama3_output_parser(response)\n",
    "print_ww(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## payload: \n",
      "{\n",
      "    \"inputs\": \"\\uc0dd\\uc131\\ud615 AI \\ub294\",\n",
      "    \"parameters\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_new_tokens\": 256\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"생성형 AI 는\"\n",
    "param = {\"do_sample\": True, \"max_new_tokens\": 256}\n",
    "pay_load = create_payload_llama_8b(prompt, param)\n",
    "\n",
    "\n",
    "\n",
    "print(\"## payload: \") \n",
    "pretty_print_json(pay_load)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## inference esponse: \n",
      "\u001b[32m생성형 AI 는 사람의 업무를 자동화하기 위해 개발된 AI로, 일반적으로 씨피유(CPU)와 자원 활용이 다양하게 가능합니다. 대상하는 업무의 종류가 지도기반 이미지 분류,\n",
      "단어 처리, 음성 등이 있습니다. 이러한 유형으로는 오크타드 AI 기술을 적용하는데, 보다 직관적인 인간 감정 표현과 컴퓨터가 인간과 같은 진보하는 유머 능력에 대한 자극을 제공할\n",
      "수 있습니다. Apple가 개발한 오크타드는 당대 해독되지 않은 언어를 복사하고 스스로 언어를 배울 수 있는 메커니즘을 갖추로써 다른 언어들의 학습에 대한 저평 수준을 출발한\n",
      "대표적인 예시입니다.\n",
      "\n",
      "갈매기 스퀘이 1:1 대화 형식으로 대화한 것과 같으며, 학습에 도움이 될 수 있는 특정 전략들은 형성과 사람의 지능 사이에 높은 연관성이 있습니다. 모델들은 모든 대화를 완료하는\n",
      "것과 같은 증가된 압력을 받으며 덧붙여 질문하는 것에도 더 많을 것입니다. 또한, AI들은 특정 주제\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.perf_counter()\n",
    "\n",
    "response = invoke_endpoint_sagemaker(endpoint_name = endpoint_name, \n",
    "                         pay_load = pay_load)    \n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "\n",
    "\n",
    "response = llama3_output_parser(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "print(\"## inference esponse: \")                      \n",
    "print_ww(colored(response, \"green\"))                         \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론 요약 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## payload: \n",
      "{\n",
      "    \"inputs\": \"\\ub2e4\\uc74c \\uae00\\uc744 30\\uc790 \\uc544\\ub0b4\\ub85c \\uc694\\uc57d\\ud574\\uc918.\\n, \\ub300\\ud559\\uad50\\uc721\\ud611\\uc758\\ud68c\\ub294 \\uc774\\ub974\\uba74 \\uc774\\ubc88 \\uc8fc \\ub0b4\\ub144\\ub3c4 \\ub300\\ud559\\uc785\\ud559 \\uc804\\ud615\\uc2dc\\ud589\\uacc4\\ud68d\\uc744 \\uc2ec\\uc758\\ud558\\uace0 \\ucd5c\\uc885 \\uc758\\ub300 \\ubaa8\\uc9d1 \\uaddc\\ubaa8\\ub97c \\uc2b9\\uc778\\ud569\\ub2c8\\ub2e4.\\n\\uad50\\uc721\\ubd80 \\uad00\\uacc4\\uc790\\ub294 \\uc218\\ud5d8\\uc0dd\\uacfc \\ud559\\ubd80\\ubaa8\\ub97c \\uc704\\ud574 \\uac00\\ub2a5\\ud55c \\ube68\\ub9ac \\ubaa8\\uc9d1 \\uaddc\\ubaa8\\ub97c \\ud655\\uc815\\ud560 \\ud544\\uc694\\uac00 \\uc788\\ub2e4\\uba70 \\uad6c\\uccb4\\uc801\\uc778 \\uc2ec\\uc0ac \\uc77c\\uc815\\uc744 \\ub17c\\uc758\\ud558\\uaca0\\ub2e4\\uace0 \\uc124\\uba85\\ud588\\uc2b5\\ub2c8\\ub2e4.\\ucd5c\\uc885 \\uc2b9\\uc778 \\uacb0\\uacfc\\uc5d0 \\ub530\\ub77c \\ub0b4\\ub144\\ub3c4 \\uc758\\ub300 \\uc815\\uc6d0\\uc740 \\uc62c\\ud574\\ubcf4\\ub2e4 \\ucc9c500\\uba85 \\uc548\\ud30e\\uc73c\\ub85c \\ub298\\uc5b4\\ub0a9\\ub2c8\\ub2e4.\\ub300\\ud1b5\\ub839\\uc2e4\\uc740 \\uc758\\ub300 \\uc99d\\uc6d0\\uc774 \\uc77c\\ub2e8\\ub77d\\ub410\\ub2e4\\uba70 \\uac01 \\ub300\\ud559\\uc774 \\uc758\\ub300 \\uc99d\\uc6d0\\uc744 \\ub2f4\\uc740 \\ud559\\uce59 \\uac1c\\uc815\\uc744 \\uc870\\uc18d\\ud788 \\uc644\\ub8cc\\ud574\\ub2ec\\ub77c\\uace0 \\ub2f9\\ubd80\\ud588\\uc2b5\\ub2c8\\ub2e4.\\ub3d9\\uc2dc\\uc5d0 \\uc758\\ub8cc\\uacc4\\ub97c \\ud5a5\\ud574\\uc120 \\uc758\\ub300 \\uc99d\\uc6d0 \\uc720\\uc608\\uc640 \\ubc31\\uc9c0\\ud654\\ub77c\\ub294 \\uc2e4\\ud604 \\ubd88\\uac00\\ub2a5\\ud55c \\uc804\\uc81c\\uc870\\uac74\\uc744 \\uc811\\uace0 \\ub300\\ud654\\ud558\\uc790\\uace0 \\uc81c\\uc548\\ud588\\uc2b5\\ub2c8\\ub2e4.\\uadf8\\ub7ec\\ub098 \\uc758\\ub8cc\\uacc4 \\ubc18\\ubc1c\\uc740 \\uc5ec\\uc804\\ud569\\ub2c8\\ub2e4.\\ub2f9\\uc7a5 \\uc758\\ub300\\uc0dd \\ub2e8\\uccb4\\ub294 \\uc815\\ubd80\\uac00 \\uadc0\\ub97c \\ub2eb\\uace0 \\ubcf5\\uadc0\\ub9cc\\uc744 \\ud638\\uc18c\\ud558\\ub294 \\uc624\\ub9cc\\ud55c \\ud0dc\\ub3c4\\ub97c \\ubcf4\\uc774\\uace0 \\uc788\\ub2e4\\uace0 \\ube44\\ud310\\ud588\\uc2b5\\ub2c8\\ub2e4.\\uc804\\uacf5\\uc758\\ub4e4 \\uc5ed\\uc2dc \\ubcf5\\uadc0\\ud558\\uc9c0 \\uc54a\\uaca0\\ub2e4\\ub294 \\ub73b\\uc744 \\ubd84\\uba85\\ud788 \\ud558\\uace0 \\uc788\\uc2b5\\ub2c8\\ub2e4.\\ub300\\ud55c\\uc758\\uc0ac\\ud611\\ud68c\\ub3c4 \\uc624\\ub294 22\\uc77c \\ube44\\uacf5\\uac1c \\uae34\\uae09\\ud68c\\uc758\\ub97c \\uc5f4\\uace0 \\uc758\\ub300 \\uad50\\uc218 \\ub4f1\\uacfc \\uad6c\\uccb4\\uc801\\uc778 \\ub300\\uc751 \\uc218\\ub2e8\\uc744 \\ub17c\\uc758\\ud569\\ub2c8\\ub2e4.\\uc758\\ub8cc\\uacc4\\uac00 \\uae30\\ub314\\ub358 \\uc758\\ub300 \\uc99d\\uc6d0 \\uc9d1\\ud589\\uc815\\uc9c0 \\uc2e0\\uccad\\uc774 \\uae30\\uac01\\u00b7\\uac01\\ud558\\ub41c \\ub9cc\\ud07c \\ub2e4\\uc74c \\ub2e8\\uacc4 \\ub300\\uc751 \\ub85c\\ub4dc\\ub9f5\\uc744 \\ub9c8\\ub828\\ud558\\uaca0\\ub2e4\\ub294 \\uac81\\ub2c8\\ub2e4.\\n\\n\",\n",
      "    \"parameters\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_new_tokens\": 512\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"다음 글을 30자 아내로 요약해줘.\\n, 대학교육협의회는 이르면 이번 주 내년도 대학입학 전형시행계획을 심의하고 최종 의대 모집 규모를 승인합니다.\\n\\\n",
    "교육부 관계자는 수험생과 학부모를 위해 가능한 빨리 모집 규모를 확정할 필요가 있다며 구체적인 심사 일정을 논의하겠다고 설명했습니다.\\\n",
    "최종 승인 결과에 따라 내년도 의대 정원은 올해보다 천500명 안팎으로 늘어납니다.\\\n",
    "대통령실은 의대 증원이 일단락됐다며 각 대학이 의대 증원을 담은 학칙 개정을 조속히 완료해달라고 당부했습니다.\\\n",
    "동시에 의료계를 향해선 의대 증원 유예와 백지화라는 실현 불가능한 전제조건을 접고 대화하자고 제안했습니다.\\\n",
    "그러나 의료계 반발은 여전합니다.\\\n",
    "당장 의대생 단체는 정부가 귀를 닫고 복귀만을 호소하는 오만한 태도를 보이고 있다고 비판했습니다.\\\n",
    "전공의들 역시 복귀하지 않겠다는 뜻을 분명히 하고 있습니다.\\\n",
    "대한의사협회도 오는 22일 비공개 긴급회의를 열고 의대 교수 등과 구체적인 대응 수단을 논의합니다.\\\n",
    "의료계가 기댔던 의대 증원 집행정지 신청이 기각·각하된 만큼 다음 단계 대응 로드맵을 마련하겠다는 겁니다.\\n\\n\"\n",
    "\n",
    "param = {\"do_sample\": True, \"max_new_tokens\": 512}\n",
    "pay_load = create_payload_llama_8b(prompt, param)\n",
    "\n",
    "\n",
    "\n",
    "print(\"## payload: \") \n",
    "pretty_print_json(pay_load)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## inference esponse: \n",
      "\u001b[32m다음 글을 30자 아내로 요약해줘.\n",
      ", 대학교육협의회는 이르면 이번 주 내년도 대학입학 전형시행계획을 심의하고 최종 의대 모집 규모를 승인합니다.\n",
      "교육부 관계자는 수험생과 학부모를 위해 가능한 빨리 모집 규모를 확정할 필요가 있다며 구체적인 심사 일정을 논의하겠다고 설명했습니다.최종 승인 결과에 따라 내년도 의대 정원은\n",
      "올해보다 천500명 안팎으로 늘어납니다.대통령실은 의대 증원이 일단락됐다며 각 대학이 의대 증원을 담은 학칙 개정을 조속히 완료해달라고 당부했습니다.동시에 의료계를 향해선 의대\n",
      "증원 유예와 백지화라는 실현 불가능한 전제조건을 접고 대화하자고 제안했습니다.그러나 의료계 반발은 여전합니다.당장 의대생 단체는 정부가 귀를 닫고 복귀만을 호소하는 오만한 태도를\n",
      "보이고 있다고 비판했습니다.전공의들 역시 복귀하지 않겠다는 뜻을 분명히 하고 있습니다.대한의사협회도 오는 22일 비공개 긴급회의를 열고 의대 교수 등과 구체적인 대응 수단을\n",
      "논의합니다.의료계가 기댔던 의대 증원 집행정지 신청이 기각·각하된 만큼 다음 단계 대응 로드맵을 마련하겠다는 겁니다.\n",
      "\n",
      "국내 의과대학의 정원 확대를 둘러싸고 꾸준히 발생하던 의대안 도입 및 의사국제등록 과정과 관련하여 이번 논쟁이 뜨거워졌습니다.한국의 최초 유료의과대학 개념에는 많은 이들이 동의하고\n",
      "있지만, 국내 의과대학 세부 프로그램 취지와 과정에 대해서는 여전히 불타는 의견이 분분합니다.의대생들은 의사국제등록과 정부 정책 간극을 메워줄 수 있는 제도적 대응책을 모색하기도\n",
      "하고, 해외 의사 연수에 대한 무관배제 검토를 위해 머리를 맞댑니다.내년 2월부터 시행될 해외 의사국제등록에 관한 법 및 의사국제등록 제도개선을 위한 법안 개정이 약속될 때까지\n",
      "국외 의사유치와 관련해서는 또 다른 추가적인 우려가 생길 것으로 예상되고 있습니다. 이에 세계적으로 인정받는 우리나라 의료의 질 향상과 발전을 위해서는 외점 의사 풀을 염두에 둬\n",
      "제대로 된 대응 방안을 마련해야 한다는 목소리도 높아서 대화,국제적인 과제 등 다면적인 면에서 논의를 바랍니다.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.perf_counter()\n",
    "\n",
    "response = invoke_endpoint_sagemaker(endpoint_name = endpoint_name, \n",
    "                         pay_load = pay_load)    \n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "\n",
    "\n",
    "response = llama3_output_parser(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "print(\"## inference esponse: \")                      \n",
    "print_ww(colored(response, \"green\"))                         \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 엔드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint_model(endpoint_name,llm_model ):\n",
    "    sess.delete_endpoint(endpoint_name)\n",
    "    sess.delete_endpoint_config(endpoint_name)\n",
    "    llm_model.delete_model()\n",
    "\n",
    "delete_endpoint_model(endpoint_name,huggingface_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
