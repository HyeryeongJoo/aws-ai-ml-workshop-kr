{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f604ee1d-3516-4b8f-8acf-6cbe8a286274",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Deploy Llama 3.1 through vLLM on SageMaker Endpoint using LMI container from DJL.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c88ae-615f-4f81-acf2-79247338e30b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use DJL with the SageMaker Python SDK\n",
    "- SageMaker Python SDK를 사용하면 Deep Java Library를 이용하여 Amazon SageMaker에서 모델을 호스팅할 수 있습니다. <BR>\n",
    "- Deep Java Library (DJL) Serving은 DJL이 제공하는 고성능 범용 독립형 모델 서빙 솔루션입니다. DJL Serving은 다양한 프레임워크로 학습된 모델을 로드하는 것을 지원합니다. <BR>\n",
    "- SageMaker Python SDK를 사용하면 DeepSpeed와 HuggingFace Accelerate와 같은 백엔드를 활용하여 DJL Serving으로 대규모 모델을 호스팅할 수 있습니다. <BR>\n",
    "- DJL Serving의 지원 버전에 대한 정보는 [AWS 문서](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html)를 참조하십시오. <BR>\n",
    "- 최신 지원 버전을 사용하는 것을 권장합니다. 왜냐하면 그곳에 우리의 개발 노력이 집중되어 있기 때문입니다. <BR>\n",
    "- SageMaker Python SDK 사용에 대한 일반적인 정보는 [SageMaker Python SDK 사용하기](https://sagemaker.readthedocs.io/en/v2.139.0/overview.html#using-the-sagemaker-python-sdk)를 참조하십시오.\n",
    "    \n",
    "REF: [BLOG] [Deploy LLM with vLLM on SageMaker in only 13 lines of code](https://mrmaheshrajput.medium.com/deploy-llm-with-vllm-on-sagemaker-in-only-13-lines-of-code-1601f780c0cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa36dc1-7a95-4a53-bb9b-8e561e9230bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Depoly model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570df89-db29-4643-adff-55e09880c3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94854307-4322-4109-85dc-ec59d1630066",
   "metadata": {
    "tags": []
   },
   "source": [
    "- [Avalable DLC (Deep Learning Containers)](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a351-1e53-4ea0-ae6f-e1ddc3c3aea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "sm_autoscaling_client = boto3.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95536d9-4e74-4279-9281-0bd2a1095a15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d4466-3ea2-47ea-99cd-ee8032056986",
   "metadata": {},
   "source": [
    " - [[DOC] DJL for serving](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e483d-0080-423f-b434-912386dc2f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-90B-Vision-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c71dfc-7ca3-4081-b7da-6ffc81ffb255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"djl-lmi\", version=\"0.30.0\", region=region\n",
    ")\n",
    "container_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1282b631-6a28-4803-8e38-cb49f5c730fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.48xlarge\"\n",
    "container_startup_health_check_timeout = 900\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"Meta-Llama-3-2-90B-Vision-Instruct\")\n",
    "\n",
    "print (f'container_uri: {container_uri}')\n",
    "print (f'container_startup_health_check_timeout: {container_startup_health_check_timeout}')\n",
    "print (f'instance_type: {instance_type}')\n",
    "print (f'endpoint_name: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca14bd8-c9d7-42cd-9bbc-3c6832359809",
   "metadata": {},
   "source": [
    "### Creat model with env variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36e275-b199-4ce3-b1b2-5ecca0554210",
   "metadata": {},
   "source": [
    "- Target model: [DeepSeek-Coder-V2-Light-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9449ef-905a-4224-97eb-9fd02aaab83e",
   "metadata": {},
   "source": [
    "- **[Backend for attention computation in vLLM](https://docs.vllm.ai/en/latest/serving/env_vars.html)**\n",
    "    - Available options:\n",
    "        - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n",
    "        - \"FLASH_ATTN\": use FlashAttention\n",
    "        - \"XFORMERS\": use XFormers\n",
    "        - \"ROCM_FLASH\": use ROCmFlashAttention\n",
    "        - \"FLASHINFER\": use flashinfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915352b-2ef9-4b3b-a795-fac419793221",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **'\"OPTION_DISABLE_FLASH_ATTN\": \"false\"'** is for HF Accelerate with Seq-Scheduler\n",
    "    - It will be ignored when using vLLM beckend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4886c8-87e2-4b09-8c65-b52b8e32889b",
   "metadata": {},
   "source": [
    "- [[DOC] DJL-Container and Model Configurations (info. about properties)](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/deployment_guide/configurations.html)\n",
    "- [[DOC] Backend Specific Configurations](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a9569-9cfd-461e-914e-6be8d8fe952d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deploy_env = {\n",
    "    \"HF_MODEL_ID\": model_id,\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"32\",\n",
    "    \"OPTION_DTYPE\":\"fp16\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"4096\",\n",
    "    \"OPTION_ENFORCE_EAGER\": \"true\", ## For llama 3.2\n",
    "    \"VLLM_ATTENTION_BACKEND\": \"XFORMERS\",\n",
    "    #\"OPTION_DISABLE_FLASH_ATTN\": \"false\", ## HF Accelerate with Seq-Scheduler\n",
    "    \"HF_TOKEN\": \"<your token>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280e310-0690-4f8f-9d8e-f77e5f37b405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = sagemaker.Model(\n",
    "    image_uri=container_uri, \n",
    "    role=role,\n",
    "    env=deploy_env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebca82-7727-4360-b144-5017f54bc68b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129dc158-7ef5-48d4-b58e-bdd4913b31dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=container_startup_health_check_timeout,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e65359c-0c75-4fcb-a813-1c4c11b8783a",
   "metadata": {},
   "source": [
    "## 2. Invocation (Generate Text using the endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b26a7-bb0e-4970-9c53-38dc0f1fcb00",
   "metadata": {},
   "source": [
    "### Get a predictor for your endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc36cb-ef32-414c-9303-798d8aa5ac2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205ee8c-ea67-4abf-b52e-e6c31d2c5fdc",
   "metadata": {},
   "source": [
    "### Make a prediction with your endpoint (with stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ae118-08e2-4dc3-970c-6fcb2ae2028f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3294351-9f0a-40be-bd89-b157da99d06e",
   "metadata": {},
   "source": [
    "- **With chat template**\n",
    "    - [DJL Chat Completions API Schema](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/chat_input_output_schema.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1126b-96d1-407c-a58b-067d69b1f11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_payload(chat):\n",
    "\n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"messages\": chat,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"stream\": True,\n",
    "        #\"stop\": terminators,\n",
    "        \"ignore_eos\": False\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06481860-93d6-405c-bd7f-a1c552ae5ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 다양한 코딩 태스크를 위한 프롬프트 리스트\n",
    "chat = [\n",
    "    #{\"role\": \"system\", \"content\": \"너는 질의응답 챗봇입니다. 사용자의 질문의 의도를 파악하여 답변합니다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I would like to get better at basketball. Can you provide me a 3 month plan to improve my skills?\"},\n",
    "    #{\"role\": \"user\", \"content\": \"철수가 20개의 연필을 가지고 있었는데 영희가 절반을 가져가고 민수가 남은 5개를 가져갔으면 철수에게 남은 연필의 갯수는 몇개인가요?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e24b5-895f-41fd-ba70-b1e520302aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Invoke the endpoint\n",
    "resp = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    # Body=json.dumps(body), \n",
    "    Body=generate_payload(chat), \n",
    "    \n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "print(\"Generated response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "buffer = \"\"\n",
    "string = \"\" \n",
    "for event in resp['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        buffer += chunk\n",
    "        try:\n",
    "            # Try to parse the buffer as JSON\n",
    "            data = json.loads(buffer)\n",
    "            if 'choices' in data:\n",
    "                print(data['choices'][0]['delta']['content'], end='', flush=True)\n",
    "                string += data['choices'][0]['delta']['content'] \n",
    "            buffer = \"\"  # Clear the buffer after successful parsing\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, keep the buffer for the next iteration\n",
    "            pass\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f8a98-ae32-426f-96a8-64e996bfc80e",
   "metadata": {},
   "source": [
    "## 3. Performance checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b89694-344d-4033-809d-a262d4923391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = \"<your token>\"\n",
    "!huggingface-cli login --token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147a728-7457-4447-923a-6007b13987d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class LLMPerformanceTester:\n",
    "    \n",
    "    def __init__(self, endpoint_name, model_id):\n",
    "        self.runtime_client = boto3.client('sagemaker-runtime')\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "    def invoke_endpoint(self, prompt):\n",
    "        \n",
    "        print (prompt)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            \n",
    "            resp = self.runtime_client.invoke_endpoint_with_response_stream(\n",
    "                EndpointName=endpoint_name, \n",
    "                # Body=json.dumps(body), \n",
    "                Body=generate_payload(prompt), \n",
    "\n",
    "                ContentType=\"application/json\"\n",
    "            )\n",
    "            print(\"Generated response:\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            buffer = \"\"\n",
    "            response_body = \"\"\n",
    "            for event in resp['Body']:\n",
    "                if 'PayloadPart' in event:\n",
    "                    chunk = event['PayloadPart']['Bytes'].decode()\n",
    "                    buffer += chunk\n",
    "                    try:\n",
    "                        # Try to parse the buffer as JSON\n",
    "                        data = json.loads(buffer)\n",
    "                        if 'choices' in data:\n",
    "                            #print(data['choices'][0]['delta']['content'], end='', flush=True)\n",
    "                            response_body += data['choices'][0]['delta']['content'] \n",
    "                        buffer = \"\"  # Clear the buffer after successful parsing\n",
    "                    except json.JSONDecodeError:\n",
    "                        # If parsing fails, keep the buffer for the next iteration\n",
    "                        pass\n",
    "            \n",
    "            end_time = time.time()\n",
    "            latency = end_time - start_time\n",
    "\n",
    "            token = len(self.tokenizer.encode(response_body))\n",
    "            print (token)\n",
    "            #print (response_body)\n",
    "            print(\"\\n\" + \"-\" * 40)\n",
    "            return {\n",
    "                'success': True,\n",
    "                'latency': latency,\n",
    "                'response': response_body,\n",
    "                'token': token\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'latency': time.time() - start_time\n",
    "            }\n",
    "\n",
    "    def run_concurrent_test(self, prompts, concurrent_requests=5):\n",
    "        results = []\n",
    "        start_time = time.time()\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_requests) as executor:\n",
    "            futures = [executor.submit(self.invoke_endpoint, prompt) for prompt in prompts]\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(prompts)):\n",
    "                results.append(future.result())\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_latency = end_time - start_time\n",
    "        return results, total_latency\n",
    "\n",
    "    def analyze_results(self, results, total_latency):\n",
    "        successful_requests = [r for r in results if r['success']]\n",
    "        failed_requests = [r for r in results if not r['success']]\n",
    "        \n",
    "        if not successful_requests:\n",
    "            return \"No successful requests\"\n",
    "            \n",
    "        latencies = [r['latency'] for r in successful_requests]\n",
    "        tokens = [r['token'] for r in successful_requests]\n",
    "        \n",
    "        analysis = {\n",
    "            'total_requests': len(results),\n",
    "            'successful_requests': len(successful_requests),\n",
    "            'failed_requests': len(failed_requests),\n",
    "            #'throughput': len(successful_requests) / sum(latencies),  # requests per second\n",
    "            'throughput': sum(tokens) / total_latency,  # requests per second\n",
    "            'latency_stats': {\n",
    "                'mean': np.mean(latencies),\n",
    "                'median': np.median(latencies),\n",
    "                'p95': np.percentile(latencies, 95),\n",
    "                'p99': np.percentile(latencies, 99),\n",
    "                'min': min(latencies),\n",
    "                'max': max(latencies)\n",
    "            }\n",
    "        }\n",
    "        return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884e6d0-d08d-4473-9533-dcd58a1fb14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 테스트 설정\n",
    "#endpoint_name = \"Meta-Llama-3-2-3B-Instruct-2024-11-08-11-00-28-868\"\n",
    "test_prompts = [\n",
    "    [{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"How does blockchain work?\"}]\n",
    "    # ... 더 많은 테스트 프롬프트 추가\n",
    "] * 20  # 각 프롬프트를 5번 반복\n",
    "\n",
    "# 테스터 초기화 및 실행\n",
    "tester = LLMPerformanceTester(endpoint_name, model_id)\n",
    "\n",
    "# 동시성 테스트 실행\n",
    "print(\"Running performance test...\")\n",
    "results, total_latency = tester.run_concurrent_test(test_prompts, concurrent_requests=60)\n",
    "\n",
    "# 결과 분석\n",
    "analysis = tester.analyze_results(results, total_latency)\n",
    "print(\"\\nPerformance Analysis:\")\n",
    "print(json.dumps(analysis, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff84a42-c092-4e91-a5df-02a6e6a77044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "181b3cc2-5678-405b-8f7d-ebc4d113238c",
   "metadata": {},
   "source": [
    "## 4. Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18484bff-7f72-4e0d-80dd-b4811460bb60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete model\n",
    "sm_client.delete_model(ModelName=model_name)\n",
    "\n",
    "# Delete endpoint configuration\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5322ba-7445-43bb-a051-08ccde0ce0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
