{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Inference: BiEncoder RoBerta with Scale-to-Zero Scheduling\n",
    "\n",
    "[KLUE RoBERTa](https://huggingface.co/klue/roberta-base) 모델을 SageMaker Inference Component로 배포하고 추론합니다.\n",
    "\n",
    "이 노트북은 **Scale-to-Zero** 기능을 사용하여 주말에는 인스턴스를 0으로 줄이고 평일에는 다시 복원하는 스케줄링을 구현합니다.\n",
    "\n",
    "**주요 기능:**\n",
    "- Inference Component를 사용한 모델 배포\n",
    "- ManagedInstanceScaling으로 MinInstanceCount=0 설정\n",
    "- EventBridge Scheduler를 사용한 자동 스케일링\n",
    "- 주말(금요일 저녁) Scale-in, 평일(월요일 아침) Scale-out\n",
    "\n",
    "#### 참조 블로그\n",
    "- [Unlock cost savings with the new scale down to zero feature in SageMaker Inference](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## [선수 작업] AWS Role 정보를 .env 파일에 아래와 같이 저장\n",
    "```\n",
    "SAGEMAKER_ROLE_ARN=arn:aws:iam::XXXXXX:role/gonsoomoon-sm-inference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 0. 환경 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/lab/16-robert-sagemaker-inference/setup/.venv/bin/python\n",
      "\u001b[2mUsing Python 3.11.0rc1 environment at: /home/ubuntu/lab/16-robert-sagemaker-inference/setup/.venv\u001b[0m\n",
      "torch                            2.5.0+cu121\n"
     ]
    }
   ],
   "source": [
    "! which python\n",
    "! uv pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('../.env')\n",
    "SAGEMAKER_ROLE_ARN = os.getenv('SAGEMAKER_ROLE_ARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n",
      "Bucket: sagemaker-us-east-1-057716757052\n",
      "Region: us-east-1\n",
      "Role: arn:aws:iam::057716757052:role/gonsoomoon-sm-inference\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "role = SAGEMAKER_ROLE_ARN\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 2. 모델 아티팩트 생성 및 S3 업로드\n",
    "\n",
    "model.tar.gz 구조로 생성을 하면, SageMaker 가 이를 인지 합니다.\n",
    "\n",
    "model.tar.gz 구조:\n",
    "```\n",
    "model.tar.gz/\n",
    "├── config.json\n",
    "├── model.safetensors\n",
    "├── special_tokens_map.json\n",
    "├── tokenizer.json\n",
    "├── tokenizer_config.json\n",
    "├── vocab.txt\n",
    "└── code/\n",
    "    ├── inference.py\n",
    "    └── requirements.txt\n",
    "```\n",
    "\n",
    "참조: [SageMaker PyTorch Documentation](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### model.tar.gz 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../model.tar.gz to s3://sagemaker-us-east-1-057716757052/klue-roberta-inference/model/model.tar.gz\n",
      "Model uploaded to: s3://sagemaker-us-east-1-057716757052/klue-roberta-inference/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ../model_artifact\n",
    "!mkdir -p ../model_artifact/code\n",
    "!cp ../src/inference.py ../model_artifact/code/\n",
    "!cp ../src/requirements.txt ../model_artifact/code/\n",
    "!cp ../model/* ../model_artifact/\n",
    "\n",
    "!cd ../model_artifact && tar -czf ../model.tar.gz *\n",
    "\n",
    "model_artifact_s3_uri = f's3://{bucket}/klue-roberta-inference/model/model.tar.gz'\n",
    "!aws s3 cp ../model.tar.gz {model_artifact_s3_uri}\n",
    "\n",
    "print(f\"Model uploaded to: {model_artifact_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. SageMaker Model 생성\n",
    "\n",
    "Inference Component를 사용하기 위해서는 먼저 SageMaker Model을 생성해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Image URI: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.5-gpu-py311\n"
     ]
    }
   ],
   "source": [
    "# PyTorch 이미지 URI 가져오기\n",
    "from sagemaker import image_uris\n",
    "\n",
    "pytorch_image_uri = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"2.5\",\n",
    "    py_version=\"py311\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    image_scope=\"inference\"\n",
    ")\n",
    "\n",
    "print(f\"PyTorch Image URI: {pytorch_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7le84fseqmo",
   "metadata": {},
   "source": [
    "#### SageMaker Model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model created: roberta-dual-encoder-1760700181-8454-model\n",
      "Model Name: roberta-dual-encoder-1760700181-8454-model\n"
     ]
    }
   ],
   "source": [
    "# SageMaker Model 생성\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"roberta-dual-encoder\")\n",
    "model_name = f\"{prefix}-model\"\n",
    "\n",
    "# 모델이 이미 존재하는지 확인\n",
    "try:\n",
    "    sagemaker_client.describe_model(ModelName=model_name)\n",
    "    print(f\"✅ Model already exists: {model_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find model\" in str(e):\n",
    "        # 모델이 존재하지 않으면 생성\n",
    "        create_model_response = sagemaker_client.create_model(\n",
    "            ModelName=model_name,\n",
    "            ExecutionRoleArn=role,\n",
    "            PrimaryContainer={\n",
    "                \"Image\": pytorch_image_uri,\n",
    "                \"ModelDataUrl\": model_artifact_s3_uri,\n",
    "                \"Environment\": {\n",
    "                    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
    "                    \"SAGEMAKER_SUBMIT_DIRECTORY\": model_artifact_s3_uri,\n",
    "                    \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
    "                    \"SAGEMAKER_REGION\": region,\n",
    "                    \"MMS_DEFAULT_WORKERS_PER_MODEL\": \"1\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        print(f\"✅ Model created: {model_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(f\"Model Name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 4. Endpoint Configuration 생성 (Scale-to-Zero 지원)\n",
    "\n",
    "**ManagedInstanceScaling**을 활성화하고 **MinInstanceCount=0**으로 설정하여 인스턴스를 0으로 줄일 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Endpoint Config created: roberta-dual-encoder-1760700181-8454-scale-to-zero-config\n",
      "Endpoint Config Name: roberta-dual-encoder-1760700181-8454-scale-to-zero-config\n"
     ]
    }
   ],
   "source": [
    "# Endpoint Configuration 설정\n",
    "endpoint_config_name = f\"{prefix}-scale-to-zero-config\"\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g4dn.xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0  # Scale-to-Zero를 위해 0으로 설정\n",
    "max_instance_count = 2\n",
    "\n",
    "# Endpoint Config가 이미 존재하는지 확인\n",
    "try:\n",
    "    sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    print(f\"✅ Endpoint Config already exists: {endpoint_config_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find endpoint configuration\" in str(e):\n",
    "        # Endpoint Config가 존재하지 않으면 생성\n",
    "        sagemaker_client.create_endpoint_config(\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "            ExecutionRoleArn=role,\n",
    "            ProductionVariants=[\n",
    "                {\n",
    "                    \"VariantName\": variant_name,\n",
    "                    \"InstanceType\": instance_type,\n",
    "                    \"InitialInstanceCount\": 1,\n",
    "                    \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "                    \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "                    \"ManagedInstanceScaling\": {\n",
    "                        \"Status\": \"ENABLED\",\n",
    "                        \"MinInstanceCount\": min_instance_count,\n",
    "                        \"MaxInstanceCount\": max_instance_count,\n",
    "                    },\n",
    "                    \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        print(f\"✅ Endpoint Config created: {endpoint_config_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(f\"Endpoint Config Name: {endpoint_config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. SageMaker Endpoint 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Endpoint creation initiated: roberta-dual-encoder-1760700181-8454-scale-to-zero-endpoint\n",
      "Endpoint Name: roberta-dual-encoder-1760700181-8454-scale-to-zero-endpoint\n"
     ]
    }
   ],
   "source": [
    "# Endpoint 생성\n",
    "endpoint_name = f\"{prefix}-scale-to-zero-endpoint\"\n",
    "\n",
    "# Endpoint가 이미 존재하는지 확인\n",
    "try:\n",
    "    endpoint_desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"✅ Endpoint already exists: {endpoint_name}\")\n",
    "    print(f\"   Status: {endpoint_desc['EndpointStatus']}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find endpoint\" in str(e):\n",
    "        # Endpoint가 존재하지 않으면 생성\n",
    "        sagemaker_client.create_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "        )\n",
    "        print(f\"✅ Endpoint creation initiated: {endpoint_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(f\"Endpoint Name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n",
      "\n",
      "Total time taken: 150.48 seconds (2.51 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Endpoint가 InService 상태가 될 때까지 대기 (~3-5분 소요)\n",
    "import time\n",
    "import sys\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = desc[\"EndpointStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 6. Inference Component 생성\n",
    "\n",
    "Inference Component를 생성하여 모델을 Endpoint에 배포합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference Component creation initiated: roberta-dual-encoder-1760700181-8454-inference-component\n",
      "Inference Component Name: roberta-dual-encoder-1760700181-8454-inference-component\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "inference_component_name = f\"{prefix}-inference-component\"\n",
    "\n",
    "# Inference Component가 이미 존재하는지 확인\n",
    "try:\n",
    "    ic_desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    print(f\"✅ Inference Component already exists: {inference_component_name}\")\n",
    "    print(f\"   Status: {ic_desc['InferenceComponentStatus']}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find inference component\" in str(e):\n",
    "        # Inference Component가 존재하지 않으면 생성\n",
    "        sagemaker_client.create_inference_component(\n",
    "            InferenceComponentName=inference_component_name,\n",
    "            EndpointName=endpoint_name,\n",
    "            VariantName=variant_name,\n",
    "            Specification={\n",
    "                \"ModelName\": model_name,\n",
    "                \"StartupParameters\": {\n",
    "                    \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "                    \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "                },\n",
    "                \"ComputeResourceRequirements\": {\n",
    "                    \"MinMemoryRequiredInMb\": 4096,\n",
    "                    \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "                },\n",
    "            },\n",
    "            RuntimeConfig={\"CopyCount\": 1},\n",
    "        )\n",
    "        print(f\"✅ Inference Component creation initiated: {inference_component_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(f\"Inference Component Name: {inference_component_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n",
      "\n",
      "Total time taken: 332.07 seconds (5.53 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Inference Component가 InService 상태가 될 때까지 대기\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 7. Endpoint 추론 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### 싱글 샘플 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ae4f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing inference component...\n",
      "   Queries: 1\n",
      "   Documents: 1\n",
      "✅ Success! Response time: 0.53s\n",
      "   Embedding dimension: 768\n",
      "\n",
      "Query embeddings shape: (1, 768)\n",
      "Document embeddings shape: (1, 768)\n",
      "\n",
      "Cosine similarity:\n",
      "   Pair 1: 0.8666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_inference_component(endpoint_name, inference_component_name, payload):\n",
    "      \"\"\"\n",
    "      간단한 추론 테스트 - Inference Component가 작동하는지 확인\n",
    "      \n",
    "      Args:\n",
    "          endpoint_name: SageMaker Endpoint 이름\n",
    "          inference_component_name: Inference Component 이름\n",
    "          payload: 추론 요청 페이로드 (dict with 'queries' and 'documents')\n",
    "      \"\"\"\n",
    "      import boto3\n",
    "      import json\n",
    "      import time\n",
    "      import numpy as np\n",
    "\n",
    "      runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "      try:\n",
    "          print(\"🧪 Testing inference component...\")\n",
    "          print(f\"   Queries: {len(payload['queries'])}\")\n",
    "          print(f\"   Documents: {len(payload['documents'])}\")\n",
    "\n",
    "          start_time = time.time()\n",
    "\n",
    "          response = runtime_client.invoke_endpoint(\n",
    "              EndpointName=endpoint_name,\n",
    "              InferenceComponentName=inference_component_name,\n",
    "              ContentType='application/json',\n",
    "              Body=json.dumps(payload)\n",
    "          )\n",
    "\n",
    "          # 응답 본문 읽기\n",
    "          response_body = response['Body'].read().decode()\n",
    "\n",
    "          # 디버깅: 응답 내용 확인\n",
    "          if not response_body:\n",
    "              print(f\"⚠️  Warning: Empty response body\")\n",
    "              return False\n",
    "\n",
    "          # JSON 파싱\n",
    "          result = json.loads(response_body)\n",
    "          elapsed_time = time.time() - start_time\n",
    "\n",
    "          print(f\"✅ Success! Response time: {elapsed_time:.2f}s\")\n",
    "          print(f\"   Embedding dimension: {result['embedding_dim']}\")\n",
    "\n",
    "          # Embedding shapes 출력\n",
    "          print(f\"\\nQuery embeddings shape: ({result['num_queries']}, {result['embedding_dim']})\")\n",
    "          print(f\"Document embeddings shape: ({result['num_documents']}, {result['embedding_dim']})\")\n",
    "\n",
    "          # 코사인 유사도 계산 및 출력\n",
    "          query_embs = np.array(result[\"query_embeddings\"])\n",
    "          doc_embs = np.array(result[\"doc_embeddings\"])\n",
    "\n",
    "          print(f\"\\nCosine similarity:\")\n",
    "          for i in range(len(query_embs)):\n",
    "              similarity = np.dot(query_embs[i], doc_embs[i])\n",
    "              print(f\"   Pair {i+1}: {similarity:.4f}\")\n",
    "\n",
    "          return True\n",
    "\n",
    "      except json.JSONDecodeError as e:\n",
    "          print(f\"❌ JSON parsing failed: {str(e)}\")\n",
    "          print(f\"   Response body (first 200 chars): {response_body[:200] if 'response_body' in locals() else 'N/A'}\")\n",
    "          return False\n",
    "      except Exception as e:\n",
    "          print(f\"❌ Failed: {str(e)}\")\n",
    "          print(f\"   Error type: {type(e).__name__}\")\n",
    "          return False\n",
    "\n",
    "# 사용 예시 1: 단일 쿼리-문서 쌍\n",
    "payload1 = {\n",
    "    \"queries\": [\"맛있는 한국 전통 음식 김치찌개\"],\n",
    "    \"documents\": [\"김치찌개와 된장찌개는 한국의 대표 전통 음식입니다.\"]\n",
    "}\n",
    "  \n",
    "test_inference_component(endpoint_name, inference_component_name, payload1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### 8개 쿼리-문서 쌍 배치 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inference completed:\n",
      "  Queries: 8\n",
      "  Documents: 8\n",
      "  Embedding dim: 768\n",
      "\n",
      "Pair-wise cosine similarities:\n",
      "  Pair 1: 0.8666\n",
      "  Pair 2: 0.7164\n",
      "  Pair 3: 0.4745\n",
      "  Pair 4: 0.6969\n",
      "  Pair 5: 0.6220\n",
      "  Pair 6: 0.5832\n",
      "  Pair 7: 0.6482\n",
      "  Pair 8: 0.6714\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create the runtime client\n",
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "batch_payload = {\n",
    "    \"queries\": [\n",
    "        \"맛있는 한국 전통 음식 김치찌개\",\n",
    "        \"최신 기술 발전\",\n",
    "        \"색깔\",\n",
    "        \"여행 계획\",\n",
    "        \"스포츠 경기\",\n",
    "        \"영화 추천\",\n",
    "        \"날씨 정보\",\n",
    "        \"건강 관리\"\n",
    "    ],\n",
    "    \"documents\": [\n",
    "        \"김치찌개와 된장찌개는 한국의 대표 전통 음식입니다.\",\n",
    "        \"인공지능 기술이 빠르게 발전하고 있습니다.\",\n",
    "        \"파리의 에펠탑은 프랑스의 상징입니다.\",\n",
    "        \"제주도는 한국의 인기 여행지입니다.\",\n",
    "        \"축구 경기가 오늘 저녁에 있습니다.\",\n",
    "        \"최근 개봉한 영화가 좋은 평가를 받고 있습니다.\",\n",
    "        \"내일은 맑은 날씨가 예상됩니다.\",\n",
    "        \"규칙적인 운동이 건강에 좋습니다.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(batch_payload)\n",
    ")\n",
    "\n",
    "batch_result = json.loads(response['Body'].read().decode())\n",
    "\n",
    "print(f\"Batch inference completed:\")\n",
    "print(f\"  Queries: {batch_result['num_queries']}\")\n",
    "print(f\"  Documents: {batch_result['num_documents']}\")\n",
    "print(f\"  Embedding dim: {batch_result['embedding_dim']}\\n\")\n",
    "\n",
    "# 각 쌍의 코사인 유사도 계산\n",
    "query_embs = np.array(batch_result['query_embeddings'])\n",
    "doc_embs = np.array(batch_result['doc_embeddings'])\n",
    "\n",
    "print(\"Pair-wise cosine similarities:\")\n",
    "for i in range(len(query_embs)):\n",
    "    similarity = np.dot(query_embs[i], doc_embs[i])\n",
    "    print(f\"  Pair {i+1}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f2ee5",
   "metadata": {},
   "source": [
    "## 수동으로 Scale-to-Zero 테스트 \n",
    "\n",
    "스케줄을 기다리지 않고 즉시 테스트하려면 아래 코드를 실행하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0004a2",
   "metadata": {},
   "source": [
    "### inference_component 를 0개로 줄이기 (Scale In)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2e32508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference Component scaled down to 0 copies\n",
      "인스턴스가 종료되는 데 몇 분이 걸릴 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# CopyCount를 0으로 설정하여 스케일 다운\n",
    "sagemaker_client.update_inference_component_runtime_config(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    DesiredRuntimeConfig={'CopyCount': 0}\n",
    ")\n",
    "\n",
    "print(\"✅ Inference Component scaled down to 0 copies\")\n",
    "print(\"인스턴스가 종료되는 데 몇 분이 걸릴 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d4b7b",
   "metadata": {},
   "source": [
    "### 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d63e8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference Component scaled down to 0 copies 요청 완료\n",
      "⏳ 인스턴스가 종료되는 과정을 모니터링합니다...\n",
      "\n",
      "[0s] 📊 Status Update:\n",
      "   IC Status: Updating\n",
      "   Current CopyCount: 0\n",
      "   Desired CopyCount: 0\n",
      "   RuntimeConfig: {\"DesiredCopyCount\": 0, \"CurrentCopyCount\": 0}\n",
      "\n",
      "✅ Scale-down 완료! CopyCount가 0이 되었습니다.\n",
      "   총 소요 시간: 0초 (0.0분)\n",
      "\n",
      "🎯 모니터링 완료\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# CopyCount를 0으로 설정하여 스케일 다운\n",
    "sagemaker_client.update_inference_component_runtime_config(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    DesiredRuntimeConfig={'CopyCount': 0}\n",
    ")\n",
    "\n",
    "print(\"✅ Inference Component scaled down to 0 copies 요청 완료\")\n",
    "print(\"⏳ 인스턴스가 종료되는 과정을 모니터링합니다...\\n\")\n",
    "\n",
    "# 스케일 다운 모니터링\n",
    "max_wait_time = 600  # 10분 타임아웃\n",
    "start_time = time.time()\n",
    "previous_copy_count = None\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # 타임아웃 체크\n",
    "        if elapsed > max_wait_time:\n",
    "            print(f\"\\n⚠️ Timeout: {max_wait_time}초 경과. 모니터링 종료.\")\n",
    "            break\n",
    "\n",
    "        # Inference Component 상태 확인\n",
    "        ic_desc = sagemaker_client.describe_inference_component(\n",
    "            InferenceComponentName=inference_component_name\n",
    "        )\n",
    "\n",
    "        status = ic_desc['InferenceComponentStatus']\n",
    "        runtime_config = ic_desc.get('RuntimeConfig', {})\n",
    "        current_copy_count = runtime_config.get('CurrentCopyCount', runtime_config.get('CopyCount', 'N/A'))\n",
    "        desired_copy_count = runtime_config.get('DesiredCopyCount', 0)\n",
    "\n",
    "        # 상태 변경 시에만 출력\n",
    "        if current_copy_count != previous_copy_count:\n",
    "            print(f\"[{elapsed:.0f}s] 📊 Status Update:\")\n",
    "            print(f\"   IC Status: {status}\")\n",
    "            print(f\"   Current CopyCount: {current_copy_count}\")\n",
    "            print(f\"   Desired CopyCount: {desired_copy_count}\")\n",
    "\n",
    "            # Full RuntimeConfig (디버깅용)\n",
    "            print(f\"   RuntimeConfig: {json.dumps(runtime_config, default=str)}\")\n",
    "            print()\n",
    "\n",
    "            previous_copy_count = current_copy_count\n",
    "\n",
    "        # CopyCount가 0이 되면 종료\n",
    "        if current_copy_count == 0:\n",
    "            print(f\"✅ Scale-down 완료! CopyCount가 0이 되었습니다.\")\n",
    "            print(f\"   총 소요 시간: {elapsed:.0f}초 ({elapsed/60:.1f}분)\")\n",
    "            break\n",
    "\n",
    "        # 10초 대기\n",
    "        time.sleep(10)\n",
    "\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if \"Could not find inference component\" in str(e):\n",
    "            print(f\"❌ Inference Component not found: {inference_component_name}\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"❌ Error: {str(e)}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error: {str(e)}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n🎯 모니터링 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561353de",
   "metadata": {},
   "source": [
    "#### 실제 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05e4edd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing inference component...\n",
      "   Queries: 1\n",
      "   Documents: 1\n",
      "❌ Failed: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.\n",
      "   Error type: ValidationError\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용 예시 1: 단일 쿼리-문서 쌍\n",
    "payload1 = {\n",
    "    \"queries\": [\"맛있는 한국 전통 음식 김치찌개\"],\n",
    "    \"documents\": [\"김치찌개와 된장찌개는 한국의 대표 전통 음식입니다.\"]\n",
    "}\n",
    "  \n",
    "test_inference_component(endpoint_name, inference_component_name, payload1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57671ff6",
   "metadata": {},
   "source": [
    "### inference_component 1 개 생성하여 Scale Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0bd64d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference Component scaled up to 1 copy\n",
      "인스턴스가 시작되는 데 몇 분이 걸릴 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# CopyCount를 1로 설정하여 스케일 업\n",
    "sagemaker_client.update_inference_component_runtime_config(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    DesiredRuntimeConfig={'CopyCount': 1}\n",
    ")\n",
    "\n",
    "print(\"✅ Inference Component scaled up to 1 copy\")\n",
    "print(\"인스턴스가 시작되는 데 몇 분이 걸릴 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ef6a0",
   "metadata": {},
   "source": [
    "### inference_component 업데이트 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d74b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "InService\n",
      "\n",
      "Total time taken: 331.88 seconds (5.53 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Inference Component가 InService 상태가 될 때까지 대기\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb559a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing inference component...\n",
      "   Queries: 1\n",
      "   Documents: 1\n",
      "✅ Success! Response time: 0.49s\n",
      "   Embedding dimension: 768\n",
      "\n",
      "Query embeddings shape: (1, 768)\n",
      "Document embeddings shape: (1, 768)\n",
      "\n",
      "Cosine similarity:\n",
      "   Pair 1: 0.8666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용 예시 1: 단일 쿼리-문서 쌍\n",
    "payload1 = {\n",
    "    \"queries\": [\"맛있는 한국 전통 음식 김치찌개\"],\n",
    "    \"documents\": [\"김치찌개와 된장찌개는 한국의 대표 전통 음식입니다.\"]\n",
    "}\n",
    "  \n",
    "test_inference_component(endpoint_name, inference_component_name, payload1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 8. EventBridge Scheduler를 사용한 Scale-to-Zero 스케줄링\n",
    "\n",
    "### 사전 요구 사항: \n",
    "- Role 에 아래와 같은 정책이 추가 되어 있어야 합니다.\n",
    "    - AmazonEC2ContainerRegistryFullAccess\n",
    "    - AmazonEventBridgeFullAccess\n",
    "    - AmazonS3FullAccess\n",
    "    - AmazonSageMakerFullAccess\n",
    "\n",
    "### 방법 1: UpdateInferenceComponentRuntimeConfig API 사용\n",
    "\n",
    "Inference Component의 CopyCount를 0으로 설정하여 스케일 다운합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### Weekend Scale-in (금요일 저녁)\n",
    "\n",
    "매주 금요일 18:00 UTC+1에 CopyCount를 0으로 설정하는 스케줄을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee1025a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::057716757052:role/gonsoomoon-sm-inference'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scale-in schedule created: roberta-dual-encoder-1760700181-8454-scale-to-zero-schedule\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "scheduler = boto3.client('scheduler')\n",
    "\n",
    "flex_window = {\"Mode\": \"OFF\"}\n",
    "\n",
    "# Scale-in 스케줄 타겟 설정\n",
    "scale_in_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:updateInferenceComponentRuntimeConfig\",\n",
    "    \"Input\": json.dumps({\n",
    "        \"DesiredRuntimeConfig\": {\"CopyCount\": 0},\n",
    "        \"InferenceComponentName\": inference_component_name\n",
    "    })\n",
    "}\n",
    "\n",
    "# 매주 금요일 18:00 UTC+9 (한국 시간)에 스케일 다운\n",
    "update_IC_scale_in_schedule = f\"{prefix}-scale-to-zero-schedule\"\n",
    "\n",
    "try:\n",
    "    scheduler.create_schedule(\n",
    "        Name=update_IC_scale_in_schedule,\n",
    "        ScheduleExpression=\"cron(00 18 ? * 6 *)\",  # 금요일 18:00\n",
    "        ScheduleExpressionTimezone=\"Asia/Seoul\",  # 한국 시간대\n",
    "        Target=scale_in_target,\n",
    "        FlexibleTimeWindow=flex_window,\n",
    "        ActionAfterCompletion=\"NONE\",  # 계속 유지\n",
    "    )\n",
    "    print(f\"✅ Scale-in schedule created: {update_IC_scale_in_schedule}\")\n",
    "except scheduler.exceptions.ConflictException:\n",
    "    print(f\"Schedule {update_IC_scale_in_schedule} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### Workweek Scale-out (월요일 아침)\n",
    "\n",
    "매주 월요일 07:00 UTC+9에 CopyCount를 1로 복원하는 스케줄을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scale-out schedule created: roberta-dual-encoder-1760700181-8454-scale-out-schedule\n"
     ]
    }
   ],
   "source": [
    "# Scale-out 스케줄 타겟 설정\n",
    "scale_out_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:updateInferenceComponentRuntimeConfig\",\n",
    "    \"Input\": json.dumps({\n",
    "        \"DesiredRuntimeConfig\": {\"CopyCount\": 1},\n",
    "        \"InferenceComponentName\": inference_component_name\n",
    "    })\n",
    "}\n",
    "\n",
    "# 매주 월요일 07:00 UTC+9 (한국 시간)에 스케일 업\n",
    "update_IC_scale_out_schedule = f\"{prefix}-scale-out-schedule\"\n",
    "\n",
    "try:\n",
    "    scheduler.create_schedule(\n",
    "        Name=update_IC_scale_out_schedule,\n",
    "        ScheduleExpression=\"cron(00 07 ? * 2 *)\",  # 월요일 07:00\n",
    "        ScheduleExpressionTimezone=\"Asia/Seoul\",  # 한국 시간대\n",
    "        Target=scale_out_target,\n",
    "        FlexibleTimeWindow=flex_window,\n",
    "        ActionAfterCompletion=\"NONE\",  # 계속 유지\n",
    "    )\n",
    "    print(f\"✅ Scale-out schedule created: {update_IC_scale_out_schedule}\")\n",
    "except scheduler.exceptions.ConflictException:\n",
    "    print(f\"Schedule {update_IC_scale_out_schedule} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### 생성된 스케줄 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Schedule: roberta-dual-encoder-1760700181-8454-scale-to-zero-schedule\n",
      "   Expression: cron(00 18 ? * 6 *)\n",
      "   Timezone: Asia/Seoul\n",
      "   State: ENABLED\n",
      "\n",
      "📅 Schedule: roberta-dual-encoder-1760700181-8454-scale-out-schedule\n",
      "   Expression: cron(00 07 ? * 2 *)\n",
      "   Timezone: Asia/Seoul\n",
      "   State: ENABLED\n"
     ]
    }
   ],
   "source": [
    "# 생성된 스케줄 목록 확인\n",
    "try:\n",
    "    schedules_to_check = [update_IC_scale_in_schedule, update_IC_scale_out_schedule]\n",
    "    \n",
    "    for schedule_name in schedules_to_check:\n",
    "        try:\n",
    "            schedule = scheduler.get_schedule(Name=schedule_name)\n",
    "            print(f\"\\n📅 Schedule: {schedule_name}\")\n",
    "            print(f\"   Expression: {schedule['ScheduleExpression']}\")\n",
    "            print(f\"   Timezone: {schedule['ScheduleExpressionTimezone']}\")\n",
    "            print(f\"   State: {schedule['State']}\")\n",
    "        except scheduler.exceptions.ResourceNotFoundException:\n",
    "            print(f\"\\n❌ Schedule not found: {schedule_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking schedules: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "## 9. 리소스 정리\n",
    "\n",
    "테스트가 완료되면 리소스를 정리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "### 스케줄 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e971638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deleted schedule: roberta-dual-encoder-1760700181-8454-scale-to-zero-schedule\n",
      "✅ Deleted schedule: roberta-dual-encoder-1760700181-8454-scale-out-schedule\n"
     ]
    }
   ],
   "source": [
    "# 생성된 스케줄 삭제\n",
    "schedules = [update_IC_scale_in_schedule, update_IC_scale_out_schedule]\n",
    "\n",
    "for schedule in schedules:\n",
    "    try:\n",
    "        scheduler.delete_schedule(Name=schedule)\n",
    "        print(f\"✅ Deleted schedule: {schedule}\")\n",
    "    except scheduler.exceptions.ResourceNotFoundException:\n",
    "        print(f\"Schedule {schedule} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### Inference Component 및 엔드포인트 등의 리소스 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db25e9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Component 삭제 중...\n",
      "✅ Inference Component 삭제 시작: roberta-dual-encoder-1760700181-8454-inference-component\n",
      "Inference Component 삭제 대기 중...\n",
      "   Status: Deleting\n",
      "   Status: Deleting\n",
      "   Status: Deleting\n",
      "✅ Inference Component 삭제 완료\n",
      "\n",
      "Endpoint 삭제 중...\n",
      "✅ Endpoint 삭제 시작: roberta-dual-encoder-1760700181-8454-scale-to-zero-endpoint\n",
      "   Endpoint Status: Deleting\n",
      "✅ Endpoint 삭제 완료\n",
      "\n",
      "Endpoint Config 삭제 중...\n",
      "✅ Endpoint Config 삭제 완료: roberta-dual-encoder-1760700181-8454-scale-to-zero-config\n",
      "\n",
      "Model 삭제 중...\n",
      "✅ Model 삭제 완료: roberta-dual-encoder-1760700181-8454-model\n",
      "\n",
      "🎉 모든 리소스 삭제 완료!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Inference Component 삭제\n",
    "print(\"Inference Component 삭제 중...\")\n",
    "try:\n",
    "    sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    sagemaker_client.delete_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    print(f\"✅ Inference Component 삭제 시작: {inference_component_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find inference component\" in str(e):\n",
    "        print(f\"ℹ️ Inference Component가 이미 삭제되었거나 존재하지 않음: {inference_component_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Inference Component가 삭제될 때까지 대기\n",
    "print(\"Inference Component 삭제 대기 중...\")\n",
    "max_wait_time = 300  # 5분 타임아웃\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        if time.time() - start_time > max_wait_time:\n",
    "            print(\"⚠️ Timeout: Inference Component 삭제 대기 시간 초과\")\n",
    "            break\n",
    "\n",
    "        desc = sagemaker_client.describe_inference_component(\n",
    "            InferenceComponentName=inference_component_name\n",
    "        )\n",
    "        status = desc.get('InferenceComponentStatus', 'Unknown')\n",
    "        print(f\"   Status: {status}\")\n",
    "        time.sleep(10)\n",
    "\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if \"Could not find inference component\" in str(e):\n",
    "            print(\"✅ Inference Component 삭제 완료\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"⚠️ Unexpected error: {str(e)}\")\n",
    "            break\n",
    "\n",
    "# Endpoint 삭제\n",
    "print(\"\\nEndpoint 삭제 중...\")\n",
    "try:\n",
    "    sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"✅ Endpoint 삭제 시작: {endpoint_name}\")\n",
    "\n",
    "    # Endpoint 삭제 대기\n",
    "    max_wait_time = 300\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            if time.time() - start_time > max_wait_time:\n",
    "                print(\"⚠️ Timeout: Endpoint 삭제 대기 시간 초과\")\n",
    "                break\n",
    "\n",
    "            desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "            status = desc.get('EndpointStatus', 'Unknown')\n",
    "            print(f\"   Endpoint Status: {status}\")\n",
    "            time.sleep(10)\n",
    "\n",
    "        except sagemaker_client.exceptions.ClientError as e:\n",
    "            if \"Could not find endpoint\" in str(e):\n",
    "                print(\"✅ Endpoint 삭제 완료\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"⚠️ Unexpected error: {str(e)}\")\n",
    "                break\n",
    "\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find endpoint\" in str(e):\n",
    "        print(f\"ℹ️ Endpoint가 이미 삭제되었거나 존재하지 않음: {endpoint_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Endpoint Config 삭제\n",
    "print(\"\\nEndpoint Config 삭제 중...\")\n",
    "try:\n",
    "    sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    print(f\"✅ Endpoint Config 삭제 완료: {endpoint_config_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find endpoint configuration\" in str(e):\n",
    "        print(f\"ℹ️ Endpoint Config가 이미 삭제되었거나 존재하지 않음: {endpoint_config_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Model 삭제\n",
    "print(\"\\nModel 삭제 중...\")\n",
    "try:\n",
    "    sagemaker_client.describe_model(ModelName=model_name)\n",
    "    sagemaker_client.delete_model(ModelName=model_name)\n",
    "    print(f\"✅ Model 삭제 완료: {model_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find model\" in str(e):\n",
    "        print(f\"ℹ️ Model이 이미 삭제되었거나 존재하지 않음: {model_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"\\n🎉 모든 리소스 삭제 완료!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "이 노트북에서는 다음을 구현했습니다:\n",
    "\n",
    "1. **RoBERTa Dual Encoder 모델**을 Inference Component로 배포\n",
    "2. **ManagedInstanceScaling**을 활성화하여 MinInstanceCount=0 설정\n",
    "3. **EventBridge Scheduler**를 사용하여:\n",
    "   - 금요일 18:00에 자동으로 CopyCount를 0으로 설정 (Scale-in)\n",
    "   - 월요일 07:00에 자동으로 CopyCount를 1로 복원 (Scale-out)\n",
    "4. **비용 절감**: 주말 동안 인스턴스가 0개로 줄어들어 컴퓨팅 비용 절감\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c669c9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
