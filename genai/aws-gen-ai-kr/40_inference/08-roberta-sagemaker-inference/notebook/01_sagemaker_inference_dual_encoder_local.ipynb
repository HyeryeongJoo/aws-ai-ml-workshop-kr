{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Inference: BiEncoder RoBerta from local\n",
    "\n",
    "[KLUE RoBERTa](https://huggingface.co/klue/roberta-base) 모델을 SageMaker Endpoint로 배포하고 추론합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [선수 작업] AWS Role 정보를 .env 파일에 아래와 같이 저장\n",
    "```\n",
    "SAGEMAKER_ROLE_ARN=arn:aws:iam::XXXXXX:role/gonsoomoon-sm-inference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 환경 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .env 에서 role 정보 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('../.env')\n",
    "SAGEMAKER_ROLE_ARN = os.getenv('SAGEMAKER_ROLE_ARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 로컬 추론 함수 테스트\n",
    "\n",
    "먼저 로컬에서 inference.py가 정상 동작하는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing SageMaker BiEncoder Inference Functions\n",
      "============================================================\n",
      "\n",
      "1. Testing model_fn...\n",
      "Loading BiEncoder model from .\n",
      "Using device: cuda\n",
      "Downloading model from HuggingFace: klue/roberta-base\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BiEncoder model loaded successfully\n",
      "   ✓ BiEncoder model loaded\n",
      "\n",
      "2. Testing input_fn...\n",
      "Received content_type: application/json\n",
      "   ✓ Processed 3 queries and 3 documents\n",
      "\n",
      "3. Testing predict_fn...\n",
      "Starting BiEncoder prediction\n",
      "Processing 3 query(s) and 3 document(s)\n",
      "BiEncoder prediction completed successfully\n",
      "   ✓ Query embeddings shape: (3, 768)\n",
      "   ✓ Document embeddings shape: (3, 768)\n",
      "\n",
      "4. Testing output_fn...\n",
      "Formatting output with accept: application/json\n",
      "   ✓ Output keys: ['query_embeddings', 'doc_embeddings', 'embedding_dim', 'num_queries', 'num_documents']\n",
      "\n",
      "5. Computing cosine similarity for each pair...\n",
      "\n",
      "   Pair-wise similarity scores:\n",
      "\n",
      "   Pair 1: [0.8666] - 상 (High)\n",
      "      Query: '맛있는 한국 전통 음식 김치찌개'\n",
      "      Doc:   '김치찌개와 된장찌개는 한국의 대표 전통 음식입니다....'\n",
      "\n",
      "   Pair 2: [0.7164] - 중 (Medium)\n",
      "      Query: '최신 기술 발전'\n",
      "      Doc:   '인공지능 기술이 빠르게 발전하고 있습니다....'\n",
      "\n",
      "   Pair 3: [0.4745] - 하 (Low)\n",
      "      Query: '색깔'\n",
      "      Doc:   '파리의 에펠탑은 프랑스의 상징입니다....'\n",
      "\n",
      "============================================================\n",
      "All tests passed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!python ../src/test_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n",
      "Bucket: sagemaker-us-east-1-057716757052\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = SAGEMAKER_ROLE_ARN\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 아티팩트 생성\n",
    "\n",
    "model.tar.gz 구조로 생성을 하면 , SageMaker 가 이를 인지 합니다.\n",
    "model.tar.gz 구조:\n",
    "```\n",
    "model.tar.gz/\n",
    "├── config.json\n",
    "├── model.safetensors\n",
    "├── special_tokens_map.json\n",
    "├── tokenizer.json\n",
    "├── tokenizer_config.json\n",
    "├── vocab.txt\n",
    "└── code/\n",
    "    ├── inference.py\n",
    "    └── requirements.txt\n",
    "```\n",
    "\n",
    "참조: [SageMaker PyTorch Documentation](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.tar.gz 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../model_artifact\n",
    "!mkdir -p ../model_artifact/code\n",
    "!cp ../src/inference.py ../model_artifact/code/\n",
    "!cp ../src/requirements.txt ../model_artifact/code/\n",
    "!cp ../model/* ../model_artifact/\n",
    "\n",
    "!cd ../model_artifact && tar -czf ../model.tar.gz *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Local SageMaker Endpoint 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_model_path:  ../model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_data_dir = \"..\"\n",
    "local_model_path = os.path.join(model_data_dir, 'model.tar.gz')\n",
    "print(\"local_model_path: \", local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU 리소스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 27 13:49:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      On  |   00000000:36:00.0 Off |                    0 |\n",
      "| N/A   38C    P8             16W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "    else:\n",
    "        instance_type = \"local\"        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local SageMaker Endpoint 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RootlessDocker not detected, falling back to remote host IP or localhost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to xrwfv8abms-algo-1-p6d1o\n",
      "xrwfv8abms-algo-1-p6d1o  | CUDA compat package should be installed for NVIDIA driver smaller than 550.163.01\n",
      "xrwfv8abms-algo-1-p6d1o  | Current installed NVIDIA driver version is 570.133.20\n",
      "xrwfv8abms-algo-1-p6d1o  | Skipping CUDA compat setup as newer NVIDIA driver is installed\n",
      "xrwfv8abms-algo-1-p6d1o  | /opt/conda/lib/python3.11/site-packages/sagemaker_pytorch_serving_container/torchserve.py:20: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "xrwfv8abms-algo-1-p6d1o  |   import pkg_resources\n",
      "xrwfv8abms-algo-1-p6d1o  | Collecting transformers>=4.30.0 (from -r /opt/ml/model/code/requirements.txt (line 1))\n",
      "xrwfv8abms-algo-1-p6d1o  |   Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.18.0)\n",
      "xrwfv8abms-algo-1-p6d1o  | Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "xrwfv8abms-algo-1-p6d1o  |   Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.2.6)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (23.2)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (6.0.1)\n",
      "xrwfv8abms-algo-1-p6d1o  | Collecting regex!=2019.12.17 (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "xrwfv8abms-algo-1-p6d1o  |   Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.32.4)\n",
      "xrwfv8abms-algo-1-p6d1o  | Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "xrwfv8abms-algo-1-p6d1o  |   Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "xrwfv8abms-algo-1-p6d1o  | Collecting safetensors>=0.4.3 (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "xrwfv8abms-algo-1-p6d1o  |   Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.66.5)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2025.7.0)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.14.1)\n",
      "xrwfv8abms-algo-1-p6d1o  | Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "xrwfv8abms-algo-1-p6d1o  |   Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.3.2)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.10)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.5.0)\n",
      "xrwfv8abms-algo-1-p6d1o  | Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2025.7.14)\n",
      "xrwfv8abms-algo-1-p6d1o  | Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m187.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m-:--:--\u001b[0m\n",
      "xrwfv8abms-algo-1-p6d1o  | \u001b[?25hDownloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m-:--:--\u001b[0m\n",
      "xrwfv8abms-algo-1-p6d1o  | \u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m153.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m-:--:--\u001b[0m\n",
      "xrwfv8abms-algo-1-p6d1o  | \u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m-:--:--\u001b[0m\n",
      "xrwfv8abms-algo-1-p6d1o  | \u001b[?25hDownloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m-:--:--\u001b[0m\n",
      "xrwfv8abms-algo-1-p6d1o  | \u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "xrwfv8abms-algo-1-p6d1o  | Installing collected packages: safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.35.1 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.56.2\n",
      "xrwfv8abms-algo-1-p6d1o  | \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "xrwfv8abms-algo-1-p6d1o  | \u001b[0m\n",
      "xrwfv8abms-algo-1-p6d1o  | \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "xrwfv8abms-algo-1-p6d1o  | \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "xrwfv8abms-algo-1-p6d1o  | ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.11/site-packages/sagemaker_pytorch_serving_container/etc/log4j2.xml', '--models', 'model=/opt/ml/model']\n",
      "xrwfv8abms-algo-1-p6d1o  | Warning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "xrwfv8abms-algo-1-p6d1o  | WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,113 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,124 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,169 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.11/site-packages/ts/configs/metrics.yaml\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,245 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "xrwfv8abms-algo-1-p6d1o  | Torchserve version: 0.12.0\n",
      "xrwfv8abms-algo-1-p6d1o  | TS Home: /opt/conda/lib/python3.11/site-packages\n",
      "xrwfv8abms-algo-1-p6d1o  | Current directory: /\n",
      "xrwfv8abms-algo-1-p6d1o  | Temp directory: /tmp\n",
      "xrwfv8abms-algo-1-p6d1o  | Metrics config path: /opt/conda/lib/python3.11/site-packages/ts/configs/metrics.yaml\n",
      "xrwfv8abms-algo-1-p6d1o  | Number of GPUs: 1\n",
      "xrwfv8abms-algo-1-p6d1o  | Number of CPUs: 32\n",
      "xrwfv8abms-algo-1-p6d1o  | Max heap size: 30688 M\n",
      "xrwfv8abms-algo-1-p6d1o  | Python executable: /opt/conda/bin/python3.11\n",
      "xrwfv8abms-algo-1-p6d1o  | Config file: /etc/sagemaker-ts.properties\n",
      "xrwfv8abms-algo-1-p6d1o  | Inference address: http://0.0.0.0:8080\n",
      "xrwfv8abms-algo-1-p6d1o  | Management address: http://0.0.0.0:8080\n",
      "xrwfv8abms-algo-1-p6d1o  | Metrics address: http://127.0.0.1:8082\n",
      "xrwfv8abms-algo-1-p6d1o  | Model Store: /.sagemaker/ts/models\n",
      "xrwfv8abms-algo-1-p6d1o  | Initial Models: model=/opt/ml/model\n",
      "xrwfv8abms-algo-1-p6d1o  | Log dir: /logs\n",
      "xrwfv8abms-algo-1-p6d1o  | Metrics dir: /logs\n",
      "xrwfv8abms-algo-1-p6d1o  | Netty threads: 0\n",
      "xrwfv8abms-algo-1-p6d1o  | Netty client threads: 0\n",
      "xrwfv8abms-algo-1-p6d1o  | Default workers per model: 1\n",
      "xrwfv8abms-algo-1-p6d1o  | Blacklist Regex: N/A\n",
      "xrwfv8abms-algo-1-p6d1o  | Maximum Response Size: 6553500\n",
      "xrwfv8abms-algo-1-p6d1o  | Maximum Request Size: 6553500\n",
      "xrwfv8abms-algo-1-p6d1o  | Limit Maximum Image Pixels: true\n",
      "xrwfv8abms-algo-1-p6d1o  | Prefer direct buffer: false\n",
      "xrwfv8abms-algo-1-p6d1o  | Allowed Urls: [file://.*|http(s)?://.*]\n",
      "xrwfv8abms-algo-1-p6d1o  | Custom python dependency for model allowed: false\n",
      "xrwfv8abms-algo-1-p6d1o  | Enable metrics API: true\n",
      "xrwfv8abms-algo-1-p6d1o  | Metrics mode: LOG\n",
      "xrwfv8abms-algo-1-p6d1o  | Disable system metrics: true\n",
      "xrwfv8abms-algo-1-p6d1o  | Workflow Store: /.sagemaker/ts/models\n",
      "xrwfv8abms-algo-1-p6d1o  | CPP log config: N/A\n",
      "xrwfv8abms-algo-1-p6d1o  | Model config: N/A\n",
      "xrwfv8abms-algo-1-p6d1o  | System metrics command: default\n",
      "xrwfv8abms-algo-1-p6d1o  | Model API enabled: false\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,252 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,267 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,273 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createTempDir /tmp/models/480c2380e09a4c60b4bd52d2aaba5fde\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,274 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createSymbolicDir /tmp/models/480c2380e09a4c60b4bd52d2aaba5fde/model\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,275 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,275 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,277 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,283 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,326 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,327 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:28,327 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "xrwfv8abms-algo-1-p6d1o  | Model server started.\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,434 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=98\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,435 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,440 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.11/site-packages/ts/configs/metrics.yaml.\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,440 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]98\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,440 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,441 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.11.9\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,443 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,447 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,451 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1758981149451\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,466 [INFO ] pool-2-thread-2 ACCESS_LOG - /172.18.0.1:39888 \"GET /ping HTTP/1.1\" 200 8\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,466 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:29,466 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981149\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:31,053 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Loading BiEncoder model from /tmp/models/480c2380e09a4c60b4bd52d2aaba5fde/model\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:31,054 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Loading BiEncoder model from /tmp/models/480c2380e09a4c60b4bd52d2aaba5fde/model\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:31,075 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Using device: cuda\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:31,075 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Using device: cuda\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:31,076 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Loading model from model_dir: /tmp/models/480c2380e09a4c60b4bd52d2aaba5fde/model\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:31,076 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Loading model from model_dir: /tmp/models/480c2380e09a4c60b4bd52d2aaba5fde/model\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:32,412 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BiEncoder model loaded successfully\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:32,413 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BiEncoder model loaded successfully\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:32,414 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2962\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:32,414 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:4134.0|#WorkerName:W-9000-model_1.0,Level:Host|#hostname:2240db8f9a7b,timestamp:1758981152\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:32,415 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981152\n",
      "!"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"local-endpoint-dual-encoder-{}\".format(int(time.time()))\n",
    "\n",
    "local_pytorch_model = PyTorchModel(model_data=local_model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='inference.py',\n",
    "                                   source_dir = '../src',\n",
    "                                   framework_version='2.5',\n",
    "                                   py_version='py311',\n",
    "                                   model_server_workers=1,\n",
    "                                  )\n",
    "\n",
    "local_predictor = local_pytorch_model.deploy(\n",
    "                           instance_type=instance_type, \n",
    "                           initial_instance_count=1, \n",
    "                           endpoint_name=endpoint_name,\n",
    "                           wait=True,\n",
    "                           log = False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Endpoint 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RootlessDocker not detected, falling back to remote host IP or localhost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,662 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,663 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1758981179663\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,665 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,665 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,665 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,665 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting BiEncoder prediction\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,665 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting BiEncoder prediction\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,666 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 1 query(s) and 1 document(s)\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,666 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 1 query(s) and 1 document(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,888 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BiEncoder prediction completed successfully\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,889 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BiEncoder prediction completed successfully\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,889 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,889 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,889 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:224.48|#ModelName:model,Level:Model|#type:GAUGE|#hostname:2240db8f9a7b,1758981179,6a311f04-2a5d-4617-b85a-1015d77444a8, pattern=[METRICS]\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,891 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:224.48|#ModelName:model,Level:Model|#hostname:2240db8f9a7b,requestID:6a311f04-2a5d-4617-b85a-1015d77444a8,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,891 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 6a311f04-2a5d-4617-b85a-1015d77444a8\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,891 [INFO ] W-9000-model_1.0 ACCESS_LOG - /172.18.0.1:49262 \"POST /invocations HTTP/1.1\" 200 230\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,892 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,892 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:228541.207|#model_name:model,model_version:default|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,892 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:130.726|#model_name:model,model_version:default|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,893 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,893 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 228\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,893 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "Query embeddings shape: (1, 768)\n",
      "Document embeddings shape: (1, 768)\n",
      "\n",
      "Cosine similarity: 0.8666\n"
     ]
    }
   ],
   "source": [
    "local_predictor.serializer = JSONSerializer()\n",
    "local_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "# BiEncoder: 단일 쿼리-문서 쌍 테스트\n",
    "result = local_predictor.predict({\n",
    "    \"queries\": [\"맛있는 한국 전통 음식 김치찌개\"],\n",
    "    \"documents\": [\"김치찌개와 된장찌개는 한국의 대표 전통 음식입니다.\"]\n",
    "})\n",
    "\n",
    "print(f\"Query embeddings shape: ({result['num_queries']}, {result['embedding_dim']})\")\n",
    "print(f\"Document embeddings shape: ({result['num_documents']}, {result['embedding_dim']})\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 유사도 계산 (이미 정규화되어 있으므로 내적만 계산)\n",
    "query_emb = np.array(result[\"query_embeddings\"])[0]\n",
    "doc_emb = np.array(result[\"doc_embeddings\"])[0]\n",
    "\n",
    "similarity = np.dot(query_emb, doc_emb)\n",
    "\n",
    "print(f\"\\nCosine similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3개의 샘플 추론 및 유사도 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RootlessDocker not detected, falling back to remote host IP or localhost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,972 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,972 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1758981179972\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,973 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,973 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,974 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,974 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting BiEncoder prediction\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,974 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting BiEncoder prediction\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,974 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 3 query(s) and 3 document(s)\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,974 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 3 query(s) and 3 document(s)\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,991 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BiEncoder prediction completed successfully\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,991 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BiEncoder prediction completed successfully\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,991 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,991 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,993 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:20.23|#ModelName:model,Level:Model|#type:GAUGE|#hostname:2240db8f9a7b,1758981179,d3d70cad-d06c-4235-9549-81bba93547ee, pattern=[METRICS]\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,994 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:20.23|#ModelName:model,Level:Model|#hostname:2240db8f9a7b,requestID:d3d70cad-d06c-4235-9549-81bba93547ee,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,994 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId d3d70cad-d06c-4235-9549-81bba93547ee\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,995 [INFO ] W-9000-model_1.0 ACCESS_LOG - /172.18.0.1:49262 \"POST /invocations HTTP/1.1\" 200 24\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,995 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,996 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:22524.71|#model_name:model,model_version:default|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,996 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:90.494|#model_name:model,model_version:default|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,996 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,996 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 22\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:52:59,996 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981179\n",
      "Encoding time: 85.2ms\n",
      "\n",
      "Pair 1: [0.8666] - 상\n",
      "  Q: 맛있는 한국 전통 음식 김치찌개\n",
      "  D: 김치찌개와 된장찌개는 한국의 대표 전통 음식입니다.\n",
      "\n",
      "Pair 2: [0.7164] - 중\n",
      "  Q: 최신 기술 발전\n",
      "  D: 인공지능 기술이 빠르게 발전하고 있습니다.\n",
      "\n",
      "Pair 3: [0.4745] - 하\n",
      "  Q: 색깔\n",
      "  D: 파리의 에펠탑은 프랑스의 상징입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.utils import test_biencoder_pairs\n",
    "\n",
    "query_doc_pairs = [\n",
    "    (\n",
    "        \"맛있는 한국 전통 음식 김치찌개\",\n",
    "        \"김치찌개와 된장찌개는 한국의 대표 전통 음식입니다.\"\n",
    "    ),\n",
    "    (\n",
    "        \"최신 기술 발전\",\n",
    "        \"인공지능 기술이 빠르게 발전하고 있습니다.\"\n",
    "    ),\n",
    "    (\n",
    "        \"색깔\",\n",
    "        \"파리의 에펠탑은 프랑스의 상징입니다.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# BiEncoder 쌍별 유사도 테스트 실행\n",
    "test_biencoder_pairs(local_predictor, query_doc_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8개 쿼리-문서 쌍 배치 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RootlessDocker not detected, falling back to remote host IP or localhost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,078 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:2240db8f9a7b,timestamp:1758981180\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,078 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1758981180078\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,079 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1758981180\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,080 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,080 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,080 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting BiEncoder prediction\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,080 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting BiEncoder prediction\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,080 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 8 query(s) and 8 document(s)\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,080 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 8 query(s) and 8 document(s)\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,099 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BiEncoder prediction completed successfully\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,099 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BiEncoder prediction completed successfully\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,099 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,099 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,105 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:25.4|#ModelName:model,Level:Model|#type:GAUGE|#hostname:2240db8f9a7b,1758981180,e54df52d-75be-4c7a-8933-263af612e741, pattern=[METRICS]\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,105 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:25.4|#ModelName:model,Level:Model|#hostname:2240db8f9a7b,requestID:e54df52d-75be-4c7a-8933-263af612e741,timestamp:1758981180\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,106 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId e54df52d-75be-4c7a-8933-263af612e741\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,107 [INFO ] W-9000-model_1.0 ACCESS_LOG - /172.18.0.1:49262 \"POST /invocations HTTP/1.1\" 200 30\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,107 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981180\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,107 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:28586.283|#model_name:model,model_version:default|#hostname:2240db8f9a7b,timestamp:1758981180\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,108 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:111.265|#model_name:model,model_version:default|#hostname:2240db8f9a7b,timestamp:1758981180\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,108 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981180\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,108 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 28\n",
      "xrwfv8abms-algo-1-p6d1o  | 2025-09-27T13:53:00,108 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:2240db8f9a7b,timestamp:1758981180\n",
      "Batch inference completed:\n",
      "  Queries: 8\n",
      "  Documents: 8\n",
      "  Embedding dim: 768\n",
      "\n",
      "Pair-wise cosine similarities:\n",
      "  Pair 1: 0.8666\n",
      "  Pair 2: 0.7164\n",
      "  Pair 3: 0.4745\n",
      "  Pair 4: 0.6969\n",
      "  Pair 5: 0.6220\n",
      "  Pair 6: 0.5832\n",
      "  Pair 7: 0.6482\n",
      "  Pair 8: 0.6714\n"
     ]
    }
   ],
   "source": [
    "batch_result = local_predictor.predict({\n",
    "    \"queries\": [\n",
    "        \"맛있는 한국 전통 음식 김치찌개\",\n",
    "        \"최신 기술 발전\", \n",
    "        \"색깔\",\n",
    "        \"여행 계획\",\n",
    "        \"스포츠 경기\",\n",
    "        \"영화 추천\",\n",
    "        \"날씨 정보\",\n",
    "        \"건강 관리\"\n",
    "    ],\n",
    "    \"documents\": [\n",
    "        \"김치찌개와 된장찌개는 한국의 대표 전통 음식입니다.\",\n",
    "        \"인공지능 기술이 빠르게 발전하고 있습니다.\",\n",
    "        \"파리의 에펠탑은 프랑스의 상징입니다.\",\n",
    "        \"제주도는 한국의 인기 여행지입니다.\",\n",
    "        \"축구 경기가 오늘 저녁에 있습니다.\",\n",
    "        \"최근 개봉한 영화가 좋은 평가를 받고 있습니다.\",\n",
    "        \"내일은 맑은 날씨가 예상됩니다.\",\n",
    "        \"규칙적인 운동이 건강에 좋습니다.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"Batch inference completed:\")\n",
    "print(f\"  Queries: {batch_result['num_queries']}\")\n",
    "print(f\"  Documents: {batch_result['num_documents']}\")\n",
    "print(f\"  Embedding dim: {batch_result['embedding_dim']}\\n\")\n",
    "\n",
    "# 각 쌍의 코사인 유사도 계산\n",
    "query_embs = np.array(batch_result['query_embeddings'])\n",
    "doc_embs = np.array(batch_result['doc_embeddings'])\n",
    "\n",
    "print(\"Pair-wise cosine similarities:\")\n",
    "for i in range(len(query_embs)):\n",
    "    similarity = np.dot(query_embs[i], doc_embs[i])\n",
    "    print(f\"  Pair {i+1}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 로컬 엔드 포인트 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully Stopping... press Ctrl+C again to force\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Endpoint 삭제 완료\n"
     ]
    }
   ],
   "source": [
    "local_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "print(\"✅ Endpoint 삭제 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
