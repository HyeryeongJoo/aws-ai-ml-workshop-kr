{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f604ee1d-3516-4b8f-8acf-6cbe8a286274",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Deploy DeepSeek-Coder-V2 on \"S3\" with vLLM on SageMaker Endpoint using LMI container from DJL.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c88ae-615f-4f81-acf2-79247338e30b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use DJL with the SageMaker Python SDK\n",
    "- SageMaker Python SDK를 사용하면 Deep Java Library를 이용하여 Amazon SageMaker에서 모델을 호스팅할 수 있습니다. <BR>\n",
    "- 이 노트북은 1_deepseek-deploy-djl-lmi.ipynb 와 거의 유사하지만 다음과 같은 점이 다릅니다.\n",
    "    - [deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct) (16 B )모델을 Hugging Face 에서 다운로드 하지 않고, S3에서 다운로드 해서 SageMaker Endpoint 를 생성 합니다.모델 파일들이 다르기에 다음과 같은 작업을 수행 합니다.\n",
    "        - deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct 를 로컬에 다운로드 합니다.\n",
    "        - 로컬에 다운로드 한 모델 파일을 S3 에 업로드 합니다. \n",
    "- 또한 이 노트북은 파인 튜닝한 모델 파일 (가중치 및 모델 정의) 들을 S3 에 업로딩하고 세이지 메이커 엔드포인트를 생성하는데 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e16e4-8943-43a1-8c17-48c2076421ee",
   "metadata": {},
   "source": [
    "### 선수 조건\n",
    "- 이 노트북을 실행 전에 [0_setup.ipynb](0_setup.ipynb) 을 실행해야 합니다.\n",
    "- 커널은 conda_pytorch_p310 을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa36dc1-7a95-4a53-bb9b-8e561e9230bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Depoly model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570df89-db29-4643-adff-55e09880c3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94854307-4322-4109-85dc-ec59d1630066",
   "metadata": {
    "tags": []
   },
   "source": [
    "- [Avalable DLC (Deep Learning Containers)](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a351-1e53-4ea0-ae6f-e1ddc3c3aea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "# 기본 버킷 이름 가져오기\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print(f\"Default SageMaker bucket name: {default_bucket}\")\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "sm_autoscaling_client = boto3.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95536d9-4e74-4279-9281-0bd2a1095a15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d4466-3ea2-47ea-99cd-ee8032056986",
   "metadata": {},
   "source": [
    " - [[DOC] DJL for serving](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html)\n",
    " - 인스턴스는 ml.g5.12xlarge 를 권장 사용합니다. \n",
    "     - ml.p4d.24xlarge 또한 더 좋은 성능을 위해서 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07253a8e-cade-4c99-ad99-64f28a18bb9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "# instance_type = \"ml.p4d.24xlarge\"\n",
    "\n",
    "container_uri =\"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu124\"\n",
    "model_id == \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"DeepSeek-Coder-V2-Instruct\")\n",
    "model_name = sagemaker.utils.name_from_base(\"DeepSeek-Coder-V2-Instruct\")\n",
    "container_startup_health_check_timeout = 120 # seconds\n",
    "\n",
    "print (f'model_id: {model_id}')\n",
    "print (f'container_uri: {container_uri}')\n",
    "print (f'instance_type: {instance_type}')\n",
    "print (f'model_name: {model_name}')\n",
    "print (f'endpoint_name: {endpoint_name}')\n",
    "print (f'container_startup_health_check_timeout: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f6199-10f2-4ef3-9295-81ac8d426ab9",
   "metadata": {},
   "source": [
    "### LMI container Image:  v1.0-djl-0.32.0-inf-lmi-14.0.0\n",
    "* Release date (Feb 8, 25) \n",
    "    * https://github.com/aws/deep-learning-containers/releases/tag/v1.0-djl-0.32.0-inf-lmi-14.0.0\n",
    "* Docker Image\n",
    "    * 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu124\n",
    "    * 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu124\n",
    "\n",
    "위의 버전은 2025.2.12 현재 SageMaker SDK 에 업데이트 되지 않은 컨테이너 임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3bc2bd-68b9-42f7-84a2-c04966a57149",
   "metadata": {},
   "source": [
    "### Download model \n",
    "- HF 에서 해당 모델을 로컬에 다운로드 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3e2b1-4a52-49dd-a452-3634b4adb56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# is_needed_downlaod_model = True\n",
    "is_needed_downlaod_model = False\n",
    "\n",
    "local_model_path = \"./deepseek-coder-v2\"\n",
    "\n",
    "if is_needed_downlaod_model:\n",
    "    model_path = snapshot_download(\n",
    "        repo_id=\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n",
    "        local_dir=local_model_path,  # 저장하고 싶은 로컬 경로\n",
    "        local_dir_use_symlinks=False  # 실제 파일을 다운로드\n",
    "    )\n",
    "else:\n",
    "    print(\"model is already downloaded\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bea3c2-5eac-486b-8794-a5b9d71f8ec7",
   "metadata": {},
   "source": [
    "## Upload model files to S3\n",
    "- 로컬의 모델을 S3 에 업로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680440e-9124-4af3-b038-e389680fbb21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "\n",
    "def upload_files_to_s3(local_path, bucket, key_prefix):\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "\n",
    "    # 로컬 파일을 S3에 업로드\n",
    "    # upload_data는 파일이나 디렉토리 경로를 받아서 S3에 업로드하고, S3 경로를 반환합니다\n",
    "    s3_path = sagemaker_session.upload_data(\n",
    "        path=local_path,\n",
    "        bucket=None,  # None으로 설정하면 기본 SageMaker 버킷을 사용합니다\n",
    "        key_prefix= key_prefix\n",
    "    )\n",
    "\n",
    "    print(f\"Uploaded to: {s3_path}\")\n",
    "    return s3_path\n",
    "    \n",
    "# SageMaker 세션 초기화\n",
    "# is_needed_upload_model = True\n",
    "is_needed_upload_model = False\n",
    "\n",
    "bucket_key_prefix = \"deepseek\"\n",
    "if is_needed_upload_model: \n",
    "    \n",
    "    s3_model_path = upload_files_to_s3(\n",
    "                        local_path=local_model_path, \n",
    "                        bucket=default_bucket, \n",
    "                        key_prefix=bucket_key_prefix\n",
    "    )    \n",
    "else:\n",
    "    s3_model_path = \"s3://sagemaker-us-east-1-057716757052/deepseek\"\n",
    "    print(\"model is already uploaded\")\n",
    "    print(\"s3 model path: \", s3_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafbe20e-2ce7-470f-b478-dcc0b682c3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 ls s3://sagemaker-us-east-1-057716757052/deepseek --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38065fa5-1968-4b85-bf64-918d84965732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! aws s3 rm s3://sagemaker-us-east-1-057716757052/deepseek --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca14bd8-c9d7-42cd-9bbc-3c6832359809",
   "metadata": {},
   "source": [
    "### Creat model with env variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36e275-b199-4ce3-b1b2-5ecca0554210",
   "metadata": {},
   "source": [
    "- Target model: [DeepSeek-Coder-V2-Light-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9449ef-905a-4224-97eb-9fd02aaab83e",
   "metadata": {},
   "source": [
    "- **[Backend for attention computation in vLLM](https://docs.vllm.ai/en/latest/serving/env_vars.html)**\n",
    "    - Available options:\n",
    "        - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n",
    "        - \"FLASH_ATTN\": use FlashAttention\n",
    "        - \"XFORMERS\": use XFormers\n",
    "        - \"ROCM_FLASH\": use ROCmFlashAttention\n",
    "        - \"FLASHINFER\": use flashinfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915352b-2ef9-4b3b-a795-fac419793221",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **'\"OPTION_DISABLE_FLASH_ATTN\": \"false\"'** is for HF Accelerate with Seq-Scheduler\n",
    "    - It will be ignored when using vLLM beckend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4886c8-87e2-4b09-8c65-b52b8e32889b",
   "metadata": {},
   "source": [
    "- [[DOC] DJL-Container and Model Configurations (info. about properties)](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/deployment_guide/configurations.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7b039-c08c-436f-a73a-a11f960fe71b",
   "metadata": {},
   "source": [
    "### 아래의 환경 변수에서 HF_MODEL_ID 삭제 함\n",
    "- 1_deepseek-deploy-djl-lmi.ipynb 노트북과 다르게 HF_MODEL_ID\": model_id 삭제 함\n",
    "- S3 에서 모델 파일을 사용하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4643e-577e-43c2-ada0-d45eec22b840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deploy_env = {\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"2\",\n",
    "    \"OPTION_DTYPE\":\"fp16\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"8192\",\n",
    "    \"VLLM_ATTENTION_BACKEND\": \"XFORMERS\",\n",
    "    \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.9\",  # GPU 메모리 사용률 제한 (기본값 0.9)\n",
    "    \"VLLM_MAX_NUM_SEQS\": \"16\",  # 동시 처리 시퀀스 수 제한    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9ca03-9c81-437f-be7e-815886980c72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_s3_path={'S3DataSource': {'S3Uri': f'{s3_model_path}/', 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n",
    "print(\"model_s3_path: \\n\", model_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280e310-0690-4f8f-9d8e-f77e5f37b405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = sagemaker.Model(\n",
    "    image_uri=container_uri, \n",
    "    model_data=model_s3_path,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    name = model_name,\n",
    "    env=deploy_env,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebca82-7727-4360-b144-5017f54bc68b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129dc158-7ef5-48d4-b58e-bdd4913b31dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=container_startup_health_check_timeout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e65359c-0c75-4fcb-a813-1c4c11b8783a",
   "metadata": {},
   "source": [
    "## 2. Invocation (Generate Text using the endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b26a7-bb0e-4970-9c53-38dc0f1fcb00",
   "metadata": {},
   "source": [
    "### Get a predictor for your endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc36cb-ef32-414c-9303-798d8aa5ac2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205ee8c-ea67-4abf-b52e-e6c31d2c5fdc",
   "metadata": {},
   "source": [
    "### Make a prediction with your endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc548d06-c631-447d-a790-6295f2627214",
   "metadata": {},
   "source": [
    "- **question candidates**\n",
    "    - write a quick sort algorithm in python.\n",
    "    - Write a piece of quicksort code in C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2dac9-2994-4dc2-bfbf-f510277c3001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = predictor.predict(\n",
    "    {\n",
    "        \"inputs\": \"write a quick sort algorithm in python and description\",\n",
    "        \"parameters\": {\"do_sample\": True, \"max_new_tokens\": 2048},\n",
    "    }\n",
    ")\n",
    "\n",
    "print(outputs[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854bba14-47af-4c1f-b899-1b230f110df8",
   "metadata": {},
   "source": [
    "- **With chat template**\n",
    "    - [DJL Chat Completions API Schema](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/chat_input_output_schema.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ff3be-c866-401c-b0fa-642ac4ff7fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works! anyway, write a quick sort algorithm in python and description\"},\n",
    "]\n",
    "\n",
    "result = predictor.predict(\n",
    "    {\"messages\": chat, \"max_tokens\": 1024}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa390f3-94ee-4b8f-8c28-3dcc13fbf321",
   "metadata": {},
   "source": [
    "## 3. Streaming output from the endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ae118-08e2-4dc3-970c-6fcb2ae2028f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673dd545-56ae-421c-b43d-9b9bf4f01d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 다양한 코딩 태스크를 위한 프롬프트 리스트\n",
    "prompts = [\n",
    "    \"write a quick sort algorithm in python.\",\n",
    "    \"Write a Python function to implement a binary search algorithm.\",\n",
    "    \"Create a JavaScript function to flatten a nested array.\",\n",
    "    \"Implement a simple REST API using Flask in Python.\",\n",
    "    \"Write a SQL query to find the top 5 customers by total purchase amount.\",\n",
    "    \"Create a React component for a todo list with basic CRUD operations.\",\n",
    "    \"Implement a depth-first search algorithm for a graph in C++.\",\n",
    "    \"Write a bash script to find and delete files older than 30 days.\",\n",
    "    \"Create a Python class to represent a deck of cards with shuffle and deal methods.\",\n",
    "    \"Write a regular expression to validate email addresses.\",\n",
    "    \"Implement a basic CI/CD pipeline using GitHub Actions.\"\n",
    "]\n",
    "\n",
    "def generate_payload():\n",
    "    # 랜덤하게 프롬프트 선택\n",
    "    prompt = random.choice(prompts)\n",
    "    \n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 400,\n",
    "            # \"return_full_text\": False  # This does not work with Phi3\n",
    "        },\n",
    "        \"stream\": True,\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c72d84-0127-4422-b29e-fd00bd054de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Invoke the endpoint\n",
    "resp = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    # Body=json.dumps(body), \n",
    "    Body=generate_payload(), \n",
    "    \n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "print(\"Generated response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "buffer = \"\"\n",
    "for event in resp['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        buffer += chunk\n",
    "        try:\n",
    "            # Try to parse the buffer as JSON\n",
    "            data = json.loads(buffer)\n",
    "            if 'token' in data:\n",
    "                print(data['token']['text'], end='', flush=True)\n",
    "            buffer = \"\"  # Clear the buffer after successful parsing\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, keep the buffer for the next iteration\n",
    "            pass\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3294351-9f0a-40be-bd89-b157da99d06e",
   "metadata": {},
   "source": [
    "- **With chat template**\n",
    "    - [DJL Chat Completions API Schema](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/chat_input_output_schema.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7caf0-8928-44ad-8f06-03c816f7d699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 다양한 코딩 태스크를 위한 프롬프트 리스트\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works! anyway, write a quick sort algorithm in python and description\"},\n",
    "]\n",
    "\n",
    "result = predictor.predict(\n",
    "    {\"messages\": chat, \"max_tokens\": 1024}\n",
    ")\n",
    "\n",
    "def generate_payload():\n",
    "    # 랜덤하게 프롬프트 선택\n",
    "    prompt = random.choice(prompts)\n",
    "    \n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"messages\": chat,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"stream\": True,\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e24b5-895f-41fd-ba70-b1e520302aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Invoke the endpoint\n",
    "resp = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    # Body=json.dumps(body), \n",
    "    Body=generate_payload(), \n",
    "    \n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "print(\"Generated response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "buffer = \"\"\n",
    "for event in resp['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        buffer += chunk\n",
    "        try:\n",
    "            # Try to parse the buffer as JSON\n",
    "            data = json.loads(buffer)\n",
    "            if 'choices' in data:\n",
    "                print(data['choices'][0]['delta']['content'], end='', flush=True)\n",
    "            buffer = \"\"  # Clear the buffer after successful parsing\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, keep the buffer for the next iteration\n",
    "            pass\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3261a-b6a5-4895-9ae0-2f2b4759ab2b",
   "metadata": {},
   "source": [
    "## 4. delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f9af8-13a4-4e0c-bd51-5c3655a2eea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016704c1-63a3-4979-812a-74178f012eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
