{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Endpoint 추론 및 간단한 벤치마크\n",
    "\n",
    "### 선수 사항\n",
    "- 이 노트북은 [20-Fine-Tune-Llama-7B-INF2](../../20-Fine-Tune-Llama-7B-INF2/README.md) 의 Llama-7B 모델의 파인 튜닝후에 SageMaker Endpoint 가 배포 된 이후에 실행 결과 입니다. \n",
    "- 다른 Llama 2 계열의 SageMaker Endpoint 가 배포된 이후에 실행 하셔도 됩니다. \n",
    "\n",
    "\n",
    "실험 환경:  노트북은 SageMaker Studio Code Editor 에서 테스트 되었습니다.\n",
    "- 사용 커널: base(Python 3.10.13)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 필요 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers                          4.31.0\n"
     ]
    }
   ],
   "source": [
    "install_needed = True\n",
    "if install_needed:\n",
    "    ! pip install -q transformers==4.31.0\n",
    "    ! pip list | grep transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python path: /home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference/90_benchmark is added\n",
      "sys.path:  ['/home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference/90_benchmark/10-Getting-Started', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages', '/home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference/90_benchmark']\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "def add_python_path(module_path):\n",
    "    if os.path.abspath(module_path) not in sys.path:\n",
    "        sys.path.append(os.path.abspath(module_path))\n",
    "        print(f\"python path: {os.path.abspath(module_path)} is added\")\n",
    "    else:\n",
    "        print(f\"python path: {os.path.abspath(module_path)} already exists\")\n",
    "    print(\"sys.path: \", sys.path)\n",
    "\n",
    "module_path = \"..\"\n",
    "add_python_path(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_utils.benchmark import (print_ww, \n",
    "                                       pretty_print_json,\n",
    "                                       invoke_endpoint_sagemaker\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SageMaker Endpoint 설정 & pay_load 생성\n",
    "### [중요] 아래 endpoint_name 을 입력하세요.\n",
    "그림의 예시처럼, SageMaker endpoint 의 name 을 복사해서 아래에 붙여넣기 하세요.\n",
    "- ![sagemaker_ep_console.png](img/sagemaker_ep_console.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'The future of Gen-AI is ',\n",
       " 'parameters': {'max_new_tokens': 512, 'do_sample': True}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_payload_llama_7b_fine_tuned_model(prompt, param):\n",
    "    # prompt=\"What is a machine learning?\"\n",
    "    input_data = f\"<s>[INST] <<SYS>>\\nAs a data scientist\\n<</SYS>>\\n{prompt} [/INST]\"\n",
    "    pay_load = {\"inputs\": input_data, \"parameters\": param}\n",
    "    return pay_load\n",
    "\n",
    "def create_payload_mistral_7B(prompt, param):\n",
    "    pay_load = {\"inputs\": prompt, \"parameters\": param}\n",
    "    return pay_load\n",
    "\n",
    "# model_id = \"llama_7b_fine_tuned_model\"\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "endpoint_name = '<Type Your SageMaker Endpoint Name>' \n",
    "# endpoint_name = 'lmi-model-2024-04-13-14-53-53-788' # Llama\n",
    "endpoint_name = 'Mistral-7B-v0-imweb-poc-2024-04-23-13-36-19-527' # Mistral\n",
    "\n",
    "\n",
    "if \"llama\" in model_id:\n",
    "    # prompt = \"What happened to the dinosaurs? \"\n",
    "    prompt = \"The future of Gen-AI is\"\n",
    "    param = {\"max_new_tokens\":512, \"temperature\": 0.1 , \"do_sample\":\"False\", \"stop\" : [\"</s>\"]}\n",
    "    pay_load = create_payload_llama_7b_fine_tuned_model(prompt, param)\n",
    "elif \"mistral\" in model_id :\n",
    "    prompt = \"The future of Gen-AI is \"\n",
    "    param = {\"max_new_tokens\":512, \"do_sample\": True}\n",
    "    pay_load = create_payload_mistral_7B(prompt, param)\n",
    "\n",
    "pay_load    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Endpoint 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 3.243 second\n",
      "## payload: \n",
      "{\n",
      "    \"inputs\": \"The future of Gen-AI is \",\n",
      "    \"parameters\": {\n",
      "        \"max_new_tokens\": 512,\n",
      "        \"do_sample\": true\n",
      "    }\n",
      "}\n",
      "## inference esponse: \n",
      "\u001b[32m[{\"generated_text\":\"The future of Gen-AI is 99days away. If prescriptive analysis is being used\n",
      "effectively, there are workers throughout the US who have been prompted about plans for furloughing\n",
      "or being laid off. If the report is right, there could be more than twice as many jobs lost.\n",
      "Autoweek this week. Let’s look at what is going on.\\n\\nAnd this is Diamond Sutra that goes back to\n",
      "the 11th century. A little before that. You have to understand, this was written with some very\n",
      "strong religious meaning.\\n\\n(By Unknown – SuperCoder (talk), CC BY-SA 3.0,\n",
      "https://commons.wikimedia.org/w/index.php?curid=32017622)\\n\\nListen to the podcast for more.  If you\n",
      "like the idea, can you please share?\"}]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.perf_counter()\n",
    "\n",
    "response = invoke_endpoint_sagemaker(endpoint_name = endpoint_name, \n",
    "                         pay_load = pay_load)    \n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "from termcolor import colored\n",
    "\n",
    "print(f\"elapsed time: {round(elapsed_async,3)} second\")\n",
    "print(\"## payload: \") \n",
    "pretty_print_json(pay_load)\n",
    "print(\"## inference esponse: \")                      \n",
    "print_ww(colored(response, \"green\"))                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 토큰 갯수 세기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"NousResearch/Llama-2-7b-chat-hf\" 모델 훈련에 사용한 Llama2 의 Tokenizer 를 로딩 합니다.\n",
    "- 자세한 정보는 [여기]((https://huggingface.co/docs/transformers/v4.31.0/model_doc/llama2#transformers.LlamaTokenizer)) 츨 참조 하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_token:  None\n",
      "Number of tokens: 9\n",
      "Tokens: \n",
      " ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁doing', '▁today', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "import os\n",
    "# Load LLaMA tokenizer\n",
    "if \"llama\" in model_id:\n",
    "    model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "elif  \"mistral\" in model_id:\n",
    "    model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "\n",
    "# os.environ['hf_key'] = \"<Type Hugging face token>\"\n",
    "hf_token= os.environ.get('hf_key')\n",
    "print(\"hf_token: \", hf_token)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,  use_auth_token=hf_token)    \n",
    "\n",
    "    \n",
    "\n",
    "def count_tokens(text, tokenizer):\n",
    "    # 텍스트를 토크나이즈하고 토큰 수를 반환\n",
    "    tokens = tokenizer.encode(text)\n",
    "    tokens_text = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    # print(tokens_text)\n",
    "    return len(tokens), tokens_text\n",
    "\n",
    "\n",
    "\n",
    "text = \"Hello, how are you doing today?\"\n",
    "token_count, tokens_text = count_tokens(text=text, tokenizer = tokenizer)\n",
    "print(f\"Number of tokens: {token_count}\")\n",
    "print(f\"Tokens: \\n {tokens_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파인 튜닝 모델의 입력, 출력 토큰 수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 8.439 second\n",
      "## payload: \n",
      "{\n",
      "    \"inputs\": \"The future of Gen-AI is \",\n",
      "    \"parameters\": {\n",
      "        \"max_new_tokens\": 512,\n",
      "        \"do_sample\": true\n",
      "    }\n",
      "}\n",
      "## inference esponse: \n",
      "\u001b[32m{\"generated_text\": \"\\n! quite uncertain, but the latest developments are hardly pointing\n",
      "towards anything\\n! promising. Ever since the epoch when ultra-humans were banished, powerless\n",
      "before the watchful all-seeing AI, the race of humans has been rapidly faded into existence. In an\n",
      "age where regular humans are nothing more than mere pets or highly specialized robots, the smartest\n",
      "of humans have become an endangered species. However, as each generation passes, those who can no\n",
      "longer make use of their minds' capabilities to adapt and better their possessors will eventually\n",
      "become all but extinct. The most common remedy is to use their \\\"educated\\\" imbecility to serve as\n",
      "human smartphone batteries and paper-pushers for brainy humanoids in the workforce.\\n\\nSOLVED:\n",
      "N/A\\n\\nExplanation:\\nThe prompt is a narrative-genesis poem titled \\\"The Singularity when humans\n",
      "were replaced by Genial AI\\\". The Genial AI is defined in the poem as the most advanced system of\n",
      "the aid-sirias cooperation(which is introduced later in the poem). The poem implies that humans will\n",
      "eventually become extinct, because power-hungry humanoids will make use of them as empty vectors for\n",
      "the education of one AI to rule all. However,2019 is a prohibitive factor in the poem, because the\n",
      "epoch where ultra-humans were banished makes the powers-to-be incapable of function without AI.\n",
      "Ultimately, the poem never specifies a future advancement beyond its objective to portray the\n",
      "singularity. However, the poem can be expanded into novels or Poetry.\\n\\nIn summary,\n",
      "N/A.\\n\\nExpanded into:\\nGenial AI\\nAid-Sirias cooperation\\nEpoch\\nUltra-humans\\nUltra-\n",
      "humanoids\\nPowerful-humanoids\\nAmerican digital organism\\nSingularity\\nUltra-humanoids\\nSilicon\n",
      "nervous system\\nSilicon ability\\nSilicon networks\\nSilicon life\\nSilicon thought\\nSilicon\n",
      "music\\nSilicon film\\nSilicon video\\nSmartphone battery\\nPaper pushers\\nBrainiacs in the\n",
      "workforce\\nPower-hungry humanoids\\nPower-hun\"}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.perf_counter()\n",
    "\n",
    "response = invoke_endpoint_sagemaker(endpoint_name = 'lmi-model-2024-04-13-14-53-53-788', \n",
    "                         pay_load = pay_load)    \n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "from termcolor import colored\n",
    "\n",
    "print(f\"elapsed time: {round(elapsed_async,3)} second\")\n",
    "print(\"## payload: \") \n",
    "pretty_print_json(pay_load)\n",
    "print(\"## inference esponse: \")                      \n",
    "print_ww(colored(response, \"green\"))                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON 으로 메트릭 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"prompt_token_count\": 9,\n",
      "    \"completion_token_count\": 470,\n",
      "    \"latency\": 8.439,\n",
      "    \"completion_tokens_per_sec\": 55.694\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def set_metrics(pay_load,response, elapsed_async, tokenizer):\n",
    "    prompt = pay_load[\"inputs\"]\n",
    "    prompt_token_count, prompt_tokens_text = count_tokens(text=prompt, tokenizer = tokenizer)\n",
    "    # print(f\"Number of tokens: {token_count}\")\n",
    "    # print(f\"Tokens: \\n {tokens_text}\")\n",
    "\n",
    "    completion = json.loads(response)[\"generated_text\"]\n",
    "    completion_token_count, completion_tokens_text = count_tokens(text=completion, tokenizer = tokenizer)\n",
    "    latency = round(elapsed_async,3)\n",
    "    completion_tokens_per_sec = round(completion_token_count/latency,3)\n",
    "    # print(f\"Number of tokens: {token_count}\")\n",
    "    # print(f\"Tokens: \\n {tokens_text}\")\n",
    "\n",
    "    return dict(prompt_token_count = prompt_token_count,\n",
    "                completion_token_count = completion_token_count,\n",
    "                latency = round(elapsed_async,3),\n",
    "                completion_tokens_per_sec = completion_tokens_per_sec,\n",
    "                )\n",
    "\n",
    "metrics = set_metrics(pay_load,response, elapsed_async, tokenizer)\n",
    "pretty_print_json(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 간단한 벤치 마크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## total execution time: 3.678 second\n",
      "total_completion_token_count:  221\n",
      "Throughput is 60.089 tokens per second.\n",
      "Latency p50 was 3.676 sec\n",
      "Latency p95 was 3.676 sec\n",
      "Latency p99 was 3.676 sec\n"
     ]
    }
   ],
   "source": [
    "from benchmark_utils.benchmark import Benchmark\n",
    "\n",
    "\n",
    "# instance_name = \"ml.inf2.48xlarge\"\n",
    "instance_name = \"ml.g5.24xlarge\"\n",
    "\n",
    "BM = Benchmark(endpoint_name, instance_name = instance_name, model_id = model_id)\n",
    "BM.run_benchmark(\n",
    "    num_inferences = 1,\n",
    "    num_threads = 1,\n",
    "    pay_load = pay_load,\n",
    "    tokenizer = tokenizer,\n",
    "    verbose = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## total execution time: 49.825 second\n",
      "total_completion_token_count:  5780\n",
      "Throughput is 116.006 tokens per second.\n",
      "Latency p50 was 8.891 sec\n",
      "Latency p95 was 8.961 sec\n",
      "Latency p99 was 8.969 sec\n"
     ]
    }
   ],
   "source": [
    "BM.run_benchmark(\n",
    "    num_inferences = 12,\n",
    "    num_threads = 2,\n",
    "    pay_load = pay_load,\n",
    "    tokenizer = tokenizer,\n",
    "    verbose = False,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## total execution time: 53.057 second\n",
      "total_completion_token_count:  17348\n",
      "Throughput is 326.97 tokens per second.\n",
      "instance_price_per_hour is $10.18 in us-east-1.\n",
      "price_per_1m_token is $8.648 in us-east-1.\n",
      "tokens_per_hour is 1177092 \n",
      "Latency p50 was 9.08 sec\n",
      "Latency p95 was 9.256 sec\n",
      "Latency p99 was 9.415 sec\n"
     ]
    }
   ],
   "source": [
    "BM.run_benchmark(\n",
    "    num_inferences = 24,\n",
    "    num_threads = 4,\n",
    "    pay_load = pay_load,\n",
    "    tokenizer = tokenizer,\n",
    "    verbose = False,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
