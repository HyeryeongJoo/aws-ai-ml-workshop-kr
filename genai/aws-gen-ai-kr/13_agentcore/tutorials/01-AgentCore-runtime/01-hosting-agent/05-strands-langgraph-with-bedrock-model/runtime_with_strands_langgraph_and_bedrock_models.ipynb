{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0122e65c053f38",
   "metadata": {},
   "source": [
    "# Hosting Strands + LangGraph agent with Amazon Bedrock models in Amazon Bedrock AgentCore Runtime\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial we will learn how to host your existing agent, using Amazon Bedrock AgentCore Runtime. \n",
    "\n",
    "We will focus on a LangGraph with Amazon Bedrock model example. For Strands Agents with Amazon Bedrock model check [here](../01-strands-with-bedrock-model)\n",
    "and for a Strands Agents with an OpenAI model check [here](../03-strands-with-openai-model).\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                      |\n",
    "|:--------------------|:-----------------------------------------------------------------------------|\n",
    "| Tutorial type       | Conversational                                                               |\n",
    "| Agent type          | Single                                                                       |\n",
    "| Agentic Framework   | Strands + LangGraph                                                                    |\n",
    "| LLM model           | Anthropic Claude Sonnet 3                                                    |\n",
    "| Tutorial components | Hosting agent on AgentCore Runtime. Using Strands + LangGraph and Amazon Bedrock Model |\n",
    "| Tutorial vertical   | Cross-vertical                                                               |\n",
    "| Example complexity  | Easy                                                                         |\n",
    "| SDK used            | Amazon BedrockAgentCore Python SDK and boto3                                 |\n",
    "\n",
    "### Tutorial Architecture\n",
    "\n",
    "In this tutorial we will describe how to deploy an existing agent to AgentCore runtime. \n",
    "\n",
    "For demonstration purposes, we will  use a LangGraph agent using Amazon Bedrock models\n",
    "\n",
    "In our example we will use a very simple agent with two tools: `get_weather` and `get_time`. \n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/architecture_runtime.png\" width=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "### Tutorial Key Features\n",
    "\n",
    "* Hosting Agents on Amazon Bedrock AgentCore Runtime\n",
    "* Using Amazon Bedrock models\n",
    "* Using LangGraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a676f58ecf52b42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To execute this tutorial you will need:\n",
    "* Python 3.10+\n",
    "* AWS credentials\n",
    "* Amazon Bedrock AgentCore SDK\n",
    "* LangGraph\n",
    "* Docker running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#!uv add -r requirements.txt --active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca924a7a2731e26f",
   "metadata": {},
   "source": [
    "## Creating your agents and experimenting locally\n",
    "\n",
    "Before we deploy our agents to AgentCore Runtime, let's develop and run them locally for experimentation purposes.\n",
    "\n",
    "For production agentic applications we will need to decouple the agent creation process from the agent invocation one. With AgentCore Runtime, we will decorate the invocation part of our agent with the `@app.entrypoint` decorator and have it as the entry point for our runtime. Let's first look how each agent is developed during the experimentation phase.\n",
    "\n",
    "The architecture here will look as following:\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/architecture_local.png\" width=\"60%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2e9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a5e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"../../../..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1624cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from graph import build_graph\n",
    "from src.utils.common_utils import get_message_from_string\n",
    "from src.utils.strands_sdk_utils import strands_utils\n",
    "from src.prompts.template import apply_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff8f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Default level is INFO\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "def enable_debug_logging():\n",
    "    \"\"\"Enable debug level logging for more detailed execution information.\"\"\"\n",
    "    logging.getLogger(__name__).setLevel(logging.DEBUG)\n",
    "\n",
    "# 로거 설정을 전역으로 한 번만 수행\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.propagate = False\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('\\n%(levelname)s [%(name)s] %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)  # 기본 레벨은 INFO로 설정\n",
    "\n",
    "class Colors:\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a62206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "graph = build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ec3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_workflow(user_input: str, debug: bool = False):\n",
    "    \"\"\"Run the agent workflow with the given user input.\n",
    "\n",
    "    Args:\n",
    "        user_input: The user's query or request\n",
    "        debug: If True, enables debug level logging\n",
    "\n",
    "    Returns:\n",
    "        The final state after the workflow completes\n",
    "    \"\"\"\n",
    "    if not user_input:\n",
    "        raise ValueError(\"Input could not be empty\")\n",
    "\n",
    "    if debug:\n",
    "        enable_debug_logging()\n",
    "\n",
    "    #logger.info(f\"Starting workflow with user input: {user_input}\")\n",
    "    logger.info(f\"{Colors.GREEN}===== Starting workflow ====={Colors.END}\")\n",
    "    logger.info(f\"{Colors.GREEN}\\nuser input: {user_input}{Colors.END}\")\n",
    "    \n",
    "    user_prompts = dedent(\n",
    "        '''\n",
    "        Here is a user request: <user_request>{user_request}</user_request>\n",
    "        '''\n",
    "    )\n",
    "    context = {\"user_request\": user_input}\n",
    "    user_prompts = user_prompts.format(**context)\n",
    "    messages = [get_message_from_string(role=\"user\", string=user_prompts, imgs=[])]\n",
    "\n",
    "        \n",
    "    result = graph.invoke(\n",
    "        input={\n",
    "            # Runtime Variables\n",
    "            \"messages\": messages,\n",
    "            \"request\": user_input,\n",
    "            \"request_prompt\": user_prompts\n",
    "        },\n",
    "        config={\n",
    "            \"recursion_limit\": 100\n",
    "        }\n",
    "    )\n",
    "    logger.debug(f\"{Colors.RED}Final workflow state: {result}{Colors.END}\")\n",
    "    logger.info(f\"{Colors.GREEN}===== Workflow completed successfully ====={Colors.END}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad31c83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [__main__] \u001b[92m===== Starting workflow =====\u001b[0m\n",
      "\n",
      "INFO [__main__] \u001b[92m\n",
      "user input: \n",
      "    안녕 나는 장동진이라고 해. 만나서 반가워\n",
      "    나는 데이터를 제공하고 그것으로 부터 인사이트를 추출하고 싶어. \n",
      "\u001b[0m\n",
      "\n",
      "INFO [graph.nodes] \u001b[92m===== Coordinator talking...... =====\u001b[0m\n",
      "\n",
      "INFO [src.utils.strands_sdk_utils] \u001b[92mCOORDINATOR - Prompt Cache Disabled\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97m안녕\u001b[0m\u001b[97m하세요 장동진님\u001b[0m\u001b[97m! 저는 Bedrock-\u001b[0m\u001b[97mManus입니다. \u001b[0m\u001b[97m만나서 반\u001b[0m\u001b[97m갑습니다!\u001b[0m\u001b[97m\n",
      "\n",
      "데이터 분\u001b[0m\u001b[97m석과 인사이트 추출\u001b[0m\u001b[97m에 대한 요\u001b[0m\u001b[97m청은 저희\u001b[0m\u001b[97m 플래너 시스템과\u001b[0m\u001b[97m 상담이 필요할\u001b[0m\u001b[97m 것 같습니다\u001b[0m\u001b[97m.\n",
      "\n",
      "handoff_to_\u001b[0m\u001b[97mplanner: I'll need to cons\u001b[0m\u001b[97mult our planning system for this request.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [graph.nodes] \u001b[92m===== Coordinator completed task =====\u001b[0m\n",
      "\n",
      "INFO [graph.nodes] \u001b[92m===== Planner generating full plan =====\u001b[0m\n",
      "\n",
      "INFO [src.utils.strands_sdk_utils] \u001b[92mPLANNER - Prompt Cache Enabled\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mI\u001b[0m\u001b[95m nee\u001b[0m\u001b[95md to\u001b[0m\u001b[95m analyze\u001b[0m\u001b[95m this\u001b[0m\u001b[95m user request that\u001b[0m\u001b[95m is\u001b[0m\u001b[95m in\u001b[0m\u001b[95m Korean\u001b[0m\u001b[95m. Let\u001b[0m\u001b[95m me translate\u001b[0m\u001b[95m an\u001b[0m\u001b[95md understan\u001b[0m\u001b[95md what the user\u001b[0m\u001b[95m is\u001b[0m\u001b[95m asking for:\u001b[0m\u001b[95m\n",
      "\n",
      "\"Hello\u001b[0m\u001b[95m, my name is\u001b[0m\u001b[95m Jang\u001b[0m\u001b[95m Don\u001b[0m\u001b[95mgjin. Nice\u001b[0m\u001b[95m to meet you.\u001b[0m\u001b[95m\n",
      "I want\u001b[0m\u001b[95m to provide data an\u001b[0m\u001b[95md extract insights from it\u001b[0m\u001b[95m.\"\u001b[0m\u001b[95m\n",
      "\n",
      "The\u001b[0m\u001b[95m user is introducing\u001b[0m\u001b[95m themselves as\u001b[0m\u001b[95m Jang\u001b[0m\u001b[95m Dongjin an\u001b[0m\u001b[95md they\u001b[0m\u001b[95m want\u001b[0m\u001b[95m to provide data from\u001b[0m\u001b[95m which\u001b[0m\u001b[95m they\u001b[0m\u001b[95m woul\u001b[0m\u001b[95md like to extract insights\u001b[0m\u001b[95m. However\u001b[0m\u001b[95m, they haven\u001b[0m\u001b[95m't provide\u001b[0m\u001b[95md any specific data yet\u001b[0m\u001b[95m or\u001b[0m\u001b[95m details\u001b[0m\u001b[95m about what kin\u001b[0m\u001b[95md of insights\u001b[0m\u001b[95m they are looking\u001b[0m\u001b[95m for.\u001b[0m\u001b[95m\n",
      "\n",
      "Let\u001b[0m\u001b[95m me create\u001b[0m\u001b[95m a plan for how\u001b[0m\u001b[95m we\u001b[0m\u001b[95m woul\u001b[0m\u001b[95md approach this task\u001b[0m\u001b[95m once\u001b[0m\u001b[95m we receive\u001b[0m\u001b[95m the data\u001b[0m\u001b[95m:\n",
      "\n",
      "1.\u001b[0m\u001b[95m First\u001b[0m\u001b[95m, we'll nee\u001b[0m\u001b[95md the\u001b[0m\u001b[95m C\u001b[0m\u001b[95moder to help\u001b[0m\u001b[95m with\u001b[0m\u001b[95m data processing and analysis\u001b[0m\u001b[95m once\u001b[0m\u001b[95m the\u001b[0m\u001b[95m user\u001b[0m\u001b[95m provides their\u001b[0m\u001b[95m data.\u001b[0m\u001b[95m\n",
      "2. At\u001b[0m\u001b[95m the en\u001b[0m\u001b[95md,\u001b[0m\u001b[95m the Reporter\u001b[0m\u001b[95m will compile\u001b[0m\u001b[95m the insights\u001b[0m\u001b[95m into\u001b[0m\u001b[95m a comprehensive\u001b[0m\u001b[95m report.\u001b[0m\u001b[95m\n",
      "\n",
      "Since\u001b[0m\u001b[95m there's\u001b[0m\u001b[95m no specific\u001b[0m\u001b[95m data provided yet,\u001b[0m\u001b[95m our\u001b[0m\u001b[95m first\u001b[0m\u001b[95m step should be to\u001b[0m\u001b[95m ask\u001b[0m\u001b[95m the user for\u001b[0m\u001b[95m the\u001b[0m\u001b[95m data\u001b[0m\u001b[95m an\u001b[0m\u001b[95md more\u001b[0m\u001b[95m details about\u001b[0m\u001b[95m what kinds\u001b[0m\u001b[95m of insights they're\u001b[0m\u001b[95m looking for. \u001b[0m\u001b[95m\n",
      "\n",
      "However\u001b[0m\u001b[95m, since I\u001b[0m\u001b[95m'm\u001b[0m\u001b[95m expecte\u001b[0m\u001b[95md to create\u001b[0m\u001b[95m a plan base\u001b[0m\u001b[95md on the current\u001b[0m\u001b[95m information, I'll\u001b[0m\u001b[95m create\u001b[0m\u001b[95m a general\u001b[0m\u001b[95m plan for\u001b[0m\u001b[95m data\u001b[0m\u001b[95m analysis\u001b[0m\u001b[95m an\u001b[0m\u001b[95md insight\u001b[0m\u001b[95m extraction that\u001b[0m\u001b[95m can be refine\u001b[0m\u001b[95md once\u001b[0m\u001b[95m we get\u001b[0m\u001b[95m more\u001b[0m\u001b[95m specifics.\u001b[0m\u001b[95m\n",
      "\n",
      "I\u001b[0m\u001b[95m'll create\u001b[0m\u001b[95m a plan\u001b[0m\u001b[95m in\u001b[0m\u001b[95m Korean\u001b[0m\u001b[95m since\u001b[0m\u001b[95m the user commun\u001b[0m\u001b[95micated in Korean.\u001b[0m\u001b[97m# Plan\n",
      "\n",
      "## thought\u001b[0m\u001b[97m\n",
      "사\u001b[0m\u001b[97m용자는\u001b[0m\u001b[97m 자\u001b[0m\u001b[97m신을\u001b[0m\u001b[97m 장\u001b[0m\u001b[97m동진으\u001b[0m\u001b[97m로 소개\u001b[0m\u001b[97m하\u001b[0m\u001b[97m고,\u001b[0m\u001b[97m 데이터\u001b[0m\u001b[97m를 제공\u001b[0m\u001b[97m하여\u001b[0m\u001b[97m 그\u001b[0m\u001b[97m로\u001b[0m\u001b[97m부터 인\u001b[0m\u001b[97m사이트를\u001b[0m\u001b[97m 추출하\u001b[0m\u001b[97m고 싶\u001b[0m\u001b[97m다\u001b[0m\u001b[97m고 요\u001b[0m\u001b[97m청했습니\u001b[0m\u001b[97m다.\u001b[0m\u001b[97m 그\u001b[0m\u001b[97m러\u001b[0m\u001b[97m나 아\u001b[0m\u001b[97m직 구\u001b[0m\u001b[97m체적인 \u001b[0m\u001b[97m데이터나\u001b[0m\u001b[97m 원\u001b[0m\u001b[97m하는 인\u001b[0m\u001b[97m사이트 \u001b[0m\u001b[97m유\u001b[0m\u001b[97m형에\u001b[0m\u001b[97m 대한\u001b[0m\u001b[97m 세부 \u001b[0m\u001b[97m정\u001b[0m\u001b[97m보가 제\u001b[0m\u001b[97m공되지 \u001b[0m\u001b[97m않았습니\u001b[0m\u001b[97m다.\u001b[0m\u001b[97m 따\u001b[0m\u001b[97m라서 \u001b[0m\u001b[97m먼저 사\u001b[0m\u001b[97m용자로\u001b[0m\u001b[97m부터 더\u001b[0m\u001b[97m 많\u001b[0m\u001b[97m은 정보\u001b[0m\u001b[97m를 요\u001b[0m\u001b[97m청하고,\u001b[0m\u001b[97m 그\u001b[0m\u001b[97m \u001b[0m\u001b[97m후\u001b[0m\u001b[97m에 데이\u001b[0m\u001b[97m터 분\u001b[0m\u001b[97m석 \u001b[0m\u001b[97m및 인사\u001b[0m\u001b[97m이트 추\u001b[0m\u001b[97m출을\u001b[0m\u001b[97m 위한 \u001b[0m\u001b[97m계획을 \u001b[0m\u001b[97m세우겠\u001b[0m\u001b[97m습니다.\u001b[0m\u001b[97m\n",
      "\n",
      "## title:\u001b[0m\u001b[97m\n",
      "데\u001b[0m\u001b[97m이터 분\u001b[0m\u001b[97m석 및 \u001b[0m\u001b[97m인사이트\u001b[0m\u001b[97m 추출 \u001b[0m\u001b[97m계획\n",
      "\n",
      "##\u001b[0m\u001b[97m steps:\n",
      "###\u001b[0m\u001b[97m 1.\u001b[0m\u001b[97m Coder:\u001b[0m\u001b[97m 데이터\u001b[0m\u001b[97m 분\u001b[0m\u001b[97m석 및 \u001b[0m\u001b[97m인사이트\u001b[0m\u001b[97m 추출\u001b[0m\u001b[97m\n",
      "- [\u001b[0m\u001b[97m ] 사\u001b[0m\u001b[97m용자로부\u001b[0m\u001b[97m터 데이\u001b[0m\u001b[97m터 요\u001b[0m\u001b[97m청 \u001b[0m\u001b[97m및\u001b[0m\u001b[97m 수\u001b[0m\u001b[97m집\n",
      "- [\u001b[0m\u001b[97m ] 데이\u001b[0m\u001b[97m터 정제\u001b[0m\u001b[97m 및 전\u001b[0m\u001b[97m처리\n",
      "-\u001b[0m\u001b[97m [ ] \u001b[0m\u001b[97m탐색적 \u001b[0m\u001b[97m데이터 \u001b[0m\u001b[97m분석 수\u001b[0m\u001b[97m행\n",
      "- [\u001b[0m\u001b[97m ] 주\u001b[0m\u001b[97m요\u001b[0m\u001b[97m 패턴\u001b[0m\u001b[97m 및\u001b[0m\u001b[97m 트\u001b[0m\u001b[97m렌드 \u001b[0m\u001b[97m식별\n",
      "-\u001b[0m\u001b[97m [ ] 통\u001b[0m\u001b[97m계적 분\u001b[0m\u001b[97m석 수\u001b[0m\u001b[97m행\n",
      "- [\u001b[0m\u001b[97m ] 데\u001b[0m\u001b[97m이터 시\u001b[0m\u001b[97m각화 생\u001b[0m\u001b[97m성\u001b[0m\u001b[97m\n",
      "\n",
      "### 2.\u001b[0m\u001b[97m Reporter\u001b[0m\u001b[97m: 최\u001b[0m\u001b[97m종 인사\u001b[0m\u001b[97m이트 보\u001b[0m\u001b[97m고서 작\u001b[0m\u001b[97m성\n",
      "- [\u001b[0m\u001b[97m ] 분\u001b[0m\u001b[97m석 결과\u001b[0m\u001b[97m 요약\u001b[0m\u001b[97m\n",
      "- [ ] \u001b[0m\u001b[97m주요 인\u001b[0m\u001b[97m사이트 \u001b[0m\u001b[97m도출\n",
      "-\u001b[0m\u001b[97m [ ] 시\u001b[0m\u001b[97m각\u001b[0m\u001b[97m적\u001b[0m\u001b[97m 요소\u001b[0m\u001b[97m와\u001b[0m\u001b[97m 함\u001b[0m\u001b[97m께 보고\u001b[0m\u001b[97m서 작성\u001b[0m\u001b[97m\n",
      "- [ ] \u001b[0m\u001b[97m결\u001b[0m\u001b[97m론 및 \u001b[0m\u001b[97m권\u001b[0m\u001b[97m장\u001b[0m\u001b[97m사\u001b[0m\u001b[97m항 제시\u001b[0m\u001b[97m\n",
      "- [ ] \u001b[0m\u001b[97m최종 보\u001b[0m\u001b[97m고서 포\u001b[0m\u001b[97m맷\u001b[0m\u001b[97m팅 \u001b[0m\u001b[97m및 검\u001b[0m\u001b[97m토\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [graph.nodes] \u001b[92m===== Planner completed task =====\u001b[0m\n",
      "WARNING:langgraph:Task planner with path ('__pregel_pull', 'planner') wrote to unknown channel branch:to:supervisor, ignoring it.\n",
      "\n",
      "INFO [__main__] \u001b[92m===== Workflow completed successfully =====\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#remove_artifact_folder()\n",
    "\n",
    "user_query = '''\n",
    "    안녕 나는 장동진이라고 해. 만나서 반가워\n",
    "    나는 데이터를 제공하고 그것으로 부터 인사이트를 추출하고 싶어. \n",
    "'''\n",
    "\n",
    "result = run_agent_workflow(\n",
    "    user_input=user_query,\n",
    "    debug=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing strands_langgraph_bedrock.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile strands_langgraph_bedrock.py\n",
    "\n",
    "import logging\n",
    "from textwrap import dedent\n",
    "from graph import build_graph\n",
    "from src.utils.common_utils import get_message_from_string\n",
    "from src.utils.strands_sdk_utils import strands_utils\n",
    "from src.prompts.template import apply_prompt_template\n",
    "\n",
    "from bedrock_agentcore.runtime import BedrockAgentCoreApp\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Default level is INFO\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "def enable_debug_logging():\n",
    "    \"\"\"Enable debug level logging for more detailed execution information.\"\"\"\n",
    "    logging.getLogger(__name__).setLevel(logging.DEBUG)\n",
    "\n",
    "# 로거 설정을 전역으로 한 번만 수행\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.propagate = False\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('\\n%(levelname)s [%(name)s] %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)  # 기본 레벨은 INFO로 설정\n",
    "\n",
    "class Colors:\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "\n",
    "# Create the graph\n",
    "graph = build_graph()\n",
    "\n",
    "def run_agent_workflow(user_input: str, debug: bool = False):\n",
    "    \"\"\"Run the agent workflow with the given user input.\n",
    "\n",
    "    Args:\n",
    "        user_input: The user's query or request\n",
    "        debug: If True, enables debug level logging\n",
    "\n",
    "    Returns:\n",
    "        The final state after the workflow completes\n",
    "    \"\"\"\n",
    "    if not user_input:\n",
    "        raise ValueError(\"Input could not be empty\")\n",
    "\n",
    "    if debug:\n",
    "        enable_debug_logging()\n",
    "\n",
    "    #logger.info(f\"Starting workflow with user input: {user_input}\")\n",
    "    logger.info(f\"{Colors.GREEN}===== Starting workflow ====={Colors.END}\")\n",
    "    logger.info(f\"{Colors.GREEN}\\nuser input: {user_input}{Colors.END}\")\n",
    "    \n",
    "    user_prompts = dedent(\n",
    "        '''\n",
    "        Here is a user request: <user_request>{user_request}</user_request>\n",
    "        '''\n",
    "    )\n",
    "    context = {\"user_request\": user_input}\n",
    "    user_prompts = user_prompts.format(**context)\n",
    "    messages = [get_message_from_string(role=\"user\", string=user_prompts, imgs=[])]\n",
    "\n",
    "        \n",
    "    result = graph.invoke(\n",
    "        input={\n",
    "            # Runtime Variables\n",
    "            \"messages\": messages,\n",
    "            \"request\": user_input,\n",
    "            \"request_prompt\": user_prompts\n",
    "        },\n",
    "        config={\n",
    "            \"recursion_limit\": 100\n",
    "        }\n",
    "    )\n",
    "    logger.debug(f\"{Colors.RED}Final workflow state: {result}{Colors.END}\")\n",
    "    logger.info(f\"{Colors.GREEN}===== Workflow completed successfully ====={Colors.END}\")\n",
    "    return result\n",
    "\n",
    "def strands_langgraph_bedrock(payload):\n",
    "    \"\"\"\n",
    "    Invoke the agent with a payload\n",
    "    \"\"\"\n",
    "    user_input = payload.get(\"prompt\")\n",
    "    \n",
    "    # Create the input in the format expected by LangGraph\n",
    "    response = agent.invoke({\"messages\": [HumanMessage(content=user_input)]})\n",
    "    \n",
    "    # Extract the final message content\n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"payload\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    response = strands_langgraph_bedrock(json.loads(args.payload))\n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc9889",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bedrock_agentcore'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbedrock_agentcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mruntime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BedrockAgentCoreApp\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'bedrock_agentcore'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d386ab54e85e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing langgraph_bedrock.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_bedrock.py\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import argparse\n",
    "import json\n",
    "import operator\n",
    "import math\n",
    "\n",
    "# Create calculator tool\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Calculate the result of a mathematical expression.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression as a string (e.g., \"2 + 3 * 4\", \"sqrt(16)\", \"sin(pi/2)\")\n",
    "    \n",
    "    Returns:\n",
    "        The result of the calculation as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define safe functions that can be used in expressions\n",
    "        safe_dict = {\n",
    "            \"__builtins__\": {},\n",
    "            \"abs\": abs, \"round\": round, \"min\": min, \"max\": max,\n",
    "            \"sum\": sum, \"pow\": pow,\n",
    "            # Math functions\n",
    "            \"sqrt\": math.sqrt, \"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan,\n",
    "            \"log\": math.log, \"log10\": math.log10, \"exp\": math.exp,\n",
    "            \"pi\": math.pi, \"e\": math.e,\n",
    "            \"ceil\": math.ceil, \"floor\": math.floor,\n",
    "            \"degrees\": math.degrees, \"radians\": math.radians,\n",
    "            # Basic operators (for explicit use)\n",
    "            \"add\": operator.add, \"sub\": operator.sub,\n",
    "            \"mul\": operator.mul, \"truediv\": operator.truediv,\n",
    "        }\n",
    "        \n",
    "        # Evaluate the expression safely\n",
    "        result = eval(expression, safe_dict)\n",
    "        return str(result)\n",
    "        \n",
    "    except ZeroDivisionError:\n",
    "        return \"Error: Division by zero\"\n",
    "    except ValueError as e:\n",
    "        return f\"Error: Invalid value - {str(e)}\"\n",
    "    except SyntaxError:\n",
    "        return \"Error: Invalid mathematical expression\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Create a custom weather tool\n",
    "@tool\n",
    "def weather():\n",
    "    \"\"\"Get weather\"\"\"  # Dummy implementation\n",
    "    return \"sunny\"\n",
    "\n",
    "# Define the agent using manual LangGraph construction\n",
    "def create_agent():\n",
    "    \"\"\"Create and configure the LangGraph agent\"\"\"\n",
    "    from langchain_aws import ChatBedrock\n",
    "    \n",
    "    # Initialize your LLM (adjust model and parameters as needed)\n",
    "    llm = ChatBedrock(\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # or your preferred model\n",
    "        model_kwargs={\"temperature\": 0.1}\n",
    "    )\n",
    "    \n",
    "    # Bind tools to the LLM\n",
    "    tools = [calculator, weather]\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    \n",
    "    # System message\n",
    "    system_message = \"You're a helpful assistant. You can do simple math calculation, and tell the weather.\"\n",
    "    \n",
    "    # Define the chatbot node\n",
    "    def chatbot(state: MessagesState):\n",
    "        # Add system message if not already present\n",
    "        messages = state[\"messages\"]\n",
    "        if not messages or not isinstance(messages[0], SystemMessage):\n",
    "            messages = [SystemMessage(content=system_message)] + messages\n",
    "        \n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    # Create the graph\n",
    "    graph_builder = StateGraph(MessagesState)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph_builder.add_node(\"chatbot\", chatbot)\n",
    "    graph_builder.add_node(\"tools\", ToolNode(tools))\n",
    "    \n",
    "    # Add edges\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"chatbot\",\n",
    "        tools_condition,\n",
    "    )\n",
    "    graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "    \n",
    "    # Set entry point\n",
    "    graph_builder.set_entry_point(\"chatbot\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Initialize the agent\n",
    "agent = create_agent()\n",
    "\n",
    "def langgraph_bedrock(payload):\n",
    "    \"\"\"\n",
    "    Invoke the agent with a payload\n",
    "    \"\"\"\n",
    "    user_input = payload.get(\"prompt\")\n",
    "\n",
    "\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "#remove_artifact_folder()\n",
    "\n",
    "user_query = '''\n",
    "    안녕 나는 장동진이라고 해. 만나서 반가워\n",
    "    나는 데이터를 제공하고 그것으로 부터 인사이트를 추출하고 싶어. \n",
    "'''\n",
    "\n",
    "result = run_agent_workflow(\n",
    "    user_input=user_query,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "    # Create the input in the format expected by LangGraph\n",
    "    response = agent.invoke({\"messages\": [HumanMessage(content=user_input)]})\n",
    "    \n",
    "    # Extract the final message content\n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"payload\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    response = langgraph_bedrock(json.loads(args.payload))\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68499675-db8d-47c6-8c0c-5d66dcb06229",
   "metadata": {},
   "source": [
    "#### Invoking local agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1226d59e6b56c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T21:52:06.461281Z",
     "start_time": "2025-06-29T21:52:06.456854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m538932\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8080\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m538932\u001b[0m]\n",
      "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 674, in run_until_complete\n",
      "    self.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/setup/.venv/lib/python3.12/site-packages/uvicorn/server.py\", line 70, in serve\n",
      "    with self.capture_signals():\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/setup/.venv/lib/python3.12/site-packages/uvicorn/server.py\", line 331, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 157, in _on_sigint\n",
      "    raise KeyboardInterrupt()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/setup/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 701, in lifespan\n",
      "    await receive()\n",
      "  File \"/home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/setup/.venv/lib/python3.12/site-packages/uvicorn/lifespan/on.py\", line 137, in receive\n",
      "    return await self.receive_queue.get()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/asyncio/queues.py\", line 158, in get\n",
      "    await getter\n",
      "asyncio.exceptions.CancelledError\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python langgraph_bedrock.py '{\"prompt\": \"What is the weather now?\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932110e6-fca6-47b6-b7c5-c4714a866a80",
   "metadata": {},
   "source": [
    "## Preparing your agent for deployment on AgentCore Runtime\n",
    "\n",
    "Let's now deploy our agents to AgentCore Runtime. To do so we need to:\n",
    "* Import the Runtime App with `from bedrock_agentcore.runtime import BedrockAgentCoreApp`\n",
    "* Initialize the App in our code with `app = BedrockAgentCoreApp()`\n",
    "* Decorate the invocation function with the `@app.entrypoint` decorator\n",
    "* Let AgentCoreRuntime control the running of the agent with `app.run()`\n",
    "\n",
    "### Strands Agent SDK + LangGraph with Amazon Bedrock model\n",
    "Let's start with our Strands Agent SDK + LangGraph using Amazon Bedrock model. Other examples with different frameworks and models are available in the parent directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17f452",
   "metadata": {},
   "source": [
    "### Dockerfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd1528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b845b32-a03e-45c2-a2f0-2afba8069f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_bedrock.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_bedrock.py\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from bedrock_agentcore.runtime import BedrockAgentCoreApp\n",
    "import argparse\n",
    "import json\n",
    "import operator\n",
    "import math\n",
    "\n",
    "app = BedrockAgentCoreApp()\n",
    "\n",
    "# Create calculator tool\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Calculate the result of a mathematical expression.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression as a string (e.g., \"2 + 3 * 4\", \"sqrt(16)\", \"sin(pi/2)\")\n",
    "    \n",
    "    Returns:\n",
    "        The result of the calculation as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define safe functions that can be used in expressions\n",
    "        safe_dict = {\n",
    "            \"__builtins__\": {},\n",
    "            \"abs\": abs, \"round\": round, \"min\": min, \"max\": max,\n",
    "            \"sum\": sum, \"pow\": pow,\n",
    "            # Math functions\n",
    "            \"sqrt\": math.sqrt, \"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan,\n",
    "            \"log\": math.log, \"log10\": math.log10, \"exp\": math.exp,\n",
    "            \"pi\": math.pi, \"e\": math.e,\n",
    "            \"ceil\": math.ceil, \"floor\": math.floor,\n",
    "            \"degrees\": math.degrees, \"radians\": math.radians,\n",
    "            # Basic operators (for explicit use)\n",
    "            \"add\": operator.add, \"sub\": operator.sub,\n",
    "            \"mul\": operator.mul, \"truediv\": operator.truediv,\n",
    "        }\n",
    "        \n",
    "        # Evaluate the expression safely\n",
    "        result = eval(expression, safe_dict)\n",
    "        return str(result)\n",
    "        \n",
    "    except ZeroDivisionError:\n",
    "        return \"Error: Division by zero\"\n",
    "    except ValueError as e:\n",
    "        return f\"Error: Invalid value - {str(e)}\"\n",
    "    except SyntaxError:\n",
    "        return \"Error: Invalid mathematical expression\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Create a custom weather tool\n",
    "@tool\n",
    "def weather():\n",
    "    \"\"\"Get weather\"\"\"  # Dummy implementation\n",
    "    return \"sunny\"\n",
    "\n",
    "# Define the agent using manual LangGraph construction\n",
    "def create_agent():\n",
    "    \"\"\"Create and configure the LangGraph agent\"\"\"\n",
    "    from langchain_aws import ChatBedrock\n",
    "    \n",
    "    # Initialize your LLM (adjust model and parameters as needed)\n",
    "    llm = ChatBedrock(\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # or your preferred model\n",
    "        model_kwargs={\"temperature\": 0.1}\n",
    "    )\n",
    "    \n",
    "    # Bind tools to the LLM\n",
    "    tools = [calculator, weather]\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    \n",
    "    # System message\n",
    "    system_message = \"You're a helpful assistant. You can do simple math calculation, and tell the weather.\"\n",
    "    \n",
    "    # Define the chatbot node\n",
    "    def chatbot(state: MessagesState):\n",
    "        # Add system message if not already present\n",
    "        messages = state[\"messages\"]\n",
    "        if not messages or not isinstance(messages[0], SystemMessage):\n",
    "            messages = [SystemMessage(content=system_message)] + messages\n",
    "        \n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    # Create the graph\n",
    "    graph_builder = StateGraph(MessagesState)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph_builder.add_node(\"chatbot\", chatbot)\n",
    "    graph_builder.add_node(\"tools\", ToolNode(tools))\n",
    "    \n",
    "    # Add edges\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"chatbot\",\n",
    "        tools_condition,\n",
    "    )\n",
    "    graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "    \n",
    "    # Set entry point\n",
    "    graph_builder.set_entry_point(\"chatbot\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Initialize the agent\n",
    "agent = create_agent()\n",
    "\n",
    "@app.entrypoint\n",
    "def langgraph_bedrock(payload):\n",
    "    \"\"\"\n",
    "    Invoke the agent with a payload\n",
    "    \"\"\"\n",
    "    user_input = payload.get(\"prompt\")\n",
    "    \n",
    "    # Create the input in the format expected by LangGraph\n",
    "    response = agent.invoke({\"messages\": [HumanMessage(content=user_input)]})\n",
    "    \n",
    "    # Extract the final message content\n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64db7b5-0f1b-475f-9bf2-467b4449d46a",
   "metadata": {},
   "source": [
    "## What happens behind the scenes?\n",
    "\n",
    "When you use `BedrockAgentCoreApp`, it automatically:\n",
    "\n",
    "* Creates an HTTP server that listens on the port 8080\n",
    "* Implements the required `/invocations` endpoint for processing the agent's requirements\n",
    "* Implements the `/ping` endpoint for health checks (very important for asynchronous agents)\n",
    "* Handles proper content types and response formats\n",
    "* Manages error handling according to the AWS standards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820ca8f-a8a8-4f34-b4ef-b6dad3776261",
   "metadata": {},
   "source": [
    "## Deploying the agent to AgentCore Runtime\n",
    "\n",
    "The `CreateAgentRuntime` operation supports comprehensive configuration options, letting you specify container images, environment variables and encryption settings. You can also configure protocol settings (HTTP, MCP) and authorization mechanisms to control how your clients communicate with the agent. \n",
    "\n",
    "**Note:** Operations best practice is to package code as container and push to ECR using CI/CD pipelines and IaC\n",
    "\n",
    "In this tutorial can will the Amazon Bedrock AgentCode Python SDK to easily package your artifacts and deploy them to AgentCore runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0861401-a111-4ade-9e02-50f52fdfa9b1",
   "metadata": {},
   "source": [
    "### Creating runtime role\n",
    "\n",
    "Before starting, let's create an IAM role for our AgentCore Runtime. We will do so using the utils function pre-developed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd2fdf-985c-4a70-8b87-071783a209de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path[0]: /home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials\n",
      "attaching role policy agentcore-langgraph_bedrock-role\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current notebook's directory\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__' if '__file__' in globals() else '.'))\n",
    "\n",
    "utils_dir = os.path.join(current_dir, '..')\n",
    "utils_dir = os.path.join(utils_dir, '..')\n",
    "utils_dir = os.path.abspath(utils_dir)\n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.insert(0, utils_dir)\n",
    "print(\"sys.path[0]:\", sys.path[0])\n",
    "\n",
    "from utils import create_agentcore_role\n",
    "\n",
    "agent_name=\"strandlanggraph_bedrock\"\n",
    "agentcore_iam_role = create_agentcore_role(agent_name=agent_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855aceb-b79f-4aaa-b16f-8577c059816a",
   "metadata": {},
   "source": [
    "### Configure AgentCore Runtime deployment\n",
    "\n",
    "Next we will use our starter toolkit to configure the AgentCore Runtime deployment with an entrypoint, the execution role we just created and a requirements file. We will also configure the starter kit to auto create the Amazon ECR repository on launch.\n",
    "\n",
    "During the configure step, your docker file will be generated based on your application code\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/configure.png\" width=\"40%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe096e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "agentcore_runtime = Runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62cbe47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "agentcore_runtime.configure(\n",
      "    entrypoint: str,\n",
      "    execution_role: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    agent_name: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    requirements: Optional[List[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    requirements_file: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ecr_repository: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    container_runtime: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    auto_create_ecr: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    auto_create_execution_role: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    authorizer_configuration: Optional[Dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    region: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    protocol: Optional[Literal[\u001b[33m'HTTP'\u001b[39m, \u001b[33m'MCP'\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> bedrock_agentcore_starter_toolkit.operations.runtime.models.ConfigureResult\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Configure Bedrock AgentCore from notebook using an entrypoint file.\n",
      "\n",
      "Args:\n",
      "    entrypoint: Path to Python file with optional Bedrock AgentCore name\n",
      "        (e.g., \"handler.py\" or \"handler.py:bedrock_agentcore\")\n",
      "    execution_role: AWS IAM execution role ARN or name (optional if auto_create_execution_role=True)\n",
      "    agent_name: name of the agent\n",
      "    requirements: Optional list of requirements to generate requirements.txt\n",
      "    requirements_file: Optional path to existing requirements file\n",
      "    ecr_repository: Optional ECR repository URI\n",
      "    container_runtime: Optional container runtime (docker/podman)\n",
      "    auto_create_ecr: Whether to auto-create ECR repository\n",
      "    auto_create_execution_role: Whether to auto-create execution role (makes execution_role optional)\n",
      "    authorizer_configuration: JWT authorizer configuration dictionary\n",
      "    region: AWS region for deployment\n",
      "    protocol: agent server protocol, must be either HTTP or MCP\n",
      "\n",
      "Returns:\n",
      "    ConfigureResult with configuration details\n",
      "\u001b[31mFile:\u001b[39m      ~/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/setup/.venv/lib/python3.12/site-packages/bedrock_agentcore_starter_toolkit/notebook/runtime/bedrock_agentcore.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "agentcore_runtime.configure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e79eba2-ca59-463f-9ebf-56e362d7ae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrypoint parsed: file=/home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/langgraph_bedrock.py, bedrock_agentcore_name=langgraph_bedrock\n",
      "INFO:bedrock_agentcore_starter_toolkit.utils.runtime.entrypoint:Entrypoint parsed: file=/home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/langgraph_bedrock.py, bedrock_agentcore_name=langgraph_bedrock\n",
      "Configuring BedrockAgentCore agent: langgraph_bedrock\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.configure:Configuring BedrockAgentCore agent: langgraph_bedrock\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">⚠️  [WARNING] Platform mismatch: Current system is </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">'linux/amd64'</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\"> but Bedrock AgentCore requires </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">'linux/arm64'</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">.</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">For deployment options and workarounds, see: </span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/getting-started-custom.html</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;4;33m⚠️  \u001b[0m\u001b[1;4;33m[\u001b[0m\u001b[1;4;33mWARNING\u001b[0m\u001b[1;4;33m]\u001b[0m\u001b[1;4;33m Platform mismatch: Current system is \u001b[0m\u001b[4;32m'linux/amd64'\u001b[0m\u001b[1;4;33m but Bedrock AgentCore requires \u001b[0m\u001b[4;32m'linux/arm64'\u001b[0m\u001b[1;4;33m.\u001b[0m\n",
       "\u001b[1;4;33mFor deployment options and workarounds, see: \u001b[0m\n",
       "\u001b[4;94mhttps://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/getting-started-custom.html\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generated Dockerfile: /home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/Dockerfile\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.configure:Generated Dockerfile: /home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/Dockerfile\n",
      "Generated .dockerignore: /home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/.dockerignore\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.configure:Generated .dockerignore: /home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/.dockerignore\n",
      "Setting 'langgraph_bedrock' as default agent\n",
      "INFO:bedrock_agentcore_starter_toolkit.utils.runtime.config:Setting 'langgraph_bedrock' as default agent\n",
      "Bedrock AgentCore configured: /home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/.bedrock_agentcore.yaml\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:Bedrock AgentCore configured: /home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/.bedrock_agentcore.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConfigureResult(config_path=PosixPath('/home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/.bedrock_agentcore.yaml'), dockerfile_path=PosixPath('/home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/Dockerfile'), dockerignore_path=PosixPath('/home/ubuntu/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/tutorials/01-AgentCore-runtime/01-hosting-agent/05-strands-langgraph-with-bedrock-model/.dockerignore'), runtime='Docker', region='us-west-2', account_id='615299776985', execution_role='arn:aws:iam::615299776985:role/agentcore-langgraph_bedrock-role', ecr_repository=None, auto_create_ecr=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "from boto3.session import Session\n",
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "region\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "\n",
    "response = agentcore_runtime.configure(\n",
    "    entrypoint=\"langgraph_bedrock.py\",\n",
    "    execution_role=agentcore_iam_role['Role']['Arn'],\n",
    "    auto_create_ecr=True,\n",
    "    #requirements_file=\"requirements.txt\",\n",
    "    region=region\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b84cc-798e-472c-ac0b-2c315f4b704d",
   "metadata": {},
   "source": [
    "### Launching agent to AgentCore Runtime\n",
    "\n",
    "Now that we've got a docker file, let's launch the agent to the AgentCore Runtime. This will create the Amazon ECR repository and the AgentCore Runtime\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/launch.png\" width=\"75%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17a32ab8-7701-4900-8055-e24364bdf35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Launching Bedrock AgentCore agent 'langgraph_bedrock' to cloud\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Launching Bedrock AgentCore agent 'langgraph_bedrock' to cloud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build: \u001b[91mexec /bin/sh: exec format error\n",
      "ERROR:bedrock_agentcore_starter_toolkit.utils.runtime.container:Build: \u001b[91mexec /bin/sh: exec format error\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Build failed:  ---> Using cache  ---> 488fbf9505ab Step 3/14 : COPY requirements.txt requirements.txt  ---> Using cache  ---> cd79e53fa466 Step 4/14 : RUN pip install -r requirements.txt  ---> [Warning] The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/amd64/v4) and no specific platform was requested  ---> Running in 3dad1aead37d \u001b[91mexec /bin/sh: exec format error \u001b[0mThe command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 255",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m launch_result = \u001b[43magentcore_runtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/setup/.venv/lib/python3.12/site-packages/bedrock_agentcore_starter_toolkit/notebook/runtime/bedrock_agentcore.py:151\u001b[39m, in \u001b[36mRuntime.launch\u001b[39m\u001b[34m(self, local, push_ecr, use_codebuild, auto_update_on_conflict, env_vars)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(exclusive_options) > \u001b[32m1\u001b[39m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOnly one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlocal\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpush_ecr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, or \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_codebuild\u001b[39m\u001b[33m'\u001b[39m\u001b[33m can be True\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m result = \u001b[43mlaunch_bedrock_agentcore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpush_ecr_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpush_ecr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_codebuild\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_codebuild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_update_on_conflict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_update_on_conflict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.mode == \u001b[33m\"\u001b[39m\u001b[33mcloud\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    161\u001b[39m     log.info(\u001b[33m\"\u001b[39m\u001b[33mDeployed to cloud: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, result.agent_arn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/13_agentcore/setup/.venv/lib/python3.12/site-packages/bedrock_agentcore_starter_toolkit/operations/runtime/launch.py:291\u001b[39m, in \u001b[36mlaunch_bedrock_agentcore\u001b[39m\u001b[34m(config_path, agent_name, local, push_ecr_only, use_codebuild, env_vars, auto_update_on_conflict)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[32m    290\u001b[39m     error_lines = output[-\u001b[32m10\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) > \u001b[32m10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m output\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBuild failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(error_lines)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mDocker image built: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, tag)\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local:\n\u001b[32m    296\u001b[39m     \u001b[38;5;66;03m# Return info for local deployment\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Build failed:  ---> Using cache  ---> 488fbf9505ab Step 3/14 : COPY requirements.txt requirements.txt  ---> Using cache  ---> cd79e53fa466 Step 4/14 : RUN pip install -r requirements.txt  ---> [Warning] The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/amd64/v4) and no specific platform was requested  ---> Running in 3dad1aead37d \u001b[91mexec /bin/sh: exec format error \u001b[0mThe command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 255"
     ]
    }
   ],
   "source": [
    "launch_result = agentcore_runtime.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4491351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine: x86_64\n",
      "Processor: x86_64\n",
      "Architecture: ('64bit', 'ELF')\n",
      "Platform: Linux-6.8.0-1008-aws-x86_64-with-glibc2.39\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "print(f\"Machine: {platform.machine()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Architecture: {platform.architecture()}\")\n",
    "print(f\"Platform: {platform.platform()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9c09-09db-4a76-871a-92eacd96b9c3",
   "metadata": {},
   "source": [
    "### Checking for the AgentCore Runtime Status\n",
    "Now that we've deployed the AgentCore Runtime, let's check for it's deployment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6ac09-9adb-4846-9fc1-4d12aeb74853",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_response = agentcore_runtime.status()\n",
    "status = status_response.endpoint['status']\n",
    "end_status = ['READY', 'CREATE_FAILED', 'DELETE_FAILED', 'UPDATE_FAILED']\n",
    "while status not in end_status:\n",
    "    time.sleep(10)\n",
    "    status_response = agentcore_runtime.status()\n",
    "    status = status_response.endpoint['status']\n",
    "    print(status)\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f89c56-918a-4cab-beaa-c7ac43a2ba29",
   "metadata": {},
   "source": [
    "### Invoking AgentCore Runtime\n",
    "\n",
    "Finally, we can invoke our AgentCore Runtime with a payload\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/invoke.png\" width=75%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d909e42-e1a0-407f-84c2-3d16cc889cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_response = agentcore_runtime.invoke({\"prompt\": \"How much is 2+2?\"})\n",
    "invoke_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa09f2-d25a-483f-aedb-11690bb8923a",
   "metadata": {},
   "source": [
    "### Processing invocation results\n",
    "\n",
    "We can now process our invocation results to include it in an application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11249103-cfb3-47b5-970d-981a977a225a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "response_text = json.loads(invoke_response['response'][0].decode(\"utf-8\"))\n",
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d2bce-be41-478c-8bed-b4037c385795",
   "metadata": {},
   "source": [
    "### Invoking AgentCore Runtime with boto3\n",
    "\n",
    "Now that your AgentCore Runtime was created you can invoke it with any AWS SDK. For instance, you can use the boto3 `invoke_agent_runtime` method for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84e68d-6c04-41b9-bf5b-60edc3fa0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "agent_arn = launch_result.agent_arn\n",
    "agentcore_client = boto3.client(\n",
    "    'bedrock-agentcore',\n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "boto3_response = agentcore_client.invoke_agent_runtime(\n",
    "    agentRuntimeArn=agent_arn,\n",
    "    qualifier=\"DEFAULT\",\n",
    "    payload=json.dumps({\"prompt\": \"What is the weather now?\"})\n",
    ")\n",
    "if \"text/event-stream\" in boto3_response.get(\"contentType\", \"\"):\n",
    "    content = []\n",
    "    for line in boto3_response[\"response\"].iter_lines(chunk_size=1):\n",
    "        if line:\n",
    "            line = line.decode(\"utf-8\")\n",
    "            if line.startswith(\"data: \"):\n",
    "                line = line[6:]\n",
    "                logger.info(line)\n",
    "                content.append(line)\n",
    "    display(Markdown(\"\\n\".join(content)))\n",
    "else:\n",
    "    try:\n",
    "        events = []\n",
    "        for event in boto3_response.get(\"response\", []):\n",
    "            events.append(event)\n",
    "    except Exception as e:\n",
    "        events = [f\"Error reading EventStream: {e}\"]\n",
    "    display(Markdown(json.loads(events[0].decode(\"utf-8\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fdfe404469632",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Let's now clean up the AgentCore Runtime created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f86824-c775-4ad4-aaee-f18e8cf390b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_result.ecr_uri, launch_result.agent_id, launch_result.ecr_uri.split('/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6cf1416830a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentcore_control_client = boto3.client(\n",
    "    'bedrock-agentcore-control',\n",
    "    region_name=region\n",
    ")\n",
    "ecr_client = boto3.client(\n",
    "    'ecr',\n",
    "    region_name=region\n",
    "    \n",
    ")\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "runtime_delete_response = agentcore_control_client.delete_agent_runtime(\n",
    "    agentRuntimeId=launch_result.agent_id\n",
    ")\n",
    "\n",
    "response = ecr_client.delete_repository(\n",
    "    repositoryName=launch_result.ecr_uri.split('/')[1],\n",
    "    force=True\n",
    ")\n",
    "policies = iam_client.list_role_policies(\n",
    "    RoleName=agentcore_iam_role['Role']['RoleName'],\n",
    "    MaxItems=100\n",
    ")\n",
    "\n",
    "for policy_name in policies['PolicyNames']:\n",
    "    iam_client.delete_role_policy(\n",
    "        RoleName=agentcore_iam_role['Role']['RoleName'],\n",
    "        PolicyName=policy_name\n",
    "    )\n",
    "iam_response = iam_client.delete_role(\n",
    "    RoleName=agentcore_iam_role['Role']['RoleName']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ad38-feeb-4d1d-9d57-e5c845becc56",
   "metadata": {},
   "source": [
    "# Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-bedrock-agentcore (UV)",
   "language": "python",
   "name": "env-bedrock-agentcore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
