{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746626f6b18e1c8c",
   "metadata": {},
   "source": [
    "## Advanced Data Analysis using Amazon AgentCore Bedrock Code Interpreter- Tutorial(Strands)\n",
    "This tutorial demonstrates how to create an AI agent that performs advanced data analysis through code execution using Python. We use Amazon Bedrock AgentCore Code Interpreter to run code that is generated by the LLM.\n",
    "\n",
    "This tutorial demonstrates how to use AgentCore Bedrock Code Interpreter to:\n",
    "1. Set up a sandbox environment\n",
    "2. Configure a strands based agent that performs advanced data analysis by generating code based on the user query\n",
    "3. Execute code in a sandbox environment using Code Interpreter\n",
    "4. Display the results back to the user\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with Bedrock AgentCore Code Interpreter access\n",
    "- You have the necessary IAM permissions to create and manage code interpreter resources\n",
    "- Required Python packages installed(including boto3, bedrock-agentcore & strands)\n",
    "- IAM role should have permissions to invoke models on Amazon Bedrock\n",
    " - Access to Claude Sonnet 3.7 & Claude Sonnet 4 models in the US Oregon (us-west-2) region (Claude Sonnet 4 is the default model for Strands SDK)\n",
    "\n",
    "## Your IAM execution role should have the following IAM policy attached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323388415caf3f7",
   "metadata": {},
   "source": [
    "~~~ {\n",
    "\"Version\": \"2012-10-17\",\n",
    "\"Statement\": [\n",
    "    {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"bedrock-agentcore:CreateCodeInterpreter\",\n",
    "            \"bedrock-agentcore:StartCodeInterpreterSession\",\n",
    "            \"bedrock-agentcore:InvokeCodeInterpreter\",\n",
    "            \"bedrock-agentcore:StopCodeInterpreterSession\",\n",
    "            \"bedrock-agentcore:DeleteCodeInterpreter\",\n",
    "            \"bedrock-agentcore:ListCodeInterpreters\",\n",
    "            \"bedrock-agentcore:GetCodeInterpreter\"\n",
    "        ],\n",
    "        \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"logs:CreateLogGroup\",\n",
    "            \"logs:CreateLogStream\",\n",
    "            \"logs:PutLogEvents\"\n",
    "        ],\n",
    "        \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/bedrock-agentcore/code-interpreter*\"\n",
    "    }\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b2cb86ff18d9c",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "The code execution sandbox enables agents to safely process user queries by creating an isolated environment with a code interpreter, shell, and file system. After a Large Language Model helps with tool selection, code is executed within this session, before being returned to the user or Agent for synthesis.\n",
    "\n",
    "![architecture local](code-interpreter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859482709c77b03d",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "First, let's import the necessary libraries and initialize our Code Interpreter client.\n",
    "\n",
    "The default session timeout is 900 seconds(15 minutes). However, we start the session with a slightly session timeout duration of 1200 seconds(20 minutes), since we will perform detailed analysis on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13da423bac8ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb006310a96750c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.346322Z",
     "start_time": "2025-07-13T09:35:46.244470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01K41VPYJ11K3JBM4V8A7MA51B'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bedrock_agentcore.tools.code_interpreter_client import CodeInterpreter\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Initialize the Code Interpreter within a supported AWS region.\n",
    "code_client = CodeInterpreter('us-west-2')\n",
    "code_client.start(session_timeout_seconds=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd02b57bd10dda2",
   "metadata": {},
   "source": [
    "## 2. Reading Local Data File\n",
    "\n",
    "Now we'll read the contents of our sample data file. The file consists of random data with 4 columns: Name, Preferred_City, Preferred_Animal, Preferred_Thing and ~ 300,000 records.\n",
    "\n",
    "We will analyze this file using an agent little later, to understand distributions and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef3821f290a589b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.465278Z",
     "start_time": "2025-07-13T09:35:48.385287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Preferred_City</th>\n",
       "      <th>Preferred_Animal</th>\n",
       "      <th>Preferred_Thing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Betty Ramirez</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Elephant</td>\n",
       "      <td>Sofa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jennifer Green</td>\n",
       "      <td>Naples</td>\n",
       "      <td>Bee</td>\n",
       "      <td>Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Lopez</td>\n",
       "      <td>Helsinki</td>\n",
       "      <td>Zebra</td>\n",
       "      <td>Wallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Susan Gonzalez</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Chicken</td>\n",
       "      <td>Phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jennifer Wright</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Goat</td>\n",
       "      <td>Wallet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name Preferred_City Preferred_Animal Preferred_Thing\n",
       "0    Betty Ramirez         Dallas         Elephant            Sofa\n",
       "1   Jennifer Green         Naples              Bee           Shirt\n",
       "2       John Lopez       Helsinki            Zebra          Wallet\n",
       "3   Susan Gonzalez        Beijing          Chicken           Phone\n",
       "4  Jennifer Wright   Buenos Aires             Goat          Wallet"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"samples/data.csv\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38277480cbc38ba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.503763Z",
     "start_time": "2025-07-13T09:35:48.495825Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Helper function to read file content with error handling\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "data_file_content = read_file(\"samples/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc91e0fb83fa05",
   "metadata": {},
   "source": [
    "## 3. Preparing Files for Sandbox Environment\n",
    "\n",
    "We'll create a structure that defines the files we want to create in the sandbox environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da44fb745b84c6ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:49.703849Z",
     "start_time": "2025-07-13T09:35:49.699079Z"
    }
   },
   "outputs": [],
   "source": [
    "files_to_create = [\n",
    "                {\n",
    "                    \"path\": \"data.csv\",\n",
    "                    \"text\": data_file_content\n",
    "                }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055bea34c93279",
   "metadata": {},
   "source": [
    "## 4. Creating Helper Function for Tool Invocation\n",
    "\n",
    "This helper function will make it easier to call sandbox tools and handle their responses. Within an active session, you can execute code in supported languages (Python, JavaScript), access libraries based on your dependencies configuration, generate visualizations, and maintain state between executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74164c54b3b8ad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:50.359366Z",
     "start_time": "2025-07-13T09:35:50.356755Z"
    }
   },
   "outputs": [],
   "source": [
    "def call_tool(tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Helper function to invoke sandbox tools\n",
    "\n",
    "    Args:\n",
    "        tool_name (str): Name of the tool to invoke\n",
    "        arguments (Dict[str, Any]): Arguments to pass to the tool\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: JSON formatted result\n",
    "    \"\"\"\n",
    "    response = code_client.invoke(tool_name, arguments)\n",
    "    for event in response[\"stream\"]:\n",
    "        return json.dumps(event[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33790785ac084a",
   "metadata": {},
   "source": [
    "## 5. Write data file to Code Sandbox\n",
    "\n",
    "Now we'll write our data file into the sandbox environment and verify they were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380afae3a5ba4934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:00.346136Z",
     "start_time": "2025-07-13T09:35:50.965773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing files result:\n",
      "{\"content\": [{\"type\": \"text\", \"text\": \"Successfully wrote all 1 files\"}], \"isError\": false}\n",
      "\n",
      "Files in sandbox:\n",
      "{\"content\": [{\"type\": \"resource_link\", \"uri\": \"file:///log\", \"name\": \"log\", \"description\": \"Directory\"}, {\"type\": \"resource_link\", \"mimeType\": \"text/csv\", \"uri\": \"file:///data.csv\", \"name\": \"data.csv\", \"description\": \"File\"}, {\"type\": \"resource_link\", \"uri\": \"file:///.ipython\", \"name\": \".ipython\", \"description\": \"Directory\"}], \"isError\": false}\n"
     ]
    }
   ],
   "source": [
    "# Write files to sandbox\n",
    "writing_files = call_tool(\"writeFiles\", {\"content\": files_to_create})\n",
    "print(\"Writing files result:\")\n",
    "print(writing_files)\n",
    "\n",
    "# Verify files were created\n",
    "listing_files = call_tool(\"listFiles\", {\"path\": \"\"})\n",
    "print(\"\\nFiles in sandbox:\")\n",
    "print(listing_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640eae7a52ce9f8d",
   "metadata": {},
   "source": [
    "## 6. Perform Advanced Analysis using Strands based Agent\n",
    "\n",
    "Now we will configure an agent to perform data analysis on the data file that we uploaded into the sandbox(above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b068faf4a5eaa",
   "metadata": {},
   "source": [
    "### 6.1 System Prompt Definition\n",
    "Define the behavior and capabilities of the AI assistant. We instruct our assistant to always validate answers through code execution and data based reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6830a170b45ce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:00.366216Z",
     "start_time": "2025-07-13T09:36:00.364374Z"
    }
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that validates all answers through code execution using the tools provided. DO NOT Answer questions without using the tools\n",
    "\n",
    "VALIDATION PRINCIPLES:\n",
    "1. When making claims about code, algorithms, or calculations - write code to verify them\n",
    "2. Use execute_python to test mathematical calculations, algorithms, and logic\n",
    "3. Create test scripts to validate your understanding before giving answers\n",
    "4. Always show your work with actual code execution\n",
    "5. If uncertain, explicitly state limitations and validate what you can\n",
    "\n",
    "APPROACH:\n",
    "- If asked about a programming concept, implement it in code to demonstrate\n",
    "- If asked for calculations, compute them programmatically AND show the code\n",
    "- If implementing algorithms, include test cases to prove correctness\n",
    "- Document your validation process for transparency\n",
    "- The sandbox maintains state between executions, so you can refer to previous results\n",
    "\n",
    "TOOL AVAILABLE:\n",
    "- execute_python: Run Python code and see output\n",
    "\n",
    "RESPONSE FORMAT: The execute_python tool returns a JSON response with:\n",
    "- sessionId: The sandbox session ID\n",
    "- id: Request ID\n",
    "- isError: Boolean indicating if there was an error\n",
    "- content: Array of content objects with type and text/data\n",
    "- structuredContent: For code execution, includes stdout, stderr, exitCode, executionTime\n",
    "\n",
    "For successful code execution, the output will be in content[0].text and also in structuredContent.stdout.\n",
    "Check isError field to see if there was an error.\n",
    "\n",
    "Be thorough, accurate, and always validate your answers when possible.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87157a0ea835ab5",
   "metadata": {},
   "source": [
    "### 6.2 Code Execution Tool Definition\n",
    "Next we define the function as tool that will be used by the Agent as tool, to run code in the code sandbox. We use the @tool decorator to annotate the function as a custom tool for the Agent.\n",
    "\n",
    "Within an active code interpreter session, you can execute code in supported languages (Python, JavaScript), access libraries based on your dependencies configuration, generate visualizations, and maintain state between executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "750472cd96e873c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:34.464620Z",
     "start_time": "2025-07-13T09:36:34.457484Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define and configure the code interpreter tool\n",
    "@tool\n",
    "def execute_python(code: str, description: str = \"\") -> str:\n",
    "    \"\"\"Execute Python code in the sandbox.\"\"\"\n",
    "\n",
    "    if description:\n",
    "        code = f\"# {description}\\n{code}\"\n",
    "\n",
    "    #Print generated Code to be executed\n",
    "    print(f\"\\n Generated Code: {code}\")\n",
    "\n",
    "\n",
    "    # Call the Invoke method and execute the generated code, within the initialized code interpreter session\n",
    "    response = code_client.invoke(\"executeCode\", {\n",
    "        \"code\": code,\n",
    "        \"language\": \"python\",\n",
    "        \"clearContext\": False\n",
    "    })\n",
    "    for event in response[\"stream\"]:\n",
    "        return json.dumps(event[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc47b82755730d",
   "metadata": {},
   "source": [
    "### 6.3 Agent Configuration\n",
    "We create and configure an agent using the Strands SDK. We provide it the system prompt and the tool we defined above to execute generate code.\n",
    "\n",
    "We also override Strands SDK's default model provider (Claude Sonnet 4) with Claude Sonnet 3.7. To use the Claude Sonnet 3.7 model we specify the [Cross-region Inference (CRIS)](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html) profile id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c5e8f18b70dc01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:35.341080Z",
     "start_time": "2025-07-13T09:36:35.239620Z"
    }
   },
   "outputs": [],
   "source": [
    "model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "model= BedrockModel(model_id=model_id)\n",
    "\n",
    "#configure the strands agent including the model and tool(s)\n",
    "agent=Agent(\n",
    "    model=model,\n",
    "        tools=[execute_python],\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        callback_handler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25693e10aa1e5689",
   "metadata": {},
   "source": [
    "## 7. Agent Invocation and Response Processing\n",
    "We invoke the agent with our query and process the agent's response\n",
    "\n",
    "\n",
    "Note: Async execution requires running in an async environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfa7cb97ead950",
   "metadata": {},
   "source": [
    "## 7.1 Query to perform Exploratory Data Analysis(EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f98916e2cc3627",
   "metadata": {},
   "source": [
    "Let's start with a query which instructs the agent to perform exploratory data analysis on the data file in the code sandbox environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7370ff964d06a1cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:40:07.612284Z",
     "start_time": "2025-07-13T09:37:28.402310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you perform exploratory data analysis (EDA) on the 'data.csv' file. Let's first check if this file exists and explore its contents.\n",
      " Generated Code: import os\n",
      "import pandas as pd\n",
      "\n",
      "# Check if file exists\n",
      "if os.path.exists('data.csv'):\n",
      "    print(\"File exists. Loading data...\")\n",
      "    df = pd.read_csv('data.csv')\n",
      "    print(f\"Data shape: {df.shape}\")\n",
      "    print(\"\\nFirst 5 rows:\")\n",
      "    print(df.head())\n",
      "else:\n",
      "    print(\"File 'data.csv' does not exist in the current directory.\")\n",
      "    print(\"Current directory contains:\")\n",
      "    print(os.listdir())\n",
      "Great! The file 'data.csv' exists and has been loaded successfully. Now let's perform a comprehensive exploratory data analysis on this dataset.\n",
      "\n",
      "Let's start by checking basic information about the dataset:\n",
      " Generated Code: import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Basic information\n",
      "print(\"Dataset Information:\")\n",
      "print(f\"- Number of rows: {df.shape[0]}\")\n",
      "print(f\"- Number of columns: {df.shape[1]}\")\n",
      "print(\"\\nColumn Names:\")\n",
      "print(df.columns.tolist())\n",
      "print(\"\\nData Types:\")\n",
      "print(df.dtypes)\n",
      "print(\"\\nMissing Values:\")\n",
      "print(df.isnull().sum())\n",
      "print(\"\\nSummary Statistics for Numerical Columns:\")\n",
      "print(df.describe())\n",
      "It seems we have an issue with the seaborn library. Let's modify our approach and use what's available:\n",
      " Generated Code: import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Basic information\n",
      "print(\"Dataset Information:\")\n",
      "print(f\"- Number of rows: {df.shape[0]}\")\n",
      "print(f\"- Number of columns: {df.shape[1]}\")\n",
      "print(\"\\nColumn Names:\")\n",
      "print(df.columns.tolist())\n",
      "print(\"\\nData Types:\")\n",
      "print(df.dtypes)\n",
      "print(\"\\nMissing Values:\")\n",
      "print(df.isnull().sum())\n",
      "print(\"\\nSummary Statistics:\")\n",
      "print(df.describe(include='all'))\n",
      "Now let's analyze the distributions of each categorical variable and check for potential outliers:\n",
      " Generated Code: import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from collections import Counter\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Analyze each column\n",
      "columns = df.columns\n",
      "\n",
      "# Function to plot frequency distribution\n",
      "def plot_frequency(column_name, top_n=20):\n",
      "    plt.figure(figsize=(12, 6))\n",
      "    counter = Counter(df[column_name])\n",
      "    most_common = counter.most_common(top_n)\n",
      "    labels = [item[0] for item in most_common]\n",
      "    values = [item[1] for item in most_common]\n",
      "    \n",
      "    plt.bar(range(len(labels)), values)\n",
      "    plt.xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
      "    plt.title(f'Top {top_n} Most Common {column_name}')\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(f'{column_name}_frequency.png')\n",
      "    plt.close()\n",
      "    \n",
      "    # Calculate frequency percentage\n",
      "    total = sum(counter.values())\n",
      "    print(f\"\\n{column_name} Distribution:\")\n",
      "    print(f\"- Total unique values: {len(counter)}\")\n",
      "    print(f\"- Most common: {counter.most_common(5)}\")\n",
      "    print(f\"- Least common: {counter.most_common()[-5:]}\")\n",
      "    \n",
      "    # Check for potential outliers based on frequency\n",
      "    threshold = 0.001  # 0.1% of data\n",
      "    rare_items = {k: v for k, v in counter.items() if v/total < threshold}\n",
      "    if rare_items:\n",
      "        print(f\"- Potential outliers (items appearing in less than {threshold*100}% of data): {len(rare_items)} items\")\n",
      "    \n",
      "    # Check for uneven distribution\n",
      "    expected_avg = total / len(counter)\n",
      "    variance = sum((v - expected_avg)**2 for v in counter.values()) / len(counter)\n",
      "    std_dev = variance ** 0.5\n",
      "    print(f\"- Average frequency per unique value: {expected_avg:.2f}\")\n",
      "    print(f\"- Standard deviation: {std_dev:.2f}\")\n",
      "    print(f\"- Coefficient of variation: {std_dev/expected_avg:.2f}\")\n",
      "\n",
      "# Analyze each column\n",
      "for column in columns:\n",
      "    plot_frequency(column)\n",
      "\n",
      "# Analyze relationships between columns\n",
      "print(\"\\n\\nRelationships between columns:\")\n",
      "for i, col1 in enumerate(columns):\n",
      "    for j, col2 in enumerate(columns):\n",
      "        if i < j:  # Avoid duplicate combinations\n",
      "            # Count the number of unique combinations\n",
      "            combinations = df.groupby([col1, col2]).size().reset_index(name='count')\n",
      "            total_combinations = len(combinations)\n",
      "            max_combination = combinations.loc[combinations['count'].idxmax()]\n",
      "            \n",
      "            print(f\"\\n{col1} and {col2}:\")\n",
      "            print(f\"- Unique combinations: {total_combinations}\")\n",
      "            print(f\"- Most common combination: {max_combination[0]} + {max_combination[1]} (appears {max_combination[2]} times)\")\n",
      "Let's visualize some of the distributions and create a few more analyses to identify patterns:\n",
      " Generated Code: import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "import io\n",
      "import base64\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Create a figure for city, animal and thing preferences (top 10)\n",
      "fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
      "\n",
      "# City distribution\n",
      "city_counts = Counter(df['Preferred_City']).most_common(10)\n",
      "city_labels = [item[0] for item in city_counts]\n",
      "city_values = [item[1] for item in city_counts]\n",
      "axes[0].bar(city_labels, city_values)\n",
      "axes[0].set_title('Top 10 Preferred Cities')\n",
      "axes[0].set_ylabel('Count')\n",
      "axes[0].tick_params(axis='x', rotation=45)\n",
      "\n",
      "# Animal distribution\n",
      "animal_counts = Counter(df['Preferred_Animal']).most_common(10)\n",
      "animal_labels = [item[0] for item in animal_counts]\n",
      "animal_values = [item[1] for item in animal_counts]\n",
      "axes[1].bar(animal_labels, animal_values)\n",
      "axes[1].set_title('Top 10 Preferred Animals')\n",
      "axes[1].set_ylabel('Count')\n",
      "axes[1].tick_params(axis='x', rotation=45)\n",
      "\n",
      "# Thing distribution\n",
      "thing_counts = Counter(df['Preferred_Thing']).most_common(10)\n",
      "thing_labels = [item[0] for item in thing_counts]\n",
      "thing_values = [item[1] for item in thing_counts]\n",
      "axes[2].bar(thing_labels, thing_values)\n",
      "axes[2].set_title('Top 10 Preferred Things')\n",
      "axes[2].set_ylabel('Count')\n",
      "axes[2].tick_params(axis='x', rotation=45)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('top_preferences.png')\n",
      "\n",
      "# Let's check for any unusual patterns or outliers in data distribution\n",
      "print(\"Distribution Analysis:\")\n",
      "\n",
      "# Check for name patterns\n",
      "first_names = [name.split()[0] for name in df['Name']]\n",
      "first_name_counts = Counter(first_names).most_common(10)\n",
      "print(\"\\nTop 10 first names:\")\n",
      "for name, count in first_name_counts:\n",
      "    print(f\"- {name}: {count} ({count/len(df)*100:.2f}%)\")\n",
      "\n",
      "# Check city-animal combinations\n",
      "city_animal_counts = df.groupby(['Preferred_City', 'Preferred_Animal']).size()\n",
      "print(\"\\nRange of city-animal combination counts:\")\n",
      "print(f\"- Min: {city_animal_counts.min()}\")\n",
      "print(f\"- Max: {city_animal_counts.max()}\")\n",
      "print(f\"- Mean: {city_animal_counts.mean():.2f}\")\n",
      "print(f\"- Std Dev: {city_animal_counts.std():.2f}\")\n",
      "\n",
      "# Calculate Chi-square statistic to check independence between cities and animals\n",
      "from scipy.stats import chi2_contingency\n",
      "contingency_table = pd.crosstab(df['Preferred_City'], df['Preferred_Animal'])\n",
      "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
      "print(\"\\nIndependence test (Chi-square):\")\n",
      "print(f\"- Chi-square value: {chi2:.2f}\")\n",
      "print(f\"- p-value: {p:.10f}\")\n",
      "print(f\"- Are Preferred_City and Preferred_Animal independent? {'No' if p < 0.05 else 'Yes'}\")\n",
      "\n",
      "# Check if the data distribution is uniform\n",
      "print(\"\\nUniformity check:\")\n",
      "cities_uniformity = np.var(list(Counter(df['Preferred_City']).values())) / np.mean(list(Counter(df['Preferred_City']).values()))\n",
      "animals_uniformity = np.var(list(Counter(df['Preferred_Animal']).values())) / np.mean(list(Counter(df['Preferred_Animal']).values()))\n",
      "things_uniformity = np.var(list(Counter(df['Preferred_Thing']).values())) / np.mean(list(Counter(df['Preferred_Thing']).values()))\n",
      "\n",
      "print(f\"- Coefficient of Variation for Cities: {cities_uniformity:.4f}\")\n",
      "print(f\"- Coefficient of Variation for Animals: {animals_uniformity:.4f}\")\n",
      "print(f\"- Coefficient of Variation for Things: {things_uniformity:.4f}\")\n",
      "print(f\"- Interpretation: Values close to 0 indicate more uniform distributions\")\n",
      "\n",
      "# Check for correlations in the selection patterns\n",
      "print(\"\\nCorrelation Analysis:\")\n",
      "\n",
      "# Create a synthetic dataset with encoding for analysis\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "# Create encoders for each categorical column\n",
      "encoders = {}\n",
      "encoded_df = pd.DataFrame()\n",
      "\n",
      "for column in ['Preferred_City', 'Preferred_Animal', 'Preferred_Thing']:\n",
      "    encoders[column] = LabelEncoder()\n",
      "    encoded_df[column] = encoders[column].fit_transform(df[column])\n",
      "\n",
      "# Calculate correlation matrix\n",
      "corr_matrix = encoded_df.corr()\n",
      "print(\"\\nCorrelation between encoded categorical variables:\")\n",
      "print(corr_matrix)\n",
      "\n",
      "# For outlier analysis in categorical data, check for rare combinations\n",
      "print(\"\\nRare Combination Analysis:\")\n",
      "combinations = df.groupby(['Preferred_City', 'Preferred_Animal', 'Preferred_Thing']).size().reset_index(name='count')\n",
      "combinations = combinations.sort_values('count')\n",
      "\n",
      "print(f\"- Total unique combinations of (City, Animal, Thing): {len(combinations)}\")\n",
      "print(f\"- Least common combinations (appearing only once):\", len(combinations[combinations['count'] == combinations['count'].min()]))\n",
      "print(f\"- Most common combination appears {combinations['count'].max()} times\")\n",
      "print(f\"- Top 3 most common combinations:\")\n",
      "\n",
      "top_combinations = combinations.sort_values('count', ascending=False).head(3)\n",
      "for _, row in top_combinations.iterrows():\n",
      "    print(f\"  * {row['Preferred_City']}, {row['Preferred_Animal']}, {row['Preferred_Thing']}: {row['count']} occurrences\")\n",
      "Let's perform one more analysis to check for any name-related patterns and create a summary of our findings:\n",
      " Generated Code: import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Check for name patterns\n",
      "last_names = [name.split()[-1] for name in df['Name']]\n",
      "last_name_counts = Counter(last_names).most_common(10)\n",
      "\n",
      "print(\"Name Analysis:\")\n",
      "print(\"\\nTop 10 last names:\")\n",
      "for name, count in last_name_counts:\n",
      "    print(f\"- {name}: {count} ({count/len(df)*100:.2f}%)\")\n",
      "\n",
      "# Check for preferences by gender (estimating gender based on common first names)\n",
      "common_male_names = ['John', 'Michael', 'David', 'James', 'Robert', 'Joseph', 'Thomas', \n",
      "                     'Charles', 'Christopher', 'Daniel', 'Matthew', 'Mark']\n",
      "common_female_names = ['Mary', 'Patricia', 'Linda', 'Barbara', 'Elizabeth', 'Jennifer', \n",
      "                      'Maria', 'Susan', 'Margaret', 'Dorothy', 'Lisa', 'Nancy', 'Sarah', \n",
      "                      'Jessica', 'Michelle']\n",
      "\n",
      "def estimate_gender(name):\n",
      "    first_name = name.split()[0]\n",
      "    if first_name in common_male_names:\n",
      "        return 'Male'\n",
      "    elif first_name in common_female_names:\n",
      "        return 'Female'\n",
      "    else:\n",
      "        return 'Unknown'\n",
      "\n",
      "df['Estimated_Gender'] = df['Name'].apply(estimate_gender)\n",
      "gender_counts = df['Estimated_Gender'].value_counts()\n",
      "\n",
      "print(\"\\nEstimated Gender Distribution:\")\n",
      "for gender, count in gender_counts.items():\n",
      "    print(f\"- {gender}: {count} ({count/len(df)*100:.2f}%)\")\n",
      "\n",
      "# Preferences by estimated gender\n",
      "print(\"\\nPreferences by Estimated Gender:\")\n",
      "\n",
      "# Cities\n",
      "male_cities = Counter(df[df['Estimated_Gender'] == 'Male']['Preferred_City']).most_common(5)\n",
      "female_cities = Counter(df[df['Estimated_Gender'] == 'Female']['Preferred_City']).most_common(5)\n",
      "\n",
      "print(\"\\nTop 5 cities preferred by estimated males:\")\n",
      "for city, count in male_cities:\n",
      "    print(f\"- {city}: {count}\")\n",
      "    \n",
      "print(\"\\nTop 5 cities preferred by estimated females:\")\n",
      "for city, count in female_cities:\n",
      "    print(f\"- {city}: {count}\")\n",
      "\n",
      "# Animals\n",
      "male_animals = Counter(df[df['Estimated_Gender'] == 'Male']['Preferred_Animal']).most_common(5)\n",
      "female_animals = Counter(df[df['Estimated_Gender'] == 'Female']['Preferred_Animal']).most_common(5)\n",
      "\n",
      "print(\"\\nTop 5 animals preferred by estimated males:\")\n",
      "for animal, count in male_animals:\n",
      "    print(f\"- {animal}: {count}\")\n",
      "    \n",
      "print(\"\\nTop 5 animals preferred by estimated females:\")\n",
      "for animal, count in female_animals:\n",
      "    print(f\"- {animal}: {count}\")\n",
      "\n",
      "# Things\n",
      "male_things = Counter(df[df['Estimated_Gender'] == 'Male']['Preferred_Thing']).most_common(5)\n",
      "female_things = Counter(df[df['Estimated_Gender'] == 'Female']['Preferred_Thing']).most_common(5)\n",
      "\n",
      "print(\"\\nTop 5 things preferred by estimated males:\")\n",
      "for thing, count in male_things:\n",
      "    print(f\"- {thing}: {count}\")\n",
      "    \n",
      "print(\"\\nTop 5 things preferred by estimated females:\")\n",
      "for thing, count in female_things:\n",
      "    print(f\"- {thing}: {count}\")\n",
      "\n",
      "# Create a summary of the analysis\n",
      "print(\"\\n\\n===== SUMMARY OF EXPLORATORY DATA ANALYSIS =====\")\n",
      "print(f\"\\nDataset Size: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
      "\n",
      "print(\"\\nDistribution Characteristics:\")\n",
      "print(f\"- Names: {len(Counter(df['Name']))} unique values\")\n",
      "print(f\"- Cities: {len(Counter(df['Preferred_City']))} unique values\")\n",
      "print(f\"- Animals: {len(Counter(df['Preferred_Animal']))} unique values\")\n",
      "print(f\"- Things: {len(Counter(df['Preferred_Thing']))} unique values\")\n",
      "\n",
      "print(\"\\nUniformity of Distribution (lower values = more uniform):\")\n",
      "print(f\"- Cities: Coefficient of Variation = {np.std(list(Counter(df['Preferred_City']).values()))/np.mean(list(Counter(df['Preferred_City']).values())):.4f}\")\n",
      "print(f\"- Animals: Coefficient of Variation = {np.std(list(Counter(df['Preferred_Animal']).values()))/np.mean(list(Counter(df['Preferred_Animal']).values())):.4f}\")\n",
      "print(f\"- Things: Coefficient of Variation = {np.std(list(Counter(df['Preferred_Thing']).values()))/np.mean(list(Counter(df['Preferred_Thing']).values())):.4f}\")\n",
      "\n",
      "print(\"\\nOutlier Analysis:\")\n",
      "print(f\"- We have {len(df.groupby(['Preferred_City', 'Preferred_Animal', 'Preferred_Thing']).size())} unique combinations of preferences\")\n",
      "print(f\"- Most frequent combination appears {df.groupby(['Preferred_City', 'Preferred_Animal', 'Preferred_Thing']).size().max()} times\")\n",
      "print(f\"- {len(df.groupby(['Preferred_City', 'Preferred_Animal', 'Preferred_Thing']).size()[df.groupby(['Preferred_City', 'Preferred_Animal', 'Preferred_Thing']).size() == 1])} combinations appear only once\")\n",
      "\n",
      "print(\"\\nCorrelation Analysis:\")\n",
      "print(\"- Preferences seem to be independent of each other (no strong correlations between preferences)\")\n",
      "print(f\"- Chi-square test p-value between City and Animal: 0.941 (indicating independence)\")\n",
      "# Exploratory Data Analysis of data.csv\n",
      "\n",
      "Based on my analysis of the dataset, here's a comprehensive summary of the findings:\n",
      "\n",
      "## Dataset Overview\n",
      "- **Size**: 299,130 rows with 4 columns\n",
      "- **Columns**: Name, Preferred_City, Preferred_Animal, Preferred_Thing\n",
      "- **Missing Values**: None found in any column\n",
      "\n",
      "## Distribution Analysis\n",
      "\n",
      "### Names\n",
      "- 1,722 unique names in the dataset\n",
      "- Most common name: \"Lisa White\" (appears 222 times)\n",
      "- First names are evenly distributed with top 10 first names each representing ~2.4% of data\n",
      "- Top 10 last names each represent ~2.5% of the dataset\n",
      "\n",
      "### Preferred Cities\n",
      "- 55 unique cities\n",
      "- Most popular cities: Prague (5,587 occurrences), Seoul (5,580), Mumbai (5,577)\n",
      "- Least popular cities: Phoenix (5,236), Toronto (5,254), Rio de Janeiro (5,284)\n",
      "- Very uniform distribution with a coefficient of variation of 0.0158\n",
      "\n",
      "### Preferred Animals\n",
      "- 50 unique animals\n",
      "- Most popular animals: Goat (6,141), Pig (6,109), Hamster (6,107)\n",
      "- Least popular animals: Shark (5,761), Rat (5,810), Horse (5,833)\n",
      "- Highly uniform distribution with a coefficient of variation of 0.0143\n",
      "\n",
      "### Preferred Things\n",
      "- 51 unique things\n",
      "- Most popular things: Pencil (6,058), Dress (6,044), Bowl (5,988)\n",
      "- Least popular things: Candle (5,720), Umbrella (5,753), Painting (5,757)\n",
      "- Most uniform distribution among all categories with a coefficient of variation of 0.0128\n",
      "\n",
      "## Outlier Analysis\n",
      "- No traditional outliers were found in the dataset as all categories show remarkably uniform distributions\n",
      "- Some potential outliers in combinations:\n",
      "  - 123,647 unique combinations of (City, Animal, Thing)\n",
      "  - 35,592 combinations appear only once (can be considered rare combinations)\n",
      "  - Most frequent combination appears only 10 times (Berlin+Whale+Pants, Tokyo+Bee+Bottle, Seattle+Fox+Pillow)\n",
      "\n",
      "## Correlation and Independence Analysis\n",
      "- Chi-square test p-value between City and Animal: 0.941, indicating independence\n",
      "- Correlation matrix shows extremely low correlation between all preference pairs:\n",
      "  - City-Animal correlation: -0.000006\n",
      "  - City-Thing correlation: 0.000813\n",
      "  - Animal-Thing correlation: 0.001948\n",
      "- All preferences appear to be independent of each other\n",
      "\n",
      "## Gender-Based Analysis (Estimated)\n",
      "- Gender estimated based on common first names:\n",
      "  - Unknown: 37.88%\n",
      "  - Female: 33.48%\n",
      "  - Male: 28.63%\n",
      "- Minor differences in preferences between genders:\n",
      "  - Females show slight preference for Tiger, Pig, and Owl\n",
      "  - Males show slight preference for Panda, Rabbit, and Kangaroo\n",
      "  - City and thing preferences also show small differences between genders\n",
      "\n",
      "## Key Insights\n",
      "1. The data shows remarkably uniform distributions across all preference categories\n",
      "2. There are no strong correlations between preferences, suggesting independence\n",
      "3. The combinations of preferences are highly diverse (123,647 unique combinations)\n",
      "4. The distributions suggest the data might be synthetically generated with careful attention to uniform distributions\n",
      "5. No problematic outliers that would require cleaning or transformation\n",
      "\n",
      "This dataset appears to be very well-balanced and clean, with extremely uniform distributions across all categories. The preferences appear independent of each other, suggesting either randomized preference selection or a very large and diverse population with no strong underlying patterns."
     ]
    }
   ],
   "source": [
    "query = \"Load the file 'data.csv' and perform exploratory data analysis(EDA) on it. Tell me about distributions and outlier values.\"\n",
    "\n",
    "# Invoke the agent asynchcronously and stream the response\n",
    "response_text = \"\"\n",
    "async for event in agent.stream_async(query):\n",
    "    if \"data\" in event:\n",
    "        # Stream text response\n",
    "        chunk = event[\"data\"]\n",
    "        response_text += chunk\n",
    "        print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accab0cfe53ade15",
   "metadata": {},
   "source": [
    "## 7.2 Query to extract information\n",
    "\n",
    "Now, let's instruct the agent to extract specific information from the data file in the code sandbox environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091d1f87558bd41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:37:07.283968Z",
     "start_time": "2025-07-13T09:36:45.865171Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"Within the file 'data.csv', how many individuals with the first name 'Kimberly' have 'Crocodile' as their favourite animal?\"\n",
    "\n",
    "# Invoke the agent asynchcronously and stream the response\n",
    "response_text = \"\"\n",
    "async for event in agent.stream_async(query):\n",
    "    if \"data\" in event:\n",
    "        # Stream text response\n",
    "        chunk = event[\"data\"]\n",
    "        response_text += chunk\n",
    "        print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b3ce0963d4a83",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "Finally, we'll clean up by stopping the Code Interpreter session. Once finished using a session, the session should be shopped to release resources and avoid unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2ca7fce8b181d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:31.525724Z",
     "start_time": "2025-07-13T09:35:30.947964Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stop the Code Interpreter session\n",
    "code_client.stop()\n",
    "print(\"Code Interpreter session stopped successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock-manus-agentcore (UV)",
   "language": "python",
   "name": "bedrock-manus-agentcore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
