{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OpenSearch 벡터 스토어 및 Amazon Bedrock를 사용한 RAG \n",
    "> 이 노트북은  SageMaker Studio* **`Data Science 3.0`** kernel 및 ml.t3.medium 인스턴스에서 테스트 되었습니다.\n",
    "---\n",
    "### 중요\n",
    "- 이 노트북은 Anthropic 의 Claude-v2 모델 접근 가능한 분만 실행 가능합니다. \n",
    "- 접근이 안되시는 분은 노트북의 코드와 결과 만을 확인 하시면 좋겠습니다.\n",
    "- 만일 실행시에는 **\"과금\"** 이 발생이 되는 부분 유념 해주시기 바랍니다.\n",
    "\n",
    "### 선수조건\n",
    "- 임베딩 모델의 세이지 메이커 엔드포인트가 액티브 된 상태를 가정 합니다.\n",
    "    - 세이지 메이커 엔드포인트에 배포하기 위해서는 아래 노트북을 실행하시고, Endpoint Name 만을 복사 하시면 됩니다.\n",
    "    - [KoSIMCSERoberta Embedding Model 배포](https://github.com/gonsoomoon-ml/Kor-LLM-On-SageMaker/blob/main/1-Lab01-Deploy-LLM/4.Kor-Embedding-Model.ipynb)\n",
    "    - SageMaker Endpoint 에 대해서는 공식 개발자 문서를 참조하세요 --> [Create your endpoint and deploy your model](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html)\n",
    "- 오픈 서치 서비스가 액티브 된 상태를 가정 합니다.\n",
    "---\n",
    "\n",
    "OpenSearch는 대규모 데이터셋에 대한 유사도 검색을 위한 강력한 엔진입니다. Amazon OpenSearch Service를 통해 쉽게 클라우드 환경에서도 이용할 수 있습니다. 이와 함께 Vector Store를 사용하면 고차원 벡터 데이터를 효율적으로 저장하고 빠르게 검색할 수 있어, 복잡한 자연어 처리 작업을 더욱 간편하게 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "opensearch = boto3.client('opensearch', region)\n",
    "\n",
    "%store -r opensearch_user_id opensearch_user_password domain_name opensearch_domain_endpoint\n",
    "\n",
    "try:\n",
    "    opensearch_user_id\n",
    "    opensearch_user_password\n",
    "    domain_name\n",
    "    opensearch_domain_endpoint\n",
    "   \n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Run 00_setup notebook first or Create Your Own OpenSearch Domain\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Bedrock Client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock, print_ww\n",
    "from utils.bedrock import bedrock_info\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "# os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "print(colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint(bedrock_info.get_list_fm_models(verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Titan Embedding 및 LLM 인 Claude-v2 모델 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM 로딩 (Claude-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - create the Anthropic Model\n",
    "llm_text = Bedrock(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Claude-V2\"),\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 512\n",
    "    },\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "llm_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding 모델 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.rag import KoSimCSERobertaContentHandler, SagemakerEndpointEmbeddingsJumpStart\n",
    "\n",
    "def get_embedding_model(is_bedrock_embeddings, is_KoSimCSERobert, aws_region, endpont_name=None):\n",
    "    if is_bedrock_embeddings:\n",
    "\n",
    "        # We will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "        from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "        llm_emb = BedrockEmbeddings(\n",
    "            client=boto3_bedrock,\n",
    "            model_id=bedrock_info.get_model_id(\n",
    "                model_name=\"Titan-Embeddings-G1\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\"Bedrock Embeddings Model Loaded\")\n",
    "        \n",
    "    elif is_KoSimCSERobert:\n",
    "        LLMEmbHandler = KoSimCSERobertaContentHandler()\n",
    "        endpoint_name_emb = endpont_name\n",
    "        llm_emb = SagemakerEndpointEmbeddingsJumpStart(\n",
    "            endpoint_name=endpoint_name_emb,\n",
    "            region_name=aws_region,\n",
    "            content_handler=LLMEmbHandler,\n",
    "        )        \n",
    "        print(\"KoSimCSERobert Embeddings Model Loaded\")\n",
    "    else:\n",
    "        llm_emb = None\n",
    "        print(\"No Embedding Model Selected\")\n",
    "    \n",
    "    return llm_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [중요] is_KoSimCSERobert == True 일시에 endpoint_name 을 꼭 넣어 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_bedrock_embeddings = True\n",
    "is_KoSimCSERobert = False\n",
    "\n",
    "aws_region = os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    "\n",
    "##############################\n",
    "# Parameters for is_KoSimCSERobert\n",
    "##############################\n",
    "if is_KoSimCSERobert: endpont_name = \"<endpoint-name>\"\n",
    "else: endpont_name = None\n",
    "##############################\n",
    "\n",
    "llm_emb = get_embedding_model(is_bedrock_embeddings, is_KoSimCSERobert, aws_region, endpont_name)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터 준비\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  신한은행 FAQ 데이터 세트로 구현\n",
    "- [중요] 저자 및 동료가 아래의 웹사이트에서 크로링한 기준으로 구성 하였습니다.\n",
    "- 인터넷뱅킹 FAQ > 스마트뱅킹 No.1 ~ N. 89 로 구성되었습니다. \n",
    "- https://www.shinhan.com/hpe/index.jsp#050101020000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 20\n",
    "\n",
    "data_file_path = \"data/fsi_smart_faq_ko.csv\"\n",
    "df = pd.read_csv(data_file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "- 여기서 no 는 제거 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    ldf = df.copy()\n",
    "\n",
    "    ldf.rename(columns={'Category': 'ask'}, inplace=True)\n",
    "    df_index = ldf.drop(['no'], axis=1)\n",
    "    # df_index.to_csv(\"rag_data_kr/fsi_smart_faq_ko_answer.csv\", index=None, header=None)\n",
    "    df_index.to_csv(\"data/fsi_smart_faq_ko_preprocess.csv\", index=None)\n",
    "\n",
    "    return df_index\n",
    "\n",
    "pre_df = preprocess_data(df)\n",
    "pre_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSVLoader 로 문서 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=\"data/fsi_smart_faq_ko_preprocess.csv\",\n",
    "    # csv_args={\n",
    "    #     \"delimiter\": \",\",\n",
    "    #     \"fieldnames\": [\"Category\", \"Information\", \"type\", \"Source\"],\n",
    "    # },    \n",
    "    source_column=\"Source\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메타 데이터 생성\n",
    "- 기존 Category 컬럼을 ask 로 변경 합니다.\n",
    "- 컬럼의 type, source 는 metadata 로 생성하고, 내용에서는 삭제 합니다.\n",
    "- 타임스탬프 및 임베딩 모델의 엔드포인트 이름을 metadata 로 추가 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "documents_fsi = loader.load()\n",
    "\n",
    "def create_metadata(docs):\n",
    "    # # add a custom metadata field, such as timestamp\n",
    "    for idx, doc in enumerate(docs):\n",
    "        # type 을 메타 데이타로 저장\n",
    "        stype = doc.page_content.split(\"type: \")[1].split(\"\\n\")[0]\n",
    "        # print(\"stype: \", stype)\n",
    "        split_content = doc.page_content.split(\"type: \")\n",
    "        content = split_content[0]        \n",
    "        metadata = split_content[1]                \n",
    "        doc.metadata['type'] = metadata.split(\"\\n\")[0]        \n",
    "        doc.page_content = content # metadata 제외하고 content 만 저장\n",
    "        # doc.metadata['type'] = doc.page_content.split(\"type: \")[1].split(\"\\n\")[0]\n",
    "        doc.metadata['timestamp'] = time.time()\n",
    "\n",
    "        \n",
    "create_metadata(documents_fsi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(documents_fsi))\n",
    "\n",
    "documents_fsi[0]\n",
    "# documents_fsi[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Spliter 로 청킹\n",
    "참고: 검색된 문서/텍스트는 질문에 대답하기에 충분한 정보를 포함할 만큼 커야 합니다. 하지만 LLM 프롬프트에 들어갈 만큼 충분히 작습니다. 또한 임베딩 모델에는 입력 토큰 길이가 512개 토큰으로 제한되어 있으며 이는 한국어의 경우에 대략 180 ~ 200 글자로 변환됩니다. 이 사용 사례를 위해 [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)를 사용하여 210자가 겹치는 약 50자의 청크를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_bedrock_embeddings:\n",
    "    chunk_size = 2048\n",
    "    chunk_overlap = 0\n",
    "elif is_KoSimCSERobert:\n",
    "    chunk_size = 800 # This is maxumum\n",
    "    chunk_overlap = 0\n",
    "    \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents_fsi)\n",
    "print(f\"Number of documents after split and chunking={len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 청킹 통계 및 내용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.rag import show_context_used, show_chunk_stat\n",
    "    \n",
    "show_chunk_stat(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_context_used(docs, limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. OpenSearch 벡터 Indexer 생성\n",
    "### 선수 조건\n",
    "- 아래의 링크를 참조해서 OpenSearch Service 를 생성하고, opensearch_domain_endpoint, http_auth 를 복사해서, 아래 셀의 내용을 대체 하세요.\n",
    "    - [OpenSearch 생성 가이드](https://github.com/gonsoomoon-ml/Kor-LLM-On-SageMaker/blob/main/2-Lab02-QA-with-RAG/4.rag-fsi-data-workshop/TASK-4_OpenSearch_Creation_and_Vector_Insertion.ipynb)\n",
    "- 랭체인 오프서처 참고 자료\n",
    "    - [Langchain Opensearch](https://python.langchain.com/docs/integrations/vectorstores/opensearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [중요] 아래에 aws parameter store 에 아래 인증정보가 먼저 입력되어 있어야 합니다.\n",
    "- 20_applications/02_qa_chatbot/01_preprocess_docs/01_parameter_store_example.ipynb 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "http_auth = (opensearch_user_id, opensearch_user_password) # Master username, Master password\n",
    "index_name = \"genai-demo-index-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오픈 서치 인덱스 유무에 따라 삭제\n",
    "오픈 서치에 해당 인덱스가 존재하면, 삭제 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.opensearch import opensearch_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os_client = opensearch_utils.create_aws_opensearch_client(\n",
    "    aws_region,\n",
    "    opensearch_domain_endpoint,\n",
    "    http_auth\n",
    ")\n",
    "\n",
    "index_exists = opensearch_utils.check_if_index_exists(\n",
    "    os_client,\n",
    "    index_name\n",
    ")\n",
    "\n",
    "if index_exists:\n",
    "    opensearch_utils.delete_index(\n",
    "        os_client,\n",
    "        index_name\n",
    "    )\n",
    "else:\n",
    "    print(\"Index does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# by default langchain would create a k-NN index and the embeddings would be ingested as a k-NN vector type\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    index_name=index_name,\n",
    "    documents=docs,\n",
    "    embedding=llm_emb,\n",
    "    opensearch_url=opensearch_domain_endpoint,\n",
    "    http_auth=http_auth,\n",
    "    bulk_size=10000,\n",
    "    timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인덱싱된 벡터 확인\n",
    "\n",
    "- [langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_db = OpenSearchVectorSearch(\n",
    "    index_name=index_name,\n",
    "    opensearch_url=opensearch_domain_endpoint,\n",
    "    embedding_function=llm_emb,\n",
    "    http_auth=http_auth, # http_auth\n",
    "    is_aoss =False,\n",
    "    engine=\"faiss\",\n",
    "    space_type=\"l2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OpenSearch Dashboard 통한 Query 및 레코드 확인\n",
    "- OpenSearch Dashboards URL 은 Amazon OpenSearch Servie 콘솔에 가시면 확인할 수 있습니다.\n",
    "OpenSearch DevTools Console DSL 방법\n",
    "- [OpenSearch_Query_DSL](https://opensearch.org/docs/latest/query-dsl/index/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![record_opensearch](img/record_opensearch.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5.오픈 서치에 \"유사 서치\" 검색\n",
    "- query 를 제공해서 실제로 유사한 내용이 검색이 되는지를 확인 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similarity_search_with_score API 정보\n",
    "    - [API: similarity_search_with_score](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html#langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.similarity_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_semantic_similar_docs(**kwargs):\n",
    "\n",
    "    search_types = [\"approximate_search\", \"script_scoring\", \"painless_scripting\"]\n",
    "    space_types = [\"l2\", \"l1\", \"linf\", \"cosinesimil\", \"innerproduct\", \"hammingbit\"]\n",
    "\n",
    "    assert \"vector_db\" in kwargs, \"Check your vector_db\"\n",
    "    assert \"query\" in kwargs, \"Check your query\"\n",
    "    assert kwargs.get(\"search_type\", \"approximate_search\") in search_types, f'Check your search_type: {search_types}'\n",
    "    assert kwargs.get(\"space_type\", \"l2\") in space_types, f'Check your space_type: {space_types}'\n",
    "\n",
    "    results = kwargs[\"vector_db\"].similarity_search_with_score(\n",
    "            query=kwargs[\"query\"],\n",
    "            k=kwargs.get(\"k\", 5),\n",
    "            search_type=kwargs.get(\"search_type\", \"approximate_search\"),\n",
    "            space_type=kwargs.get(\"space_type\", \"l2\"),\n",
    "            #boolean_filter=kwargs.get(\"boolean_filter\", {}),        \n",
    "            boolean_filter=opensearch_utils.get_filter(\n",
    "                filter=kwargs.get(\"boolean_filter\", [])\n",
    "            ),        \n",
    "            # fetch_k=3,\n",
    "        )\n",
    "\n",
    "    print (\"\\nsemantic search args: \")\n",
    "    pprint ({\n",
    "        \"k\": kwargs.get(\"k\", 5),\n",
    "        \"search_type\": kwargs.get(\"search_type\", \"approximate_search\"),\n",
    "        \"space_type\": kwargs.get(\"space_type\", \"l2\"),\n",
    "        \"boolean_filter\": opensearch_utils.get_filter(filter=kwargs.get(\"boolean_filter\", []))\n",
    "    })\n",
    "    \n",
    "    if kwargs.get(\"hybrid\", False) and results:            \n",
    "        max_score = results[0][1]\n",
    "        new_results = []\n",
    "        for doc in results:\n",
    "            nomalized_score = float(doc[1]/max_score)\n",
    "            new_results.append((doc[0], nomalized_score))\n",
    "        results = copy.deepcopy(new_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filter 없이 실행. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '간편조회서비스는 회원가입해야하나요?'\n",
    "#query = \"타기관OTP 등록 방법 알려주세요\"\n",
    "\n",
    "pre_similar_doc = get_semantic_similar_docs(\n",
    "    vector_db=vector_db,\n",
    "    query=query,\n",
    "    k=3\n",
    ")\n",
    "opensearch_utils.opensearch_pretty_print_documents(pre_similar_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metadata의 type, source 에 필터를 걸어서 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter01 = \"인터넷뱅킹\"\n",
    "#filter01 = \"인증서\"\n",
    "filter02 = \"신한은행\"\n",
    "#filter02 = \"아마존은행\"\n",
    "\n",
    "boolean_filter = [\n",
    "    {\"term\": {\"metadata.type\": filter01}},\n",
    "    {\"term\": {\"metadata.source\": filter02}},\n",
    "]\n",
    "\n",
    "pre_similar_doc = get_semantic_similar_docs(\n",
    "    vector_db=vector_db,\n",
    "    query=query,\n",
    "    boolean_filter=boolean_filter,\n",
    "    k=3\n",
    ")\n",
    "opensearch_utils.opensearch_pretty_print_documents(pre_similar_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.사용자 정의 가능한 옵션\n",
    "이제 벡터 저장소가 준비되었으므로 질문을 시작할 수 있습니다.\n",
    "\n",
    "Vector Store를 둘러싸서 LLM 입력을 받는 LangChain에서 제공하는 래퍼를 사용할 수 있습니다.\n",
    "이 래퍼는 뒤에서 다음 단계를 수행합니다.\n",
    "- 질문을 입력합니다.\n",
    "- 질문 임베딩 생성\n",
    "- 관련 문서 가져오기\n",
    "- 프롬프트에 문서와 질문을 채워 넣습니다.\n",
    "- 프롬프트로 모델을 호출하고 사람이 읽을 수 있는 방식으로 답변을 생성합니다.\n",
    "\n",
    "위 시나리오에서는 질문에 대한 상황 인식 답변을 빠르고 쉽게 얻을 수 있는 방법을 탐색했습니다. 이제 문서를 가져오는 방법을 사용자 정의할 수 있는 [RetrievalQA](https://python.langchain.com/en/latest/modules/chains/index_examples/Vector_db_qa.html)의 도움으로 더 사용자 정의 가능한 옵션을 살펴보겠습니다. `chain_type` 매개변수를 사용하여 프롬프트에 추가해야 합니다. 또한 검색해야 하는 관련 문서 수를 제어하려면 아래 셀에서 'k' 매개변수를 변경하여 다른 출력을 확인하세요. 많은 시나리오에서 LLM이 답변을 생성하는 데 사용한 소스 문서가 무엇인지 알고 싶을 수 있습니다. LLM 프롬프트의 컨텍스트에 추가된 문서를 반환하는 `return_source_documents`를 사용하여 출력에서 ​​해당 문서를 가져올 수 있습니다. 'RetrievalQA'를 사용하면 모델에 특정한 사용자 정의 [프롬프트 템플릿](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html)을 제공할 수도 있습니다.\n",
    "\n",
    "참고: 이 예에서는 Amazon Bedrock에서 LLM으로 Anthropic Claude를 사용하고 있습니다. 이 특정 모델은 입력이 'Human:' 아래에 제공되고 모델이 'Assistant:' 다음에 출력을 생성하도록 요청되는 경우 가장 잘 수행됩니다. 아래 셀에는 LLM이 기본 상태를 유지하고 컨텍스트 외부에서 응답하지 않도록 프롬프트를 제어하는 ​​방법의 예가 나와 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TIP] Prompt의 instruction의 경우 한글보다 **영어**로 했을 때 더 좋은 결과를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [[REF] Using langchain for Question Answering on Own Data](https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt_template = \"\"\"\n",
    "# \\n\\nHuman: 다음 문맥의 Information을 사용하여 고객 서비스 센터 직원처럼, 마지막 질문에 대한 목차 형식으로 답변을 제공하세요. 응답을 모르면 모른다고 말하고 응답을 만들려고 하지 마세요.\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \\n\\nAssistant:\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\\n\\nHuman: Use the following pieces of context to provide a concise answer to the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def run_RetrievalQA(**kwargs):\n",
    "\n",
    "    chain_types = [\"stuff\", \"map_reduce\", \"refine\"]\n",
    "\n",
    "    assert \"llm\" in kwargs, \"Check your llm\"\n",
    "    assert \"query\" in kwargs, \"Check your query\"\n",
    "    assert \"prompt\" in kwargs, \"Check your prompt\"\n",
    "    assert \"vector_db\" in kwargs, \"Check your vector_db\"\n",
    "    assert kwargs.get(\"chain_type\", \"stuff\") in chain_types, f'Check your chain_type, {chain_types}'\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=kwargs[\"llm\"],\n",
    "        chain_type=kwargs.get(\"chain_type\", \"stuff\"),\n",
    "        retriever=kwargs[\"vector_db\"].as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\n",
    "                \"k\": kwargs.get(\"k\", 5),\n",
    "                \"boolean_filter\": opensearch_utils.get_filter(\n",
    "                    filter=kwargs.get(\"boolean_filter\", [])\n",
    "                ),\n",
    "            }\n",
    "        ),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": kwargs[\"prompt\"],\n",
    "            \"verbose\": kwargs.get(\"verbose\", False),\n",
    "        },\n",
    "        verbose=kwargs.get(\"verbose\", False)\n",
    "    )\n",
    "\n",
    "    return qa(kwargs[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.rag import run_RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query = '간편조회서비스는 회원가입해야하나요?'\n",
    "#query = \"타기관OTP 등록 방법 알려주세요\"\n",
    "query = \"홈페이지 탈퇴하는방법 알려줘\"\n",
    "\n",
    "result = run_RetrievalQA(\n",
    "    query=query,\n",
    "    llm=llm_text,\n",
    "    prompt=PROMPT,\n",
    "    vector_db=vector_db,\n",
    "    verbose=False,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"##################################\")\n",
    "print(\"query: \", query)\n",
    "print(\"##################################\")\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_context_used(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"홈페이지 탈퇴하는방법 알려줘\"\n",
    "\n",
    "filter01 = \"홈페이지\"\n",
    "# filter01 = \"인증서\"\n",
    "filter02 = \"신한은행\"\n",
    "# filter02 = \"아마존은행\"\n",
    "\n",
    "boolean_filter = [\n",
    "    {\"term\": {\"metadata.type\": filter01}},\n",
    "    {\"term\": {\"metadata.source\": filter02}},\n",
    "]\n",
    "\n",
    "result = run_RetrievalQA(\n",
    "    query=query,\n",
    "    boolean_filter=boolean_filter,\n",
    "    llm=llm_text,\n",
    "    prompt=PROMPT,\n",
    "    vector_db=vector_db,\n",
    "    verbose=False,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"##################################\")\n",
    "print(\"query: \", query)\n",
    "print(\"boolean_filter: \", boolean_filter)\n",
    "print(\"##################################\")\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_context_used(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"홈페이지 이용자아이디 여러 개 사용할 수 있나요?\"\n",
    "filter01 = \"홈페이지\"\n",
    "# filter01 = \"인증서\"\n",
    "filter02 = \"신한은행\"\n",
    "#filter02 = \"아마존은행\"\n",
    "\n",
    "boolean_filter = [\n",
    "    {\"term\": {\"metadata.type\": filter01}},\n",
    "    {\"term\": {\"metadata.source\": filter02}},\n",
    "]\n",
    "\n",
    "result = run_RetrievalQA(\n",
    "    query=query,\n",
    "    boolean_filter=boolean_filter,\n",
    "    llm=llm_text,\n",
    "    prompt=PROMPT,\n",
    "    vector_db=vector_db,\n",
    "    verbose=True,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"##################################\")\n",
    "print(\"query: \", query)\n",
    "print(\"boolean_filter: \", boolean_filter)\n",
    "print(\"##################################\")\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_context_used(result['source_documents'])"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
