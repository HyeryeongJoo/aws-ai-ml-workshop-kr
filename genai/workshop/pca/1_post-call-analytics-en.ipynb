{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547672b4-4914-40a6-b55a-7aa8b9b7fb61",
   "metadata": {},
   "source": [
    "# Post Call Analytics\n",
    "\n",
    "Welcome to this training module on post-call analytics use cases using Amazon SageMaker JumpStart. \n",
    "\n",
    "As businesses continue to interact with customers through various channels, it becomes increasingly important to analyze these interactions to gain insights into customer behavior and preferences. Post-call analytics is one such method that involves analyzing customer interactions after the call has ended. The use of large language models can greatly enhance the effectiveness of post-call analytics by enabling more accurate sentiment analysis, identifying specific customer needs and preferences, and improving overall customer experience. \n",
    "\n",
    "In this sample notebook, we will explore following topics to demonstrate the various benefits of using Bedrock for post-call analytics and businesses gain a competitive edge in the modern marketplace.\n",
    "\n",
    "- [수정필요] Choice of LLM models in Amazon SageMaker JumpStart\n",
    "- One model handling multiple PCA tasks\n",
    "- Handling long call transcripts\n",
    "\n",
    "## Step 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92150b51-40e6-4fcd-9d93-a990cc716e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd768d18-d7de-4cad-abed-67e398a5b2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f64b47-53c8-42dd-98df-044bc80e1ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82933858-b359-44b8-952f-1f8d3b2485da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install -U termcolor\n",
    "    !{sys.executable} -m pip install -U langchain\n",
    "\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153cf6e-82d0-4869-a222-79a6c9d74eee",
   "metadata": {},
   "source": [
    "## Step 1. Prepare Large Language Model (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adae66bf-ab9c-4a34-b75f-aab4b8ab6cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from termcolor import colored\n",
    "from sagemaker.session import Session\n",
    "from langchain.llms import AmazonAPIGateway\n",
    "from lib_en import Llama2ContentHandlerAmazonAPIGateway, FalconContentHandlerEndpoint, FalconContentHandlerAmazonAPIGateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe9eca9e-7861-4426-b09c-d5617b28fc60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a53d62e7-2014-43ad-b68d-6bb37043d523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"FALCON-40B\" #LLAMA2-7B, FALCON-40B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e4e24a-c12a-42cf-a97f-2440f95b60af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME: FALCON-40B\n",
      "LLM_URL: https://tgegz13tr1.execute-api.us-west-2.amazonaws.com/api/llm/falcon_40b\n"
     ]
    }
   ],
   "source": [
    "#RESTAPI_ID = \"6bk4r5mo4f\" ## us-east-1\n",
    "RESTAPI_ID = \"tgegz13tr1\" ## us-west-2\n",
    "\n",
    "URL = f'https://{RESTAPI_ID}.execute-api.{aws_region}.amazonaws.com/api/'.replace('\"','')\n",
    "LLM_INFO = {\n",
    "    \"FALCON-40B\": f\"{URL}llm/falcon_40b\",\n",
    "    \"LLAMA2-7B\": f\"{URL}llm/llama2_7b\",\n",
    "}\n",
    "LLM_URL = LLM_INFO[MODEL_NAME]\n",
    "HEADERS = {    \n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "}\n",
    "\n",
    "print (f'MODEL_NAME: {MODEL_NAME}\\nLLM_URL: {LLM_URL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17108899-41b7-46df-aa7e-667c149ff04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = AmazonAPIGateway(api_url=LLM_URL, headers=HEADERS)\n",
    "\n",
    "if MODEL_NAME == \"FALCON-40B\": llm.content_handler = FalconContentHandlerAmazonAPIGateway()\n",
    "elif MODEL_NAME == \"LLAMA2-7B\": llm.content_handler = Llama2ContentHandlerAmazonAPIGateway()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc03fd-5396-4172-bb26-ae8160cb34c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2. Load transcript files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d70819-9d6b-4c96-943c-044109d222bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_files = [\n",
    "    \"./call_transcripts/negative-refund.txt\",\n",
    "    \"./call_transcripts/neutral-short.txt\",\n",
    "    \"./call_transcripts/positive-partial-refund.txt\",\n",
    "    \"./call_transcripts/aws-short.txt\",\n",
    "    \"./call_transcripts/aws.txt\"\n",
    "]\n",
    "transcripts = []\n",
    "\n",
    "for file_name in transcript_files:\n",
    "    with open(file_name, \"r\") as file:\n",
    "        transcripts.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4adc6069-b40a-4bd6-b213-e0baa3d67428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcript #1: timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "Agent: Thank you for calling our retail support line. My name is ABC. How can I assist you today?\n",
      "\n",
      "Customer: Yes, I have received a defective product, and I am extremely angry about it! This is unacceptable, and I want it resolved immediately!\n",
      "\n",
      "Agent: I'm sorry\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "transcript #2: timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up y\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "transcript #3: timestamp: 2022-12-28 08:26:49.219717\n",
      "\n",
      "Agent: Thank you for calling [Retailer], my name is [Agent Name]. How may I assist you today?\n",
      "\n",
      "Customer: Hi, I wanted to check on the status of my order. It was supposed to arrive today, but I haven't received it yet.\n",
      "\n",
      "Agent: I'm sorry to hear that. Can I have \n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "transcript #4: What is AWS? AWS or Amazon Web Services is a cloud service provider that offers various computing services that are accessible over the public internet.\n",
      "\n",
      "AWS and other public cloud vendors — like Google Cloud Platform (GCP) and Microsoft Azure — manage and maintain hardware and infrastructure, savin\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "transcript #5: What is AWS? AWS or Amazon Web Services is a cloud service provider that offers various computing services that are accessible over the public internet.\n",
      "\n",
      "AWS and other public cloud vendors — like Google Cloud Platform (GCP) and Microsoft Azure — manage and maintain hardware and infrastructure, savin\n",
      "\n",
      "====================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, trans in enumerate(transcripts):\n",
    "    print(f\"transcript #{i+1}: {trans[:300]}\\n\")\n",
    "    print(\"====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8823ce9-676a-4e36-a80a-0f855c689dbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3. Post Call Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78474678-208e-4cf1-924d-e89e1eb791dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2d0fb-5b2f-462c-bbc9-8f294c0a2083",
   "metadata": {},
   "source": [
    "### Step 3.1. Prompt Template\n",
    "In this notebook, we'll be performing four different analyses(**Summary, Sentiment, Intent and Resolution**), and we'll need a template for each one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37038a11-9d72-4ef0-8cb9-c3c9bae0d4cc",
   "metadata": {},
   "source": [
    "* Summary template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ec04a3-530b-4f47-b945-f0306be3af34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "Analyze the retail support call transcript below. Provide a detail summary of the conversation in complete sentence:\n",
    "\n",
    "context: {transcript}\n",
    "\n",
    "summary:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a6f09-971b-41d1-9508-c186c80830c8",
   "metadata": {},
   "source": [
    "* Sentiment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7afdc5-4dfb-4d3b-860a-c821d212e980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_template = \"\"\"\n",
    "This is a sentiment analysis program. What is the customer sentiment using following classes \n",
    "[\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]. classify the conversation into one and exact one of these classes. \n",
    "If you don't know or not sure, please use [\"NEUTRAL\"] class. Do not try to make up a class:\n",
    "\n",
    "context: {transcript}\n",
    "\n",
    "sentiment: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43171cd-ae26-4803-a2ec-b9ccb302455f",
   "metadata": {},
   "source": [
    "* intent template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5c2107-31de-43ec-8ca2-2de40ef097fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intent_template = \"\"\"\n",
    "This is a intent classification program. What is the purpose of the customer call using following classes\n",
    "[\"SHIPMENT_DELAY\", \"COMPLAIN_PRODUCT_DEFECT\", \"ACCOUNT_QUESTION\"]. classify the conversation into one and exact one of these classes.\n",
    "If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class. \n",
    "\n",
    "context: {transcript}\n",
    "\n",
    "intent: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5515c8-0107-400d-9c2d-4f4bd91ebcd4",
   "metadata": {},
   "source": [
    "### Step 3.2. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89e48c47-c57f-4ad5-872c-d2f6998171ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analysis(llm, transcript, params, template=\"\", max_tokens=50):\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"transcript\"])\n",
    "    analysis_prompt = prompt.format(transcript=transcript)\n",
    "    llm.model_kwargs = params\n",
    "\n",
    "    print (colored(analysis_prompt, 'green'))\n",
    "\n",
    "    response = llm(analysis_prompt)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "067e7a3e-baab-4b77-99be-f351f1968837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"FALCON-40B\": {\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"max_length\": 1024,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.01,\n",
    "        \"return_full_text\": False,\n",
    "        \"include_prompt_in_result\": False\n",
    "    },\n",
    "    \"LLAMA2-7B\": {\n",
    "        'max_new_tokens': 128,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.1,\n",
    "        'return_full_text': False\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4059f5-8e9b-40bb-8d6b-a5d5a0628bd1",
   "metadata": {},
   "source": [
    "* Summary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8704d41-2711-4da5-b5e1-ecb848eddcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "Analyze the retail support call transcript below. Provide a detail summary of the conversation in complete sentence:\n",
      "\n",
      "context: timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "Agent: Thank you for calling our retail support line. My name is ABC. How can I assist you today?\n",
      "\n",
      "Customer: Yes, I have received a defective product, and I am extremely angry about it! This is unacceptable, and I want it resolved immediately!\n",
      "\n",
      "Agent: I'm sorry to hear that you received a defective product. Can you please let me know what the issue is?\n",
      "\n",
      "Customer: The product I received is broken and unusable. I spent a lot of money on it, and now I can't even use it! This is unacceptable, and I demand a solution right now!\n",
      "\n",
      "Agent: I completely understand your frustration, and I'm sorry for any inconvenience this has caused you. Can you please provide me with your order number so that I can look into this for you?\n",
      "\n",
      "Customer: 2357894561\n",
      "\n",
      "Agent: Thank you. I'm sorry to hear about the defective product you received. We can definitely offer you a full refund for it. Would that be acceptable to you?\n",
      "\n",
      "Customer: Yes, a refund would be acceptable, but I'm still very disappointed that I received a defective product in the first place.\n",
      "\n",
      "Agent: I understand your disappointment, and we apologize for any inconvenience this has caused you. We will process the refund for you right away. Can you please provide me with your email address so that we can send you a confirmation of the refund?\n",
      "\n",
      "Customer: 123@456.com\n",
      "\n",
      "Agent: Thank you. We have processed the refund, and you should receive the confirmation email within the next 24 hours. Is there anything else I can assist you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you for offering the refund, but I'm still very disappointed with the product I received.\n",
      "\n",
      "Agent: I understand, and once again, I apologize for any inconvenience this has caused you. Thank you for contacting our retail support line, and have a great day.\n",
      "\n",
      "summary:\u001b[0m\n",
      "\n",
      "\n",
      "The customer is angry and frustrated because they received a defective product. They demand a solution immediately and demand a refund. The agent apologizes for the inconvenience and offers a full refund. The customer is still disappointed with the product and the situation. The agent processes the refund and offers to assist with anything else. The customer declines further assistance and ends the call.\n",
      "CPU times: user 13.7 ms, sys: 0 ns, total: 13.7 ms\n",
      "Wall time: 4.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[0],\n",
    "    params=PARAMS[MODEL_NAME],\n",
    "    template=summary_template\n",
    ")\n",
    "\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf7aca-4518-416d-84d5-9717348a6cdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "037e6c6b-e75d-4d56-ae7a-e3e86677949f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "This is a sentiment analysis program. What is the customer sentiment using following classes \n",
      "[\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]. classify the conversation into one and exact one of these classes. \n",
      "If you don't know or not sure, please use [\"NEUTRAL\"] class. Do not try to make up a class:\n",
      "\n",
      "context: timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "Agent: Thank you for calling our retail support line. My name is ABC. How can I assist you today?\n",
      "\n",
      "Customer: Yes, I have received a defective product, and I am extremely angry about it! This is unacceptable, and I want it resolved immediately!\n",
      "\n",
      "Agent: I'm sorry to hear that you received a defective product. Can you please let me know what the issue is?\n",
      "\n",
      "Customer: The product I received is broken and unusable. I spent a lot of money on it, and now I can't even use it! This is unacceptable, and I demand a solution right now!\n",
      "\n",
      "Agent: I completely understand your frustration, and I'm sorry for any inconvenience this has caused you. Can you please provide me with your order number so that I can look into this for you?\n",
      "\n",
      "Customer: 2357894561\n",
      "\n",
      "Agent: Thank you. I'm sorry to hear about the defective product you received. We can definitely offer you a full refund for it. Would that be acceptable to you?\n",
      "\n",
      "Customer: Yes, a refund would be acceptable, but I'm still very disappointed that I received a defective product in the first place.\n",
      "\n",
      "Agent: I understand your disappointment, and we apologize for any inconvenience this has caused you. We will process the refund for you right away. Can you please provide me with your email address so that we can send you a confirmation of the refund?\n",
      "\n",
      "Customer: 123@456.com\n",
      "\n",
      "Agent: Thank you. We have processed the refund, and you should receive the confirmation email within the next 24 hours. Is there anything else I can assist you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you for offering the refund, but I'm still very disappointed with the product I received.\n",
      "\n",
      "Agent: I understand, and once again, I apologize for any inconvenience this has caused you. Thank you for contacting our retail support line, and have a great day.\n",
      "\n",
      "sentiment: \u001b[0m\n",
      "\"NEGATIVE\"\n",
      "CPU times: user 8.72 ms, sys: 4.34 ms, total: 13.1 ms\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[0],\n",
    "    params=PARAMS[MODEL_NAME],\n",
    "    template=sentiment_template\n",
    ")\n",
    "\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ada3e82-1f22-4546-92e1-815caeb3653a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "This is a intent classification program. What is the purpose of the customer call using following classes\n",
      "[\"SHIPMENT_DELAY\", \"COMPLAIN_PRODUCT_DEFECT\", \"ACCOUNT_QUESTION\"]. classify the conversation into one and exact one of these classes.\n",
      "If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class. \n",
      "\n",
      "context: timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "Agent: Thank you for calling our retail support line. My name is ABC. How can I assist you today?\n",
      "\n",
      "Customer: Yes, I have received a defective product, and I am extremely angry about it! This is unacceptable, and I want it resolved immediately!\n",
      "\n",
      "Agent: I'm sorry to hear that you received a defective product. Can you please let me know what the issue is?\n",
      "\n",
      "Customer: The product I received is broken and unusable. I spent a lot of money on it, and now I can't even use it! This is unacceptable, and I demand a solution right now!\n",
      "\n",
      "Agent: I completely understand your frustration, and I'm sorry for any inconvenience this has caused you. Can you please provide me with your order number so that I can look into this for you?\n",
      "\n",
      "Customer: 2357894561\n",
      "\n",
      "Agent: Thank you. I'm sorry to hear about the defective product you received. We can definitely offer you a full refund for it. Would that be acceptable to you?\n",
      "\n",
      "Customer: Yes, a refund would be acceptable, but I'm still very disappointed that I received a defective product in the first place.\n",
      "\n",
      "Agent: I understand your disappointment, and we apologize for any inconvenience this has caused you. We will process the refund for you right away. Can you please provide me with your email address so that we can send you a confirmation of the refund?\n",
      "\n",
      "Customer: 123@456.com\n",
      "\n",
      "Agent: Thank you. We have processed the refund, and you should receive the confirmation email within the next 24 hours. Is there anything else I can assist you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you for offering the refund, but I'm still very disappointed with the product I received.\n",
      "\n",
      "Agent: I understand, and once again, I apologize for any inconvenience this has caused you. Thank you for contacting our retail support line, and have a great day.\n",
      "\n",
      "intent: \u001b[0m\n",
      "[\"SHIPMENT_DELAY\", \"COMPLAIN_PRODUCT_DEFECT\", \"ACCOUNT_QUESTION\"]\n",
      "\n",
      "Based on the conversation, the customer is calling to complain about a defective product and is demanding a refund. The agent offers a refund and apologizes for the inconvenience. Therefore, the intent classification for this conversation is \"COMPLAIN_PRODUCT_DEFECT\".\n",
      "CPU times: user 9.63 ms, sys: 3.94 ms, total: 13.6 ms\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[0],\n",
    "    params=PARAMS[MODEL_NAME],\n",
    "    template=intent_template\n",
    ")\n",
    "\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a44c03-e8a9-4cb7-a5a9-950627d3eeaf",
   "metadata": {},
   "source": [
    "## Handling long call transcripts\n",
    "We'll cover how to handle long transcripts that exceed the limits of the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d8870-9ed1-452a-8614-00d16cf5179a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1757f8f-0556-4abc-a36d-2ed58e85e76b",
   "metadata": {
    "tags": []
   },
   "source": [
    "* prompting to divide and conquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b732f-fe1e-4562-be00-a80864318322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt_template = \"\"\"\n",
    "Please provide a summary of the following text.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "chuck_prompt_template = \"\"\"\n",
    "Please provide a summary of the following text.\n",
    "Please answer in one sentence.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "chunk_prompt = PromptTemplate(\n",
    "    template=chuck_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "Write a concise summary of the following text.\n",
    "Return your response in bullet points which covers the key points of the text.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66e394-3c16-4951-bad7-162184165d1f",
   "metadata": {},
   "source": [
    "* summarize chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1212b3-f95e-4c24-9b6a-86d4c03b7bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# summary_chain = load_summarize_chain(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"map_reduce\",\n",
    "#     verbose=True\n",
    "# ) # map_reduce, refine\n",
    "# transcript = summary_chain(docs)\n",
    "'''\n",
    "\n",
    "\n",
    "def summary_chain_init(chain_type, llm):\n",
    "    \n",
    "    if chain_type == \"STUFF\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"stuff\",\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    elif chain_type == \"MAP_REDUCE\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"map_reduce\",\n",
    "            map_prompt=chunk_prompt,\n",
    "            combine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif chain_type == \"REFINE\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=chunk_prompt,\n",
    "            refine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fda098-0069-4a57-9580-2b5356aeb847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def long_call_analysis(llm, transcript, params, template=\"\", chain_type=\"MAP_REDUCE\", max_tokens=50):\n",
    "\n",
    "    \n",
    "    llm.model_kwargs = params\n",
    "    num_tokens = llm.get_num_tokens(transcript) #raise warnning\n",
    "\n",
    "    if num_tokens > max_tokens:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\\n\"],\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        docs = text_splitter.create_documents([transcript])\n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "        print(f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")\n",
    "\n",
    "        \n",
    "        summary_chain = summary_chain_init(\n",
    "            chain_type=chain_type, \n",
    "            llm=llm\n",
    "        )\n",
    "        response = summary_chain(\n",
    "            {\"input_documents\": docs}\n",
    "        )\n",
    "        \n",
    "        print (\"Intermediate_steps: \\n\")\n",
    "        for idx, step in enumerate(response[\"intermediate_steps\"]):\n",
    "            print (colored(f'step {idx}: \\n', \"green\"))\n",
    "            print (colored(f'{step}\\n', \"green\"))\n",
    "        \n",
    "        return response[\"output_text\"]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        prompt = PromptTemplate(template=stuff_prompt_template, input_variables=[\"text\"])\n",
    "        analysis_prompt = prompt.format(text=transcript)\n",
    "        print (colored(analysis_prompt, 'green'))\n",
    "        \n",
    "        response = llm(analysis_prompt)\n",
    "        \n",
    "        return response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3824af9-d55f-4baf-9296-8266b9de43bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"FALCON-40B\": {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"max_length\": 1024,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.2,\n",
    "        \"return_full_text\": False,\n",
    "        \"include_prompt_in_result\": False\n",
    "    },\n",
    "    \"LLAMA2-7B\": {\n",
    "        'max_new_tokens': 128,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.1,\n",
    "        'return_full_text': False\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005333e-f359-4ab1-98ae-8338c0e74d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = long_call_analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[3],\n",
    "    params=PARAMS[MODEL_NAME],\n",
    "    template=summary_template,\n",
    "    chain_type=\"REFINE\" # REFINE, MAP_REDUCE\n",
    ")\n",
    "\n",
    "print (\"Results: \\n\")\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad0fff-7d02-45a1-932d-2fd6dd88ca6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67011a72-d7a1-4782-8116-36b4f8a3bb39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
