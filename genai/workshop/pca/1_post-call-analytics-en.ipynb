{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547672b4-4914-40a6-b55a-7aa8b9b7fb61",
   "metadata": {},
   "source": [
    "# Post Call Analytics\n",
    "\n",
    "Welcome to this training module on post-call analytics use cases using Amazon SageMaker JumpStart. \n",
    "\n",
    "As businesses continue to interact with customers through various channels, it becomes increasingly important to analyze these interactions to gain insights into customer behavior and preferences. Post-call analytics is one such method that involves analyzing customer interactions after the call has ended. The use of large language models can greatly enhance the effectiveness of post-call analytics by enabling more accurate sentiment analysis, identifying specific customer needs and preferences, and improving overall customer experience. \n",
    "\n",
    "In this sample notebook, we will explore following topics to demonstrate the various benefits of using Bedrock for post-call analytics and businesses gain a competitive edge in the modern marketplace.\n",
    "\n",
    "- [수정필요] Choice of LLM models in Amazon SageMaker JumpStart\n",
    "- One model handling multiple PCA tasks\n",
    "- Handling long call transcripts\n",
    "\n",
    "## Step 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92150b51-40e6-4fcd-9d93-a990cc716e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd768d18-d7de-4cad-abed-67e398a5b2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../templates')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153cf6e-82d0-4869-a222-79a6c9d74eee",
   "metadata": {},
   "source": [
    "## Step 1. Prepare Large Language Model (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae66bf-ab9c-4a34-b75f-aab4b8ab6cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from termcolor import colored\n",
    "from sagemaker.session import Session\n",
    "from langchain.llms import AmazonAPIGateway\n",
    "from lib_en import Llama2ContentHandlerAmazonAPIGateway, FalconContentHandlerEndpoint, FalconContentHandlerAmazonAPIGateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9eca9e-7861-4426-b09c-d5617b28fc60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "\n",
    "from ssm import parameter_store\n",
    "pm = parameter_store(aws_region)\n",
    "pm_restapi_id_key = f\"RESTAPI-ID-{aws_region.upper()}\"\n",
    "\n",
    "try:\n",
    "    RESTAPI_ID = pm.get_params(key=pm_restapi_id_key)\n",
    "    URL = f'https://{RESTAPI_ID}.execute-api.{aws_region}.amazonaws.com/api/'.replace('\"','')\n",
    "    print(\"RESTAPI_ID = \", RESTAPI_ID) # YOUR RESTAPI ID\n",
    "    print(\"API GATEWAY URL = \", URL)    \n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Please run rag/0_setup.ipynb first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4e24a-c12a-42cf-a97f-2440f95b60af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"FALCON-40B\" #LLAMA2-7B, FALCON-40B\n",
    "\n",
    "LLM_INFO = {\n",
    "    \"LLAMA2-7B\": f\"{URL}llm/llama2_7b\",\n",
    "    \"FALCON-40B\": f\"{URL}llm/falcon_40b\",    \n",
    "    \"KULLM-12-8B\": f\"{URL}llm/kkulm_12_8b\",\n",
    "}\n",
    "\n",
    "LLM_URL = LLM_INFO[MODEL_NAME]\n",
    "HEADERS = {    \n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "}\n",
    "\n",
    "print (f'MODEL_NAME: {MODEL_NAME}\\nLLM_URL: {LLM_URL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17108899-41b7-46df-aa7e-667c149ff04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = AmazonAPIGateway(api_url=LLM_URL, headers=HEADERS)\n",
    "\n",
    "if MODEL_NAME == \"FALCON-40B\": llm.content_handler = FalconContentHandlerAmazonAPIGateway()\n",
    "elif MODEL_NAME == \"LLAMA2-7B\": llm.content_handler = Llama2ContentHandlerAmazonAPIGateway()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc03fd-5396-4172-bb26-ae8160cb34c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2. Load transcript files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d70819-9d6b-4c96-943c-044109d222bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_files = [\n",
    "    \"./call_transcripts/negative-refund.txt\",\n",
    "    \"./call_transcripts/neutral-short.txt\",\n",
    "    \"./call_transcripts/positive-partial-refund.txt\",\n",
    "    \"./call_transcripts/aws-short.txt\",\n",
    "    \"./call_transcripts/aws.txt\"\n",
    "]\n",
    "transcripts = []\n",
    "\n",
    "for file_name in transcript_files:\n",
    "    with open(file_name, \"r\") as file:\n",
    "        transcripts.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc6069-b40a-4bd6-b213-e0baa3d67428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, trans in enumerate(transcripts):\n",
    "    print(f\"transcript #{i+1}: {trans[:300]}\\n\")\n",
    "    print(\"====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8823ce9-676a-4e36-a80a-0f855c689dbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3. Post Call Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78474678-208e-4cf1-924d-e89e1eb791dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2d0fb-5b2f-462c-bbc9-8f294c0a2083",
   "metadata": {},
   "source": [
    "### Step 3.1. Prompt Template\n",
    "In this notebook, we'll be performing four different analyses(**Summary, Sentiment, Intent and Resolution**), and we'll need a template for each one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37038a11-9d72-4ef0-8cb9-c3c9bae0d4cc",
   "metadata": {},
   "source": [
    "* Summary template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec04a3-530b-4f47-b945-f0306be3af34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "Analyze the retail support call transcript below. Provide a detail summary of the conversation in complete sentence:\n",
    "\n",
    "context: {transcript}\n",
    "\n",
    "summary:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a6f09-971b-41d1-9508-c186c80830c8",
   "metadata": {},
   "source": [
    "* Sentiment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7afdc5-4dfb-4d3b-860a-c821d212e980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_template = \"\"\"\n",
    "This is a sentiment analysis program. What is the customer sentiment using following classes \n",
    "[\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]. classify the conversation into one and exact one of these classes. \n",
    "If you don't know or not sure, please use [\"NEUTRAL\"] class. Do not try to make up a class:\n",
    "\n",
    "context: {transcript}\n",
    "\n",
    "sentiment: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43171cd-ae26-4803-a2ec-b9ccb302455f",
   "metadata": {},
   "source": [
    "* intent template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c2107-31de-43ec-8ca2-2de40ef097fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intent_template = \"\"\"\n",
    "This is a intent classification program. What is the purpose of the customer call using following classes\n",
    "[\"SHIPMENT_DELAY\", \"COMPLAIN_PRODUCT_DEFECT\", \"ACCOUNT_QUESTION\"]. classify the conversation into one and exact one of these classes.\n",
    "If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class. \n",
    "\n",
    "context: {transcript}\n",
    "\n",
    "intent: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5515c8-0107-400d-9c2d-4f4bd91ebcd4",
   "metadata": {},
   "source": [
    "### Step 3.2. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e48c47-c57f-4ad5-872c-d2f6998171ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analysis(llm, transcript, params, template=\"\", max_tokens=50):\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"transcript\"])\n",
    "    analysis_prompt = prompt.format(transcript=transcript)\n",
    "    llm.model_kwargs = params\n",
    "\n",
    "    print (colored(analysis_prompt, 'green'))\n",
    "\n",
    "    response = llm(analysis_prompt)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e7a3e-baab-4b77-99be-f351f1968837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"FALCON-40B\": {\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"max_length\": 1024,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.01,\n",
    "        \"return_full_text\": False,\n",
    "        \"include_prompt_in_result\": False\n",
    "    },\n",
    "    \"LLAMA2-7B\": {\n",
    "        'max_new_tokens': 128,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.1,\n",
    "        'return_full_text': False\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4059f5-8e9b-40bb-8d6b-a5d5a0628bd1",
   "metadata": {},
   "source": [
    "* Summary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8704d41-2711-4da5-b5e1-ecb848eddcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[0],\n",
    "    params=PARAMS[MODEL_NAME],\n",
    "    template=summary_template\n",
    ")\n",
    "\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf7aca-4518-416d-84d5-9717348a6cdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e6c6b-e75d-4d56-ae7a-e3e86677949f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[0],\n",
    "    params=PARAMS[MODEL_NAME],\n",
    "    template=sentiment_template\n",
    ")\n",
    "\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada3e82-1f22-4546-92e1-815caeb3653a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[0],\n",
    "    params=PARAMS[MODEL_NAME],\n",
    "    template=intent_template\n",
    ")\n",
    "\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a44c03-e8a9-4cb7-a5a9-950627d3eeaf",
   "metadata": {},
   "source": [
    "## Handling long call transcripts\n",
    "We'll cover how to handle long transcripts that exceed the limits of the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d8870-9ed1-452a-8614-00d16cf5179a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1757f8f-0556-4abc-a36d-2ed58e85e76b",
   "metadata": {
    "tags": []
   },
   "source": [
    "* prompting to divide and conquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b732f-fe1e-4562-be00-a80864318322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt_template = \"\"\"\n",
    "Please provide a summary of the following text.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "chuck_prompt_template = \"\"\"\n",
    "Please provide a summary of the following text.\n",
    "Please answer in one sentence.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "chunk_prompt = PromptTemplate(\n",
    "    template=chuck_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "Write a concise summary of the following text.\n",
    "Return your response in bullet points which covers the key points of the text.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66e394-3c16-4951-bad7-162184165d1f",
   "metadata": {},
   "source": [
    "* summarize chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1212b3-f95e-4c24-9b6a-86d4c03b7bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# summary_chain = load_summarize_chain(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"map_reduce\",\n",
    "#     verbose=True\n",
    "# ) # map_reduce, refine\n",
    "# transcript = summary_chain(docs)\n",
    "'''\n",
    "\n",
    "\n",
    "def summary_chain_init(chain_type, llm):\n",
    "    \n",
    "    if chain_type == \"STUFF\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"stuff\",\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    elif chain_type == \"MAP_REDUCE\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"map_reduce\",\n",
    "            map_prompt=chunk_prompt,\n",
    "            combine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif chain_type == \"REFINE\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=chunk_prompt,\n",
    "            refine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fda098-0069-4a57-9580-2b5356aeb847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def long_call_analysis(llm, transcript, params, template=\"\", chain_type=\"MAP_REDUCE\", max_tokens=50):\n",
    "\n",
    "    \n",
    "    llm.model_kwargs = params\n",
    "    num_tokens = llm.get_num_tokens(transcript) #raise warnning\n",
    "\n",
    "    if num_tokens > max_tokens:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\\n\"],\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        docs = text_splitter.create_documents([transcript])\n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "        print(f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")\n",
    "\n",
    "        \n",
    "        summary_chain = summary_chain_init(\n",
    "            chain_type=chain_type, \n",
    "            llm=llm\n",
    "        )\n",
    "        response = summary_chain(\n",
    "            {\"input_documents\": docs}\n",
    "        )\n",
    "        \n",
    "        print (\"Intermediate_steps: \\n\")\n",
    "        for idx, step in enumerate(response[\"intermediate_steps\"]):\n",
    "            print (colored(f'step {idx}: \\n', \"green\"))\n",
    "            print (colored(f'{step}\\n', \"green\"))\n",
    "        \n",
    "        return response[\"output_text\"]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        prompt = PromptTemplate(template=stuff_prompt_template, input_variables=[\"text\"])\n",
    "        analysis_prompt = prompt.format(text=transcript)\n",
    "        print (colored(analysis_prompt, 'green'))\n",
    "        \n",
    "        response = llm(analysis_prompt)\n",
    "        \n",
    "        return response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3824af9-d55f-4baf-9296-8266b9de43bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"FALCON-40B\": {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"max_length\": 1024,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.2,\n",
    "        \"return_full_text\": False,\n",
    "        \"include_prompt_in_result\": False\n",
    "    },\n",
    "    \"LLAMA2-7B\": {\n",
    "        'max_new_tokens': 128,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.1,\n",
    "        'return_full_text': False\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005333e-f359-4ab1-98ae-8338c0e74d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = long_call_analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[3],\n",
    "    params=PARAMS[MODEL_NAME],\n",
    "    template=summary_template,\n",
    "    chain_type=\"REFINE\" # REFINE, MAP_REDUCE\n",
    ")\n",
    "\n",
    "print (\"Results: \\n\")\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af00b77-0716-4429-bef9-d0445ac02add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
